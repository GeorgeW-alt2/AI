Meaning, Evolution, and the Structure of Society
Roland MÃ¼hlenbernd and Michael Franke
Seminar fÃ¼r Sprachwissenschaft, UniversitÃ¤t TÃ¼bingen

Abstract. Leading models of language evolution seek to explain key properties
of language as emerging from repeated interactions of language-using agents.
This paper will explore some of the consequences that integrating a more realistic
social interaction structure into established models of language evolution in terms
of evolutionary game theory has and reflects on prospective applications.

Leading models of language evolution seek to explain key properties of language as
emerging from repeated goal-oriented interactions of language-using agents [26, 32, 4,
2, 31]. Models of meaning evolution delineate exact conditions under which semantic
meaningfulness can emerge, by imitation of others or by rationally responding to the
behavior of oneâ€™s environment. It is obvious that the psychology of language users plays
a pivotal role here: among other things, the particulars of agentsâ€™ perception and memory, their disposition to adapt their behavior, including the extent to which they make
rational choices will heavily influence the way linguistic behavior evolves over time.
Consequently, there has been a lot of research into the impact of different agent-based
learning dynamics [18, 17, 3, 21, 8, 13]. Moreover, the sociology of language-using
populations plays a role in determining the time course and outcome of evolutionary
processes. Many evolutionary models make explicit assumptions about the interaction
patterns within a population of language users, such as who interacts with whom [25].
Different interaction structures may give different predictions about uniformity or diversity of language [37, 34, 23]: if everybody in a large population were to interact with
everybody else equally, language uniformity is to be expected; but given a tendency to
interact mostly with nearby kinsmen, we expect dialects and regional variations. Further obvious consequences of different social interaction structures are the readiness
and speed with which new innovations may spread in a population [16, 6], or the social
standing of the innovators [6, 24].
The aim of this paper is to highlight where and how psychological and sociological
constraints interact and influence the outcome of evolutionary processes relevant for
the evolution of meaning. Our goal is to communicate ideas, not mathematical detail.
Section 1 introduces basic notions of evolutionary game theory and formal network theory. Section 2 demonstrates how restrictions on social interaction patterns influence the
equilibrium outcomes for a simple game that has been used frequently in the literature
to study structural iconicity in language [5]. We show that more realistic interaction
structures make it more likely that iconic language use evolves. Section 3 recaps the
conditions under which simple signaling game models of meaning evolution give rise to
regional variability, and Section 4 zooms in on the global and local network properties
that characterize, among others, language boundaries or origins of conventionalization.

t1

m1

a1

t1

m1

a1

t1

m1

a1

t2

m2

a2

t2

m2

a2

t2

m2

a2

(a) strategy profile s1

(b) strategy profile s2

(c) strategy profile s3

Fig. 1. Different strategy profiles for a signaling game with 2 states/messages/actions.

1

Background: Evolutionary Games & Networks

Signaling Games. We will restrict our attention to signaling games [20]. A sender has
private information which a receiver lacks. The sender sends a message, which has no
pre-established meaning. The receiver then takes an action in response to observing the
senderâ€™s message. If the receiverâ€™s action matches the senderâ€™s private information, the
game is a success; if not, itâ€™s a failure. For illustration, we consider signaling games
with a set of two states T = {t1 , t2 }, a set of two messages M = {m1 , m2 } and a set of two
receiver actions A = {a1 , a2 }. With probability p state t1 occurs, and with probability
1 âˆ’ p it is t2 . The utilities for sender and receiver of one round of play are given by
functions US ,R : T Ã— M Ã— A â†’ R as follows:
ï£±
ï£´
ï£´
ï£²1 if i = k
UR (ti , mj , ak ) = ï£´
US (ti , mj , ak ) = UR (ti , mj , ak ) âˆ’  j
ï£´
ï£³0 otherwise
where 0 â‰¤ 1 â‰¤ 2 < 1 are costs of sending messages m1 and m2 respectively. When
the state probabilities are equal p = .5 and the message costs are zero 1 = 2 = 0,
then we call the game a Lewis game. We also consider the case where p > .5 and
0 â‰¤ 1 < 2 and call it a Horn game. This case has been used frequently to study
the structural iconicity principle a.k.a. Hornâ€™s division of pragmatic labor [11] from
an evolutionary perspective, namely the observation that most often languages encode
frequent or stereotypical meanings with short and economic forms [27, 19, 15, 7, 23].
A pure sender strategy is a function s : T â†’ M from states to messages; a pure
receiver strategy is a function r : M â†’ A from messages to acts. A pair of pure strategies is a pure strategy profile. There are 4 pure sender and 4 pure receiver strategies for
the signaling games we look at here and consequently 16 pure strategy profiles. Next to
pure strategies, there are also mixed strategies. These are probability distributions over
pure strategies. So, a mixed sender (receiver) strategy sÌƒ (rÌƒ) is a function assigning a
probability to each pure sender (receiver) strategy. In an evolutionary context, we interpret these as giving the frequency with which each pure strategy occurs in a population
of agents. A mixed strategy profile is then a pair of mixed sender and receiver strategies that describes the current population state, i.e., the frequencies of pure sender and
receiver strategies occurring in the population at a given time.
Static Solutions. For the Lewis game, the two pure strategy profiles s1 and s2 in Figure 1 are of particular interest. In each of these, all of the agents play the same strategy
in either sender or receiver role. Lewis called profiles like these, where agents succeed

to communicate perfectly, signaling systems. Although messages are initially meaningless, they become meaningful if a population of agents uses them in the way described
by a signaling system. E.g., in strategy profile s1 from Figure 1(a), message m1 denotes state t1 . The population states in s1 and s2 are evolutionarily stable states (esss)
[22, 35]. When, as we assume here, agents hold both a sender and receiver strategy
independently, the esss of the game are just the strict Nash equilibria [30].
Dynamic Solutions. Knowing which population states are esss is important, but we may
also wish to know whether and how a population which is not in an ess develops if every
agent adapts her behavior in some way or another. The most common evolutionary
dynamic studied for that purpose is the replicator dynamic of [33]. The general idea
behind the replicator dynamic is that the frequency sÌƒt (s) of pure sender strategy s at time
t in the population changes proportional to how successful s fares against the averaged
receiver behavior rÌƒt at time t. If s does better than the average over all pure sender
strategies, then the frequency of s increases proportionally to how much better it does
than average; if s does worse, then its frequency decreases proportionally to how much
worse it does in comparison with other sender strategies. Similarly, for the receiver. A
fixed point of the replicator dynamic is a population state that does not change under the
replicator dynamic. An attractor is a fixed point such that population states close to it
converge towards that fixed point. For each attractor, we call its basin of attraction the
region of population states that converge to it. To simplify parlor, we restrict attention
to relevant attractors that have a non-negligible basin of attraction.
The behavior of signaling games under the replicator dynamic is well-studied [12,
31, 13]. For populations where sender and receiver role are treated separately, we obtain
the following picture. The two signaling systems s1 and s2 in Figure 1 are the only
relevant attractors of the replicator dynamic for the Lewis game and have equally sized
basins of attraction. For the Horn game, there are three relevant attractors (Figure 1):
strategy s1 is called the Horn state since it captures the structural iconicity principle
mentioned above according to which the more efficient form is linked with the more
frequent meaning; strategy s2 is called the Anti-Horn state, since it operates exactly the
other way around; and strategy s3 is called the Smolensky state [11, 5, 19]. The Horn
state is the more efficient way of using language than the Anti-Horn state, although both
achieve perfect communication throughout. The Smolensky state is worse than Horn or
Anti-Horn, as communication is achieved in only p times cases. This is reflected in the
relative sizes of the basins of attraction. To estimate these by numerical simulations,
we generated 1 million randomly chosen initial population states, ran the replicator
dynamic for a Horn game (p = .75, 1 = 0, 2 = .1) and recorded the number of times
the system converged to one of the three states. 54.5% or trials converged to the Horn
state, 33.0% to the Anti-Horn state, and 11.9% to the Smolensky state.
The bigger size of the basin of attraction of the Horn state could be taken as a partial
explanation for the relative prevalence of the structural iconicity principle. However, although Horn and Anti-Horn states support full communication, the Smolensky state is
communicatively inefficient but still attracts almost 12% of initial population states for
the chosen parameter values. Interestingly, this changes when we consider communication in structured populations, which is what we will work towards next.

Network Games. The replicator dynamic describes how the frequencies of various
strategies in a population of agents develop over time. This is a macro-level perspective, because there is no mention in the formulation of the replicator dynamic of what
each individual agent is doing. But we can also link the replicator dynamic to a microlevel perspective. It can be shown that the replicator dynamic describes the most likely
path of strategy distributions in a virtually infinite and homogeneous population when
every agent updates her behavior by conditional imitation [9, 29]. A population is homogenous if every agent repeatedly interacts with everybody else equally. An agent who
updates her behavior by conditional imitation plays a constant pure strategy for a while,
but, every now and then, checks how well all neighbors fare with the strategy that they
play. If some set X of neighbors do better than the agent, he adopts the pure strategy
of some x âˆˆ X, where x is chosen from X with a probability that is proportional to the
relative success of xâ€™s strategy if compared with that of the other elements in X.
The conditional imitation rule adopts a micro-level, agent-based perspective. The
micro-level perspective makes it possible to dispense with the assumption that the population is virtually infinite and homogeneous. Instead, we can fix an explicit interaction
structure for a population by defining which agents can interact with one another. At
the same time, we can also look at different update rules, i.e. different ways of changing
behavior over time. We refer to a fixed interaction structure and a fixed update rule as a
network game, when the interaction structure of the agents is given by a social network
of relations. Such a network is formally represented as an undirected graph G = hN, Ei
where N = {1, . . . , n} is the set of nodes, representing the agents, and E âŠ† N Ã— N
is an irreflexive and symmetric ordering on N. Agents i and j interact if and only if
hi, ji âˆˆ E. In the following, we look at some relevant graph structures whose interaction
with various update rules we will explore thereafter.
Network Types. The most trivial graph structure is a completely connected network:
every agent interacts with everybody else equally; the population is homogeneous. In
a k-ring network all of the agents are ordered and connected in a circular way to their
k-nearest neighbors. In a grid network the agents are arranged on an n Ã— m toroid structure, where every agent interacts with the 8 nearest neighbors. Rings and grids are easy
to implement and facilitate proofs, but they are quite unrealistic models of human interaction patterns. We therefore also consider two more complicated graph types that
are created with a random component. The first are so-called Î²-graphs. A Î²-graph is
obtained by considering a k-ring network and subsequently, for each node, rewiring
its k/2 left neighbors to a random vertex n with probability Î² [36]. Î²-graphs have socalled small-world properties: a short characteristic-path length and a high clustering
coefficient, features that also show, for instance, in the analysis of human friendship
networks [14]. Nodes in a Î²-graph almost all have the same number of connections,
but it seems more realistic to assume that most agents interact with a smaller number
of agents, but there are also agents who interact with many. If the frequency of agents
with ever larger numbers of connections follows a power-law distribution, the network
is said to be scale-free. We consider a special kind of scale-free networks here, which
is both scale-free and has small-world properties [1]. These are constructed by a simple
parameterized preferential attachement algorithm [10].

Update Rules. We will here only consider general classes of update rules, namely the
imitation class and the learning class. The imitation class comprises update rules where
agents would adopt other agentsâ€™ behavior under certain conditions. The conditional
imitation update rule that we linked to the replicator dynamic above is, of course, a first
example. Moreover, we consider the imitate-the-best update rule used by [37] and [34].
This update rule is the special case of the conditional imitation where agents adopt the
strategy of the neighbor who is doing best with probability 1 if this neighbor is doing
better than the agent himself.
When imitation is conditional on success of neighboring strategies, imitation-based
update rules are not as innocuous and resource efficient as they may seem at first. Payoffdependent imitation requires keeping track of the neighborsâ€™ average success. It may
be easier to keep track just of oneâ€™s own. For that reason, we also look at two specimen from the learning class of update dynamics. Learning-based dynamics assume
that agents try to optimize their behavior by keeping track of the past interactions that
they were engaged in. Whereas imitation-based dynamics have agents change their entire pure strategy when they imitate, learning-based update rules often induce only local
and gradual changes to the agentsâ€™ behavioral strategies. A behavioral sender strategy Ïƒ
is a function from T to probability distributions over M. Similarly, a behavioral receiver
strategy Ï is a function from M to probability distributions over A.
Reinforcement learning (RL) is the most popular learning dynamic when it comes
to signaling games [31]. Here, agents are more likely to repeat a certain choice the more
successful is was in the past. This can be pictured as a process of updating PÃ³lya urns
[28]. An urn models a behavioral strategy, in the sense that the probability of making a
particular decision is proportional to the number of balls in the urn that correspond to
that action choice. By adding or removing balls from an urn after each encounter, an
agentâ€™s behavior is gradually adjusted. For signaling games, the sender has an urn â„¦t
for each state t âˆˆ T , which contains balls for different messages m âˆˆ M. The number
of balls of type m in urn â„¦t is m(â„¦t ), the overall number of balls in urn â„¦t is |â„¦t |. If the
sender is faced with a state t she draws a ball from urn â„¦t and sends message m, if the
ball is of type m. The resulting behavioral strategy is: Ïƒ(m|t) = m(â„¦t )/|â„¦t |. (Similarly, for
the receiver.) The learning dynamics are realized by changing the urn content dependent on the communicative success. If communication via t, m and a is successful, the
number of balls in urn â„¦t is increased by Î± âˆˆ N balls of type m and reduced by Î³ âˆˆ N
balls of type m0 , m. Similarly, for the receiver. In this way successful communicative
behavior is more probable to reappear in subsequent rounds. In our experiments, all
urns were initially filled with 100 balls and we set Î± = 10 and Î³ = 4.

2

Network Structure Shifts the Basin of Attraction

We have seen above that there are three relevant attractor states for the replicator dynamic in the Horn game (Figure 1), with differently sized basins of attraction. Letâ€™s see
what happens when we look at various network games instead. To isolate the impact of
interaction structure, we look at a network game with the conditional imitation update
rule. This is because, as explained above, for huge and completely connected networks,
average behavior of conditional imitation imitates that of the replicator dynamic. Com-

Smolensky

Smolensky
45.9%
36.5%
16.0%

50.1%
41.0%
6.5%

Anti-Horn
Horn

Anti-Horn
Horn

(a) Homogeneous population

(b) Heterogeneous population

Fig. 2. Comparing the basins of attraction for the conditional imitation rule by numerical simulation for different network topologies. Figure 2(a) shows the results for a completely connected
network, while 2(b) shows the results for a small-world network, both for 100 agents.

paring completely connected networks to other types of interaction structure, we get a
good glimpse at the consequences of the assumption of homogeneity that is hidden in
the elegant formulation of the replicator dynamic.
We fix a Horn game with p = .75, 1 = 0 and 2 = .1, as before, and compare,
by numerical simulation, several trials on both homogeneous and heterogeneous populations. Figure 2 shows the averaged outcomes of several trials of a discrete traversal of the strategy space. Each dot in the graph corresponds to an initial state of the
sender population. (The picture for the receiver would be largely parallel.) Since population states for this game are probability distributions over four strategies, it suffices
to describe these states by three numbers, leaving implicit the frequency of strategies
{t1 , t2 } â†’ m2 and {m1 , m2 } â†’ a2 . The color of the dots represents the most frequent
sender strategy to which the initial strategy eventually evolved. Depicted are trials over
100 runs. Figure 2(a) visualizes the basins of attraction for a homogeneous population
of 100 agents with a completely connected network as interaction structure. Notice that
these results are for a very small population only, but still meet the predictions of the
replicator dynamic in 93.2% of the trials. Figure 2(b) shows the results for a heterogeneous population of 100 agents located on a Î²-graph (k = 4; Î² = .1) as the underlying
interaction structure. The pictures in Figure 2 show clearly that conditional imitation on
small-world networks leads to fewer Smolensky outcomes.
Table 1 compares basins of attraction (BoA), not only for complete and small-world
networks, but also for scale-free network (with m = 3 and p = .8) and grid topologies.
They all differ not only in this distribution, but also in the correspondence to the basin of
attraction of the replicator dynamic (RD-correspondence). These results show clearly
that social factors of interaction have non-trivial effects on the results of evolutionary
processes of language games.

Table 1. Basins of attraction and replicator dynamic correspondence for different topologies

BoA Horn BoA Anti-Horn BoA Smolensky RD-corresp.
complete network
scale-free
Î²-graph
grid topology

L1

L2

45.9%
49.7%
50.1%
50.1%

36.5%
38.3%
41.0%
40.8%

other
(a) Lewis game

16.0%
10.5%
6.5%
6.7%

Horn

Anti-Horn

93.2%
91.0%
89.1%
88.2%

other

(b) Horn game

Fig. 3. Exemplary distribution of players with different strategies. Figure 3(a) shows the result of
a Lewis game, Figure 3(b) of a Horn game with p = .6 and 1 = .1, 2 = .2.

3

Emergence of Regional Meaning

[37] analyzed the Lewis game played on 100 Ã— 100 grid under the imitate the best rule.
The main finding was that all agents learned a signaling system and that the population
was divided in multiple language regions: regions of agents playing either the one or the
other signaling system. In other words, heterogeneous interaction structures can lead to
the emergence of regional meaning.
[23] applied reinforcement learning to a Horn-game, played on a 30 Ã— 30 grid network. Multiple language regions emerged in ca. 80% of all simulation runs and, on
average, the number of Anti-Horn players amounted to ca. 3% (30 agents) of all agents.
Furthermore, language regions were always separated by border players, who never
learned a language, but rather switched between different strategies continuously. Exemplary distributions for Lewis and Horn game are depicted in Figure 3.
[23] also extended the grid structure to a social map [25]: agents communicate not
only with their immediate neighbors on the toroid structure, but with all agents of the
population, whereby the probability of communicating with another agent was inversely
proportional to the Manhattan distance. A parameter degree of locality Î³ determines the

percentage
100%

probability

1
0.8

80%

0.6

60%

0.4

40%

0.2

20%

0

Î³
0 1 2 3 4 5 6 7 8

Lewis game
Horn game 1
Horn game 2

Î³

0%

0

1

2

3

4

5

6

7

8

(a) Prob. of neighborhood communication (b) Percentage of trials with multiple language reas a function of degree of locality Î³
gions as a function of degree of locality Î³
Fig. 4. The probability of neighborhood communication increases with degree of locality Î³ (4(a)).
The emergence of a society of multiple language regions is supported by high Î³-values (4(b)).

relation between distance and probability. While for Î³ = 0 each agent in the network
is chosen as interlocutor with the same probability, by increasing Î³, the probability of
choosing a close agent increases. Figure 4(a) depicts the probability to communicate
with a direct neighbor dependent on Î³ for a 30 Ã— 30 social map. By increasing Î³ the
social map structure approximates neighborhood communication on a grid network.
We can then examine how the emergence of multiple language regions depends on the
degree of locality Î³ in three different games: the Lewis game, Horn game 1 (p = .6,
1 = .1, 2 = .2) and Horn game 2 (p = .7, 1 = .1, 2 = .2). Since for Î³ = 0 the social
map corresponds to a complete network, in any trial for any game only one signaling
system emerges, as expected. For Î³ = 8 the social map equals a grid structure and all
trials of the Lewis game, ca. 80% of the trials of Horn game 1 and only ca. 10% of
Horn game 2 result in a society of multiple language regions. Figure 4(b) depicts how
the percentage increases with Î³ for 0 â‰¤ Î³ â‰¤ 8.
In sum, the emergence of variability depends strongly on the network topology, especially the level of connectedness. Moreover, [34] showed for Î²-graphs that the number and kind of language regions depends on global network properties like clustering
coefficient and average path length. In the next section we extend this line of research
by looking at the evolution of signaling conventions, not only on Î²-graphs, but also
scale-free networks. Adding to previous work, we investigate the network properties
associated with regions of agents that have successfully learned a language.

4

Structure of Regional Meaning

Deepening our understanding of synthetic evolutionary processes in structured populations us useful for a more thorough understanding of the sociological factors of linguistic variability. Most previous work has focused on studying which network structures
are conducive to innovation and its spread [16, 6], less has been done to investigate the

structure of the sub-regions of agents that have successfully learned a language. Towards this goal, we modeled structured populations as Î²-graph and scale-free networks
and created appropriate 10 networks of each type with 300 nodes. For each network, 20
simulation runs were conducted. Agents played the Lewis game with randomly chosen
neighbors on the network, and each agentâ€™s behavior was updated via reinforcement
learning, separately after each round of play. We recorded strategies of agents in suitably chosen regular intervals. Each trial ran until at least 90% of agents had acquired a
language, or each network connection had been used 3000 times in either direction.
In order to determine which global network properties best characterize where, on
average, learning would be most likely successful, we looked at what we will call language regions. A language region is a maximal subset of agents that have acquired the
same language that forms a connected subgraph. Despite using reinforcemant learning
instead of imitation dynamics, our data confirmed Wagnerâ€™s [34] results that in smallworld networks like ours the number of language regions is small while the size of
language regions is relatively big. On Î²-graphs multiple language regions emerged for
each simulation run, whereas on scale-free networks more than one language region
emerged in ca. 90% of all runs. However, most of the time, only two big language
regions formed on scale-free networks and Î²-graphs as well, one for each signaling
convention.
To analyze the properties of language regions, we looked at relevant properties of
(sub-)graphs: density, average clustering and transitivity.1 Our simulation data show
that each connected language region of a given type had always a higher average clustering value than the expected value for a random connected subgraph of the same size,
whereas the density value didnâ€™t exhibit such a divergence (Figure 5). So, a regular
local clique structure supports locally coherent language, but mere number of connections does not. For Î²-graphs this conclusion is also supported by the fact, that language
regions always had a higher transitivity value than expected from random sampling.
Interestingly this doesnâ€™t hold for scale-free networks. This divergence results from the
nature of both network types: in a diffuse network like Î²-graphs transitivity and clustering roughly correspond to each other, whereas scale-free networks can have high individual clustering because of the cliquishness of sub-communities, but unions of those
communities are connected by only few hubs and therefore the more global transitivity
value stays low, or doesnâ€™t exceed the expected average value.

Conclusion & Outlook

5

We presented cursorily data from a number of studies that all support a very general
conclusion: the social structure in a society matters to the evolution of conventional
meaning. Not only can different interaction structures in network games lead to shifts
in the basins of attraction of different strategies under identical dynamics, constraints
on social interaction patterns can also lead to the emergence of regional variety. Using
notions from formal network theory, this paper tried to dig deeper into the relation
between social structure and evolutionary dynamics of meaning by investigation where
languages are most likely situated in the population.
1

For the formal definitions, we refer to [14].

Î²-graph

scale-free

density
1

density
1
0.8

0.8

0.6

0.6

0.4

0.4
0.2

0.2

n

0
0

50

100 150 200 250 300

n

0
0

50

clustering
1

clustering
1

0.8

0.8

0.6

0.6

0.4

0.4

0.2

100 150 200 250 300

0.2

n
0

50

n

100 150 200 250 300

0

50

transitivity
1

transitivity
1

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

n
0

50

100 150 200 250 300

100 150 200 250 300

n
0

50

100 150 200 250 300

Fig. 5. Comparing observed density, clustering and transitivity of a language region (o) with
expected values from randomly chosen subgraphs (solid lines, subgraph size n along the x-axis).

The models and results we presented explore the impact of social factors on language evolution on a very high level of abstraction. But it is nonetheless clear that this
approach promises to lend itself to a number of concrete linguistic applications. We
therefore consider the presented material here, as a first step into new terrain. The most
obvious future applications of this line of work are language contact phenomena and
the sociolinguistics of innovation and innovation-adoption. But also, for example, by
looking at scale-free networks with a few super-agents connected to (almost) all other
agents, the impact of modern media on language change can be formally studied. Moreover, once we move beyond static interaction structures and also take generational models of language evolution into account, it becomes possible to study the way changes in
social interaction structures could have interacted with, i.e., causing and being caused
by, language use.


MIND
HAS NO
GENDER

WE ONLY THINK WHEN
WE ARE CONFRONTED
WITH PROBLEMS

I THINK
THEREFORE
I AM

MAN IS THE
MEASURE OF
ALL THINGS

IMAGINATION
DECIDES
EVERYTHING

THE UNIVERSE
HAS NOT ALWAYS
EXISTED

TO BE IS TO BE
MAN IS
PERCEIVED

MAN WAS BORN FREE,
YET EVERYWHERE
HE IS IN CHAINS

THE

AN ANIMAL
THAT MAKES
BARGAINS

PHILOSOPHY
BOOK

BIG IDEAS SIMPLY EXPLAINED
HAPPY IS HE WHO
HAS OVERCOME
HIS EGO

THERE IS
NOTHING
OUTSIDE OF
THE TEXT

MAN IS A
MACHINE

MAN IS AN
INVENTION OF
RECENT DATE

THE END JUSTIFIES
THE MEANS

ACT AS IF WHAT
YOU DO MAKES
A DIFFERENCE

LIFE WILL BE LIVED
ALL THE BETTER IF
IT HAS NO MEANING

OVER HIS OWN
BODY AND MIND,
THE INDIVIDUAL
IS SOVEREIGN

THE

PHILOSOPHY
BOOK

THE

PHILOSOPHY
BOOK

LONDON, NEW YORK, MELBOURNE,
MUNICH, AND DELHI

DK LONDON

DK DELHI

First American Edition 2011

PROJECT ART EDITOR
Anna Hall

PROJECT ART EDITOR
Neerja Rawat

SENIOR EDITOR
Sam Atkinson

ART EDITOR
Shriya Parameswaran

Published in the United States by
DK Publishing
375 Hudson Street
New York, New York 10014

EDITORS
Cecile Landau, Andrew Szudek,
Sarah Tomley

ASSISTANT ART EDITORS
Showmik Chakraborty, Devan Das,
Niyati Gosain, Neha Sharma

EDITORIAL ASSISTANT
Manisha Majithia

MANAGING ART EDITOR
Arunesh Talapatra

US EDITORS
Liza Kaplan, Rebecca Warren
MANAGING ART EDITOR
Karen Self
MANAGING EDITOR
Camilla Hallinan
ART DIRECTOR
Philip Ormerod
ASSOCIATE PUBLISHER
Liz Wheeler
PUBLISHER
Jonathan Metcalf

PRODUCTION EDITOR
Luca Frassinetti
PRODUCTION CONTROLLER
Sophie Argyris

001â€“176426â€“Feb/2011
Copyright Â© 2011
Dorling Kindersley Limited
All rights reserved

PRODUCTION MANAGER
Pankaj Sharma
DTP MANAGER/CTS
Balwant Singh
DTP DESIGNERS
Bimlesh Tiwary, Mohammad Usman
DTP OPERATOR
Neeraj Bhatia

styling by

STUDIO8 DESIGN

ILLUSTRATIONS
James Graham
PICTURE RESEARCH
Ria Jones, Myriam Megharbi

11 12 13 14 15 10 9 8 7 6 5 4 3 2 1

Without limiting the rights under
copyright reserved above, no part
of this publication may be reproduced,
stored in or introduced into a retrieval
system, or transmitted, in any form, or
by any means (electronic, mechanical,
photocopying, recording, or otherwise),
without the prior written permission of
both the copyright owner and the
above publisher of this book.
Published in Great Britain by Dorling
Kindersley Limited.
A catalog record for this book is
available from the Library of Congress.

DK books are available at special
discounts when purchased in bulk
for sales promotions, premiums,
fund-raising, or educational use.
For details, contact: DK Publishing
Special Markets, 375 Hudson Street,
New York, New York 10014 or
SpecialSales@dk.com.

ISBN 978-0-7566-6861-7
Printed and bound in Singapore
by Star Standard
Discover more at
www.dk.com

CONTRIBUTORS
WILL BUCKINGHAM

JOHN MARENBON

A philosopher, novelist, and lecturer, Will Buckingham
is particularly interested in the interplay of philosophy
and narrative. He currently teaches at De Montfort
University, Leicester, UK, and has written several
books, including Finding our Sea-Legs: Ethics,
Experience and the Ocean of Stories.

A Fellow of Trinity College, Cambridge, UK,
John Marenbon studies and writes on medieval
philosophy. His books include Early Medieval
Philosophy 480â€“1150: An Introduction.

MARCUS WEEKS
DOUGLAS BURNHAM
A professor of philosophy at Staffordshire University,
UK, Douglas Burnham is the author of many books
and articles on modern and European philosophy.

CLIVE HILL
A lecturer in political theory and British history,
Clive Hill has a particular interest in the role of
the intellectual in the modern world.

PETER J. KING
A doctor of philosophy who lectures at Pembroke
College, University of Oxford, UK, Peter J. King is the
author of the recent book One Hundred Philosophers:
A Guide to the Worldâ€™s Greatest Thinkers.

A writer and musician, Marcus Weeks studied
philosophy and worked as a teacher before embarking
on a career as an author. He has contributed to many
books on the arts and popular sciences.

OTHER CONTRIBUTORS
The publishers would also like to thank Richard
Osborne, lecturer of philosophy and critical theory at
Camberwell College of Arts, UK, for his enthusiasm
and assistance in planning this book, and Stephanie
Chilman for her help putting the Directory together.

CONTENTS
10 INTRODUCTION

46

The life which is
unexamined is not
worth living
Socrates

50

Earthly knowledge is
but shadow Plato

56

Truth resides in the world
around us Aristotle

64

Death is nothing to us
Epicurus

The Dao that can be told
is not the eternal Dao
Laozi

66

He has the most who is
most content with the least
Diogenes of Sinope

Number is the ruler
of forms and ideas
Pythagoras

67

THE ANCIENT
WORLD
700 BCEâ€“250 CE
22

24

26

30

34

250â€“1500
72

God is not the parent
of evils
St. Augustine of Hippo

74

God foresees our free
thoughts and actions
Boethius

76

The soul is distinct
from the body Avicenna

80

Just by thinking about God
we can know he exists
St. Anselm

Happy is he who has
overcome his ego
Siddhartha Gautama

82

Philosophy and religion
are not incompatible
Averroes

Hold faithfulness and
sincerity as ï¬rst principles
Confucius

84

God has no attributes
Moses Maimonides

86

Donâ€™t grieve. Anything
you lose comes round in
another form
Jalal ad-Din Muhammad Rumi

88

The universe has not
always existed
Thomas Aquinas

96

God is the not-other
Nikolaus von Kues

97

To know nothing is
the happiest life
Desiderius Erasmus

Everything is made
of water
Thales of Miletus

40

Everything is ï¬‚ux
Heraclitus

41

All is one Parmenides

42

Man is the measure of
all things Protagoras

44

When one throws to me
a peach, I return to him
a plum Mozi

45

THE MEDIEVAL
WORLD

Nothing exists except
atoms and empty space
Democritus and Leucippus

The goal of life is living
in agreement with nature
Zeno of Citium

RENAISSANCE
AND THE AGE
OF REASON
1500â€“1750

102 The end justiï¬es the means

THE AGE OF
REVOLUTION
1750â€“1900

146 Doubt is not a pleasant

condition, but certainty
is absurd Voltaire

NiccolÃ² Machiavelli
108 Fame and tranquillity

can never be bedfellows
Michel de Montaigne
110 Knowledge is power

148 Custom is the great guide

of human life David Hume
154 Man was born free yet

everywhere he is in chains
Jean-Jacques Rousseau

Francis Bacon
112 Man is a machine

Thomas Hobbes

160 Man is an animal that

makes bargains
Adam Smith

186 Every man takes the limits

of his own ï¬eld of vision
for the limits of the world
Arthur Schopenhauer
189 Theology is anthropology

Ludwig Andreas Feuerbach
116 I think therefore I am

RenÃ© Descartes
124 Imagination decides

164 There are two worlds:

our bodies and the
external world
Immanuel Kant

everything Blaise Pascal
126 God is the cause of all

things, which are in him
Benedictus Spinoza
130 No manâ€™s knowledge

here can go beyond his
experience John Locke

172 Society is indeed a contract

Edmund Burke
174 The greatest happiness

for the greatest number
Jeremy Bentham
175 Mind has no gender

Mary Wollstonecraft
134 There are two kinds of

truths: truths of reasoning
and truths of fact
Gottfried Leibniz
138 To be is to be perceived

George Berkeley

176 What sort of philosophy

one chooses depends on
what sort of person one is
Johann Gottlieb Fichte
177 About no subject is there

less philosophizing than
about philosophy
Friedrich Schlegel
178 Reality is a historical

process Georg Hegel

190 Over his own body and

mind, the individual
is sovereign
John Stuart Mill
194 Anxiety is the dizziness

of freedom
SÃ¸ren Kierkegaard
196 The history of all hitherto

existing society is the
history of class struggles
Karl Marx
204 Must the citizen ever

resign his conscience
to the legislator?
Henry David Thoreau
205 Consider what effects

things have
Charles Sanders Peirce
206 Act as if what you do

makes a difference
William James

THE MODERN
WORLD
1900â€“1950

214 Man is something to

be surpassed
Friedrich Nietzsche
222 Men with self-conï¬dence

come and see and conquer
Ahad Haâ€™am
223 Every message is made

of signs
Ferdinand de Saussure
224 Experience by itself is

not science Edmund Husserl
226 Intuition goes in the very

direction of life
Henri Bergson
228 We only think when we are

confronted with problems
John Dewey
232 Those who cannot

remember the past are
condemned to repeat it
George Santayana
233 It is only suffering that

makes us persons
Miguel de Unamuno
234 Believe in life

William du Bois
236 The road to happiness lies

in an organized diminution
of work Bertrand Russell
240 Love is a bridge from

poorer to richer knowledge
Max Scheler

241 Only as an individual can

man become a philosopher
Karl Jaspers
242 Life is a series of collisions

with the future
JosÃ© Ortega y Gasset
244 To philosophize, ï¬rst one

must confess
Hajime Tanabe
246 The limits of my language

are the limits of my world
Ludwig Wittgenstein
252 We are ourselves the

entities to be analyzed
Martin Heidegger
256 The individualâ€™s only true

moral choice is through
self-sacriï¬ce for the
community
Tetsuro Watsuji
257 Logic is the last scientiï¬c

ingredient of philosophy
Rudolf Carnap
258 The only way of knowing

a person is to love them
without hope
Walter Benjamin
259 That which is cannot

be true Herbert Marcuse

268 Existence precedes

essence
Jean-Paul Sartre
272 The banality of evil

Hannah Arendt
273 Reason lives in language

Emmanuel Levinas
274 In order to see the world

we must break with our
familiar acceptance of it
Maurice Merleau-Ponty
276 Man is deï¬ned as

a human being and
woman as a female
Simone de Beauvoir
278 Language is a social art

Willard Van Orman Quine
260 History does not belong

to us but we belong to it
Hans-Georg Gadamer
262 In so far as a scientiï¬c

statement speaks about
reality, it must be
falsiï¬able Karl Popper
266 Intelligence is a moral

category Theodor Adorno

280 The fundamental sense of

freedom is freedom from
chains Isaiah Berlin
282 Think like a mountain

Arne Naess
284 Life will be lived all the

better if it has no meaning
Albert Camus

322 Thought has always

CONTEMPORARY
PHILOSOPHY

worked by opposition
HÃ©lÃ¨ne Cixous

1950â€“PRESENT

323 Who plays God in present-

day feminism?
Julia Kristeva

290 Language is a skin

324 Philosophy is not only

Roland Barthes

a written enterprise
Henry Odera Oruka

292 How would we manage

without a culture?
Mary Midgley

325 In suffering, the animals

are our equals
Peter Singer

293 Normal science does not

aim at novelties of fact
or theory Thomas Kuhn
294 The principles of justice

are chosen behind a veil
of ignorance
John Rawls
296 Art is a form of life

300 For the black man, there

is only one destiny and it
is white Frantz Fanon

Paul Feyerabend
298 Knowledge is produced

to be sold
Jean-FranÃ§ois Lyotard

analyses are always
analyses of a failure
Slavoj Å½iÅ¾ek

302 Man is an invention of

recent date
Michel Foucault

Richard Wollheim
297 Anything goes

326 All the best Marxist

304 If we choose, we can live

in a world of comforting
illusion Noam Chomsky
306 Society is dependent upon

a criticism of its own
traditions JÃ¼rgen Habermas
308 There is nothing outside

of the text
Jacques Derrida
314 There is nothing deep

down inside us except
what we have put there
ourselves Richard Rorty
320 Every desire has a relation

to madness Luce Irigaray
321 Every empire tells itself

and the world that it is
unlike all other empires
Edward Said

330 DIRECTORY
340 GLOSSARY
344 INDEX
351 ACKNOWLEDGMENTS

INTRODU

CTION

12 INTRODUCTION

P

hilosophy is not just the
preserve of brilliant but
eccentric thinkers that it is
popularly supposed to be. It is what
everyone does when theyâ€™re not
busy dealing with their everyday
business and get a chance simply
to wonder what life and the
universe are all about. We human
beings are naturally inquisitive
creatures, and canâ€™t help wondering
about the world around us and our
place in it. Weâ€™re also equipped with
a powerful intellectual capability,
which allows us to reason as well
as just wonder. Although we may
not realize it, whenever we reason,
weâ€™re thinking philosophically.
Philosophy is not so much about
coming up with the answers to
fundamental questions as it is
about the process of trying to ï¬nd
these answers, using reasoning
rather than accepting without
question conventional views or
traditional authority. The very ï¬rst
philosophers, in ancient Greece and
China, were thinkers who were not
satisï¬ed with the established
explanations provided by religion
and custom, and sought answers
which had rational justiï¬cations.
And, just as we might share our
views with friends and colleagues,
they discussed their ideas with
one another, and even set up

â€œschoolsâ€ to teach not just the
conclusions they had come to, but
the way they had come to them.
They encouraged their students to
disagree and criticize ideas as a
means of reï¬ning them and coming
up with new and different ones. A
popular misconception is that of
the solitary philosopher arriving at
his conclusions in isolation, but this
is actually seldom the case. New
ideas emerge through discussion
and the examination, analysis, and
criticism of other peopleâ€™s ideas.

Debate and dialogue
The archetypical philosopher in
this respect was Socrates. He
didnâ€™t leave any writings, or even

Wonder is very much the
affection of a philosopher;
for there is no other
beginning of philosophy
than this.
Plato

any big ideas as the conclusions of
his thinking. Indeed, he prided
himself on being the wisest of men
because he knew he didnâ€™t know
anything. His legacy lay in the
tradition he established of debate
and discussion, of questioning the
assumptions of other people to gain
deeper understanding and elicit
fundamental truths. The writings
of Socratesâ€™ pupil, Plato, are almost
invariably in the form of dialogues,
with Socrates as a major character.
Many later philosophers also
adopted the device of dialogues
to present their ideas, giving
arguments and counterarguments
rather than a simple statement of
their reasoning and conclusions.
The philosopher who presents
his ideas to the world is liable to
be met with comments beginning
â€œYes, but ...â€ or â€œWhat if ...â€ rather
than wholehearted acceptance.
In fact, philosophers have ï¬ercely
disagreed with one another about
almost every aspect of philosophy.
Plato and his pupil Aristotle, for
example, held diametrically
opposed views on fundamental
philosophical questions, and their
different approaches have divided
opinions among philosophers ever
since. This has, in turn, provoked
more discussion and prompted yet
more fresh ideas.

INTRODUCTION 13

But how can it be that these
philosophical questions are still
being discussed and debated?
Why havenâ€™t thinkers come up
with deï¬nitive answers? What are
these â€œfundamental questionsâ€ that
philosophers through the ages have
wrestled with?

Existence and knowledge
When the ï¬rst true philosophers
appeared in ancient Greece some
2,500 years ago, it was the world
around them that inspired their
sense of wonder. They saw the
Earth and all the different forms of
life inhabiting it; the sun, moon,
planets, and stars; and natural
phenomena such as the weather,
earthquakes, and eclipses. They
sought explanations for all these
thingsâ€”not the traditional myths
and legends about the gods, but
something that would satisfy their
curiosity and their intellect. The
ï¬rst question that occupied these
early philosophers was â€œWhat is the
universe made of?â€, which was soon
expanded to become the wider
question of â€œWhat is the nature
of whatever it is that exists?â€
This is the branch of philosophy
we now call metaphysics. Although
much of the original question has
since been explained by modern
science, related questions of

metaphysics such as â€œWhy is there
something rather than nothing?â€
are not so simply answered.
Because we, too, exist as a part
of the universe, metaphysics also
considers the nature of human
existence and what it means to be
a conscious being. How do we
perceive the world around us, and
do things exist independently of
our perception? What is the
relationship between our mind and
body, and is there such a thing as
an immortal soul? The area of
metaphysics concerned with
questions of existence, ontology, is
a huge one and forms the basis for
much of Western philosophy.
Once philosophers had started
to put received wisdom to the test
of rational examination, another
fundamental question became
obvious: â€œHow can we know?â€ The
study of the nature and limits of
knowledge forms a second main
branch of philosophy, epistemology.
At its heart is the question of
how we acquire knowledge, how
we come to know what we know;
is some (or even all) knowledge
innate, or do we learn everything
from experience? Can we know
something from reasoning alone?
These questions are vital to
philosophical thinking, as we need
to be able to rely on our knowledge

in order to reason correctly. We also
need to determine the scope and
limits of our knowledge. Otherwise
we cannot be sure that we actually
do know what we think we know,
and havenâ€™t somehow been â€œtrickedâ€
into believing it by our senses.

Logic and language
Reasoning relies on establishing
the truth of statements, which can
then be used to build up a train of
thought leading to a conclusion. This
might seem obvious to us now, but
the idea of constructing a rational
argument distinguished philosophy
from the superstitious and religious
explanations that had existed before
the first philosophers. These
thinkers had to devise a way of
ensuring their ideas had validity. â¯â¯

Superstition sets the
whole world in ï¬‚ames;
philosophy quenches them.
Voltaire

14 INTRODUCTION

What emerged from their thinking
was logic, a technique of reasoning
that was gradually reï¬ned over time.
At ï¬rst simply a useful tool for
analyzing whether an argument
held water, logic developed rules
and conventions, and soon became
a ï¬eld of study in its own right,
another branch of the expanding
subject of philosophy.
Like so much of philosophy,
logic has intimate connections
with science, and mathematics in
particular. The basic structure of
a logical argument, starting from
a premise and working through
a series of steps to a conclusion, is
the same as that of a mathematical
proof. Itâ€™s not surprising then that
philosophers have often turned to
mathematics for examples of selfevident, incontrovertible truths, nor
that many of the greatest thinkers,
from Pythagoras to RenÃ© Descartes
and Gottfried Leibniz, were also
accomplished mathematicians.
Although logic might seem to
be the most exact and â€œscientiï¬câ€
branch of philosophy, a ï¬eld where
things are either right or wrong,
a closer look at the subject shows
that it is not so simple. Advances
in mathematics in the 19th century
called into question the rules of
logic that had been laid down by
Aristotle, but even in ancient times

Zeno of Eleaâ€™s famous paradoxes
reached absurd conclusions from
apparently faultless arguments.
A large part of the problem is
that philosophical logic, unlike
mathematics, is expressed in words
rather than numbers or symbols,
and is subject to all the ambiguities
and subtleties inherent in language.
Constructing a reasoned argument
involves using language carefully
and accurately, examining our
statements and arguments to make
sure they mean what we think they
mean; and when we study other
peopleâ€™s arguments, we have to
analyze not only the logical steps
they take, but also the language
they use, to see if their conclusions
hold water. Out of this process came
yet another ï¬eld of philosophy that
ï¬‚ourished in the 20th century, the
philosophy of language, which
examined terms and their meanings.

Morality, art, and politics
Because our language is imprecise,
philosophers have attempted to
clarify meanings in their search for
answers to philosophical questions.
The sort of questions that Socrates
asked the citizens of Athens tried
to get to the bottom of what they
actually believed certain concepts
to be. He would ask seemingly
simple questions such as â€œWhat is

justice?â€ or â€œWhat is beauty?â€ not
only to elicit meanings, but also to
explore the concepts themselves.
In discussions of this sort, Socrates
challenged assumptions about the
way we live our lives and the things
we consider to be important.
The examination of what it
means to lead a â€œgoodâ€ life, what
concepts such as justice and
happiness actually mean and how
we can achieve them, and how we
should behave, forms the basis for
the branch of philosophy known as
ethics (or moral philosophy); and the
related branch stemming from the
question of what constitutes beauty
and art is known as aesthetics.

O philosophy, lifeâ€™s guide!
O searcher-out of virtue
and expeller of vices!
What could we and every
age of men have been
without thee?
Cicero

INTRODUCTION 15

From considering ethical questions
about our individual lives, it is a
natural step to start thinking about
the sort of society we would like to
live inâ€”how it should be governed,
the rights and responsibilities of
its citizens, and so on. Political
philosophy, the last of the major
branches of philosophy, deals with
these ideas, and philosophers have
come up with models of how they
believe society should be organized,
ranging from Platoâ€™s Republic to
Karl Marxâ€™s Communist Manifesto.

Religion: East and West
The various branches of philosophy
are not only interlinked, but overlap
considerably, and it is sometimes
difï¬cult to say in which area a
particular idea falls. Philosophy also
encroaches on many completely
different subjects, including the
sciences, history, and the arts. With
its beginnings in questioning the
dogmas of religion and superstition,
philosophy also examines religion
itself, speciï¬cally asking questions
such as â€œDoes god exist?â€ and â€œDo
we have an immortal soul?â€ These
are questions that have their roots
in metaphysics, but they have
implications in ethics too. For
example, some philosophers have
asked whether our morality comes
from god or whether it is a purely

human constructâ€”and this in turn
has raised the whole debate as to
what extent humanity has free will.
In the Eastern philosophies
that evolved in China and India
(particularly Daoism and Buddhism)
the lines between philosophy and
religion are less clear, at least to
Western ways of thinking. This
marks one of the major differences
between Western and Eastern
philosophies. Although Eastern
philosophies are not generally a
result of divine revelation or
religious dogma, they are often
intricately linked with what we
would consider matters of faith.
Even though philosophical
reasoning is frequently used to
justify faith in the Judeo-Christian
and Islamic world, faith and belief

There is nothing either
good or bad, but thinking
makes it so.
William Shakespeare

form an integral part of Eastern
philosophy that has no parallel in
the West. Eastern and Western
philosophy also differ in their
starting points. Where the ancient
Greeks posed metaphysical
questions, the ï¬rst Chinese
philosophers considered these
adequately dealt with by religion,
and instead concerned themselves
with moral and political philosophy.

Following the reasoning
Philosophy has provided us with
some of the most important and
inï¬‚uential ideas in history. What
this book presents is a collection
of ideas from the best-known
philosophers, encapsulated in well
known quotes and pithy summaries
of their ideas. Perhaps the bestknown quotation in philosophy is
Descartesâ€™ â€œcogito, ergo sumâ€ (often
translated from the Latin as â€œI think,
therefore I amâ€). It ranks as one of
the most important ideas in the
history of philosophy, and is widely
considered a turning point in
thinking, leading us into the modern
era. On its own however, the
quotation doesnâ€™t mean much. It is
the conclusion of a line of argument
about the nature of certainty, and
only when we examine the
reasoning leading to it does the
idea begin to make sense. And â¯â¯

16 INTRODUCTION

itâ€™s only when we see where
Descartes took the ideaâ€”what the
consequences of that conclusion
areâ€”that we see its importance.
Many of the ideas in this book
may seem puzzling at ï¬rst glance.
Some may appear self-evident,
others paradoxical or ï¬‚ying in the
face of common sense. They might
even appear to prove Bertrand
Russellâ€™s ï¬‚ippant remark that â€œthe
point of philosophy is to start with
something so simple as not to seem
worth stating, and to end with
something so paradoxical that no
one will believe it.â€ So why are
these ideas important?

Systems of thought
Sometimes the theories presented
in this book were the ï¬rst of their
kind to appear in the history of
thought. While their conclusions
may seem obvious to us now, in
hindsight, they were startlingly
new in their time, and despite their
apparent simplicity, they may make
us reexamine things that we take
for granted. The theories presented
here that seem to be paradoxes and
counter-intuitive statements are the
ideas that really call into question
our assumptions about ourselves
and the worldâ€”and they also make
us think in new ways about how
we see things. There are many

ideas here that raise issues that
philosophers still puzzle over.
Some ideas may relate to other
thoughts and theories in different
ï¬elds of the same philosopherâ€™s
thinking, or have come from an
analysis or criticism of another
philosopherâ€™s work. These latter
ideas form part of a line of
reasoning that may extend over
several generations or even
centuries, or be the central idea of
a particular â€œschoolâ€ of philosophy.
Many of the great philosophers
formed integrated â€œsystemsâ€ of
philosophy with interconnecting
ideas. For example, their opinions
about how we acquire knowledge
led to a particular metaphysical
view of the universe and manâ€™s
soul. This in turn has implications
for what kind of life the philosopher
believes we should lead and what
type of society would be ideal. And
in turn, this entire system of ideas
has been the starting point for
subsequent philosophers.
We must remember too that
these ideas never quite become
outdated. They still have much to
tell us, even when their conclusions
have been proved wrong by
subsequent philosophers and
scientists. In fact, many ideas that
had been dismissed for centuries
were later to be proved startlingly

prescientâ€”the theories of the
ancient Greek atomists for example.
More importantly, these thinkers
established the processes of
philosophy, ways of thinking and
organizing our thoughts. We must
remember that these ideas are only
a small part of a philosopherâ€™s
thinkingâ€”usually the conclusion
to a longer line of reasoning.

Science and society
These ideas spread their inï¬‚uence
beyond philosophy too. Some have
spawned mainstream scientiï¬c,
political, or artistic movements.
Often the relationship between
science and philosophy is a backand-forth affair, with ideas from one
informing the other. Indeed, there
is a whole branch of philosophy
that studies the thinking behind

Scepticism is the ï¬rst
step towards truth.
Denis Diderot

INTRODUCTION 17

scientiï¬c methods and practices.
The development of logical thinking
affected how math evolved and
became the basis for the scientiï¬c
method, which relies on systematic
observation to explain the world.
Ideas about the nature of the self
and consciousness have developed
into the science of psychology.
The same is true of philosophyâ€™s
relationship with society. Ethics of
all sorts found adherents in political
leaders throughout history, shaping
the societies we live in today, and
even prompting revolutions. The
ethical decisions made in all kinds
of professions have moral dimensions
that are informed by the ideas of
the great thinkers of philosophy.

Behind the ideas
The ideas in this book have come
from people living in societies and
cultures which have shaped those
ideas. As we examine the ideas, we
get a picture of certain national and
regional characteristics, as well as
a ï¬‚avor of the times they lived in.
The philosophers presented here
emerge as distinct personalitiesâ€”
some thinkers are optimistic, others
pessimistic; some are meticulous
and painstaking, others think in
broad sweeps; some express
themselves in clear, precise
language, others in a poetic way,

and still more in dense, abstract
language that takes time to unpick.
If you read these ideas in the
original texts, you will not only
agree or disagree with the what
they say, and follow the reasoning
by which they reached their
conclusions, but also get a feeling
of what kind of person is behind it.
You might, for example, warm to
the witty and charming Hume,
appreciating his beautifully clear
prose, while not altogether feeling
at home with what he has to say; or
ï¬nd Schopenhauer both persuasive
and a delight to read, while getting
the distinct feeling that he was not
a particularly likeable man.
Above all these thinkers were
(and still are) interesting and
stimulating. The best were also
great writers too, and reading
their original writings can be as
rewarding as reading literature; we
can appreciate not just their literary
style, but also their philosophical
style, the way they present their
arguments. As well as being
thought-provoking, it can be as
uplifting as great art, as elegant as
a mathematical proof, and as witty
as an after-dinner speaker.
Philosophy is not simply about
ideasâ€”itâ€™s a way of thinking. There
are frequently no right or wrong
answers, and different philosophers

often come to radically different
conclusions in their investigations
into questions that science cannot
â€”and religion does notâ€”explain.

Enjoying philosophy
If wonder and curiosity are human
attributes, so too are the thrill of
exploration and the joy of discovery.
We can gain the same sort of
â€œbuzzâ€ from philosophy that we
might get from physical activity,
and the same pleasure that we
enjoy from an appreciating the arts.
Above all, we gain the satisfaction
of arriving at beliefs and ideas that
are not handed down or forced upon
us by society, teachers, religion, or
even philosophers, but through our
own individual reasoning. â– 

The beginning of thought
is in disagreementâ€”not
only with others but also
with ourselves.
Eric Hoffer

THE ANC
WORLD
700 â€“250
BCE

CE

IENT

20 INTRODUCTION
Thales of Miletus,
the ï¬rst known Greek
philosopher, seeks
rational answers
to questions about
the world we live in.

Traditional date of
birth of Kong Fuzi
(Confucius), whose
philosophy is centered
on respect and
tradition.

Death of Siddhartha
Gautama, the Buddha,
founder of the religion and
philosophy of Buddhism.

Empedocles proposes
his theory of the four
Classical elements;
he is the last Greek
philosopher to record
his ideas in verse.

624â€“546 BCE

551 BCE

480 BCE

C.460 BCE

F

569 BCE

508 BCE

469 BCE

404 BCE

Birth of Pythagoras,
the Greek thinker who
combined philosophy
and mathematics.

The powerful Greek
city-state of Athens
adopts a democratic
constitution.

Birth of Socrates, whose
methods of questioning
in Athens formed the
basis for much of later
Western philosophy.

Defeat in the
Peloponnesian
War leads to the
decline of Athensâ€™
political power.

rom the beginning of human
history, people have asked
questions about the world
and their place within it. For early
societies, the answers to the most
fundamental questions were found
in religion: the actions of the gods
explained the workings of the
universe, and provided a framework
for human civilizations.
Some people, however, found the
traditional religious explanations
inadequate, and they began to
search for answers based on reason
rather than convention or religion.
This shift marked the birth of
philosophy, and the ï¬rst of the great
thinkers that we know of was Thales
of Miletusâ€”Miletus was a Greek
settlement in modern-day Turkey.
Thales used reason to inquire into
the nature of the universe, and
encouraged others to do likewise.

He passed on to his followers not
only his answers, but the process
of thinking rationally, together with
an idea of what kind of explanations
could be considered satisfactory.
For this reason Thales is generally
regarded as the ï¬rst philosopher.
The main concern of the early
philosophers centered around
Thalesâ€™ basic question: â€œWhat is
the world made of?â€ Their answers
form the foundations of scientiï¬c
thought, and forged a relationship
between science and philosophy
that still exists today. The work of
Pythagoras marked a key turning
point, as he sought to explain the
world not in terms of primal matter,
but in terms of mathematics. He and
his followers described the
structure of the cosmos in numbers
and geometry. Although some of
these mathematical relationships

acquired mystical signiï¬cance for
Pythagoras and his followers, their
numerical explanation of the cosmos
had a profound inï¬‚uence on the
beginnings of scientiï¬c thought.

Classical Greek philosophy
As the Greek city-states grew in
stature, philosophy spread across
the Greek world from Ionia, and in
particular to Athens, which was
rapidly becoming the cultural
center of Greece. It was here that
philosophers broadened the scope of
philosophy to include new questions,
such as â€œHow do we know what we
know?â€ and â€œHow should we live
our lives?â€ It was an Athenian,
Socrates, who ushered in the short
but hugely inï¬‚uential period of
Classical Greek philosophy. Although
he left no writings, his ideas were so
important that they steered the

THE ANCIENT WORLD 21

Plato founds his
hugely inï¬‚uential
Academy in
Athens.

Zeno of Citium
formulates his stoic
philosophy, which
goes on to ï¬nd favor
in the Roman Empire.

Ptolemy, a Roman
citizen of Egypt,
proposes the idea that
Earth is at the center
of the universe and
does not move.

Galen of Pergamum
produces extraordinary
medical research that
remains unsurpassed until
the work of Vesalius in 1543.

C.385 BCE

C.332â€“265 BCE

C.100â€“178 CE

C.150 BCE

335 BCE

323 BCE

122 CE

220 CE

Aristotle, Platoâ€™s
student, opens his own
school in Athensâ€”the
Lyceum.

The death of Alexander
the Great signals the end
of the cultural and political
dominance of Greece in
the ancient world.

Construction begins
on Hadrianâ€™s Wall in
Britain, marking the
northernmost border
of the Roman Empire.

The collapse of the
Han Dynasty
marks the end of
a uniï¬ed China.
The Period of
Disunity begins.

future course of philosophy, and
all philosophers before him became
known as the pre-socratics. His pupil
Plato founded a philosophical school
in Athens called the Academy (from
which the word â€œacademicâ€ derives)
where he taught and developed his
masterâ€™s ideas, passing them on to
students such as Aristotle, who was
a pupil and teacher there for 20 years.
The contrasting ideas and methods
of these great thinkersâ€”Socrates,
Plato, and Aristotleâ€”form the basis
of Western philosophy as we know
it today, and their differences of
opinion have continued to divide
philosophers throughout history.
The Classical period of ancient
Greece effectively came to an end
with the death of Alexander the
Great in 323 BCE. This great leader
had uniï¬ed Greece, and Greek citystates that had worked together

once again became rivals. Following
the death of Aristotle in 322 BCE,
philosophy also divided into very
different schools of thought, as the
cynics, sceptics, epicureans, and
stoics argued their positions.
Over the next couple of centuries,
Greek culture waned as the Roman
Empire grew. The Romans had
little time for Greek philosophy
apart from stoicism, but Greek
ideas persisted, mainly because
they were preserved in the
manuscripts and translations of
the Arab world. They resurfaced
later, during medieval times, with
the rise of Christianity and Islam.

Eastern philosophies
Thinkers throughout Asia were also
questioning conventional wisdom.
Political upheaval in China from
771 to 481 BCE led to a collection of

philosophies that were less
concerned with the nature of
the universe than with how best
to organize a just society and
provide moral guidelines for the
individuals within it; in the process
examining what constitutes a
â€œgoodâ€ life. The so-called â€œHundred
Schools of Thoughtâ€ ï¬‚ourished in
this period, and the most signiï¬cant
of these were Confucianism and
Daoism, both of which continued
to dominate Chinese philosophy
until the 20th century.
To the south of China an equally
inï¬‚uential philosopher appeared:
Siddhartha Gautama, later known
as the Buddha. From his teaching
in northern India around 500 BCE,
his philosophy spread across the
subcontinent and over most of
southern Asia, where it is still
widely practiced. â– 

22

EVERYTHING
IS MADE
OF WATER

THALES OF MILETUS (C.624â€“546 BCE)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Monism
BEFORE
2500â€“900 BCE The Minoan
civilization in Crete and the
later Mycenaean civilization
in Greece rely on religion to
explain physical phenomena.
c.1100 BCE The Babylonian
creation myth, EnÃ»ma EliÅ¡,
describes the primal state of
the world as a watery mass.
c.700 BCE Theogony by the
Greek poet Hesiod relates how
the gods created the universe.
AFTER
Early 5th century BCE
Empedocles proposes the four
basic elements of the cosmos:
earth, water, air, and ï¬re.
c.400 BCE Leucippus and
Democritus conclude that the
cosmos is made up solely of
atoms and empty space.

From observation, Thales deduced that speciï¬c
weather conditions, not appeals to the gods, led to a good
harvest. Predicting a high yield of olives one year, he is
said to have bought up all the local olive presses, then
proï¬ted by renting them out to meet increased demand.

D

uring the Archaic period
(mid-8thâ€“6th century BCE),
the peoples of the Greek
peninsula gradually settled into a
group of city-states. They developed
an alphabetical system of writing,
as well as the beginnings of what
is now recognized as Western
philosophy. Previous civilizations
had relied on religion to explain
phenomena in the world around
them; now a new breed of thinkers
emerged, who attempted to ï¬nd
natural, rational explanations.
The ï¬rst of these new scientiï¬c
thinkers that we are aware of was
Thales of Miletus. Nothing survives
of his writings, but we know that
he had a good grasp of geometry
and astronomy, and is reputed to

have predicted the total eclipse of
the sun in 585 BCE. This practical
turn of mind led him to believe that
events in the world were not due to
supernatural intervention, but had
natural causes that reason and
observation would reveal.

Fundamental substance
Thales needed to establish a ï¬rst
principle from which to work, so
he posed the question, â€œWhat is
the basic material of the cosmos?â€
The idea that everything in the
universe can be ultimately reduced
to a single substance is the theory
of monism, and Thales and his
followers were the ï¬rst to propose
it within Western philosophy.
Thales reasons that the fundamental

THE ANCIENT WORLD 23
See also: Anaximander 330 â–  Anaximenes of Miletus 330 â–  Pythagoras 26â€“29
Empedocles 330 â–  Democritus and Leucippus 45 â–  Aristotle 56â€“63

â– 

What is the basic
material of the cosmos?

It must beâ€¦

Thales of Miletus

â€¦something
from which
everything
can be formed.

â€¦essential
to life.

â€¦capable
of motion.

â€¦capable
of change.

Everything is
made of water.

material of the universe had to be
something out of which everything
else could be formed, as well as
being essential to life, and capable
of motion and therefore of change.
He observes that water is clearly
necessary to sustain all forms of
life, and that it moves and changes,
assuming different forms â€“ from
liquid to solid ice and vaporous
mist. So Thales concludes that all
matter, regardless of its apparent
properties, must be water in some
stage of transformation.
Thales also notes that every
landmass appears to come to an
end at the waterâ€™s edge. From this
he deduces that the whole of the
earth must be ï¬‚oating on a bed of
water, from which it has emerged.

When anything occurs to cause
ripples or tremors in this water,
Thales states, we experience
them as earthquakes.
However, as interesting as
the details of Thalesâ€™ theories are,
they are not the main reason why
he is considered a major ï¬gure in
the history of philosophy. His true
importance lies in the fact that he
was the ï¬rst known thinker to seek
naturalistic, rational answers to
fundamental questions, rather than
to ascribe objects and events to the
whims of capricious gods. By doing
so, he and the later philosophers
of the Milesian School laid the
foundations for future scientiï¬c
and philosophical thought across
the Western world. â– 

Although we know that
Thales was born and lived in
Miletus, on the coast of what
is now Turkey, we know very
little about his life. None of his
writings, if indeed he left any,
have survived. However, his
reputation as one of the key
early Greek thinkers seems
deserved, and he is referred
to in some detail by both
Aristotle and Diogenes
Laertius, the 3rd-century
biographer of the ancient
Greek philosophers.
Anecdotal evidence
suggests that as well as
being a philosopher, Thales
was actively involved in
politics and was a very
successful businessman. He
is thought to have traveled
widely around the eastern
Mediterranean, and while
visiting Egypt, to have learned
the practical geometry that
was to become the basis of his
deductive reasoning.
However, Thales was
above all a teacher, the ï¬rst of
the so-called Milesian School
of philosophers. Anaximander,
his pupil, expanded his
scientiï¬c theories, and in
turn became a mentor to
Anaximenes, who is believed
to have taught the young
mathematician Pythagoras.

24

THE DAO THAT CAN
BE TOLD IS NOT
THE
ETERNAL
DAO
LAOZI ( .6TH CENTURY )
C

IN CONTEXT
TRADITION
Chinese philosophy
APPROACH
Daoism
BEFORE
1600â€“1046 BCE During the
Shang Dynasty, people believe
fate is controlled by deities and
practice ancestor worship.
1045â€“256 BCE Under the Zhou
Dynasty, the Mandate of
Heaven (god-given authority)
justiï¬es political decisions.
AFTER
5th century BCE Confucius
(Kong Fuzi) sets out his rules
for personal development and
for ethical government.
4th century BCE Philosopher
Zhuangzi moves the focus of
Daoist teaching more toward
the actions of the individual,
rather than those of the state.
3rd century CE Scholars Wang
Bi and Guo Xiang create a
Neo-Daoist school.

I

BCE

n the 6th century BCE, China
moved toward a state of
internal warfare as the ruling
Zhou Dynasty disintegrated. This
change bred a new social class of
administrators and magistrates
within the courts, who occupied
themselves with the business of
devising strategies for ruling more
effectively. The large body of ideas

The source of
all existence.

that was produced by these ofï¬cials
became known as the Hundred
Schools of Thought.
All this coincided with the
emergence of philosophy in Greece,
and shared some of its concerns,
such as seeking stability in a
constantly changing world, and
alternatives to what had previously
been prescribed by religion. But

Dao
(the Way)â€¦

The root of
all things, seen
and unseen.

â€¦is achieved
throughâ€¦
Acting
thoughtfully,
not impulsively.

A solitary
life of meditation
and reï¬‚ection.

â€¦wu wei
(non-action).
Living in peace,
simplicity, and
tranquility.

Acting in
harmony
with nature.

THE ANCIENT WORLD 25
See also: Siddhartha Gautama 30â€“33

Chinese philosophy evolved from
practical politics and was therefore
concerned with morality and ethics
rather than the nature of the cosmos.
One of the most important ideas
to appear at this time came from
the Daode jing (The Way and its
Power), which has been attributed
to Laozi (Lao Tzu). It was one of the
ï¬rst attempts to propose a theory
of just rule, based on de (virtue),

Living in harmony with nature is
one path the Daode jing prescribes for
a well-balanced life. For this man that
could mean respecting the ecological
balance of the lake and not over-ï¬shing.

Laozi

â– 

Confucius 34â€“39

â– 

Mozi 44

â– 

Wang Bi 331

â– 

Hajime Tanabe 244â€“45

which could be found by following
dao (the Way), and forms the basis
of the philosophy known as Daoism.

Cycles of change
In order to understand the concept
of dao, it is necessary to know how
the ancient Chinese viewed the
ever-changing world. For them, the
changes are cyclical, continually
moving from one state to another,
such as from night to day, summer
to winter, and so on. They saw the
different states not as opposites,
but as related, one arising from the
other. These states also possess
complementary properties that
together make up a whole. The
process of change is seen as an
expression of dao, and leads to the
10,000 manifestations that make up
the world. Laozi, in the Daode jing,
says that humans are merely one
of these 10,000 manifestations and
have no special status. But because
of our desire and free will, we can
stray from the dao, and disturb the
worldâ€™s harmonious balance. To live
a virtuous life means acting in
accordance with the dao.
So little is known for certain about
the author of the Daode jing, who
is traditionally assumed to be
Laozi (Lao Tzu). He has become
an almost mythical ï¬gure; it has
even been suggested that the
book was not by Laozi, but is in
fact a compilation of sayings by a
number of scholars. What we do
know is that there was a scholar
born in the state of Chu, with the
name Li Er or Lao Tan, during
the Zhou dynasty, who became
known as Laozi (the Old Master).
Several texts indicate that he was
an archivist at the Zhou court, and
that Confucius consulted him on

Knowing others
is intelligence; knowing
yourself is true wisdom.
Laozi

Following the dao, however, is not
a simple matter, as the Daode jing
acknowledges. Philosophizing
about dao is pointless, as it is
beyond anything that humans can
conceive of. It is characterized by
wu (â€œnot-beingâ€), so we can only
live according to the dao by wu
wei, literally â€œnon-action.â€ By this
Laozi does not mean â€œnot doingâ€,
but acting in accordance with
natureâ€”spontaneously and
intuitively. That in turn entails
acting without desire, ambition,
or recourse to social conventions. â– 
rituals and ceremonies. Legend
states that Laozi left the court
as the Zhou dynasty declined,
and journeyed west in search
of solitude. As he was about to
cross the border, one of the
guards recognized him and
asked for a record of his wisdom.
Laozi wrote the Daode jing for
him, and then continued on his
way, never to be seen again.
Key works
c.6th century BCE
Daode jing (also known
as the Laozi)

26

NUMBER IS
THE RULER
OF FORMS
AND IDEAS
PYTHAGORAS (C.570â€“495 BCE)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Pythagoreanism
BEFORE
6th century BCE Thales
proposes a non-religious
explanation of the cosmos.
AFTER
c.535â€“c.475 BCE Heraclitus
dismisses Pythagoreanism
and says that the cosmos is
governed by change.
c.428 BCE Plato introduces
his concept of perfect Forms,
which are revealed to the
intellect and not the senses.
c.300 BCE Euclid, a Greek
mathematician, establishes
the principles of geometry.
1619 German mathematician
Johannes Kepler describes the
relationship between geometry
and physical phenomena.

W

estern philosophy was
in its infancy when
Pythagoras was born.
In Miletus, Greece, a group of
philosophers known collectively as
the Milesian School had started to
seek rational explanations for natural
phenomena only a generation or so
earlier, marking the beginning of
the Western philosophical tradition.
Pythagoras spent his childhood not
far from Miletus, so it is very likely
that he knew of them, and may
even have studied in their academy.
Like Thales, the founder of the
Milesian School, Pythagoras is
said to have learnt the rudiments
of geometry during a trip to Egypt.
With this background, it is not

THE ANCIENT WORLD 27
See also: Thales of Miletus 22â€“23

Everything in the
universe conforms
to mathematical
rules and ratios.

â– 

Siddhartha Gautama 30â€“33

So if we
understand number
and mathematical
relationships...

Number is the
ruler of forms.

surprising that he should approach
philosophical thinking in a
scientiï¬c and mathematical way.

The Pythagorean academy
Pythagoras was also, however, a
deeply religious and superstitious
man. He believed in reincarnation
and the transmigration of souls, and
he established a religious cult, with
himself cast as a virtual messiah, in
Croton, southern Italy. His disciples
lived in a collective commune,

Pythagoras

â– 

Heraclitus 40

â– 

Plato 50â€“55

...we come to
understand the
structure of
the cosmos.

â– 

RenÃ© Descartes 116â€“23

Mathematics is
the key model
for philosophical
thought.

Number is the
ruler of ideas.

following strict behavioral and
dietary rules, while studying his
religious and philosophical theories.
The Pythagoreans, as his disciples
were known, saw his ideas as
mystical revelations, to the extent
that some of the discoveries
attributed to him as â€œrevelationsâ€
may in fact have come from others
in the community. His ideas were
recorded by his students, who
included his wife, Theano of Crotona,
and daughters. The two sides of

Pythagorasâ€™s beliefsâ€”the mystical
and the scientiï¬câ€”seem to be
irreconcilable, but Pythagoras
himself does not see them as
contradictory. For him, the goal
of life is freedom from the cycle
of reincarnation, which can be
gained by adhering to a strict
set of behavioral rules, and by
contemplation, or what we would
call objective scientiï¬c thinking.
In geometry and mathematics he
found truths that he regarded â¯â¯

Little is known about Pythagorasâ€™s
life. He left no writings himself,
and unfortunately, as the Greek
philosopher Porphyry noted in his
Vita Pythagorae, â€œNo one knows
for certain what Pythagoras told
his associates, since they observed
an unusual silence.â€ However,
modern scholars believe that
Pythagoras was probably born on
the island of Samos, off the coast
of modern-day Turkey. As a young
man, he travelled widely, perhaps
studying at the Milesian School,
and probably visiting Egypt, which
was a centrer of learning. At
the age of about 40, he set up a

community of around 300 people
in Croton, southern Italy. Its
members studied a mixture of
mystical and academic studies,
and despite its collective nature,
Pythagoras was clearly the
communityâ€™s leader. At the age
of 60, he is said to have married
a young girl, Theano of Crotona.
Growing hostility toward the
Pythagorean cult eventually
forced him to leave Croton, and
he ï¬‚ed to Metapontum, also in
southern Italy, where he died
soon after. His community had
virtually disappeared by the end
of the 4th century BCE.

28 PYTHAGORAS
Pythagorasâ€™s Theorem showed that shapes
and ratios are governed by principles that
can be discovered. This suggested that it
might be possible, in time, to work out the
structure of the entire cosmos.

c2
b2

There is geometry in
the humming of the strings,
there is music in the
spacing of the spheres.
Pythagoras

b c
a

a2

a2

+

2

b

as self-evident, as if god-given, and
worked out mathematical proofs that
had the impact of divine revelation.
Because these mathematical
discoveries were a product of pure
reasoning, Pythagoras believes
they are more valuable than mere
observations. For example, the
Egyptians had discovered that a
triangle whose sides have ratios of
3:4:5 always has a right angle, and
this was useful in practice, such as
in architecture. But Pythagoras
uncovered the underlying principle
behind all right-angled triangles
(that the square of the hypotenuse
equals the sum of the squares of the
other two sides) and found it to be
universally true. This discovery was
so extraordinary, and held such
potential, that the Pythagoreans
took it to be divine revelation.
Pythagoras concludes that the
whole cosmos must be governed
by mathematical rules. He says

=

c2

that number (numerical ratios and
mathematical axioms) can be used
to explain the very structure of the
cosmos. He does not totally dismiss
the Milesian idea that the universe
is made up of one fundamental
substance, but he shifts the enquiry
from substance to form.
This was such a profound change
in the way of looking at the world,
that we should probably forgive
Pythagoras and his disciples for
getting somewhat carried away,
and giving numbers a mystical
signiï¬cance. Through exploring the
relationship between numbers and
geometry, they discoved the square
numbers and cube numbers that
we speak of today, but they also
attributed characteristics to them,
such as â€œgoodâ€ to the even numbers
and â€œevilâ€ to the odd ones, and even
speciï¬cs such as â€œjusticeâ€ to the
number four, and so on. The number
ten, in the form of the tetractys (a

triangular shape made up of rows of
dots) had a particular signiï¬cance
in Pythagorean ritual. Less
contentiously, they saw the number
one as a single point, a unity, from
which other things could be derived.
The number two, in this way of
thinking, was a line, number three a
surface or plane, and four a solid; the
correspondence with our modern
concept of dimensions is obvious.
The Pythagorean explanation of
the creation of the universe followed
a mathematical pattern: on the
Unlimited (the inï¬nite that existed
before the universe), God imposed a
Limit, so that all that exists came to
have an actual size. In this way God
created a measurable unity from
which everything else was formed.

Numerical harmonies
Pythagorasâ€™s most important
discovery was the relationships
between numbers: the ratios and
proportions. This was reinforced by
his investigations into music, and
in particular into the relationships
between notes that sounded pleasant
together. The story goes that he
ï¬rst stumbled onto this idea when
listening to blacksmiths at work. One
had an anvil half the size of the other,
and the sounds they made when

THE ANCIENT WORLD 29
hit with a hammer were exactly an
octave (eight notes) apart. While
this may be true, it was probably by
experimenting with a plucked string
that Pythagoras determined the
ratios of the consonant intervals
(the number of notes between two
notes that determines whether they
will sound harmonious if struck
together). What he discovered was
that these intervals were harmonious
because the relationship between
them was a precise and simple
mathematical ratio. This series,
which we now know as the harmonic
series, conï¬rmed for him that the
elegance of the mathematics he had
found in abstract geometry also
existed in the natural world.

The stars and elements
Pythagoras had now proved not
only that the structure of the
universe can be explained in
mathemathical termsâ€”â€œnumber
is the ruler of formsâ€â€”but also
that acoustics is an exact science,
and number governs harmonious
proportions. He then started to
apply his theories to the whole
cosmos, demonstrating the
harmonic relationship of the stars,
planets, and elements. His idea
of harmonic relationships between
the stars was eagerly taken up
by medieval and Renaissance
astronomers, who developed whole
theories around the idea of the music
of the spheres, and his suggestion
that the elements were arranged
harmoniously was revisited over
2,000 years after his death. In 1865
English chemist John Newlands
discovered that when the chemical
elements are arranged according to
Classical architecture follows
Pythagorean mathematical ratios.
Harmonious shapes and ratios are used
throughout, scaled down in the smaller
parts, and up for the overall structure.

atomic weight, those with similar
properties occur at every eighth
element, like notes of music. This
discovery became known as the
Law of Octaves, and it helped lead
to the development of the Periodic
Law of chemical elements still
used today.
Pythagoras also established the
principle of deductive reasoning,
which is the step-by-step process
of starting with self-evident axioms
(such as â€œ2 + 2 = 4â€) to build toward
a new conclusion or fact. Deductive
reasoning was later reï¬ned by
Euclid, and it formed the basis
of mathematical thinking into
medieval times and beyond.
One of Pythagorasâ€™s most
important contributions to the
development of philosophy was
the idea that abstract thinking
is superior to the evidence of the
senses. This was taken up by
Plato in his theory of Forms, and
resurfaced in the philosophical
method of the rationalists in the
17th century. The Pythagorean
attempt to combine the rational
with the religious was the ï¬rst

Reason is immortal,
all else mortal.
Pythagoras

attempt to grapple with a problem
that has dogged philosophy and
religion in some ways ever since.
Almost everything we know
about Pythagoras comes to us from
others; even the bare facts of his life
are largely conjecture. Yet he has
achieved a near-legendary status
(which he apparently encouraged) for
the ideas attributed to him. Whether
or not he was in fact the originator
of these ideas does not really matter;
what is important is their profound
effect on philosophical thought. â– 

30

HAPPY IS
HE WHO HAS
OVERCOME
HIS EGO
SIDDHARTHA GAUTAMA (C.563â€“483 BCE)

IN CONTEXT
TRADITION
Eastern philosophy
APPROACH
Buddhism
BEFORE
c.1500 BCE Vedism reaches
the Indian subcontinent.
c.10thâ€“5th centuries BCE
Brahmanism replaces
Vedic beliefs.
AFTER
3rd century BCE Buddhism
spreads from the Ganges
valley westward across India.
1st century BCE The
teachings of Siddhartha
Gautama are written down
for the ï¬rst time.
1st century CE Buddhism
starts to spread to China
and Southeast Asia. Different
schools of Buddhism begin
to evolve in different areas.

S

iddhartha Gautama, later
known as the Buddha, â€œthe
enlightened oneâ€, lived in
India during a period when religious
and mythological accounts of the
world were being questioned. In
Greece, thinkers such as Pythagoras
were examining the cosmos using
reason, and in China, Laozi and
Confucius were detaching ethics
from religious dogma. Brahmanism,
a religion that had evolved from
Vedismâ€”an ancient belief based
on the sacred Veda textsâ€”was
the dominant faith in the Indian
subcontinent in the 6th century BCE,
and Siddhartha Gautama was the
ï¬rst to challenge its teachings with
philosophical reasoning.

THE ANCIENT WORLD 31
See also: Laozi 24â€“25 â–  Pythagoras 26â€“29 â–  Confucius 34â€“39 â– 
David Hume 148â€“53 â–  Arthur Schopenhauer 186â€“188 â–  Hajime Tanabe 244â€“45

The Four Noble Truths
inherent part off
existence from birth, through
sickness and old age, to death.

The truth of suffering
(Dukkha)

The cause of suffering is
desire: craving for sensual
pleasures and attachment to
worldly possessions and power.

The truth of
the origin of suffering
(Samudaya)

Suffering can be ended
by detaching oneself from
craving and attachment.

The truth of the
ending of suffering
(Nirodha)

The Eightfold Path is the
means to eliminate desire and
overcome the ego.

The truth of the path
to the ending of
suffering (Magga)

Gautama, although revered by
Buddhists for his wisdom, was
neither a messiah nor a prophet,
and he did not act as a medium
between God and Man. His ideas
were arrived at through reasoning,
not divine revelation, and it is this
that marks Buddhism out as a
philosophy as much as (perhaps
even more than) a religion. His
quest was philosophicalâ€”to
discover truthsâ€”and he
maintained that these truths are
available to all of us through the
power of reason. Like most Eastern
philosophers, he was not interested
in the unanswerable questions of
metaphysics that preoccupied the
Greeks. Dealing with entities

beyond our experience, this kind of
enquiry was senseless speculation.
Instead, he concerned himself with
the question of the goal of life,
which in turn involved examining
the concepts of happiness, virtue,
and the â€œgoodâ€ life.

The middle way
In his early life, Gautama enjoyed
luxury and, we are told, all the
sensual pleasures. However, he
realized that these were not enough
on their own to bring him true
happiness. He was acutely aware
of the suffering in the world, and
saw that it was largely due to
sickness, old age, and death, and
the fact that people lack what â¯â¯

Siddhartha Gautama
Almost all we know of
Siddhartha Gautamaâ€™s life
comes from biographies
written by his followers
centuries after his death, and
which differ widely in many
details. What is certain is
that he was born in Lumbini,
modern-day Nepal, some time
around 560 BCE. His father
was an ofï¬cial, possibly
the leader of a clan, and
Siddhartha led a privileged
life of luxury and high status.
Dissatisï¬ed with this,
Siddhartha left his wife and
son to ï¬nd a spiritual path,
and discovered the â€œmiddle
wayâ€ between sensual
indulgence and asceticism.
He experienced enlightenment
while thinking in the shade of
a bodhi tree, and devoted the
rest of his life to traveling
throughout India, preaching.
After his death, his teachings
were passed down orally for
some 400 years before being
written down in the Tipitaka
(Three Baskets).
Key works
1st century CE
Tipitaka (recounted by
his followers), comprising:
Vinaya-pitaka, Sutta-pitaka,
Abhidhamma-pitaka

32

SIDDHARTHA GAUTAMA
may bring short-term gratiï¬cation,
but not happiness in the sense of
contentment and peace of mind.

The â€œnot-selfâ€

The Buddha cut off his hair as part of
his renunciation of the material world.
According to Buddhist teaching, the
temptations of the world are the source
of all suffering, and must be resisted.

they need. He also recognized that
the sensual pleasure we indulge
in to relieve suffering is rarely
satisfying, and that when it is, the
effects are transitory. He found the
experience of extreme asceticism
(austerity and abstinence) equally
dissatisfying, bringing him no
nearer to an understanding of how
to achieve happiness.
Gautama came to the conclusion
that there must be a â€œmiddle wayâ€
between self-indulgence and selfmortiï¬cation. This middle way,
he believed, should lead to true
happiness, or â€œenlightenmentâ€,
and to ï¬nd it he applied reason
to his own experiences.
Suffering, he realized, is
universal. It is an integral part of
existence, and the root cause of our
suffering is the frustration of our
desires and expectations. These
desires he calls â€œattachmentsâ€, and
they include not only our sensual
desires and worldly ambitions,
but our most basic instinct for
self-preservation. Satisfying
these attachments, he argues,

The next step in Gautamaâ€™s
reasoning is that the elimination
of attachments will prevent any
disappointment, and so avoid
suffering. To achieve this, he
suggests a root cause of our
attachmentsâ€”our selï¬shness,
and by selï¬shness he means more
than just our tendency to seek
gratiï¬cation. For Gautama,
selï¬shness is self-centeredness
and self-attachmentâ€”the domain
of what today we would call the
â€œego.â€ So, to free ourselves from
attachments that cause us pain,
it is not enough merely to renounce
the things we desireâ€”we must
overcome our attachment to that
which desiresâ€”the â€œself.â€
But how can this be done?
Desire, ambition, and expectation
are part of our nature, and for
most of us constitute our very
reasons for living. The answer,
for Gautama, is that the egoâ€™s
world is illusoryâ€”as he shows,
again, by a process of reasoning.
He argues that nothing in the
universe is self-caused, for
everything is the result of some
previous action, and each of us is
only a transitory part of this eternal
processâ€”ultimately impermanent
and without substance. So, in
reality, there is no â€œselfâ€ that is not
part of the greater wholeâ€”or the
â€œnot-selfâ€â€”and suffering results
from our failure to recognize this.
This does not mean that we should
deny our existence or personal
identity, rather that we should
understand them for what they
areâ€”transient and insubstantial.
Grasping the concept of being a
constituent part of an eternal â€œnotselfâ€, rather than clinging to the

Believe nothing,
no matter where you
read it, or who said it,
unless it agrees with
your own reason.
Siddhartha Gautama

notion of being a unique â€œselfâ€, is
the key to losing that attachment,
and ï¬nding a release from suffering.

The Eightfold Path
Gautamaâ€™s reasoning from the
causes of suffering to the way to
achieve happiness is codiï¬ed in
Buddhist teachings in the Four
Noble Truths: that suffering is
universal; that desire is the cause
of suffering; that suffering can be
avoided by eliminating desire;
that following the Eightfold Path
will eliminate desire. This last
Truth refers to what amounts to
a practical guide to the â€œmiddle
wayâ€ that Gautama laid out for his
followers to achieve enlightenment.

Peace comes
from within. Do not
seek it without.
Siddhartha Gautama

THE ANCIENT WORLD 33
The Eightfold Path (right action,
right intention, right livelihood,
right effort, right concentration,
right speech, right understanding,
and right mindfulness) is in effect
a code of ethicsâ€”a prescription for
a good life and the happiness that
Gautama ï¬rst set out to ï¬nd.

Nirvana
Gautama sees the ultimate goal of
life on Earth to be the ending of the
cycle of suffering (birth, death, and
rebirth) into which we are born. By
following the Eightfold Path, a man
can overcome his ego and live a
life free from suffering, and through
his enlightenment he can avoid the
pain of rebirth into another life of
suffering. He has realized his place
in the â€œnot-selfâ€, and become at one
with the eternal. He has attained
the state of Nirvanaâ€”which is
variously translated as â€œnonattachmentâ€, â€œnot-beingâ€, or literally
â€œblowing outâ€ (as of a candle).
In the Brahmanism of Gautamaâ€™s
time, and the Hindu religion that
followed, Nirvana was seen as
becoming one with god, but
Gautama carefully avoids any
mention of a deity or of an ultimate
purpose to life. He merely describes
Nirvana as â€œunborn, unoriginated,
uncreated, and unformedâ€, and
transcending any sensory

experience. It is an eternal and
unchanging state of not-being,
and so the ultimate freedom from
the suffering of existence.
Gautama spent many years
after his enlightenment traveling
around India, preaching and
teaching. During his lifetime, he
gained a considerable following,
and Buddhism became established
as a major religion as well as a
philosophy. His teachings were
passed down orally from generation
to generation by his followers, until
the 1st century CE, when they were
written down for the ï¬rst time.
Various schools began to appear
as Buddhism spread across India,
and later spread eastward into
China and Southeast Asia, where
it rivalled Confucianism and
Daoism in its popularity.

Gautamaâ€™s teachings spread as far
as the Greek empire by the 3rd
century BCE, but had little inï¬‚uence
on Western philosophy. However,
there were similarities between
Gautamaâ€™s approach to philosophy
and that of the Greeks, not least
Gautamaâ€™s emphasis on reasoning
as a means of ï¬nding happiness, and
his disciplesâ€™ use of philosophical
dialogues to elucidate his teachings.
His thoughts also ï¬nd echoes in the
ideas of later Western philosophers,
such as in Humeâ€™s concept of the
self and Schopenhauerâ€™s view of
the human condition. But it was
not until the 20th century that
Buddhism was to have any direct
inï¬‚uence on Western thinking.
Since then, more and more
Westerners have turned to it
for guidance on how to live. â– 

The dharma wheel, one of the oldest
Buddhist symbols, represents the
Eightfold Path to Nirvana. In Buddhism,
the word â€œdharmaâ€ refers to the teachings
of the Buddha.

Right
Mindfulness
Right
Action

Right
Understanding

Right
Speech

The
Eightfold
Path

Right
Intention

The mind is
everything. What you
think, you become.
Siddhartha Gautama
Right
Concentration

Right
Livelihood
Right
Effort

HOLD
FAITHFULNESS
AND SINCERITY
AS
FIRST
PRINCIPLES
CONFUCIUS (551â€“479 )
BCE

36 CONFUCIUS
IN CONTEXT
TRADITION
Chinese philosophy
APPROACH
Confucianism
BEFORE
7th century BCE The Hundred
Schools of Thought emerge.
6th century BCE Laozi
proposes acting in accordance
with the dao (the Way).
AFTER
c.470â€“c.380 BCE Chinese
philosopher Mozi argues
against Confucian ideas.
372â€“289 BCE Chinese thinker
Meng Zi revives Confucianism.
221â€“202 BCE Confucianism is
suppressed by the Qin Dynasty.
136 BCE The Han Dynasty
introduces civil service
examinations modelled on
Confucian texts.
9th century CE Confucianism
is reborn as Neo-Confucianism.

Confucius

F

rom 770 to 220 BCE, China
enjoyed an era of great
cultural development, and
the philosophies that emerged
at this time were known as the
Hundred Schools of Thought. By
the 6th century BCE, the Zhou
Dynasty was in declineâ€”moving
from the stability of the Spring
and Autumn Period to the aptly
named Warring States Periodâ€”
and it was during this time that
Kong Fuzi, the Master Kong, or
Confucius, was born. Like other
philosophers of the ageâ€”such as
Thales, Pythagoras, and Heraclitus
of Greeceâ€”Confucius sought
constants in a world of change,
and for him this meant a search
for moral values that could enable
rulers to govern justly.

The Analects
Unlike many of the early Chinese
philosophers, Confucius looked
to the past for his inspiration. He
was conservative by nature, and
had a great respect for ritual and
ancestor worshipâ€”both of which
were maintained by the Zhou
Dynasty, whose rulers received
authority from the gods via the
so-called Heavenly Mandate.
According to tradition, Confucius
was born in 551 BCE in Qufu, in
the state of Lu, China. His name
was originally Kong Qiu, and only
later did he earn the title Kong
Fuzi, or â€œMaster Kong.â€ Little is
known about his life, except that
he was from a well-to-do family,
and that as a young man he
worked as a servant to support
his family after his father died.
He nevertheless managed to ï¬nd
time to study, and became an
administrator in the Zhou court,
but when his suggestions to the
rulers were ignored he left to
concentrate on teaching.

The superior man does
what is proper to the station
in which he is; he does not
desire to go beyond this.
Confucius

A rigid social hierarchy existed in
China, but Confucius was part of
a new class of scholars who acted
as advisors to the courtsâ€”in effect
a class of civil servantsâ€”and they
achieved their status not through
inheritance, but by merit. It was
Confuciusâ€™s integration of the
old ideals with the emerging
meritocracy that produced his
unique new moral philosophy.
The main source we have for
the teachings of Confucius is the
Analects, a collection of fragments
of his writings and sayings compiled
by his disciples. It is primarily
a political treatise, made up of
As a teacher he traveled
throughout the empire, and at
the end of his life he returned
to Qufu, where he died in 479
BCE. His teaching survives in
fragments and sayings passed
down orally to his disciples,
and collected in the Analects
and anthologies compiled by
Confucian scholars.
Key works
5th century BCE
Analects
Doctrine of the Mean
Great Learning

THE ANCIENT WORLD 37
See also: Thales of Miletus 22â€“23
Hajime Tanabe 244â€“45

â– 

Laozi 24â€“25

aphorisms and anecdotes that form
a sort of rule book for good
governmentâ€”but his use of the
word junzi (literally â€œgentlemanâ€) to
denote a superior, virtuous man,
indicates that his concerns were as
much social as political. Indeed,
many passages of the Analects
read like a book of etiquette. But
to see the Analects as merely a
social or political treatise is to miss
its central point. At its heart lies a
comprehensive ethical system.

The virtuous life
Before the appearance of the
Hundred Schools of Thought,
the world had been explained by
mythology and religion, and power
and moral authority were generally
accepted to be god-given. Confucius
is pointedly silent about the gods,
but he often refers to tian, or

â– 

Pythagoras 26â€“29

â– 

Siddhartha Gautama 30â€“33

Heaven, as the source of moral
order. According to the Analects,
we humans are the agents that
Heaven has chosen to embody its
will and to unite the world with
the moral orderâ€”an idea that was
in line with traditional Chinese
thinking. What breaks with
tradition, however, is Confuciusâ€™s
belief that deâ€”virtueâ€”is not
something Heaven-sent for the
ruling classes, but something that
can be cultivatedâ€”and cultivated
by anyone. Having himself risen to
be a minister of the Zhou court,
he believed that it was a duty of
the middle classes, as well as the
rulers, to strive to act with virtue
and benevolence (ren) to achieve
a just and stable society.
To reconcile the fact that society
was a rigid class system with his
belief that all men can receive the

â– 

Heraclitus 40

â– 

blessing of the Heavenly Mandate,
Confucius argues that the virtuous
man is not simply one who stands
at the top of the social hierarchy,
but one who understands his
place within that hierarchy and
embraces it to the full. And to
deï¬ne the various means of acting
in accordance with deâ€”virtueâ€”he
turns to traditional Chinese values:
zhong, loyalty; xiao, ï¬lial piety; li,
ritual propriety; and shu, reciprocity.
The person who sincerely observes
these values Confucius called junzi,
the gentleman or superior man, by
which he means a man of virtue,
learning, and good manners.
The values of de had evolved
within the ruling classes but had
become little more than empty
gestures in the disintegrating
world of the Zhou Dynasty.
Confucius is attempting to â¯â¯

Faithfulness
and sincerity...

...are shown in traditional
rituals and ceremonies.

These qualities in these
settings allow virtue to
become visible.

Others are
transformed
by virtue.

Virtue can then
be seen by others.

Virtue is then
made manifest
in the world.

Faithfulness and sincerity
hold the power of
transformation.

38 CONFUCIUS
The Five Constant
Relationships

Sovereignâ€”Subject
Rulers should be benevolent,
and subjects loyal.

Fatherâ€”Son
A parent is to be loving,
a child obedient.

Husbandâ€”Wife
Husbands are to be good and
fair, and wives understanding.

Elder Br
B otthe
h râ€”
Younger Brother
An elder sibling is to be
gentle, and younger
siblings respectful.

Friendâ€”Friend
Older friends are to
be considerate, younger
friends reverential.

persuade the rulers to return to
these ideals and to restore a just
government, but he also believes in
the power of benevolenceâ€”arguing
that ruling by example rather than
by fear would inspire the people to
follow a similarly virtuous life. The
same principle, he believes, should
govern personal relationships.

Loyalty and ritual
In his analysis of relationships,
Confucius uses zhongâ€”the virtue
of loyaltyâ€”as a guiding principle.
To begin with, he stresses the
importance of the loyalty of a
minister to his sovereign, then
shows that a similar relation holds
between father and son, husband
and wife, elder brother and younger
brother, and between friends. The
order in which he arranges these is
signiï¬cantâ€”political loyalty ï¬rst,
then family and clan loyalties, then
loyalties to friends and strangers.
For Confucius, this hierarchy
reï¬‚ects the fact that each person
should know his station in society
as a whole, as well his place in the
family and the clan.
This aspect of â€œknowing oneâ€™s
stationâ€ is exempliï¬ed by xiaoâ€”
ï¬lial pietyâ€”which for Confucius
was much more than just respect
for oneâ€™s parents or elders. In fact,
this is the closest he gets to
religious ideas in the Analects, for
xiao is connected to the traditional
practice of ancestor worship. Above
all, xiao reinforced the relationship
of inferior to superior, which was
central to his thinking.
It is in his insistence on liâ€”
ritual proprietyâ€”that Confucius
is at his most conservative. Li did
not simply refer to rituals such as
ancestor worship, but also to the
social norms that underpinned
every aspect of contemporary
Chinese life. These ranged from
ceremonies such as marriages,

Ritual and tradition, for Confucius,
are vital for binding an individual
to his community. By knowing his
place in society, the individual is free
to become junzi, a man of virtue.

funerals, and sacriï¬ces to the
etiquette of receiving guests,
presenting gifts, and the simple,
everyday gestures of politeness,
such as bowing and using the
correct mode of address. These are,
according to Confucius, the outward
signs of an inner deâ€”but only when
they are performed with sincerity,
which he considers to be the way of
Heaven. Through the outward show
of loyalty with inner sincerity, the
superior man can transform society.

Sincerity
For Confucius, society can be
changed by example. As he writes:
â€œSincerity becomes apparent.
From being apparent, it becomes
manifest. From being manifest,
it becomes brilliant. Brilliant, it
affects others. Affecting others,
they are changed by it. Changed by
it, they are transformed. Only he
who is possessed of the most
complete sincerity that can exist
under Heaven, can transform.â€
Here, Confucius is at his least
conservative, and he explains that
the process of transformation can
work both ways. The concept of
zhong (faithfulness) also has an

THE ANCIENT WORLD 39

What you know,
you know;
what you donâ€™t know,
you donâ€™t know.
This is true wisdom.
Confucius

implication of â€œregard for others.â€
He took the view that one can learn
to become a superior man by ï¬rst
recognizing what one does not know
(an idea echoed a century later by
the Greek philosopher Socrates,
who claimed that his wisdom lay
in accepting that he knew nothing),
and then by watching other people:
if they show virtue, try to become
their equal; if they are inferior,
be their guide.

Self-reï¬‚ection
This notion of zhong as a regard
for others is also tied to the last of
the Confucian values of de: shu,
reciprocity, or â€œself-reï¬‚ectionâ€, which
should govern our actions toward
others. The so-called Golden Rule,
â€œdo as you would be done byâ€,
appears in Confucianism as a
negative: â€œwhat you do not desire
for yourself, do not do to others.â€
The difference is subtle but crucial:
Confucius does not prescribe
what to do, only what not to do,
emphasizing restraint rather than
Confuciusâ€™s devotion to the idea
of establishing a humane society led
him to travel the Chinese empire for
12 years, teaching the virtues of
faithfulness and sincerity.

action. This implies modesty and
humilityâ€”values traditionally held
in high regard in Chinese society,
and which for Confucius express
our true nature. Fostering these
values is a form of loyalty to oneself,
and another kind of sincerity.

Confucianism
Confucius had little success in
persuading contemporary rulers to
adopt his ideas in government, and
turned his attention to teaching.
His disciples, including Meng Zi
(Mencius), continued to anthologize
and expand on his writings, which
survived the repressive Qin
Dynasty, and inspired a revival of
Confucianism in the Han Dynasty
of the early Common Era. From
then on, the impact of Confuciusâ€™s
ideas was profound, inspiring
almost every aspect of Chinese
society, from administration to
politics and philosophy. The major
religions of Daoism and Buddhism
had also been ï¬‚ourishing in
Confuciusâ€™s time, replacing
traditional beliefs, and although
Confucius offered no opinion on

them, remaining silent about the
gods, he nevertheless inï¬‚uenced
aspects of both new faiths.
A Neo-Confucian school
revitalized the movement in the 9th
century, and reached its peak in the
12th century, when its inï¬‚uence
was felt across Southeast Asia into
Korea and Japan. Although Jesuit
missionaries brought back Kong
Fuziâ€™s ideas to Europe (and
Latinized his name to Confucius)
in the 16th century, Confucianism
was alien to European thought
and had limited inï¬‚uence until
translations of his work appeared
in the late 17th century.
Despite the fall of imperial
China in 1911, Confucian ideas
continued to form the basis of
many Chinese moral and social
conventions, even if they were
ofï¬cially frowned upon. In recent
years the Peopleâ€™s Republic of China
has shown a renewed interest in
Confucius, integrating his ideas
with both modern Chinese thought
and Western philosophy, creating
a hybrid philosophy known as
â€œNew Confucianism.â€ â– 

40

EVERYTHING
IS FLUX
HERACLITUS (C.535â€“475 BCE)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Monism
BEFORE
6th century BCE The Milesian
philosophers claim that the
cosmos is made up of a single
speciï¬c substance.
6th century BCE Pythagoras
states that the universe has
an underlying structure that
can be deï¬ned mathematically.
AFTER
Early 5th century BCE
Parmenides uses logical
deduction to prove change
is impossible.
Late 4th century BCE Plato
describes the world as being
in a state of ï¬‚ux, but dismisses
Heraclitus as contradictory.
Early 19th century Georg
Hegel bases his dialectic
system of philosophy on the
integration of opposites.

W

here other early Greek
philosophers seek to
uncover scientiï¬c
explanations for the physical nature
of the cosmos, Heraclitus sees it as
being governed by a divine logos.
Sometimes interpreted to mean
â€œreasonâ€ or â€œargumentâ€, Heraclitus
considers the logos to be a universal,
cosmic law, according to which all
things come into being, and by
which all the material elements of
the universe are held in balance.
It is the balancing of opposites,
such as day and night and hot and
cold, which Heraclitus believes

The road up and
the road down are
one and the same.
Heraclitus

leads to the unity of the universe,
or the idea everything is part of a
single fundamental process or
substanceâ€”the central tenet of
monism. But he also states that
tension is constantly generated
between these pairs of opposites,
and he therefore concludes that
everything must be in a permanent
state of ï¬‚ux, or change. Day, for
instance, changes into night, which
in turn changes back again to day.
Heraclitus offers the example
of a river to illustrate his theory:
â€œYou can never step into the same
river twice.â€ By this, he means that
at the very moment you step into a
river, fresh waters will immediately
replace those into which you initially
placed your foot, and yet the river
itself is always described as one
ï¬xed and unchanging thing.
Heraclitusâ€™s belief that every
object in the universe is in a state
of constant ï¬‚ux runs counter to the
thinking of the philosophers of the
Milesian school, such as Thales
and Anaximenes, who deï¬ne all
things by their quintessentially
unchanging essence. â– 

See also: Thales of Miletus 22â€“23 â–  Anaximenes of Miletus 330 â– 
Pythagoras 26â€“29 â–  Parmenides 41 â–  Plato 50â€“55 â–  Georg Hegel 178â€“85

THE ANCIENT WORLD 41

ALL IS ONE

PARMENIDES (C.515â€“445 BCE)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Monism
BEFORE
6th century BCE Pythagoras
sees mathematical structure,
rather than a substance, as
the foundation of the cosmos.
c.500 BCE Heraclitus says that
everything is in a state of ï¬‚ux.
AFTER
Late 5th century BCE Zeno
of Elea presents his paradoxes
to demonstrate the illusory
nature of our experience.
c.400 BCE Democritus and
Leucippus say the cosmos is
composed of atoms in a void.
Late 4th century BCE Plato
presents his theory of Forms,
claiming that abstract ideas
are the highest form of reality.
1927 Martin Heidegger writes
Being and Time, reviving the
question of the sense of being.

T

he ideas put forward by
Parmenides mark a key
turning point in Greek
philosophy. Inï¬‚uenced by the
logical, scientiï¬c thinking of
Pythagoras, Parmenides employs
deductive reasoning in an attempt
to uncover the true physical nature
of the world. His investigations lead
him to take the opposite view to
that of Heraclitus.
From the premise that something
exists (â€œIt isâ€), Parmenides deduces
that it cannot also not exist (â€œIt is
notâ€), as this would involve a logical
contradiction. It follows therefore
that a state of nothing existing is
impossibleâ€”there can be no void.
Something cannot then come from
nothing, and so must always have
existed in some form. This
permanent form cannot change,
because something that is
permanent cannot change into
something else without it ceasing
to be permanent. Fundamental
change is therefore impossible.
Parmenides concludes from this
pattern of thought that everything
that is real must be eternal and

Understanding the cosmos is one of
the oldest philosophical quests. In the
20th century, evidence from quantum
physics emerged to support ideas that
Parmenides reached by reason alone.

unchanging, and must have an
indivisible unityâ€”â€œall is one.â€
More importantly for subsequent
philosophers, Parmenides shows by
his process of reasoning that our
perception of the world is faulty and
full of contradictions. We seem to
experience change, and yet our
reason tells us that change is
impossible. The only conclusion
we can come to is that we can
never rely on the experience that
is delivered to us by our senses. â– 

See also: Pythagoras 26â€“29 â–  Heraclitus 40 â–  Democritus and Leucippus 45
Zeno of Elea 331 â–  Plato 50â€“55 â–  Martin Heidegger 252â€“255

â– 

42

MAN IS THE
MEASURE OF
ALL THINGS
PROTAGORAS (C.490â€“420 BCE)

IN CONTEXT

It is a spring day
in Athens.

BRANCH
Ethics
APPROACH
Relativism

A visitor from Sweden says
the weather is warm.

BEFORE
Early 5th century BCE
Parmenides argues that we
can rely more on reason than
the evidence of our senses.
AFTER
Early 4th century BCE
Platoâ€™s theory of Forms states
that there are â€œabsolutesâ€ or
ideal forms of everything.
1580 French writer Michel de
Montaigne espouses a form of
relativism to describe human
behavior in his Essays.
1967â€“72 Jacques Derrida uses
his technique of deconstruction
to show that any text contains
irreconcilable contradictions.
2005 Benedict XVI warns
â€œwe are moving towards a
dictatorship of relativismâ€ in
his ï¬rst public address as pope.

A visitor from Egypt
says the weather is cold.

Both people are
speaking the truth.

The truth depends on
perspective and is
therefore relative.

D

uring the 5th century BCE,
Athens evolved into an
important and prosperous
city-state, and under the leadership
of Pericles (445â€“429 BCE) it entered
a â€œGolden Ageâ€ of scholarship and
culture. This attracted people from
all parts of Greece, and for those
who knew and could interpret the
law, there were rich pickings to be
had. The city was run on broadly
democratic principles, with an
established legal system. Anyone

Man is the
measure of
all things.

taken to court was required to
plead his own case; there were no
advocates, but a recognized class
of advisors soon evolved. Among
this group was Protagoras.

Everything is relative
Protagoras lectured in law and
rhetoric to anybody who could
afford him. His teachings were
essentially about practical matters,
arguing to win a civil case rather
than to prove a point, but he could

THE ANCIENT WORLD 43
See also: Parmenides 41

â– 

Socrates 46â€“49

Many things prevent
knowledge, including
the obscurity of
the subject and the
brevity of human life.
Protagoras

see the philosophical implications
of what he taught. For Protagoras,
every argument has two sides,
and both may be equally valid.
He claims that he can â€œmake the
worse case the betterâ€, proving not
the worth of the argument, but the
persuasiveness of its proponent. In
this way, he recognizes that belief
is subjective, and it is the man
holding the view or opinion that is
the measure of its worth. This style
of reasoning, common in law and

Protagoras

â– 

Plato 50â€“55

â– 

Michel de Montaigne 108â€“09

â– 

Jacques Derrida 308â€“13

politics at that time, was new to
philosophy. By placing human
beings at its center, it continued
a tradition of taking religion out
of philosophical argument, and it
also shifted the focus of philosophy
away from an understanding of
the nature of the universe to an
examination of human behavior.
Protagoras is mainly interested in
practical questions. Philosophical
speculations on the substance of
the cosmos or about the existence
of the gods seem pointless to him,
as he considers such things to be
ultimately unknowable.
The main implication of â€œman
is the measure of all thingsâ€ is that
belief is subjective and relative.
This leads Protagoras to reject the
existence of absolute deï¬nitions
of truth, justice, or virtue. What is
true for one person may be false for
another, he claims. This relativism
also applies to moral values, such
as what is right and what is wrong.
To Protagoras, nothing is inherently
good in itself. Something is ethical,
or right, only because a person or
society judges it to be so.

Protagoras was the most inï¬‚uential
of a group of itinerant teachers of
law and rhetoric that became
known as the Sophists (from the
Greek sophia, meaning wisdom).
Socrates and Plato derided the
Sophists as mere rhetoricians,
but with Protagoras there was a
signiï¬cant step in ethics toward
the view that there are no absolutes
and that all judgements, including
moral judgements, are subjective. â– 

Protagoras was born in Abdera,
in northeast Greece, but traveled
widely as an itinerant teacher. At
some stage, he moved to Athens,
where he became advisor to the
ruler of the city-state, Pericles,
who commissioned him to write
the constitution for the colony of
Thurii in 444 BCE. Protagoras was
a proponent of agnosticism, and
legend has it that he was later
tried for impiety, and that his
books were publicly burned.
Only fragments of his writings
survive, although Plato discusses
the views of Protagoras at length
in his dialogues.

Protagoras is believed to have
lived to the age of 70, but his
exact date and place of death
are unknown.

According to Protagoras, any â€œtruthâ€
uncovered by these two philosophers,
depicted on a 5th-century BCE Greek
drinking vessel, will depend on their
use of rhetoric and their debating skill.

Key works
5th century BCE
On the Gods
Truth
On Being
The Art of Controversy
On Mathematics
On the State
On Ambition
On Virtues
On the Original State of Things

44

WHEN ONE THROWS
TO ME A PEACH,
I RETURN TO HIM
AMOZIPLUM
( .470â€“391 )
C

IN CONTEXT
TRADITION
Chinese philosophy
APPROACH
Mohism
BEFORE
6th century BCE Laozi states
that to live according to the
dao means acting intuitively
and in accordance with nature.
Late 6th century BCE
Confuciusâ€™s moral philosophy
stresses the importance of
family ties and traditions.
AFTER
Mid-4th century BCE
The Confucian philosophy
of Mencius stresses manâ€™s
innate goodness.
Mid-4th century BCE Daoist
philosopher Zhuangzi criticizes
Confucianism and Mohism.
3rd century BCE Legalism is
adopted by the Qin dynasty. It
opposes Mohism, advocating
strong laws to keep manâ€™s
essentially evil nature in check.

BCE

orn in 479 BCE, shortly after
the death of Confucius,
Mozi had a traditional
Chinese education based on the
classic texts. Later, however, he
came to dislike the emphasis on
clan relationships that runs through
Confucianism, and this led him
to set up his own school of thought,
advocating universal love or jian ai.
By jian ai, Mozi means that we
should care for all people equally,
regardless of their status or their
relationship to us. He regards this
philosophy, which became known
as Mohism and which â€œnourishes
and sustains all lifeâ€, as being
fundamentally benevolent and in
accordance with the way of heaven.
Mozi believes that there is
always reciprocity in our actions.
By treating others as we would
wish to be treated ourselves, we
will receive similar treatment in
return. This is the meaning behind
â€œwhen one throws to me a peach, I
return to him a plum.â€ When this
principle of caring for everyone
impartially is applied by rulers,
Mozi states that it avoids conï¬‚ict

B

Mao Zedong regarded Mozi as the
true philosopher of the people, because
of his humble origins. Moziâ€™s view that
everyone should be treated equally has
been encouraged in modern China.

and war; when the same principle
is practiced by everyone, it leads to
a more harmonious and therefore
more productive society. This idea
is similar in spirit to that of the
Utilitarianism proposed by Western
philosophers of the 19th century. â– 

See also: Laozi 24â€“25 â–  Siddhartha Gautama 30â€“33 â–  Confucius 34â€“39
Wang Bi 331 â–  Jeremy Bentham 174 â–  Hajime Tanabe 244â€“45

â– 

THE ANCIENT WORLD 45

NOTHING EXISTS
EXCEPT ATOMS
AND
EMPTY
SPACE
DEMOCRITUS ( . 460â€“371 )
C

BCE

AND LEUCIPPUS (EARLY 5TH CENTURY BCE)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Atomism
BEFORE
Early 6th century BCE Thales
says that the cosmos is made
of one fundamental substance.
c.500 BCE Heraclitus declares
that everything is in a state of
constant ï¬‚ux, or change.
AFTER
c.300 BCE The Epicurians
conclude that there is no
afterlife, as the bodyâ€™s atoms
disperse after death.
1805 British chemist John
Dalton proposes that all pure
substances contain atoms of
a single type that combine
to form compounds.
1897 The British physicist
J.J. Thomson discovers that
atoms can be divided into
even smaller particles.

F

rom the 6th century BCE
onward, philosophers began
to consider whether the
universe was made from a single
fundamental substance. During the
5th century BCE, two philosophers
from Abderra in Greece, named
Democritus and Leucippus,
suggested that everything was
made up of tiny, indivisible, and
unchangeable particles, which they
called atoms (atomos is Greek for
uncuttable).

First atomic theory
Democritus and Leucippus also
claim that a void or empty space
separates atoms, allowing them to
move around freely. As the atoms
move, they may collide with each
other to form new arrangements of
atoms, so that objects in the world
will appear to change. The two
thinkers consider that there are
an inï¬nite number of these eternal
atoms, but that the number of
different combinations they can
arrange themselves into is ï¬nite.
This explains the apparent ï¬xed
number of different substances that
See also: Thales of Miletus 22â€“23

â– 

exist. The atoms that make up our
bodies, for example, do not decay
and disappear when we die, but are
dispersed and can be reconstituted.
Known as atomism, the theory
that Democritus and Leucippus
devised offered the ï¬rst complete
mechanistic view of the universe,
without any recourse to the notion
of a god or gods. It also identiï¬ed
fundamental properties of matter
that have proved critical to the
development of the physical
sciences, particularly from the 17th
century onward, right up to the
atomic theories that revolutionized
science in the 20th century.â– 

Man is a microcosm
of the universe.
Democritus

Heraclitus 40

â– 

Epicurus 64â€“65

46

THE LIFE WHICH
IS UNEXAMINED
IS NOT WORTH
LIVING
SOCRATES (469â€“399 BCE)

IN CONTEXT
BRANCH
Epistemology
APPROACH
Dialectical method
BEFORE
c.600â€“450 BCE Pre-Socratic
philosophers in Ionia and Italy
attempt to explain the nature
of the cosmos.
Early 5th century BCE
Parmenides states that we
can only understand the
universe through reasoning.
c.450 BCE Protagoras and the
Sophists apply rhetoric to
philosophical questions.
AFTER
c.399â€“355 BCE Plato portrays
the character of Socrates in
the Apology and numerous
other dialogues.
4th century BCE Aristotle
acknowledges his debt to
Socratesâ€™ method.

S

ocrates is often referred to
as one of the founders of
Western philosophy, and
yet he wrote nothing, established
no school, and held no particular
theories of his own. What he did do,
however, was persistently ask the
questions that interested him, and
in doing so evolved a new way of
thinking, or a new way of examining
what we think. This has been called
the Socratic, or dialectical, method
(â€œdialecticalâ€ because it proceeds
as a dialogue between opposing
views), and it earned him many
enemies in Athens, where he lived.
He was viliï¬ed as a Sophist
(someone who argues for the sake
of deception), and was sentenced to

THE ANCIENT WORLD 47
See also: Thales of Miletus 22â€“23
Parmenides 41 â–  Protagoras 42â€“43

â– 
â– 

Pythagoras 26â€“29 â–  Heraclitus 40
Plato 50â€“55 â–  Aristotle 56â€“63

â– 

The only life worth
living is a good life.

I can only live a good
life if I really know what
â€œgoodâ€ and â€œevilâ€ are.

An unquestioning life
is one of ignorance,
without morality.

â€œGoodâ€ and â€œevilâ€ are not
relative; they are absolutes
that can only be found by
a process of questioning
and reasoning.

In this way, morality
and knowledge are
bound together.

The life which is
unexamined is not
worth living.

death on charges of corrupting the
young with ideas that undermined
tradition. But he also had many
followers, and among them was
Plato, who recorded Socratesâ€™ ideas
in a series of written works, called
dialogues, in which Socrates sets
about examining various ideas. It is
largely thanks to these dialoguesâ€”
which include the Apology, Phaedo,
and the Symposiumâ€”that Socratesâ€™
thought survived at all, and that it
went on to guide the course of
Western philosophy.

The purpose of life
Socrates lived in Athens in the
second half of the 5th century BCE.
As a young man he is believed to

have studied natural philosophy,
looking at the various explanations
of the nature of the universe, but
then became involved in the politics
of the city-state and concerned
with more down-to-earth ethical
issues, such as the nature of justice.
However, he was not interested in
winning arguments, or arguing
for the sake of making moneyâ€”a
charge that was leveled at many of
his contemporaries. Nor was he
seeking answers or explanationsâ€”
he was simply examining the
basis of the concepts we apply to
ourselves (such as â€œgoodâ€, â€œbadâ€,
and â€œjustâ€), for he believed that
understanding what we are is
the ï¬rst task of philosophy. â¯â¯

Socrates
Born in Athens in 469 BCE,
Socrates was the son of a
stonemason and a midwife.
It is likely that he pursued his
fatherâ€™s profession, and had
the opportunity to study
philosophy, before he was
called up for military service.
After distinguishing himself
during the Peloponnesian War,
he returned to Athens, and for
a while involved himself in
politics. However, when his
father died he inherited
enough money to live with
his wife Xanthippe without
having to work.
From then on, Socrates
became a familiar sight around
Athens, involving himself in
philosophical discussions with
fellow citizens and gaining a
following of young students.
He was eventually accused of
corrupting the minds of young
Athenians, and was sentenced
to death. Although he was
offered the choice of exile, he
accepted the guilty verdict
and was given a fatal dose
of hemlock in 399 BCE.
Key works
4thâ€“3rd century BCE
Platoâ€™s record of Socratesâ€™ life
and philosophy in the Apology
and numerous dialogues.

48 SOCRATES

I am a citizen
of the world.
Socrates

Q. So you think
that the gods
know everything?

Socratesâ€™ central concern, then,
was the examination of life, and it
was his ruthless questioning of
peopleâ€™s most cherished beliefs
(largely about themselves) that
earned him his enemiesâ€”but he
remained committed to his task
until the very end. According to the
account of his defence at his trial,
recorded by Plato, Socrates chose
death rather than face a life of
ignorance: â€œThe life which is
unexamined is not worth living.â€
Socratesâ€™ dialectical method
was a simple method of questioning
that brought to light the often false
assumptions on which particular
claims to knowledge are based.

A. Yes, because
they are gods.
Q. Do some gods
disagree with others?

Q. So gods disagree
about what is
true and right?

A. Yes, of course
they do. They are
always ï¬ghting.

A. I suppose
they must do.
Q. So some gods
can be wrong
sometimes?

A. I suppose
that is true.

Therefore the gods
cannot know
everything!

But what exactly is involved in this
examination of life? For Socrates it
was a process of questioning the
meaning of essential concepts that
we use every day but have never
really thought about, thereby
revealing their real meaning and
our own knowledge or ignorance.
Socrates was one of the ï¬rst
philosophers to consider what it
was that constituted a â€œgoodâ€ life;
for him it meant achieving peace of
mind as a result of doing the right
thing, rather than living according to
the moral codes of society. And the
â€œright thingâ€ can only be determined
through rigorous examination.
Socrates rejected the notion
that concepts such as virtue were
relative, insisting instead that they
were absolutes, applicable not just to
citizens of Athens, or Greece, but to
all people in the world. He believed
that virtue (aretÃ© in Greek, which at
the time implied excellence and
fulï¬lment) was â€œthe most valuable
of possessionsâ€, and that no-one
actually desires to do evil. Anyone
performing evil actions would be
acting against their conscience and
would therefore feel uncomfortable;
and as we all strive for peace of
mind it is not something we would
do willingly. Evil, he thought, was
done because of lack of wisdom and
knowledge. From this he concluded
that â€œthere is only one good:
knowledge; and one evil: ignorance.â€
Knowledge is inextricably bound to
moralityâ€”it is the â€œonly one
goodâ€â€”and for this reason we must
continually â€œexamineâ€ our lives.

Care of the soul
For Socrates, knowledge may also
play a part in life after death. In the
Apology, Platoâ€™s Socrates prefaces
his famous quote about the
unexamined life by saying: â€œI tell
you that to let no day pass without
discussing goodness and all the

THE ANCIENT WORLD 49
other subjects about which you
hear me talking, and that examining
both myself and others is really the
very best thing a man can do.â€
This gaining of knowledge, rather
than wealth or high status, is the
ultimate goal of life. It is not a matter
of entertainment or curiosityâ€”it is
the reason why we exist. Moreover,
all knowledge is ultimately selfknowledge, for it creates the person
you are within this world, and
fosters the care of the immortal soul.
In Phaedo, Socrates says that an
unexamined life leads the soul to
be â€œconfused and dizzy, as if it
were drunkâ€, while the wise soul
achieves stability, its straying
ï¬nally brought to an end.

Dialectical method
Socrates quickly became a wellknown ï¬gure in Athens, with a
reputation for an enquiring mind.
A friend of his, so the story goes,
asked the priestess of Apollo at
Delphi who the wisest man in the
world was: the oracular reply was
that there was no-one wiser than
Socrates. When Socrates heard
about this, he was astounded, and
went to the most knowledgeable
people he could ï¬nd to try to
disprove it. What he discovered
was that these people only thought
they knew a great deal; under
examination, their knowledge was
proved to be either limited or false.
What was more important,
however, was the method he used
to question their knowledge. He
took the standpoint of someone who
knew nothing, and merely asked
questions, exposing contradictions
in arguments and gaps in knowledge
Socrates was put to death in 399 BCE,
ultimately for questioning the basis of
Athenian morality. Here he accepts the
bowl of hemlock that will kill him, and
gestures deï¬antly at the heavens.

to gradually elicit insights. He
likened the process to his motherâ€™s
profession of midwife, assisting
in the birth of ideas.
Through these discussions,
Socrates came to realize that the
Delphic oracle had been right â€“
he was the wisest man in Athens,
not because of his knowledge but
because he professed to know
nothing. He also saw that the
inscription on the entrance to the
temple at Delphi, gnothi seauton
(â€œknow thyselfâ€), was just as
signiï¬cant. To gain knowledge
of the world and oneself it was
necessary to realize the limits of
oneâ€™s own ignorance and to remove
all preconceptions. Only then could
one hope to determine the truth.
Socrates set about engaging the
people of Athens in discussion on
topics such as the nature of love,
justice, and loyalty. His mission,
misunderstood at the time as a
dangerous form of Sophistryâ€”or
cleverness for the sake of itâ€”was
not to instruct the people, nor even
simply to learn what they knew, but
to explore the ideas that they had.
It was the conversation itself, with
Socrates guiding it, that provided
him with insights. Through a series
of questions, he revealed the ideas
and assumptions his opponent held,

I know nothing except
the fact of my ignorance.
Socrates

then exposed the contradictions
within them and brought them to
agree to a new set of conclusions.
This method of examining an
argument by rational discussion
from a position of ignorance marked
a complete change in philosophical
thinking. It was the ï¬rst known
use of inductive argument, in
which a set of premises based
on experience is ï¬rst established
to be true, and then shown to lead
to a universal truth in conclusion.
This powerful form of argument
was developed by Aristotle, and
later by Francis Bacon, who used
it as the starting point of his
scientiï¬c method. It became,
therefore, the foundation not
only of Western philosophy, but
of all the empirical sciences. â– 

EARTHLY
KNOWLEDGE IS BUT
SHADOW
PLATO (C.427â€“347 BCE)

52 PLATO
IN CONTEXT
BRANCH
Epistemology

world
of Ideas, which contains
the Ideal Forms of everything.

APPROACH
Rationalism
BEFORE
6th century BCE The Milesian
philosophers propose theories
to explain the nature and
substance of the cosmos.

We are born
with the concepts of
these Ideal Forms
in our minds.

The illusory world in which
we liveâ€”the world of the
sensesâ€”contains imperfect
copies of the Ideal Forms.

c.500 BCE Heraclitus argues
that everything is constantly
in a state of ï¬‚ux or change.
c.450 BCE Protagoras says
that truth is relative.
AFTER
c.335 BCE Aristotle teaches
that we can ï¬nd truth by
observing the world around us.
c.250 CE Plotinus founds
the Neo-Platonist school, a
religious take on Platoâ€™s ideas.
386 St. Augustine of Hippo
integrates Platoâ€™s theories into
Christian doctrine.

I

n 399 BCE, Platoâ€™s mentor
Socrates was condemned to
death. Socrates had left no
writings, and Plato took it upon
himself to preserve what he had
learnt from his master for
posterityâ€”ï¬rst in the Apology, his
retelling of Socratesâ€™ defense at his
trial, and later by using Socrates as
a character in a series of dialogues.
In these dialogues, it is sometimes
difï¬cult to untangle which are
Socratesâ€™ thoughts and which are
the original thoughts of Plato, but a
picture emerges of Plato using the
methods of his master to explore
and explain his own ideas.

We recognize things in the world,
such as dogs, because we recognize
they are imperfect copies of the
concepts in our minds.

Everything in this world is
a â€œshadowâ€ of its Ideal Form
in the world of Ideas.

Initially Platoâ€™s concerns were very
much those of his mentor: to search
for deï¬nitions of abstract moral
values such as â€œjusticeâ€ and
â€œvirtueâ€, and to refute Protagorasâ€™s
notion that right and wrong are
relative terms. In the Republic,
Plato set out his vision of the ideal
city-state and explored aspects of
virtue. But in the process, he also
tackled subjects outside moral
philosophy. Like earlier Greek
thinkers, he questioned the nature
and substance of the cosmos, and
explored how the immutable and
eternal could exist in a seemingly
changing world. However, unlike

his predecessors, Plato concluded
that the â€œunchangingâ€ in nature is
the same as the â€œunchangingâ€ in
morals and society.

Seeking the Ideal
In the Republic, Plato describes
Socrates posing questions about
the virtues, or moral concepts, in
order to establish clear and precise
deï¬nitions of them. Socrates had
famously said that â€œvirtue is
knowledgeâ€, and that to act justly,
for example, you must ï¬rst ask what
justice is. Plato decides that before
referring to any moral concept in
our thinking or reasoning, we must

THE ANCIENT WORLD 53
See also: Thales of Miletus 22â€“23
St. Augustine of Hippo 72â€“73

â– 

Heraclitus 40

ï¬rst explore both what we mean by
that concept and what makes it
precisely the kind of thing that it is.
He raises the question of how we
would recognize the correct, or
perfect, form of anythingâ€”a form
that is true for all societies and for
all time. By doing so, Plato is
implying that he thinks some kind
of ideal form of things in the world
we inhabitâ€”whether those things
are moral concepts or physical
objectsâ€”must actually exist, of
which we are in some way aware.
Plato talks about objects in the
world around us, such as beds.
When we see a bed, he states, we
know that it is a bed and we can
recognize all beds, even though
they may differ in numerous ways.
Dogs in their many species are
even more varied, yet all dogs share
the characteristic of â€œdogginessâ€,
which is something we can
recognize, and that allows us to
say we know what a dog is. Plato
argues that it is not just that a
shared â€œdogginessâ€ or â€œbednessâ€
exists, but that we all have in our
minds an idea of an ideal bed or
dog, which we use to recognize any
particular instance.
Taking a mathematical example
to further his argument, Plato shows
that true knowledge is reached by
reasoning, rather than through our
senses. He states that we can work
out in logical steps that the square
of the hypotenuse of a right-angled
triangle is equal to the sum of the
squares of the other two sides, or
that the sum of the three interior
The Allegory of the Cave, in which
knowledge of the world is limited to
mere shadows of reality and truth, is
used by Plato to explain his idea of
a world of perfect Forms, or Ideas.

â– 

Protagoras 42â€“43

â– 

Socrates 46â€“49

angles of any triangle is always
180 degrees. We know the truth of
these statements, even though the
perfect triangle does not exist
anywhere in the natural world. Yet
we are able to perceive the perfect
triangleâ€”or the perfect straight
line or circleâ€”in our minds, using
our reason. Plato, therefore, asks
whether such perfect forms can
exist anywhere.

â– 

Aristotle 56â€“63

â– 

Plotinus 331

â– 

If particulars are to
have meaning, there
must be universals.
Plato

World of Ideas
Reasoning brings Plato to only one
conclusionâ€”that there must be a
world of Ideas, or Forms, which is
totally separate from the material
world. It is there that the Idea of the
perfect â€œtriangleâ€, along with the
Idea of the perfect â€œbedâ€ and â€œdogâ€
exists. He concludes that human
senses cannot perceive this place
directlyâ€”it is only perceptible to us
through reason. Plato even goes on
to state that this realm of Ideas is
â€œrealityâ€, and that the world around
us is merely modelled upon it.
To illustrate his theory, Plato
presents what has become known
as the â€œAllegory of the Cave.â€ He

asks us to imagine a cave in which
people have been imprisoned since
birth, tied up facing the back wall
in the darkness. They can only face
straight ahead. Behind the prisoners
is a bright ï¬re, which casts shadows
onto the wall they are facing. There
is also a rampart between the ï¬re
and the prisoners along which
people walk and hold up various
objects from time to time, so that
the shadows of these objects are
cast on the wall. These shadows
are all the prisoners know of the â¯â¯

54 PLATO
According to Platoâ€™s theory of Forms, every
horse that we encounter in the world around us is
a lesser version of an â€œidealâ€, or perfect, horse that
exists in a world of Forms or Ideasâ€”a realm that
humans can only access through
their ability to reason.

The world
d of ideas

other of what Plato considers to be
reality, also solves the problem of
ï¬nding constants in an apparently
changing world. The material world
may be subject to change, but
Platoâ€™s world of Ideas is eternal and
immutable. Plato applies his theory
not just to concrete things, such as
beds and dogs, but also to abstract
concepts. In Platoâ€™s world of Ideas,
there is an Idea of justice, which is
true justice, and all the instances of
justice in the material world around
us are models, or lesser variants, of
it. The same is true of the concept
of goodness, which Plato considers
to be the ultimate Ideaâ€”and the
goal of all philosophical enquiry.

Innate knowledge

The world of the senses
world; they have no concept of the
actual objects themselves. If one
of the prisoners manages to untie
himself and turn around, he will
see the objects themselves. But
after a lifetime of entrapment, he
is likely to be confused, as well as
dazzled by the ï¬re, and will most
likely turn back toward the wall
and the only reality he knows.
Plato believes that everything
that our senses perceive in the
material world is like the images
on the cave wall, merely shadows
of reality. This belief is the basis
of his theory of Forms, which is that
for every earthly thing that we have

the power to perceive with our
senses, there is a corresponding
â€œFormâ€ (or â€œIdeaâ€)â€”an eternal and
perfect reality of that thingâ€”in the
world of Ideas. Because what we
perceive via our senses is based
on an experience of imperfect or
incomplete â€œshadowsâ€ of reality,
we can have no real knowledge of
those things. At best, we may have
opinions, but genuine knowledge
can only come from study of the
Ideas, and that can only ever be
achieved through reason, rather
than through our deceptive senses.
This separation of two distinct
worlds, one of appearance, the

The problem remains of how we
can come to know these Ideas, so
that we have the ability to recognize
the imperfect instances of them in
the world we inhabit. Plato argues
that our conception of Ideal Forms
must be innate, even if we are not
aware of this. He believes that
human beings are divided into two
parts: the body and the soul. Our
bodies possess the senses, through
which we are able to perceive the
material world, while the soul
possesses the reason with which
we can perceive the realm of Ideas.
Plato concludes that our soul, which
is immortal and eternal, must have

The soul of
man is immortal
and imperishable.
Plato

THE ANCIENT WORLD 55

Marcus Aurelius, Roman Emperor
from 161 to 180 CE, was not just a
powerful ruler, he was a noted scholar
and thinkerâ€”a realization of Platoâ€™s idea
that philosophers should lead society.

inhabited the world of Ideas before
our birth, and still yearns to return
to that realm after our death. So
when we see variations of the Ideas
in the world with our senses, we
recognize them as a sort of
recollection. Recalling the innate

Plato

memories of these Ideas requires
reasonâ€”an attribute of the soul.
For Plato, the philosopherâ€™s job
is to use reason to discover the
Ideal Forms or Ideas. In the
Republic, he also argues that it is
philosophers, or rather those who
are true to the philosopherâ€™s calling,
who should be the ruling class.
This is because only the true
philosopher can understand the
exact nature of the world and the
truth of moral values. However, just
like a prisoner in the â€œAllegory of
the Caveâ€ who sees the real objects
rather than their shadows, many
will just turn back to the only world
they feel comfortable with. Plato
often found it difï¬cult to convince
his fellow philosophers of the true
nature of their calling.

Unsurpassed legacy
Plato himself was the embodiment
of his ideal, or true, philosopher. He
argued on questions of ethics that
had been raised previously by the
followers of Protagoras and Socrates,
but in the process, he explored for
the ï¬rst time the path to knowledge
itself. He was a profound inï¬‚uence
on his pupil Aristotleâ€”even if they
fundamentally disagreed about the
Despite the large proportion of
writings attributed to Plato that
have survived, little is known
about his life. He was born into a
noble family in Athens in around
427 BCE and named Aristocles, but
acquired the nickname â€œPlatoâ€
(meaning â€œbroadâ€). Although
probably destined for a life in
politics, he became a pupil of
Socrates. When Socrates was
condemned to death, Plato is said
to have become disillusioned with
Athens, and left the city. He
travelled widely, spending some
time in southern Italy and Sicily,
before returning to Athens around

What we call learning
is only a process
of recollection.
Plato

theory of Forms. Platoâ€™s ideas later
found their way into the philosophy
of medieval Islamic and Christian
thinkers, including St. Augustine of
Hippo, who combined Platoâ€™s ideas
with those of the Church.
By proposing that the use of
reason, rather than observation, is
the only way to acquire knowledge,
Plato also laid the foundations of
17th-century rationalism. Platoâ€™s
inï¬‚uence can still be felt todayâ€”
the broad range of subjects he
wrote about led the 20th-century
British logician Alfred North
Whitehead to say that subsequent
Western philosophy â€œconsists of a
set of footnotes to Plato.â€ â– 
385 BCE. Here he founded a
school known as the Academy
(from which the word â€œacademicâ€
comes), remaining its head until
his death in 347 BCE.
Key works
c.399â€“387 BCE Apology, Crito,
Giorgias, Hippias Major, Meno,
Protagoras (early dialogues)
c.380â€“360 BCE Phaedo, Phaedrus,
Republic, Symposium (middle
dialogues)
c.360â€“355 BCE Parmenides,
Sophist, Theaetetus (late
dialogues)

TRUTH

RESIDES IN THE WORLD

AROUND US

ARISTOTLE (384â€“322 BCE)

58 ARISTOTLE

A

IN CONTEXT
BRANCH
Epistemology
APPROACH
Empiricism
BEFORE
399 BCE Socrates argues that
virtue is wisdom.
c.380 BCE Plato presents his
theory of Forms in his Socratic
dialogue, The Republic.
AFTER
9th century CE Aristotleâ€™s
writings are translated
into Arabic.
13th century Translations
of Aristotleâ€™s works appear
in Latin.
1690 John Locke establishes
a school of British empiricism.
1735 Zoologist Carl Linnaeus
lays the foundations of modern
taxonomy in Systema Naturae,
based on Aristotleâ€™s system
of biological classiï¬cation.

different
instances of â€œdogâ€ in
the world around us.

ristotle was 17 years old
when he arrived in Athens
to study at the Academy
under the great philosopher Plato.
Plato himself was 60 at the time,
and had already devised his theory
of Forms. According to this theory,
all earthly phenomena, such as
justice and the color green, are
shadows of ideal counterparts, called
Forms, which give their earthly
models their particular identities.
Aristotle was a studious type,
and no doubt learnt a great deal from
his master, but he was also of a very
different temperament. Where Plato
was brilliant and intuitive, Aristotle
was scholarly and methodical.
Nevertheless, there was an obvious
mutual respect, and Aristotle stayed
at the Academy, both as a student
and a teacher, until Plato died 20
years later. Surprisingly, he was not
chosen as Platoâ€™s successor, and so
he left Athens and took what would
prove to be a fruitful trip to Ionia.

Platoâ€™s theory questioned
The break from teaching gave
Aristotle the opportunity to indulge
his passion for studying wildlife,
which intensiï¬ed his feeling that
Platoâ€™s theory of Forms was wrong.

It is tempting to imagine that
Aristotleâ€™s arguments had already
had some inï¬‚uence on Plato, who
in his later dialogues admitted
some ï¬‚aws in his earlier theories,
but it is impossible to know for
certain. We do know, though, that
Plato was aware of the Third Man
argument, which Aristotle used to
refute his theory of Forms. This
argument runs as follows: if there
exists in a realm of Forms a perfect
Form of Man on which earthly men
are modelled, this Form, to have
any conceivable content, would
have to be based on a Form of the
Form of Manâ€”and this too would
have to be based on a higher Form
on which the Forms of the Forms
are based, and so on ad inï¬nitum.
Aristotleâ€™s later argument
against the theory of Forms was
more straightforward, and more
directly related to his studies of the
natural world. He realized that it
was simply unnecessary to assume
that there is a hypothetical realm
of Forms, when the reality of things
can already be seen here on Earth,
inherent in everyday things.
Perhaps because his father
had been a physician, Aristotleâ€™s
scientiï¬c interests lay in what we

Using our senses and our
reason, we understand what
makes a dog a dog.

We recognize the
common characteristics
of dogs in the world.

We ï¬nd the truth
from evidence
gained in the world
around us.

THE ANCIENT WORLD 59
See also: Socrates 46â€“49 â–  Plato 50â€“55 â–  Avicenna 76â€“79 â–  Averroes 82â€“83 â–  RenÃ© Descartes 116â€“123 â– 
John Locke 130â€“33 â–  Gottfried Leibniz 134â€“37 â–  George Berkeley 138â€“41 â–  David Hume 148â€“53 â–  Immanuel Kant 164â€“71

Plato and Aristotle differed in their
opinion of the nature of universal
qualities. For Plato, they reside in the
higher realm of the Forms, but for
Aristotle they reside here on Earth.

of the world what the shared
characteristics are that make
things what they areâ€”and
the only way of experiencing
the world is through our senses.

The essential form of things

now call the biological sciences,
whereas Platoâ€™s background had
been ï¬rmly based in mathematics.
This difference in background
helps to explain the difference in
approach between the two men.
Mathematics, especially geometry,
deals with abstract concepts that
are far removed from the everyday
world, whereas biology is very much
about the world around us, and is
based almost solely on observation.
Plato sought conï¬rmation of a realm
of Forms from notions such as the
perfect circle (which cannot exist
in nature), but Aristotle found that
certain constants can be discovered
by examining the natural world.

Trusting the senses
What Aristotle proposed turned
Platoâ€™s theory on its head. Far from
mistrusting our senses, Aristotle
relied on them for the evidence

to back up his theories. What he
learnt from studying the natural
world was that by observing the
characteristics of every example
of a particular plant or animal that
he came across, he could build up
a complete picture of what it was
that distinguished it from other
plants or animals, and deduce what
makes it what it is. His own studies
conï¬rmed what he already
believedâ€”that we are not born
with an innate ability to recognize
Forms, as Plato maintained.
Each time a child comes across
a dog, for example, it notes what it
is about that animal that it has in
common with other dogs, so that
it can eventually recognize the
things that make something a
dog. The child now has an idea
of â€œdogginessâ€, or the â€œformâ€, as
Aristotle puts it, of a dog. In this
way, we learn from our experience

Like Plato, then, Aristotle is
concerned with ï¬nding some kind of
immutable and eternal bedrock in a
world characterized by change, but
he concludes that there is no need
to look for this anchor in a world of
Forms that are only perceptible to
the soul. The evidence is here in the
world around us, perceptible through
the senses. Aristotle believes that
things in the material world are not
imperfect copies of some ideal
Form of themselves, but that the
essential form of a thing is actually
inherent in each instance of that
thing. For example, â€œdogginessâ€
is not just a shared characteristic
of dogsâ€”it is something that is
inherent in each and every dog. â¯â¯

Everything that
depends on the action
of nature is by nature
as good as it can be.
Aristotle

60 ARISTOTLE

All men by nature
desire to know.
Aristotle

Aristotle classiï¬ed many of the
different strands of knowledge and
learning that we have today, such
as physics, logic, metaphysics, poetics,
ethics, politics, and biology.

By studying particular things,
therefore, we can gain insight into
their universal, immutable nature.
What is true of examples in the
natural world, Aristotle reasons,
is also true of concepts relating
to human beings. Notions such
as â€œvirtueâ€, â€œjusticeâ€, â€œbeautyâ€, and
â€œgoodâ€ can be examined in exactly
the same way. As he sees it, when
we are born our minds are like
â€œunscribed tabletsâ€, and any ideas
that we gain can only be received
through our senses. At birth, we
have no innate ideas, so we can
have no idea of right or wrong. As
we encounter instances of justice
throughout our lives, however, we
learn to recognize the qualities that
these instances have in common,
and slowly build and reï¬ne our
understanding of what justice is.
In other words, the only way we
can come to know the eternal,
immutable idea of justice, is by
observing how it is manifested
in the world around us.
Aristotle departs from Plato,
then, not by denying that universal
qualities exist, but by questioning
both their nature and the means

by which we come to know them
(the latter being the fundamental
quesion of â€œepistemologyâ€, or the
theory of knowledge). And it was
this difference of opinion on how
we arrive at universal truths that
later divided philosophers into two
separate camps: the rationalists
(including RenÃ© Descartes,
Immanuel Kant, and Gottfried
Leibniz), who believe in a priori,
or innate, knowledge; and the
empiricists (including John Locke,
George Berkeley, and David Hume),
who claim that all knowledge
comes from experience.

Biological classiï¬cation
The manner in which Plato and
Aristotle arrive at their theories tells
us much about their temperaments.
Platoâ€™s theory of Forms is grand and
otherworldly, which is reï¬‚ected in
the way he argues his case, using
highly imaginative ï¬ctionalized
dialogues between Socrates and
his contemporaries. By contrast,
Aristotleâ€™s theory is much more
down to earth, and is presented in
more prosaic, academic language.
Indeed, so convinced was Aristotle

that the truth of the world is to be
found here on Earth, and not in
some higher dimension, that he set
about collecting specimens of ï¬‚ora
and fauna, and classiï¬ed them
according to their characteristics.
For this biological classiï¬cation,
Aristotle devised a hierarchical
systemâ€”the ï¬rst of its kind, and so
beautifully constructed that it forms
the basis of the taxonomy still in
use today. First, he divides the
natural world into living and
nonliving things, then he turns his
attention to classifying the living
world. His next division is between
plants and animals, which involves
the same kind of thinking that
underpins his theory of universal
qualities: we may be able to
distinguish between a plant and
an animal almost without thinking,
but how do we know how to make
that distinction? The answer, for
Aristotle, is in the shared features
of either category. All plants share
the form â€œplantâ€, and all animals
share the form â€œanimal.â€ And once
we understand the nature of those
forms, we can then recognize them
in each and every instance.
This fact becomes more apparent
the more Aristotle subdivides the
natural world. In order to classify a
specimen as a ï¬sh, for example, we
have to recognize what it is that
makes a ï¬sh a ï¬shâ€”which, again,
can be known through experience
and requires no innate knowledge
at all. As Aristotle builds up a
complete classiï¬cation of all living
things, from the simplest organisms
to human beings, this fact is
conï¬rmed again and again.

Teleological explanation
Another fact that became obvious
to Aristotle as he classiï¬ed the
natural world is that the â€œformâ€
of a creature is not just a matter
of its physical characteristics, such

THE ANCIENT WORLD 61
as its skin, fur, feather, or scales,
but also a matter of what it does,
and how it behavesâ€”which, for
Aristotle, has ethical implications.
To understand the link with
ethics, we need ï¬rst to appreciate
that for Aristotle everything in
the world is fully explained by four
causes that fully account for a
thingâ€™s existence. These four causes
are: the material cause, or what a
thing is made of; the formal cause,
or the arrangement or shape of a

thing; the efï¬cient cause, or how
a thing is brought into being; and
the ï¬nal cause, or the function or
purpose of a thing. And it is this
last type of cause, the â€œï¬nal causeâ€,
that relates to ethicsâ€”a subject
which, for Aristotle, is not separate
from science, but rather a logical
extension of biology.
An example that Aristotle gives
is that of an eye: the ï¬nal cause
of an eyeâ€”its functionâ€”is to see.
This function is the purpose, or

Aristotleâ€™s classiï¬cation of living things is
the ï¬rst detailed examination of the natural world.
It proceeds from general observations about the
characteristics shared by all animals, and then
subdivides into ever more precise categories.

telos, of the eyeâ€”telos is a Greek
word that gives us â€œteleologyâ€, or
the study of purpose in nature. A
teleological explanation of a thing
is therefore an account of a thingâ€™s
purpose, and to know the purpose
of a thing is also to know what a
â€œgoodâ€ or a â€œbadâ€ version of a thing
isâ€”a good eye for example, is one
that sees well.
In the case of humans, a â€œgoodâ€
life is therefore one in which we
fulï¬ll our purpose, or use all the
characteristics that make us
human to the full. A person can be
considered â€œgoodâ€ if he uses the
characteristics he was born with,
and can only be happy by using all
his capabilities in the pursuit of
virtueâ€”the highest form of which,
for Aristotle, is wisdom. Which
brings us full circle back to the
question of how we can recognize
the thing that we call virtueâ€”and
for Aristotle, again, the answer is
by observation. We understand the
nature of the â€œgood lifeâ€ by seeing
it in the people around us.

The syllogism
In the process of classiï¬cation,
Aristotle formulates a systematic
form of logic which he applies
to each specimen to determine â¯â¯

Does it ï¬‚y?
Yes

Does it have feathers?

Yes

No

No

Does it have scales?

Yes

No

Linnaeus and Cuvier
have been my two gods,
though in very different
ways, but they were mere
schoolboys to old Aristotle.
Charles Darwin

62 ARISTOTLE
â€œSocrates is mortalâ€ is the undeniable conclusion
to the most famous syllogism in history. Aristotleâ€™s
syllogismâ€”a simple deduction from two premises
to a conclusionâ€”was the ï¬rst formal system of logic.

All men are mortal.

Socrates is a man.

Therefore Socrates
is mortal.

whether it belongs to a certain
category. For example, one of the
characteristics common to all
reptiles is that they are cold-blooded;
so, if this particular specimen is
warm-blooded, then it cannot be a
reptile. Likewise, a characteristic
common to all mammals is that
they suckle their young; so, if this
specimen is a mammal, it will suckle
its young. Aristotle sees a pattern
in this way of thinkingâ€”that of
three propositions consisting of
two premises and a conclusion, for
example in the form: if As are Xs,
and B is an A, then B is an X.
The â€œsyllogismâ€, as this form of
reasoning is known, is the ï¬rst
formal system of logic ever devised,
and it remained the basic model for
logic up until the 19th century.
But the syllogism was more than
simply a by-product of Aristotleâ€™s
systematic classiï¬cation of the
natural world. By using analytical
reasoning in the form of logic,
Aristotle realized that the power
of reason was something that did
not rely on the senses, and that
it must therefore be an innate
characteristicâ€”part of what it is
to be human. Although we have no
innate ideas, we do possess this

innate faculty, which is necessary
for us to learn from experience.
And as he applied this fact to his
hierarchical system, he saw that
the innate power of reason is what
distinguishes us from all other
living creatures, and placed us at
the top of the hierarchy.

Decline of Classical Greece
The sheer scope of Aristotleâ€™s ideas,
and the revolutionary way in which
he overturns Platoâ€™s theory of Forms,
should have ensured that his
philosophy had a far greater impact
than it did during his lifetime. That
is not to say that his work was
without faultâ€”his geography and
astronomy were ï¬‚awed; his ethics
supported the use of slaves and
considered women to be inferior
human beings; and his logic was
incomplete by modern standards.
However, what he got right
amounted to a revolution both
in philosophy and in science.
But Aristotle lived at the end of
an era. Alexander the Great, whom
he taught, died shortly before him,
and so began the Hellenistic period
of Greek history which saw a decline
in Athensâ€™ inï¬‚uence. The Roman
Empire was becoming the dominant

Every action must be
due to one or other of
seven causes: chance,
nature, compulsion,
habit, reasoning,
anger, or appetite.
Aristotle

power in the Mediterranean, and the
philosophy it adopted from Greece
was that of the Stoics. The rival
schools of Plato and Aristotleâ€”
Platoâ€™s Academy and the Lyceum
Aristotle founded in Athensâ€”
continued to operate, but they
had lost their former eminence.
As a result of this neglect, many
of Aristotleâ€™s writings were lost. It
is believed that he wrote several
hundred treatises and dialogues
explaining his theories, but all that
remain are fragments of his work,
mainly in the form of lectures and
teacherâ€™s notes. Luckily for posterity,
these were preserved by his
followers, and there is enough
contained in them to give a picture
of the full range of his work.

Aristotleâ€™s legacy
With the emergence of Islam in the
7th century CE, Aristotleâ€™s works
were translated into Arabic and
spread throughout the Islamic world,
becoming essential reading for
Middle Eastern scholars such as
Avicenna and Averroes. In Western
Europe, however, Boethiusâ€™s Latin
translation of Aristotleâ€™s treatise on
logic (made in the 6th century CE)
remained the only work of Aristotleâ€™s

THE ANCIENT WORLD 63

The inï¬‚uence of Aristotle on the
history of thought can be seen in
the Great Chain of Being, a medieval
Christian depiction of life as a hierarchy
in which with God presides over all.

available until the 9th century CE,
when all of Aristotleâ€™s works began
to be translated from Arabic into
Latin. It was also at this time that
his ideas were collected into the
the books we know todayâ€”such as
Physics, The Nicomachean Ethics,

Aristotle

and the Organon. In the 13th
century, Thomas Aquinas braved
a ban on Aristotleâ€™s work and
integrated it into Christian
philosophy, in the same way that
St. Augustine had adopted Plato,
and Plato and Aristotle came to
lock horns again.
Aristotleâ€™s notes on logic (laid
out in the Organon) remained the
standard text on logic until the
emergence of mathematical logic
in the 19th century. Likewise,
his classiï¬cation of living things
dominated Western thinking
throughout the Middle Ages,
becoming the Christian scala
naturae (the â€œladder of natureâ€),
or the Great Chain of Being. This
depicted the whole of creation
dominated by man, who stood
second only to God. And during the
Renaissance, Aristotleâ€™s empirical
method of enquiry held sway.
In the 17th century, the debate
between empiricists and rationalists
reached its zenith after RenÃ©
Descartes published his Discourse
on the Method. Descartes, and
Leibniz and Kant after him, chose
the rationalist route; in response,
Locke, Berkeley, and Hume lined
up as the empiricist opposition.
Born in Stagira, Chalcidice, in
the northeast region of modern
Greece, Aristotle was the son of
a physician to the royal family
of Macedon, and was educated as
a member of the aristocracy. He
was sent to Platoâ€™s Academy in
Athens at the age of 17, and spent
almost 20 years there both as a
student and a teacher. When
Plato died, Aristotle left Athens
for Ionia, and spent several years
studying the wildlife of the area.
He was then appointed tutor at
the Macedonian court, where he
taught the young Alexander the
Great and continued his studies.

There is nothing in
the mind except was
ï¬rst in the senses.
John Locke

Again, the differences between the
philosophers were as much about
temperament as they were about
substanceâ€”the Continental versus
the English, the poetic versus the
academic, the Platonic versus the
Aristotelian. Although the debate
died down in the 19th century,
there has been a revival of interest
in Aristotle in recent times, and
a reappraisal of his signiï¬cance.
His ethics in particular have
been of great appeal to modern
philosophers, who have seen in
his functional deï¬nition of â€œgoodâ€
a key to understanding the way
we use ethical language. â– 
In 335 BCE he returned to Athens,
encouraged by Alexander, and
set up the Lyceum, a school to
rival Platoâ€™s. It was here that
he did most of his writing, and
formalized his ideas. After
Alexander died in 323 BCE,
anti-Macedonian feeling ï¬‚ared
up in Athens, and Aristotle
ï¬‚ed to Chalcis, on the island
of Euboea, where he died
the following year.
Key works
Organon, Physics (as compiled in
book form in the 9th century).

64

DEATH IS
NOTHING TO US
EPICURUS (341â€“270 BCE)

IN CONTEXT
BRANCH
Ethics
APPROACH
Epicureanism
BEFORE
Late 5th century BCE
Socrates states that seeking
knowledge and truth is the
key to a worthwhile life.
c.400 BCE Democritus and
Leucippus conclude that
the cosmos consists solely of
atoms, moving in empty space.

E

picurus grew up in a time
when the philosophy of
ancient Greece had already
reached a pinnacle in the ideas of
Plato and Aristotle. The main focus
of philosophical thinking was
shifting from metaphysics toward
ethicsâ€”and also from political to
personal ethics. Epicurus, however,
found the seeds of a new school of
thought in the quests of earlier
philosophers, such as Socratesâ€™
examination of the truth of basic
human concepts and values.

AFTER
c.50 BCE Roman philosopher
Lucretius writes De rerum
natura, a poem exploring
Epicurusâ€™s ideas.

Fear of death

1789 Jeremy Bentham
advocates the utilitarian idea
of â€œthe greatest happiness for
the greatest number.â€
1861 John Stuart Mill argues
that intellectual and spiritual
pleasures have more value
than physical pleasures.

Central to the philosophy that
Epicurus developed is the view
that peace of mind, or tranquillity,
is the goal of life. He argues that
pleasure and pain are the roots of
good and evil, and qualities such
as virtue and justice derive from
these roots, as â€œit is impossible to
live a pleasant life without living
wisely, honorably, and justly, and
it is impossible to live wisely,
honorably, and justly without living
pleasantly.â€ Epicurianism is often
mistakenly interpreted as simply
being about the pursuit of sensual
pleasures. For Epicurus, the
greatest pleasure is only attainable
through knowledge and friendship,
and a temperate life, with freedom
from fear and pain.

Terrifying images of the merciless
god of death Thanatos were used to
depict the pain and torment ancient
Greeks might incur for their sins, both
when they died and in the afterlife.

One of the obstacles to enjoying the
peace of a tranquil mind, Epicurus
reasons, is the fear of death, and
this fear is increased by the
religious belief that if you incur
the wrath of the gods, you will be
severely punished in the afterlife.
But rather than countering this fear
by proposing an alternative state
of immortality, Epicurus tries to
explain the nature of death itself.
He starts by proposing that when

THE ANCIENT WORLD 65
See: Democritus and Leucippus 45 â–  Socrates 46â€“49 â–  Plato 50â€“55
Aristotle 56â€“63 â–  Jeremy Bentham 174 â–  John Stuart Mill 190â€“93

The goal of life
is happiness.

Death is the end
of sensation,
so cannot be
physically
painful.

â– 

Death is the end
of consciousness,
so cannot be
emotionally
painful.

Epicurus
Our unhappiness
is caused by fear,
and our main
fear is of death.

Death is
nothing
to fear.

If we can
overcome fear
of death, we
can be happy.

we die, we are unaware of our
death, since our consciousness
(our soul) ceases to exist at the
point of death. To explain this,
Epicurus takes the view that the
entire universe consists of either
atoms or empty space, as argued
by the atomist philosophers
Democritus and Leucippus.
Epicurus then reasons that the soul
could not be empty space, because
it operates dynamically with the
body, so it must be made up of
atoms. He describes these atoms
of the soul as being distributed
around the body, but as being so
fragile that they dissolve when
we die, and so we are no longer
capable of sensing anything. If you

are unable to feel anything, mentally
or physically, when you die, it is
foolish to let the fear of death cause
you pain while you are still alive.
Epicurus attracted a small but
devoted following in his lifetime,
but he was perceived as being
dismissive of religion, which made
him unpopular. His thinking was
largely ignored by mainstream
philosophy for centuries, but it
resurfaced in the 18th century, in
the ideas of Jeremy Bentham and
John Stuart Mill. In revolutionary
politics, the tenets of Epicureanism
are echoed in the words of the
United Statesâ€™ Declaration of
Independence: â€œlife, liberty, and
the pursuit of happiness.â€ â– 

Born to Athenian parents on
the Aegean island of Samos,
Epicurus was ï¬rst taught
philosophy by a disciple of
Plato. In 323 BCE, Alexander
the Great died and, in the
political conï¬‚icts that
followed, Epicurus and his
family were forced to move
to Colophon (now in Turkey).
There he continued his studies
with Nausiphanes, a follower
of Democritus.
Epicurus taught brieï¬‚y
in Mytilene on the island of
Lesbos, and in Lampsacus on
the Greek mainland, before
moving to Athens in 306 BCE.
He founded a school, known
as the The Garden, consisting
of a community of friends and
followers. There he set down
in great detail the philosophy
that was to become known
as Epicureanism.
Despite frequent ill health,
and often being in great pain,
Epicurus lived to the age
of 72. True to his beliefs, he
described the last day of his
life as a truly happy day.
Key works
Early 3rd century BCE
On Nature
Prinicipal Doctrines,
Vatican Sayings

66

HE HAS THE MOST
WHO IS MOST
CONTENT WITH
THE
LEAST
DIOGENES OF SINOPE ( .404â€“323 )
C

IN CONTEXT
BRANCH
Ethics
APPROACH
Cynicism
BEFORE
Late 5th century BCE
Socrates teaches that the
ideal life is one spent in
search of truth.
Early 4th century BCE
Socratesâ€™ pupil Antisthenes
advocates an ascetic life, lived
in harmony with nature.
AFTER
c.301 BCE Inï¬‚uenced by
Diogenes, Zeno of Citium
founds a school of Stoics.
4th century CE St. Augustine
of Hippo denounces the often
shameless behavior of the
Cynics, although they become
the model for several ascetic
Christian orders.
1882 Friedrich Nietzsche
refers to Diogenes and his
ideas in The Gay Science.

BCE

P

lato once described
Diogenes as â€œa Socrates
gone mad.â€ Although this
was meant as an insult, it is not
far from the truth. Diogenes shares
Socratesâ€™ passion for virtue and
rejection of material comfort, but
takes these ideas to the extreme.
He argues that in order to lead a
good life, or one that is worth living,
it is necessary to free oneself from
the external restrictions imposed
by society, and from the internal
discontentment that is caused
by desire, emotion, and fear. This
can be achieved, he states, by
being content to live a simple life,
governed by reason and natural
impulses, rejecting conventions
without shame, and renouncing
the desire for property and comfort.
Diogenes was the ï¬rst of a group
of thinkers who became known as
the Cynics, a term taken from the
Greek kunikos, meaning â€œdog-like.â€
It reï¬‚ects the determination of the
Cynics to spurn all forms of social
custom and etiquette, and instead
live in as natural a state as possible.
They asserted that the more one

Rejecting worldly values, Diogenes
chose to live on the streets. He ï¬‚outed
convention, by eating only discarded
scraps and dressingâ€”when he actually
bothered to do soâ€”in ï¬lthy rags.

can do this, as Diogenes himself
did by living a life of poverty with
only an abandoned tub for shelter,
the nearer one will be to leading
the ideal life.
The happiest person, who in
Diogenesâ€™ phrase, â€œhas the mostâ€,
is therefore someone who lives
in accordance with the rhythms
of the natural world, free from
the conventions and values of
civilized society, and â€œcontent
with the least.â€ â– 

See also: Socrates 46â€“49 â–  Plato 50â€“55 â–  Zeno of Citium 67
St. Augustine of Hippo 72â€“73 â–  Friedrich Nietzsche 214â€“21

â– 

THE ANCIENT WORLD 67

THE GOAL OF
LIFE IS LIVING
IN AGREEMENT
WITH
NATURE
ZENO OF CITIUM ( .332â€“265 )
C

IN CONTEXT
BRANCH
Ethics
APPROACH
Stoicism
BEFORE
c.380 BCE Plato states his
thoughts on ethics and the
city-state in The Republic.
4th century BCE Diogenes
of Sinope lives in extreme
poverty to demonstrate his
Cynic principles.
AFTER
c.40â€“45 CE Roman statesman
and philosopher Seneca the
Younger continues the Stoic
tradition in his Dialogues.
c.150â€“180 Roman emperor
Marcus Aurelius writes his
12-volume Meditations on
Stoic philosophy.
1584 Flemish humanist
Justus Lipsius writes De
Constantia, combining
Stoicism with Christianity to
found a school of Neo-Stoicism.

BCE

T

wo main schools of
philosophical thought
emerged after Aristotleâ€™s
death. These were the hedonistic,
godless ethic of Epicurus, which
had limited appeal, and the more
popular and longer-lasting Stoicism
of Zeno of Citium.
Zeno studied with a disciple of
Diogenes of Sinope, the Cynic, and
shared his no-nonsense approach
to life. He had little patience with
metaphysical speculation and came
to believe that the cosmos was
governed by natural laws that were
ordained by a supreme lawgiver.
Man, he declares, is completely
powerless to change this reality,
and in addition to enjoying its
many beneï¬ts, man also has to
accept its cruelty and injustice.

control, and be indifferent to pain
and pleasure, poverty and riches.
But if a person does so, Zeno is
convinced that he will achieve a
life that is in harmony with nature
in all its aspects, good or bad, and
live in accordance with the rulings
of the supreme lawgiver.
Stoicism was to ï¬nd favor across
much of Hellenistic Greece. But it
drew in even more followers in the
expanding Roman empire, where it
ï¬‚ourished as a basis for ethicsâ€”
both personal and politicalâ€”until it
was supplanted by Christianity in
the 6th century. â– 

Happiness is a good
ï¬‚ow of life.
Zeno of Citium

Free will
However, Zeno also declares that
man has been given a rational soul
with which to exercise free will.
No one is forced to pursue a â€œgoodâ€
life. It is up to the individual to
choose whether to put aside the
things over which he has little or no
See also: Plato 50â€“55

â– 

Aristotle 56â€“63

â– 

Epicurus 64â€“65

â– 

Diogenes of Sinope 66

THE MED
WORLD
250â€“1500

IEVAL

70 INTRODUCTION

Plotinus founds
Neo-Platonism,
a school of mystical
philosophy based on
the writings of Plato.

Crises brought on by both
internal and external forces
lead to the division of the
Roman Empire into east and
west. The western empire
falls within a century.

Boethius begins to
translate Aristotleâ€™s
work on logic.

The prophet Muhammad
performs the Hejira, his
journey from Mecca to Medina,
marking the beginning of
the Muslim era.

C.260

395

C.510

622

P

313

397â€“98

618

711

Constantine I proclaims
religious freedom within
the Roman Empire in the
Edict of Milan.

St. Augustine of
Hippo writes his
Confessions.

The Tang dynasty is
established in China,
bringing a Golden Age
of cultural development.

Conquest of
Christian Iberia
(now Spain and
Portugal) by
Muslim invaders.

hilosophy did not play a
large part in Roman culture,
other than Stoicism, which
was admired by the Romans for
its emphasis on virtuous conduct
and doing oneâ€™s duty. The broader
philosophical tradition that had
been established by the Classical
Greeks was therefore effectively
marginalized under the Roman
Empire. Philosophy continued to be
taught in Athens, but its inï¬‚uence
dwindled, and no signiï¬cant
philosophers emerged until Plotinus
in the 3rd century CE, who founded
an important Neo-Platonist school.
During the ï¬rst millennium of
the Common Era, Roman inï¬‚uence
also waned, both politically and
culturally. Christianity became
assimilated into the Roman culture,
and after the fall of the empire in
the 5th century, the Church

became the dominant authority in
Western Europe, remaining so for
almost 1,000 years. The Greek idea
of philosophy as rational examination
independent of religious doctrine
sat uncomfortably with the rise of
Christianity. Questions about the
nature of the universe and what
constitutes a virtuous life were held
to be answered in the scriptures;
they were not considered subjects
for philosophical discussion.
Early Christian philosophers such
as St. Augustine of Hippo sought
to integrate Greek philosophy into
the Christian religion. This process
was the main task of scholasticism,
a philosophical approach that
stemmed from the monastic schools
and was renowned for its rigorous
dialectical reasoning. The work of
scholastic philosophers such as
Augustine was not so much an

exploration of questions such as
â€œIs there a God?â€ or â€œDoes man
have an immortal soul?â€ as a search
for a rational justiï¬cation for the
belief in God and an immortal soul.

The Dark Ages
As the Roman Empire shrank and
eventually fell, Europe sank into the
â€œDark Agesâ€ and most of the culture
it had inherited from Greece and
Rome disappeared. The Church
held the monopoly on learning,
and the only true philosophy that
survived was a form of Platonism
deemed compatible with
Christianity, and Boethiusâ€™s
translation of Aristotleâ€™s Logic.
Elsewhere, however, culture
thrived. China and Japan in
particular enjoyed a â€œGolden Ageâ€
of poetry and art, while traditional
eastern philosophies coexisted

THE MEDIEVAL WORLD 71
The â€œHouse of
Wisdomâ€ is
established in
Baghdad, attracting
scholars from around
the world to share
and translate ideas.

St. Anselm
writes the
Proslogion.

The Black Death
reaches Europe, killing
more than a third of the
continentâ€™s population.

Fall of the Byzantine
Empire, the eastern
remnant of the Roman
Empire, when its capital
Constantinople is captured
by the Ottoman Turks.

832

1077â€“78

1347

1453

C.1014â€“20

1099

1445

1492

Avicenna (Ibn Sina)
writes his Kitab al-Shifa
(The Book of Healing).

Christian crusaders
capture the holy city
of Jerusalem.

Johannes Gutenberg
of Germany invents the
printing press, allowing
for a greater dissemination
of knowledge.

Christopher
Columbus crosses
the Atlantic and
reaches the
West Indies.

happily with their religions. In
the lands that had been part of
Alexander the Greatâ€™s empire, the
Greek legacy commanded more
respect than in Europe. Arabic and
Persian scholars preserved and
translated the works of the Classical
Greek philosophers, incorporating
their ideas into Islamic culture from
the 6th century onward.
As Islam spread eastward into
Asia and across north Africa and
into Spain, its inï¬‚uence began to be
felt in Europe. By the 12th century,
news of ideas and inventions from
the Islamic world were reaching as
far north as Britain, and European
scholars started to rediscover
Greek mathematics and philosophy
through Islamic sources. The works
of Aristotle in particular came as
something of a revelation, and they
sparked a resurgence of philosophical

thinking within the medieval
Christian Church. But whereas
Platoâ€™s philosophy had been
comparatively easy to assimilate
into Christian thought, because it
provided rational justiï¬cation for
belief in God and the immortal
human soul, Aristotle was treated
with suspicion by the Church
authorities. Nevertheless, Christian
philosophers including Roger
Bacon, Thomas Aquinas, Duns
Scotus, and William of Ockham
enthusiastically embraced the new
Aristotelianism and eventually
convinced the Church of its
compatibility with Christian faith.

A new rationality
Along with the philosophy that
revitalized the Church, the Islamic
world also introduced a wealth of
technological and scientiï¬c

knowledge to medieval Europe.
Aristotleâ€™s scientiï¬c methods had
been reï¬ned to sophisticated levels
in Persia, and advances in chemistry,
physics, medicine, and particularly
astronomy undermined the authority
of the Church when they arrived
in Europe.
The re-introduction of Greek
thinking and the new ideas that led
to Europeâ€™s Renaissance in the late
15th century sparked a change of
mood as people began to look more
toward reason rather than faith to
provide them with answers. There
was dissent even within the
Church, as humanists such as
Erasmus provoked the Reformation.
Philosophers themselves turned
their attention away from questions
of God and the immortal soul
toward the problems posed by
science and the natural world. â– 

72

GOD IS NOT
THE PARENT
OF EVILS

ST. AUGUSTINE OF HIPPO (354â€“430 CE)

IN CONTEXT
BRANCH
Ethics

Humans are
rational beings.

APPROACH
Christian Platonism
BEFORE
c.400 BCE In Gorgias, Plato
argues that evil is not a thing,
but an absence of something.

In order to be
rational, humans must
have free will.

3rd century CE Plotinus
revives Platoâ€™s view of
good and evil.
AFTER
c.520 Boethius uses an
Augustinian theory of evil in
The Consolation of Philosophy.

This means they must
be able to choose
between good or evil.

c.1130 Pierre Abelard rejects
the idea that there are not
evil things.
1525 Martin Luther, the
German priest who inspired
the Protestant reformation,
publishes On the Bondage
of the Will, arguing that the
human will is not free.

Humans can therefore
act badly or well.

God is not the
parent of evils.

A

ugustine was especially
interested in the problem
of evil. If God is entirely
good and all-powerful, why is there
evil in the world? For Christians
such as Augustine, as well as for
adherents of Judaism and Islam, this
was, and remains, a central question.
This is because it makes an obvious
fact about the worldâ€”that it
contains evilâ€”into an argument
against the existence of God.
Augustine is able to answer
one aspect of the problem quite
easily. He believes that although
God created everything that exists,
he did not create evil, because evil is
not a thing, but a lack or deï¬ciency
of something. For example, the evil
suffered by a blind man is that he is
without sight; the evil in a thief is
that he lacks honesty. Augustine
borrowed this way of thinking from
Plato and his followers.

An essential freedom
But Augustine still needs to explain
why God should have created the
world in such a way as to allow
there to be these natural and moral
evils, or deï¬ciencies. His answer
revolves around the idea that
humans are rational beings. He
argues that in order for God to

THE MEDIEVAL WORLD 73
See also: Plato 50â€“55
David Hume 148â€“53

â– 

Plotinus 331

â– 

create rational creatures, such as
human beings, he had to give them
freedom of will. Having freedom of
will means being able to choose,
including choosing between good
and evil. For this reason God had
to leave open the possibility that
the ï¬rst man, Adam, would choose
evil rather than good. According
to the Bible this is exactly what
happened, as Adam broke Godâ€™s
command not to eat fruit from the
Tree of Knowledge.
In fact, Augustineâ€™s argument
holds even without referring to
the Bible. Rationality is the ability
to evaluate choices through the
process of reasoning. The process is
only possible where there is freedom
of choice, including the freedom to
choose to do wrong.
Augustine also suggests a third
solution to the problem, asking us
to see the world as a thing of beauty.
He says that although there is evil
in the universe, it contributes to an
overall good that is greater than it

Boethius 74â€“75

â– 

Pierre Abelard 333

â– 

could be without evilâ€”just as
discords in music can make a
harmony more lovely, or dark patches
add to the beauty of a picture.

Explaining natural evils
Since Augustineâ€™s time, most
Christian philosophers have tackled
the problem of evil using one of his
approaches, while their opponents,
such as David Hume, have pointed
to their weaknesses as arguments
against Christianity. Calling sickness,
for instance, an absence of health
seems to be just playing with words:
illness may be due to a deï¬ciency of
something, but the suffering of the
sick person is real enough. And
how are natural evils, such as
earthquakes and plagues, explained?
Someone without a prior belief
in God might still argue that the
presence of evil in the world proves
that there is no all-powerful and
benevolent God. But for those who do
already believe in God, Augustineâ€™s
arguments might hold the answer. â– 

What made Adam
capable of obeying Godâ€™s
commands also made
him able to sin.
St. Augustine of Hippo

St. Augustine of Hippo
Aurelius Augustine was born
in 354 CE in Thagaste, a small
provincial town in North
Africa, to a Christian mother
and a pagan father. He was
educated to be a rhetorician,
and he went on to teach
rhetoric in his home town,
and at Carthage, Rome, and
Milan, where he occupied
a prestigious position.
For a while Augustine
followed Manichaeismâ€”a
religion that sees good and
evil as dual forces that rule
the universeâ€”but under the
inï¬‚uence of Archbishop
Ambrose of Milan, he became
attracted to Christianity.
In 386, he suffered a spiritual
crisis and underwent a
conversion. He abandoned his
career and devoted himself to
writing Christian works, many
of a highly philosophical
nature. In 395 he became
Bishop of Hippo, in North
Africa, and he held this post
for the rest of his life. He died
in Hippo, aged 75, when the
town was beseiged and
sacked by the Vandals.
Key works

A world without evil, Augustine says,
would be a world without usâ€”rational
beings able to choose their actions.
Just as for Adam and Eve, our moral
choices allow for the possibility of evil.

c.388â€“95 On Free Will
c.397â€“401 Confessions
c.413â€“27 On the City of God

74

GOD FORSEES
OUR FREE THOUGHTS
AND
ACTIONS
BOETHIUS ( .480â€“525 )
C

CE

IN CONTEXT
BRANCH
Epistemology

God lives in the
eternal present.

God knows the future
as if it were the present.

I am free not to go
to the cinema today.

God knows that I will
go to the cinema today.

APPROACH
Christian Platonism
BEFORE
c.350 BCE Aristotle outlines the
problems of claiming as true
any statement about the
outcome of a future event.
c.300 BCE Syrian philosopher
Iamblichus says that what can
be known depends upon the
knowerâ€™s capacity.
AFTER
c.1250â€“70 Thomas Aquinas
agrees with Boethius that God
exists outside of time, and so
is transcendent and beyond
human understanding.
c.1300 John Duns Scotus says
that human freedom rests on
Godâ€™s own freedom to act, and
that God knows our future, free
actions by knowing his own,
unchangingâ€”but freeâ€”will.

God foresees our free
thoughts and actions.

T

he Roman philosopher
Boethius was trained in
the Platonist tradition of
philosophy, and was also a Christian.
He is famous for his solution to a
problem that predates Aristotle:
if God already knows what we are
going to do in the future, how can
we be said to have free will?
The best way to understand the
dilemma is to imagine a situation in
everyday life. For instance, this

afternoon I might go to the cinema,
or I might spend time writing. As it
turns out, I go to the cinema. That
being the case, it is true now (before
the event) that I will go the cinema
this afternoon. But if it is true now,
then it seems that I do not really have
the choice of spending the afternoon
writing. Aristotle was the ï¬rst to
deï¬ne this problem, but his answer
to it is not very clear; he seems to
have thought that a sentence such

THE MEDIEVAL WORLD 75
See also: Aristotle 56â€“63
Immanuel Kant 164â€“71

â– 

Thomas Aquinas 88â€“95

as â€œI shall go to the cinema this
afternoonâ€ is neither true nor false,
or at least not in the same way as
â€œI went to the cinema yesterday.â€

A God beyond time
Boethius faced a harder version
of the same problem. He believed
that God knows everything; not only
the past and the present, but also
the future. So if I am going to go
to the cinema this afternoon, God
knows it now. It seems, therefore,
that I am not really free to choose

Everything is
known, not according to
itself, but according to the
capacity of the knower.
Boethius

Boethius

â– 

John Duns Scotus 333

â– 

Benedictus Spinoza 126â€“29

to spend the afternoon writing, since
that would conï¬‚ict with what God
already knows.
Boethius solves the problem by
arguing that the same thing can be
known in different ways, depending
on the nature of the knower. My dog,
for instance, knows the sun only as
something with qualities he can
senseâ€”by sight and touch. A person,
however, can also reason about the
category of thing the sun is, and
may know which elements it is made
of, its distance from Earth, and so on.
Boethius considers time in a
similar kind of way. As we live in
the ï¬‚ow of time, we can only know
events as past (if they have occurred),
present (if they are happening now),
or future (if they will come to pass).
We cannot know the outcome of
uncertain future events. God, by
contrast, is not in the ï¬‚ow of time.
He lives in an eternal present, and
knows what to us are past, present,
and future in the same way that we
know the present. And just as my
knowledge that you are sitting now
does not interfere with your freedom
to stop, so too Godâ€™s knowledge of
Anicius Boethius was a Christian
Roman aristocrat, born at a time
when the Roman Empire was
disintegrating and the Ostrogoths
ruled Italy. He became an orphan
at the age of seven and was
brought up by an aristocratic
family in Rome. He was extremely
well educated, speaking ï¬‚uent
Greek and having an extensive
knowledge of Latin and Greek
literature and philosophy. He
devoted his life to translating
and commenting on Greek texts,
especially Aristotleâ€™s works on
logic, until he was made chief
adviser to the Ostrogothic king

â– 

Lady Philosophy and Boethius discuss
free will, determinism, and Godâ€™s vision
of the eternal present in his inï¬‚uential
book, The Consolation of Philosophy.

our future actions, as if they were
present, does not stop them from
being free.
Some thinkers today argue that
since I have not yet decided whether
I shall go to the cinema this
afternoon, there is simply nothing
to be known about it, so even a God
who is all-knowing does not, and
cannot, know if I shall go or not. â– 
Theoderic. Some ï¬ve years later
he became a victim of court
intrigue, was wrongly accused
of treason, and sentenced to
death. He wrote his most
famous work, The Consolation
of Philosophy, while in prison
awaiting execution.
Key works
c.510 Commentaries on
Aristotleâ€™s â€œCategoriesâ€
c.513â€“16 Commentaries on
Aristotleâ€™s â€œOn Interpretationâ€
c.523â€“26 The Consolation of
Philosophy

76

THE SOUL
IS DISTINCT
FROM
THE BODY
AVICENNA (980â€“1037)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Arabic Aristotelianism
BEFORE
c.400 BCE Plato argues that
mind and body are distinct
substances.
4th century BCE Aristotle
argues that mind is the â€œformâ€
of the body.
c.800â€“950 CE Aristotleâ€™s works
are translated into Arabic for
the ï¬rst time.
AFTER
1250sâ€“60s Thomas Aquinas
adapts Aristotleâ€™s account of
the mind and body.
1640 RenÃ© Descartes argues
for dualism in his Meditations.
1949 Gilbert Ryle describes
dualism as a â€œcategory mistakeâ€
in The Concept of Mind.

A

vicenna, also known as
Ibn SÃ®nÃ¢, is the most
important philosopher in
the Arabic tradition, and one of the
worldâ€™s greatest thinkers. Like his
predecessors, al-KindÃ® and al-FÃ¢rÃ¢bÃ®,
and his successor, Averroes,
Avicenna self-consciously marked
himself out as a philosopher rather
than an Islamic theologian, choosing
to follow Greek wisdom and the
path of reasoning and proof. In
particular, he saw himself as a
follower of Aristotle, and his main
writings are encyclopedias of
Aristotelian philosophy.
However, these works explain
Aristotleâ€™s philosophy as re-thought
and synthesized by Avicenna. On

THE MEDIEVAL WORLD 77
See also: Plato 50â€“55 â–  Aristotle 56â€“63 â–  Al-KindÃ® 332 â–  Al-FÃ¢rÃ¢bÃ® 332
Thomas Aquinas 88â€“95 â–  RenÃ© Descartes 116â€“23 â–  Gilbert Ryle 337

If I were blindfolded
and suspended in the
air, touching nothingâ€¦

â– 

â€¦I would not know
that I have a body.

Avicenna
So my soul is not
a body, but something
different.

But I would know
that Iâ€”my â€œselfâ€
or â€œsoulâ€â€”exists.

The soul is
distinct from
the body.

some doctrines, such as the idea
that the universe has always existed,
Avicenna kept to the Aristotelian
view despite the fact that it clashed
with Islamic orthodoxy, but in other
areas he felt free to depart radically
from Aristotle. One striking example
is his explanation of the relationship
between mind (self or soul) and body.

Mind and body are distinct
Aristotle claims that the body and
mind of humans (and other animals)
are not two different things (or
â€œsubstancesâ€), but one unit, and that
the mind is the â€œformâ€ of the human
body. As such, it is responsible for
all the activities a human being can
perform, including thinking. For this

reason Aristotle does not seem to
think it possible for anything to
survive the death of the body.
By contrast, Avicenna is one of
the most famous â€œdualistsâ€ in the
history of philosophyâ€”he thinks
that the body and the mind are two
distinct substances. His great
predecessor in this view was Plato,
who thought of the mind as a
distinct thing that was imprisoned
in the body. Plato believed that at
the point of death, the mind would
be released from its prison, to be
later reincarnated in another body.
In seeking to prove the divided
nature of mind and body, Avicenna
devised a thought-experiment
known as the â€œFlying Manâ€. This â¯â¯

Ibn SÃ®nÃ¢, or Avicenna as the
Europeans called him, was
born in 980 in a village near
Bukhara, now in Uzbekhistan.
Although he wrote mainly in
Arabic, the language of
learning throughout the
Islamic world, he was a native
Persian speaker. Avicenna
was a child prodigy, rapidly
surpassing his teachers not
only in logic and philosophy,
but also in medicine. While
still in his teens, he became
known to the Samanid ruler
Nuh ibn Mansur as a brilliant
physician, and was given the
use of his magniï¬cent library.
Avicennaâ€™s life was spent
in the service of various
princes, both as physician and
political adviser. He started
writing at the age of 21, and
went on to write more than
200 texts, on subjects as
diverse as metaphysics,
animal physiology, mechanics
of solids, and Arabic syntax.
He died when his medications
for colic were altered, possibly
maliciously, while on campaign
with his patron AlÃ¢ al-Dawla.
Key works
c.1014â€“20 Book of Healing
c.1015 Canon of Medicine
c.1030 Pointers and Reminders

78 AVICENNA
appears as a treatise, On the Soul,
within his Book of Healing, and it
aims to strip away any knowledge
that can possibly be disproved, and
leave us only with absolute truths.
It remarkably anticipates the much
later work of Descartes, the famous
dualist of the 17th century, who also
decided to believe nothing at all
except that which he himself could
know for certain. Both Avicenna
and Descartes want to demonstrate
that the mind or self exists because
it knows it exists; and that it is
distinct from the human body.

The Flying Man
In the Flying Man experiment,
Avicenna wants to examine what
we can know if we are effectively
robbed of our senses, and cannot
depend on them for information.
He asks us each to imagine this:
suppose I have just come into
existence, but I have all my normal
intelligence. Suppose, too, that I am
blindfolded and that I am ï¬‚oating in
the air, and my limbs are separated

from each other, so I can touch
nothing. Suppose I am entirely
without any sensations. None the
less, I will be sure that I myself exist.
But what is this self, which is me?
It cannot be any of the parts of my
body, because I do not know that I
have any. The self that I afï¬rm as
existing does not have length or
breadth or depth. It has no extension,
or physicality. And, if I were able
to imagine, for instance, a hand,
I would not think that it belonged
to this self which I know exists.
It follows from this that the
human selfâ€”what I amâ€”is distinct
from my body, or anything physical.
The Flying Man experiment, says
Avicenna, is a way of alerting and
reminding oneself of the existence
of the mind as something other
than, and distinct from, the body.
Avicenna also has other ways
to show that the mind cannot be
something material. Most are
based on the fact that the type of
intellectual knowledge the mind
can grasp cannot not be contained

The secret conversation
is a direct encounter
between God and the soul,
abstracted from all
material constraints.
Avicenna

by anything material. It is easy to
see how the parts of physical, shaped
things ï¬t with the parts of a physical,
shaped sense organ: the image of
the wall that I see is stretched over
the lens of my eye, each of its parts
corresponding to a part of the lens.
But the mind is not a sense organ;
what it grasps are deï¬nitions, such
as â€œMan is a rational, mortal animalâ€.
The parts of this phrase need to be
grasped at once, together. The mind
therefore cannot be in any way like
or part of the body.

The immortal soul
Avicenna goes on to draw the
conclusion that the mind is not
destroyed when the body dies, and
that it is immortal. This did not
help to make his thinking more
palatable to orthodox Muslims, who
believe that the whole person, body
and mind, is resurrected and enjoys
the afterlife. Consequently, Avicenna
was attacked in the 12thÂ century
by the great Islamic theologian
al-GhazÃ¢lÃ®, who called him a heretic
Avicennaâ€™s medical knowledge
was so vast that it won him royal
patronage. His Canon of Medicine
inï¬‚uenced European schools of
medicine until the mid-17th century.

THE MEDIEVAL WORLD 79

But what is it
that I am?
A thinking thing.
RenÃ© Descartes

thing that he cannot be deceived
about, he realizes, is that he exists.
This self is exactly the self which
Avicennaâ€™s Flying Man is sure of,
when he has no other knowledge.
Like Avicenna, Descartes can then
conclude that the â€œIâ€, or self, is
completely distinct from the body,
and that it must be immortal.

The ghost in the machine

for abandoning the central Islamic
tenet of the resurrection of the dead.
But in the sameÂ century Avicennaâ€™s
work was also translated into Latin,
and his dualism became popular
among Christian philosophers and
theologians. They liked the way his
interpretations of Aristotleâ€™s texts
made them easily compatible with
the idea of an immortal soul.

The indubitable self
Some 200 years later, in the 1250s,
Thomas Aquinas championed a
more faithful interpretation of
Aristotle, in which the mind and
body are much more closely tied
together, and his views were widely
accepted by the theologians of the
16th and 17th centuries. But in 1640
Descartes returned to a dualism
that was nearer to Platoâ€™s than
Aristotleâ€™s, and his argument for
it was very like Avicennaâ€™s.
Descartes imagines that there
is a demon who is trying to deceive
him about everything on which he
might possibly be deceived. The one
Philip Pullmanâ€™s tale, Northern Lights,
picks up on the ancient Greek idea of a
personâ€™s soul, or daimon, being separate
to the body, by presenting it as an
entirely separate animal, such as a cat.

One very strong objection to the
dualism of Avicenna or Descartes
is the argument used by Aquinas.
He says that the self which thinks
is the same as the self which feels
sensations in the body. For instance,
I do not just observe that there is
a pain in my leg, in the way that a
sailor might notice a hole in his ship.
The pain belongs to me as much as
my thoughts about philosophy, or
what I might have for lunch.
Most contemporary philosophers
reject mind-body dualism, largely
because of the increasing scientiï¬c
knowledge of the brain. Avicenna
and Descartes were both very
interested in physiology and they
produced scientiï¬c accounts of
activities such as movement and
sensation. But the process of
rational thinking was inexplicable
with the scientiï¬c tools of their

times. We are now able to explain
quite precisely how thinking goes
on in different areas of the brainâ€”
though whether this means that we
can explain thinking without
reference to a self is not so clear.
An inï¬‚uential 20th-century British
philosopher, Gilbert Ryle, caricatured
the dualistsâ€™ self as â€œa ghost in the
machineâ€, and tried to show that
we can explain how human beings
perceive and function within the
world without resorting to this
â€œghostâ€ of a self.
Today philosophers are divided
between a small number of dualists,
a larger number of thinkers who say
that the mind is simply a brain, and
the majority, who agree that thinking
is the result of the physical activity
of the brain, but still insist there is
a distinction between the physical
states of the brain (the gray matter,
the neurons, and so on), and the
thinking which derives from them.
Many philosophers, especially
continental European thinkers, still
accept the results of Avicennaâ€™s
thought experiment in one central
way. It shows, they say, that we each
have a self with a ï¬rst-person view
of the world (the â€œIâ€) that cannot be
accommodated by the objective
view of scientiï¬c theories. â– 

80

JUST BY THINKING
ABOUT GOD WE CAN
KNOW
HE
EXISTS
ST. ANSELM (1033â€“1109)
IN CONTEXT
BRANCH
Philosophy of religion
APPROACH
Platonic-Aristotelian
BEFORE
c.400 CE St. Augustine of
Hippo argues for Godâ€™s
existence through our grasp
of unchanging truths.
1075 In his Monologion,
Anselm develops Augustineâ€™s
proof of Godâ€™s existence.
AFTER
1260s Thomas Aquinas
rejects Anselmâ€™s Ontological
Argument.
1640 RenÃ© Descartes uses a
form of Anselmâ€™s Ontological
Argument in his Meditations.
1979 American philosopher
Alvin Plantinga reformulates
Anselmâ€™s Ontological Argument
using a form of modal logic
to establish its truth.

A

lthough Christian thinkers
believe as a matter of faith
that God exists, in the
Middle Ages they were keen to
show that Godâ€™s existence could
also be proved by rational argument.
The Ontological Argument invented
by Anselmâ€”an 11th-century Italian
philosopher who worked on the
basis of Aristotelian logic, Platonic
thinking, and his own geniusâ€”is
probably the most famous of all.
Anselm imagines himself
arguing with a Fool, who denies
that God exists (see opposite). The
argument rests on an acceptance
of two things: ï¬rst, that God is
â€œthat than which nothing greater

We believe that
You [God] are that
than which nothing
greater can be thought.
St. Anselm

can be thoughtâ€, and second,
that existence is superior to
non-existence. By the end of the
argument the Fool is forced to
either take up a self-contradictory
position or admit that God exists.
The argument has been accepted
by many great philosophers, such as
RenÃ© Descartes and Baruch Spinoza.
But there have been many others
who took up the Foolâ€™s side. One
contemporary of Anselmâ€™s, Gaunilo
of Marmoutiers, said that we could
use the same argument to prove that
there exists somewhere a marvellous
island, greater than any island that
can be thought. In the 18th century
Immanuel Kant objected that the
argument treats existence as if it
were an attribute of thingsâ€”as if I
might describe my jacket like this:
â€œitâ€™s green, made of tweed, and it
exists.â€ Existing is not like being
green: if it did not exist, there would
be no jacket to be green or tweed.
Kant holds that Anselm is also
wrong to say that what exists in
reality as well as in the mind is
greater than what exists in the
mind alone, but other philosophers
disagree. Is there not a sense in
which a real painting is greater
than the mental concept the painter
has before he starts work? â– 

THE MEDIEVAL WORLD 81
See also: Plato 50â€“55 â–  St. Augustine of Hippo 72â€“73
RenÃ© Descartes 116â€“23 â–  Benedictus Spinoza 126â€“29

Anselm

â– 

Thomas Aquinas 88â€“95

â– 

The Fool

Do you agree that if
God existed he would be the
greatest thing that there could beâ€”
â€œthat than which nothing greater
can be thought?â€
Yes.

St. Anselm
And do you agree that â€œthat
than which nothing greater can be
thoughtâ€ exists in your mind?
Yes, in my mindâ€”
but not in reality.
But would you agree
that something that exists in
reality as well as in the mind is
greater than something that
exists in the mind alone?

Yes, I suppose soâ€”
an ice cream in my
hand is better than
one thatâ€™s just in
my imagination.

So if â€œthat than which
nothing greater can be thoughtâ€
exists only in the mind, it is less great
than if it existed also in reality.
Thatâ€™s true.
The being that
really exists would
be greater.
So now you are saying
that there is something greater
than â€œthat than which nothing
greater exists?â€
That doesnâ€™t
even make sense.
Exactly. And the only
way around this contradiction
is to admit that God (â€œthat than which
nothing greater existsâ€) does existâ€”
both in thought and reality.

Anselmâ€™s Ontological
Argument was written
in 1077â€“78, but acquired
its title from the German
philosopher Kant in 1781.

St. Anselm of Canterbury was
born in Aosta in Italy in 1033.
He left home in his twenties to
study at the monastery of Bec,
in France, under an eminent
logician, grammarian, and
Biblical commentator named
Lanfranc. Anselm became a
monk of Bec in 1060, then
prior, and eventually abbot in
1078. He traveled to England,
and in 1093 was made
Archbishop of Canterbury,
despite his protestations of
ill-health and lack of political
skills. This position put him in
conï¬‚ict with the Anglo-Norman
kings William II and Henry I,
as he tried to uphold the
Church against royal power.
These disputes led to two
periods of exile from England
for Anselm, during which he
visited the pope to plead the
case for the English Church
and his own removal from
ofï¬ce. Ultimately reconciled
with King Henry I, Anselm
died in Canterbury aged 76.
Key works
1075â€“76 Monologion
1077â€“78 Proslogion
1095â€“98 Why did God
become Man?
1080â€“86 On the Fall of
the Devil

82

PHILOSOPHY AND
RELIGION ARE NOT
INCOMPATIBLE
AVERROES (1126â€“1198)
IN CONTEXT
BRANCH
Philosophy of religion
APPROACH
Arabic Aristotelian
BEFORE
1090s AbÃ» HÃ¢mid al-GhazÃ¢lÃ®
launches an attack on Islamic
Aristotelian philosophers.
1120s Ibn BÃ¢jja (Avempace)
establishes Aristotelian
philosophy in Islamic Spain.

A

verroes worked in the legal
profession; he was a qÃ¢dÃ®
(an Islamic judge) who
worked under the Almohads, one of
the strictest Islamic regimes in the
Middle Ages. Yet he spent his nights
writing commentaries on the work
of an ancient pagan philosopher,
Aristotleâ€”and one of Averroesâ€™ avid
readers was none other than the
Almohad ruler, AbÃ» YacqÃ»b YÃ»suf.
Averroes reconciles religion and
philosophy through a hierarchical
theory of society. He thinks that
only the educated elite are capable
of thinking philosophically, and

everyone else should be obliged to
accept the teaching of the Qurâ€™an
literally. Averroes does not think
that the Qurâ€™an provides a completely
accurate account of the universe if
read in this literal way, but says that
it is a poetic approximation of the
truth, and this is the most that the
uneducated can grasp.
However, Averroes believes that
educated people have a religious
obligation to use philosophical
reasoning. Whenever reasoning
shows the literal meaning of the
Qurâ€™an to be false, Averroes says
that the text must be â€œinterpretedâ€;

AFTER
1270 Thomas Aquinas
criticizes the Averroists for
accepting conï¬‚icting truths
from Christianity and
Aristotelian philosophy.
1340s Moses of Narbonne
publishes commentaries on
Averroesâ€™ work.
1852 French philosopher
Ernest Renan publishes a
study of Averroes, on the
basis of which he becomes an
important inï¬‚uence on modern
Islamic political thought.

true.

But some parts of it are
demonstrably false.

Philosophy and
religion are not
incompatible.

The text is a poetic truth,
and must be interpreted using
philosophical reasoning.

THE MEDIEVAL WORLD 83
See also: Plato 50â€“55 â–  Aristotle 56â€“63
Moses of Narbonne 334

Philosophers believe
that religious laws are
necessary political arts.
Averroes

that is to say the obvious meaning
of the words should be disregarded
and the scientiï¬c theory
demonstrated by Aristotelian
philosophy accepted in its place.

The immortal intellect
Averroes is willing to sacriï¬ce some
widely-held Islamic doctrines in
order to maintain the compatibility
of philosophy and religion. For
instance, almost all Muslims believe
that the universe has a beginning,
but Averroes agrees with Aristotle
that it has always existed, and says
that there is nothing in the Qurâ€™an

Averroes

â– 

Al-GhazÃ¢lÃ® 332

â– 

Ibn BÃ¢jja 333

â– 

Thomas Aquinas 88â€“95

to contradict this view. However, the
resurrection of the dead, a basic
tenet of Islam, is harder to include
within an Aristotelian universe.
Averroes accepts that we must
believe in personal immortality,
and that anyone who denies this is
a heretic who should be executed.
But he takes a different position
from his predecessors by saying that
Aristotleâ€™s treatise On the Soul does
not state that individual humans
have immortal souls. According to
Averroesâ€™ interpretation, Aristotle
claims that humanity is immortal
only through a shared intellect.
Averroes seems to be saying that
there are truths discoverable by
humans that hold good for ever, but
that you and I as individuals will
perish when our bodies die.

â– 

of Aristotle and Averroes became
known as Averroists, and they
included Jewish scholars such
as Moses of Narbonne, and Latin
scholars such as Anicius Boethius
and Siger of Brabant. The Latin
Averroists acccepted Aristotle as
interpreted by Averroes as the truth
according to reasonâ€”despite also
afï¬rming an apparently conï¬‚icting
set of Christian â€œtruths.â€ They have
been described as advocating a
â€œdouble truthâ€ theory, but their view
is rather that truth is relative to the
context of enquiry. â– 

Later Averroists
Averroesâ€™ advocacy of Aristotelian
philosophy (if only for the elite) was
shunned by his fellow Muslims. But
his works, translated into Hebrew
and Latin, had enormous inï¬‚uence
in the 13th and 14th centuries.
Scholars who supported the opinions
Ibn Rushd, known in Europe as
Averroes, was born in 1126 in
Cordoba, then part of Islamic
Spain. He belonged to a family of
distinguished lawyers and trained
in law, science, and philosophy.
His friendship with another doctor
and philosopher, Ibn Tufayl, led
to an introduction to the Caliph
AbÃ» YacqÃ»b YÃ»suf, who appointed
Averroes chief judge and later
court physician. AbÃ» YacqÃ»b
also shared Averroesâ€™ interest in
Aristotle, and commissioned him
to write a series of paraphrases of
all Aristotleâ€™s works, designed for
non-specialists such as himself.

Some Muslims did not view philosophy
as a legitimate subject for study in the
12th century, but Averroes argued that
it was essential to engage with religion
critically and philosophically.

Despite the increasingly liberal
views of the Almohads, the
public disapproved of Averroesâ€™
unorthodox philosophy, and
public pressure led to a banning
of his books and personal exile in
1195. Reprieved two years later,
Averroes returned to Cordoba
but died the following year.
Key works
1179â€“80 Decisive Treatise
1179â€“80 The Incoherence of the
Incoherence
c.1186 Great Commentary on
Aristotleâ€™s â€˜On the Soulâ€™

84

GOD HAS
NO ATTRIBUTES
MOSES MAIMONIDES (1135â€“1204)

IN CONTEXT
BRANCH
Philosophy of religion
APPROACH
Jewish Aristotelian
BEFORE
c.400 CE The philosopher
Pseudo-Dionysius establishes
the tradition of Christian
negative theology, which
states that God is not being,
but more than being.

M

aimonides wrote on both
Jewish law (in Hebrew)
and Aristotelian thought
(in Arabic). In both areas, one of his
central concerns was to guard
against anthropomorphizing God,
which is the tendency to think
about God in the same way as a
human being. For Maimonides, the
worst mistake of all is to take the
Torah (the ï¬rst part of the Hebrew
Bible) as literal truth, and to think
that God is a bodily thing. Anyone

860s John Scotus Eriugena
suggests that God creates
the universe from the nothing
which is himself.
AFTER
1260s Thomas Aquinas
moderates Maimonidesâ€™
negative theology in his
Summa Theologiae.
Early 1300s Meister Eckhart
develops his negative theology.
1840â€“50s SÃ¸ren Kierkegaard
claims that it is impossible
to provide any form of external
description of God.

who thinks this, he says, should
be excluded from the Jewish
community. But in the Guide of the
Perplexed, Maimonides pushes this
idea to its farthest extent, developing
a strand of thought known as
â€œnegative theology.â€ This already
existed in Christian theology, and
it focuses on describing God only
in terms of what God is not.
God, Maimonides says, has no
attributes. We cannot rightly say
that God is â€œgoodâ€ or â€œpowerful.â€

Attributes are eitherâ€¦

â€¦accidental.

â€¦essential.

But God has
no accidents.

Essential attributes
deï¬ne.

God has
no attributes.

But God
is indeï¬nable.

THE MEDIEVAL WORLD 85
See also: Johannes Scotus Eriugena 332 â–  Thomas Aquinas 88â€“95
Meister Eckhart 333 â–  SÃ¸ren Kierkegaard 194â€“95

This is because an attribute is
either accidental (capable of change)
or essential. One of my accidental
attributes, for example, is that I am
sitting; others are that I have gray
hair and a long nose. But I would
still be what I essentially am even if
I were standing, red-haired, and had
a snub-nose. Being humanâ€”that is,
being a rational, mortal animalâ€”is
my essential attribute: it deï¬nes
me. God, it is generally agreed, has
no accidental attributes, because
God is unchanging. In addition,
says Maimonides, God cannot have
any essential attributes either,
because they would be deï¬ning,
and God cannot be deï¬ned. So God
has no attributes at all.

Speaking about God
Maimondes claims that we can say
things about God, but they must be
understood as telling us about Godâ€™s
actions, rather than Godâ€™s being.
Most discussions in the Torah should
be understood in this way. So when

â– 

When the intellects
contemplate Godâ€™s essence,
their apprehension turns
into incapacity.
Maimonides

we are told that â€œGod is a creatorâ€,
we must understand this as stating
what God does, rather than the
sort of thing God is. If we were to
consider the sentence â€œJohn is a
writerâ€, we might normally take it
to mean that being a writer is Johnâ€™s
profession. But Maimonides asks us
to consider only what has been
done: in this instance John has
written words. The writing has been
brought about by John but it does
not tell us anything about him.
Maimonides also accepts that
statements which seem to attribute
qualities to God can be understood
if they are taken as double negatives.
â€œGod is powerfulâ€: should be taken
to mean that God is not powerless.
Imagine a game in which I think of
a thing and tell you what it is not
(it is not large, it is not red...) until
you guess what it is. The difference
in the case of God is that we have
only the negations to guide us: we
cannot say what God is. â– 
The Mishneh Torah was a complete
restatement of Jewish Oral Law, which
Maimonides wrote in plain Hebrew so
that â€œyoung and oldâ€ could know and
understand all the Jewish observances.

Moses Maimonides
Moses Maimonides (also
known as Rambam) was born
in 1135 in Cordoba, Spain, into
a Jewish family. His childhood
was rich in cross-cultural
inï¬‚uences: he was educated
in both Hebrew and Arabic,
and his father, a rabbinic
judge, taught him Jewish law
within the context of Islamic
Spain. His family ï¬‚ed Spain
when the Berber Almohad
dynasty came to power in
1148, and lived nomadically
for 10 years until they settled
ï¬rst in Fez (now in Morocco)
and then Cairo. The familyâ€™s
ï¬nancial problems led
Maimonides to train as a
physician, and his skill led to
a royal appointment within
only a few years. He also
worked as a rabbinic judge,
but this was an activity for
which he thought it wrong
to accept any payment. He
was recognized as head of
the Jewish community of
Cairo in 1191, and after his
death his tomb became a
place of Jewish pilgrimage.
Key works
1168 Commentary on the
Mishna
1168â€“78 Mishneh Torah
1190 Guide of the Perplexed

86

DONâ€™T GRIEVE.
ANYTHING YOU LOSE
COMES ROUND IN
ANOTHER
FORM
JALAL AD-DIN MUHAMMAD RUMI (1207â€“1273)

ss

ï¬‚o
w

of l

the

an
d

ng
Anythi

um

uï¬sm, the mystical and
aesthetic interpretation of
the Qurâ€™an, had been part
of Islam since its foundation in the
7th century, but had not always
been accepted by mainstream
Islamic scholars. Jalal ad-Din
Muhammad Rumi, better known
simply as Rumi, was brought up in
orthodox Islam, and ï¬rst came into
contact with Suï¬sm when his

ife.

nu
endl
ess conti

e
ur
present to the fut

in

on
e

an, is art of an
p

tc
th a

e

a

a
lw
is a

S

in one form

.

nt
ast
se
e
is lin
r
ked to the p

cl
ud
ing
m

AFTER
1273 Rumiâ€™s followers found
the Mawlawi Order of Suï¬sm.

Today Rumiâ€™s works continue
to be translated into many
languages around the world.

he
p

T

n
,i
se

10th century Aliâ€™s mystical
interpretation of the Qurâ€™an
becomes the basis for Suï¬sm.

1925 After the founding of a
secular Republic of Turkey,
the Mawlawi Order is banned
in Turkey. It remains illegal
until 1954, when it receives
the right to perform on
certain occasions.

y

644 Ali ibn Abi Talib,
Mohammedâ€™s cousin and
successor, becomes Caliph.

.
rm
another fo

BEFORE
610 Islam is founded by the
Prophet Mohammed.

rn i
n

r
v
e
ive
r
ythi
ng in the un

APPROACH
Suï¬sm

sr
eb
o

E

BRANCH
Islamic philosophy

se
st
o
exis
t

IN CONTEXT

dl
en

e

family moved from the eastern
edges of Persia to Anatolia in the
mid-13th century. The Suï¬ concept
of uniting with God through love
caught his imagination, and from
this he developed a version of
Suï¬sm that sought to explain the
relationship of man with the divine.
Rumi became a teacher in a Suï¬
order, and as such he believed he
was a medium between God and

THE MEDIEVAL WORLD 87
See also: Siddhartha Gautama 30â€“33 â–  Avicenna 76â€“79
Hajime Tanabe 244â€“45 â–  Arne Naess 282â€“83

â– 

Averroes 82â€“83

â– 

death, and nor should we grieve a
loss. In order to ensure our growth
from one form to another, however,
we should strive for spiritual growth
and an understanding of the
divineâ€“human relationship. Rumi
believes that this understanding
comes from emotion rather than
from reasonâ€”emotion enhanced
by music, song, and dance.

Rumiâ€™s legacy
The Mawlawi Order, or Whirling
Dervishes, dance as part of the Suï¬
Sema ceremony. The dance represents
the spiritual journey of man from
ignorance to perfection through love.

man. In contrast to general Islamic
practice, he placed much emphasis
on dhikrâ€”ritual prayer or litanyâ€”
rather than rational analysis of the
Qurâ€™an for divine guidance, and
became known for his ecstatic
revelations. He believed it was his
task to communicate the visions
he experienced, and so he wrote
them down in the form of poetry.
Central to his visionary philosophy
is the idea that the universe and
everything in it is an endless ï¬‚ow
of life, in which God is an eternal
presence. Man, as part of the
universe, is also a part of this
continuum, and Rumi seeks to
explain our place within it.
Man, he believes, is a link
between the past and future in a
continual process of life, death, and
rebirthâ€”not as a cycle, but in a
progression from one form to another
stretching into eternity. Death and
decay are inevitable and part of
this endless ï¬‚ow of life, but as
something ceases to exist in one
form, it is reborn in another. Because
of this, we should have no fear of

The mystical elements of Rumiâ€™s
ideas were inspirational within
Suï¬sm, and inï¬‚uenced mainstream
Islam too. They were also pivotal
in converting much of Turkey from
Orthodox Christianity to Islam. But
this aspect of his thinking did not
hold much sway in Europe, where
rationalism was the order of the
day. In the 20th century, however,
his ideas became very popular
in the West, mainly because his
message of love chimed with the
New Age values of the 1960s.
Perhaps his greatest admirer in
the 20th century was the poet
and politician Muhammed Iqbal,
advisor to Muhammad Ali Jinnah,
who campaigned for an Islamic
state of Pakistan in the 1930s. â– 

I died as a mineral
and became a plant,
I died as a plant and
rose to animal, I died as
animal and I was Man.
Jalal ad-Din Rumi

Jalal ad-Din
Muhammad Rumi
Jalal ad-Din Muhammad Rumi,
also known as Mawlana (Our
Guide) or simply Rumi, was
born in Balkh, in a province
of Persia. When the Mongol
invasions threatened the
region, his family settled in
Anatolia, Turkey, where Rumi
met the Persian poets Attar
and Shams al-Din Tabrizi.
He decided to devote himself
to Suï¬sm, and went on to
write thousands of verses
of Persian and Arabic poetry.
In 1244 Rumi became
the shaykh (Master) of a Suï¬
order, and taught his mysticalemotional interpretation of the
Qurâ€™an and the importance of
music and dance in religious
ceremony. After his death,
his followers founded the
Mawlawi Order of Suï¬sm,
which is famous for its
Whirling Dervishes who
perform a distinctive dance
in the Sema ceremonyâ€”a form
of dhikr unique to the sect.
Key works
Earlyâ€“mid-13th century
Rhyming Couplets of Profound
Spiritual Meaning
The Works of Shams of Tabriz
What is Within is Within
Seven Sessions

THE UNIVERSE

HAS NOT ALWAYS

EXISTED
THOMAS AQUINAS (C. 1225â€“1274)

90 THOMAS AQUINAS
IN CONTEXT
BRANCH
Metaphysics
APPROACH
Christian Aristotelian
BEFORE
c.340 BCE Aristotle says that
the universe is eternal.
c.540 CE John Philoponus
argues that the universe must
have a beginning.
1250sâ€“60s French theologians
adopt Philoponusâ€™s argument.
AFTER
1290s French philosopher
Henry of Ghent criticizes
Aquinas, saying the universe
cannot have always existed.
1781 Immanuel Kant claims
he can show that the universe
has always existed, and that
it has not always existed.
1931 Belgian priest and
scientist Georges LemaÃ®tre
proposes the â€œBig Bangâ€ theory
of the origins of the universe.

Thomas Aquinas

T

he opinions of people today
are still divided into those
that hold that the universe
had a beginning, and those that
hold that it has always existed.
Today we tend to look to physics
and astronomy for an answer, but
in the past this was a question for
philosophers and theologians. The
answer given by the Catholic priest
and philosopher Thomas Aquinas,
the most famous of all medieval
Christian philosophers, is especially
interesting. It is still a plausible
way of thinking about the problem,
and it also tells us a great deal about
how Aquinas combined his faith
with his philosophical reasoning,
despite their apparent contradictions.

Aristotleâ€™s inï¬‚uence
The central ï¬gure in Aquinasâ€™s
thinking is Aristotle, the ancient
Greek philosopher whose work was
intensively studied by medieval
thinkers. Aristotle was certain that
the universe has always existed,
and that it has always been home
to different things, from inanimate
objects like rocks, to living species,
such as humans, dogs, and horses.
He argued that the universe is
changing and moving, and this
Thomas Aquinas was born in
1225 at Roccasecca in Italy. He
studied at the University of
Naples and then joined the
Dominican order (a new, highly
intellectual order of friars) against
the wishes of his family. As a
novitiate he studied in Paris and
then in Cologne under the German
Aristotelian theologian, Albert
the Great. Returning to Paris, he
became Master (professor) of
theology, before leaving to travel
around Italy teaching for 10 years.
Unusually, Aquinas was then
offered a second period of tenure
as Master at Paris. In 1273 he

can only be caused by change and
motion. So there could never have
been a ï¬rst change or motion: the
universe must have been moving
and changing for ever.
The great Arabic philosophers,
Avicenna and Averroes, were
willing to accept Aristotleâ€™s view,
even though it put them at odds
with Islamic orthodoxy. Medieval
Jewish and Christian thinkers,
however, struggled to do so. They
held that, according to the Bible,
the universe has a beginning, so
Aristotle must be wrong: the
universe has not always existed.
But was this view something that
had to be accepted on faith, or
could it be refuted by reasoning?
John Philoponus, a Greek
Christian writer of the 6thÂ century,
believed that he had found an
argument to show that Aristotle
must be wrong, and that the
universe had not always existed.
His reasoning was copied and
developed by a number of thinkers
in the 13th century, who needed to
ï¬nd a ï¬‚aw in Aristotleâ€™s reasoning
in order to protect the teachings of
the Church. Their line of argument
was especially clever, because it
took Aristotleâ€™s own ideas about
experienced something that
has been considered both some
sort of vision and a possible
stroke; after it, he said that all
he had done was â€œmere strawâ€,
and he never wrote again. He
died at the age of 49, and was
recognized as a saint by the
Catholic Church in 1323.
Key works
1256â€“59 Disputed Questions on
Truth
c.1265â€“74 Summa Theologica
1271 On the Eternity of the
Universe

THE MEDIEVAL WORLD 91
See also: Aristotle 56â€“63 â–  Avicenna 76â€“79
Pierre AbÃ©lard 333 â–  William of Ockham 334

â– 
â– 

Averroes 82â€“83 â–  John Philoponus 332
Immanuel Kant 164â€“71

Aristotle says that
the universe has
always existed.

â– 

John Duns Scotus 333

â– 

The Bible says that
the universe has not
always existed.

The world did have a
beginning, but God could
have created it in such a way
that it existed eternally.

Aquinas is ï¬‚anked by Aristotle
and Plato in The Triumph of Thomas
Aquinas. His understanding of ancient
philosophy was considered greater than
that of Averroes, who lies at his feet.

inï¬nity as a point of departure, but
turned them against his view of
the universe as eternal.

An inï¬nity of humans
According to Aristotle, the inï¬nite
is what has no limit. For instance,
the sequence of numbers is inï¬nite,
because for each number, there is
another higher number that follows.
Similarly, the universe has existed
for an inï¬nite time, because for each
day, there is a preceding day. In
Aristotleâ€™s opinion, however, this is
a â€œpotentialâ€ inï¬nity, as these days
do not coexist at the same time;
an â€œactualâ€ inï¬nityâ€”in which an
inï¬nite number of things all exist
at the same timeâ€”is impossible.
Philoponus and his 13th-century
followers, however, think that this
argument presents problems that
Aristotle had not noticed. They point
to the fact that he believes that all

the types of living beings in the
universe have always existed. If this
were true, they say, it would mean
that there were already an inï¬nite
number of human beings by the
time Socrates was bornâ€”because
if they have always existed, they
existed then. But since Socratesâ€™
time, many more humans have been
born, and so the number of humans
born up until now must be greater
than inï¬nity. But no number can
be greater than inï¬nity.
In addition, these writers add,
Christian thinkers believe that
human souls are immortal. If this
is so, and an inï¬nite number of
humans has already existed, there
must be an inï¬nite number of human
souls in existence now. So there is
an actual inï¬nity of souls, not a
potential inï¬nity; and Aristotle has
said actual inï¬nity is impossible.
With these two arguments,
using Aristotleâ€™s own principles as
a starting point, Philoponus and his
followers were conï¬dent they had
demonstrated that the universe
cannot always have existed.

Aristotle was therefore wrong; the
universe is not eternal, and this ï¬ts
perfectly with the Christian
doctrine that God created the world.
Aquinas has little time for this
line of reasoning. He points out that
the universe could have existed for
ever but that species such as
humans and other animals might
have had a beginning, and so the
difï¬culties raised by Philoponus
and his followers can be avoided.
Despite his defence of Aristotleâ€™s
reasoning, Aquinas does not â¯â¯

There never was
a time when there
was not motion.
Aristotle

92 THOMAS AQUINAS

God could have
made the universe
without humans and
then made them.
Thomas Aquinas

beginningâ€”but he also wants
to show that there is no ï¬‚aw in
Aristotleâ€™s reasoning. He claims
that his Christian contemporaries
have confused two different points:
the ï¬rst is that God created the
universe, and the second is that the
universe had a beginning. Aquinas
set out to prove that in fact
Aristotleâ€™s positionâ€”that the
universe has always existedâ€”
could be true, even if it is also true
that God created the universe.

Creating the eternal
accept Aristotleâ€™s assertion that the
universe is eternal, because the
Christian faith says otherwise; but
he doesnâ€™t think that Aristotleâ€™s
position is illogical. Like Philoponus
and his followers, Aquinas wants
to show that the universe had a

Aquinas steps away from Philoponus
and his followers by insisting that
although it is true, as the Bible says,
that the universe had a beginning,
this is not a necessary (undeniable)
truth on logical grounds. As they all
agree, God created the universe
with a beginning, but he could just

as easily have created an eternal
one. If something is created by God,
then it owes its whole existence to
God, but that does not mean that
there must have been a time when
it did not exist at all. It is therefore
quite possible to believe in an
eternal universe that had been
created by God.
Aquinas gives an example of
how this might work. Suppose
there was a foot making a footprint
in the sand and it had been there
for ever. Although there would
never have been a moment before
the footprint was made, we would
still recognize the foot as the cause
of the footprint: if it were not for the
foot, there would not be a footprint.

Aquinas and synthesis
Historians sometimes say that
Aquinas â€œsynthesizedâ€ Christianity
and Aristotelian philosophy, as if
he took the parts he wanted from
each and made them into a smooth
mixture. In fact, for Aquinasâ€”as
for most Christiansâ€”the teachings
of the Church must all be accepted,
without exception or compromise.
Aquinas was unusual, however,
because he thought that, properly
understood, Aristotle did not
contradict Christian teaching. The
question of whether the universe
always existed is the exception
that proves the rule. In this
particular case Aquinas thinks
that Aristotle was wrong, but he
was not wrong in principle, or in
his reasoning. The universe really
might have existed for ever, as far
as the ancient philosophers knew.
It was just that Aristotle, not having
access to Christian revelation, had
Aquinas believed the creation story
on faith, but claimed that some elements
of Christian belief could be rationally
demonstrated. For Aquinas, the Bible
and reason need never conï¬‚ict.

THE MEDIEVAL WORLD 93
Aristotle believed that the universe was inï¬nite,
as each hour and day is succeed by another. Aquinas
disagreed, believing that the universe had a beginning,
but his respect for Aristotleâ€™s philosophy led him to
argue that Aristotle could have been correct.

no way of knowing that it had not.
Aquinas believes that there are
a number of other doctrines central
to Christianity that the ancient
philosophers did not know and
could not have knownâ€”such as
the belief that God is a Trinity
made up of three persons, and that
one person of the Trinity, the Son,
became a human. But in Aquinasâ€™s
opinion, whenever humans reason
correctly, they cannot come to any
conclusion which contradicts
Christian doctrine. This is because
both human reason and Christian
teaching come from the same
sourceâ€”Godâ€”and so they can
never contradict each other.
Aquinas taught in convents
and universities in France and Italy,
and the idea that human reason
could never conï¬‚ict with Christian
doctrine often placed him in ï¬erce
conï¬‚ict with some of his academic
contemporaries, especially those
who specialized in the sciences,
which at the time were derived
from the work of Aristotle. Aquinas
accused his fellow scholars of
accepting certain positions on
faithâ€”for example, the position
that we each have an immortal

soulâ€”but of saying at the same
time that according to reason,
these positions could be shown
to be wrong.

How we gain knowledge
Aquinas keeps to these principles
throughout his work, but they are
particularly clear in two central
areas of his thought: his account
of how we gain knowledge and his
treatment of the relation between
mind and body. According to
Aquinas, human beings acquire
knowledge through using their
senses: sight, hearing, smell, touch,
and taste. These sense-impressions,
however, only tell us what things
are like superï¬cially. For example,
from where John sits, he has a visual
impression of a tree-shaped object,
which is green and brown. I, on the
other hand, am standing next to the
tree, and can feel the roughness of
its bark and smell the scent of the
forest. If John and I were dogs, our
knowledge of the tree would be
limited to these sense-impressions.
But as human beings we are able to
go beyond them and grasp what a
tree is in a rational way, deï¬ning it
and distinguishing it from other

types of plants and of living things.
Aquinas calls this â€œintellectual
knowledgeâ€, because we gain it
by using the innate power of our
intellect to seize, on the basis of
sense-impressions, the reality that
lies behind them. Animals other
than humans lack this inborn
capacity, which is why their
knowledge cannot stretch beyond
the senses. All of our scientiï¬c
understanding of the world is based
on this intellectual knowledge.
Aquinasâ€™s theory of knowledge
owes much to Aristotle, although
he clariï¬es and elaborates upon â¯â¯

We should see whether
there is a contradiction
between something being
created by God, and its
existing forever.
Thomas Aquinas

94 THOMAS AQUINAS
the latterâ€™s thinking. For Aquinas, as
a Christian thinker, human beings
are only one type of the various
sorts of beings that are capable of
knowing things intellectually: souls
separated from their bodies in the
afterlife, angels, and God himself
can also do this. These other
knowing beings do not have
to acquire knowledge through the
senses. They can directly grasp
the deï¬nitions of things. This
aspect of Aquinasâ€™s theory has
no parallel in Aristotle, but it is a
coherent development of Aristotleâ€™s
principles. Once again Aquinas
is able to hold Christian beliefs
without contradicting Aristotle,
but going beyond him.

The human soul
According to Aristotle, the intellect
is the life-principle or â€œsoulâ€ of a
human being. All living things have
a soul, he believes, which explains
their capacity for different levels of

what he calls â€œlife-activityâ€, such
as growing and reproducing, for
plants; moving, sensing, seeking,
and avoiding, for animals; and
thinking for humans.
Aristotle believes that â€œformâ€ is
what makes matter into the thing
that it is. Within the human body,
this form is the soul, which makes
the body into the living thing that
it is by giving it a particular set of
life-activities. As such, the soul is
tied to the body, and so Aristotle
thinks that, even in the case of
humans, the life-soul survives only
so long as it animates a body, and
at death it perishes.
Aquinas follows Aristotleâ€™s
teaching about living things and
their souls, and he insists that a
human being has just one form:
his or her intellect. Although other
13th- and 14th-century thinkers
also adopted the main lines of
Aristotleâ€™s view, they cut the
connection Aristotle had made

The laws of cause and effect lead us to look for the
cause of any event, even the beginning of the universe.
Aristotle supposed that God set the universe into
motion, and Aquinas agreed, but added that the
â€œPrime Moverâ€â€”Godâ€”must itself be uncaused.

?

between the intellect and the body,
so they could accommodate the
Christian teaching that the human
soul survives death. Aquinas,
however, refuses to distort
Aristotleâ€™s position. This made it
far more difï¬cult for him to argueâ€”
as he didâ€”for the immortality of
the human soul, in yet another
example of his resolve to be a good
Aristotelian, and philosopher, while
remaining a faithful Christian.

After Aquinas
Since the Middle Ages, Aquinas
has come to be regarded as the
ofï¬cial orthodox philosopher of
the Catholic Church. In his own
time, when translations of Greek
philosophy were being made from
Arabic, complete with Arabic
commentaries, he was one of the
thinkers keenest to follow Aristotleâ€™s
train of philosophical reasoning,
even when it did not ï¬t neatly with
Christian doctrine. He always

caused this newtonâ€™s cradle
to swing. But does the
existence of the universe
itself have a cause?

THE MEDIEVAL WORLD 95
Cosmic background radiation
provides evidence of the â€œBig Bangâ€
that started the universe, but we can
still argue, like Aquinas, that this was
not the only possible way for it to exist.

remained faithful to the Churchâ€™s
teachings, but this did not prevent
his thought from almost being
condemned as heretical shortly
after his death. The great thinkers
and teachers of the following
century, such as the secular
philosopher Henry of Ghent, and
the Franciscans John Duns Scotus
and William of Ockham, were all
far more willing to say that purely
philosophical reasoning, as best
represented by Aristotleâ€™s
arguments, is often mistaken.
Scotus thought that Aquinasâ€™s
Aristotelian view of the soul was
inadequate, and Ockham rejected
Aristotleâ€™s account of knowledge
almost entirely. Henry of Ghent
explicitly criticized Aquinasâ€™s view
that God could have created a
universe that always exists. If it
always existed, he argued, there
would be no possibility of its not
existing, and so God would not
have been free to create or not
create it. Aquinasâ€™s supreme
conï¬dence in the power of reason
meant that he had more in common
with the greatest philosopher of
the previous century, the French
philosopher and theologian Pierre
AbÃ©lard, than he did with his
contemporaries and successors.

Coherent belief
Both Aquinasâ€™s general view on
the relation between philosophy
and Christian doctrine, and his
particular treatment of the eternity
of the universe, remain relevant
in the 21st century. Today few
philosophers believe that religious
positions, such as the existence of
God or the immortality of the soul,

can be proved by philosophical
reasoning. But what some claim for
philosophy is that it can demonstrate
that although religious believers
hold certain doctrines as a matter
of faith, their overall views are no
less rational or coherent than those
of agnostics or atheists. This view
is an extension and development of
Aquinasâ€™s constant endeavor to
develop a philosophically coherent
system of thought, while holding
on to his Christian beliefs. Reading
Aquinasâ€™s works is a lesson in
tolerance, for Christians and
non-Christians alike.

science for an explanation of how
the universe began, the arguments
of Aquinas show that philosophy is
still relevant to how we think about
the subject. He demonstrates how
philosophy can provide the tools for
intelligent enquiry, allowing us to
investigate not what happens to be
the case, but what is possible and
what is impossible, and what are
intelligible questions to ask. Is it or
is it not coherent to believe that the
universe had a beginning? This is
still a question for philosophers, and
no amount of theoretical physics
will be able to answer it. â– 

The role of philosophy
Today, we do not look to philosophy
to tell us whether or not the universe
has always existed, and most of us
do not turn to the Bible, as Aquinas
and other medieval philosophers
did. Instead we look to physics,
in particular to the theory of the
â€œBig Bangâ€ proposed by modern
scientists, including the British
physicist and cosmologist Stephen
Hawking. This theory states that
the universe expanded from a state
of extremely high temperature and
density at a particular point in time.
Though most of us now turn to

One may say that
time had a beginning at
the Big Bang, in the sense
that earlier times simply
would not be deï¬ned.
Stephen Hawking

96

GOD IS THE
NOT-OTHER

NIKOLAUS VON KUES (1401â€“1464)

IN CONTEXT
BRANCH
Philosophy of religion
APPROACH
Christian Platonism
BEFORE
380â€“360 BCE Plato writes on
â€œthe Goodâ€ or â€œthe Oneâ€ as
the ultimate source of reason,
knowledge, and all existence.
Late 5th century CE
The Greek theologian and
philosopher Dionysius the
Areopagite describes God
as â€œabove being.â€
c.860 Johannes Scotus
Eriugena promotes the ideas
of Dionysius the Areopagite.
AFTER
1492 Giovanni Pico della
Mirandolaâ€™s On Being and
the One marks a turning
point in Renaissance
thinking about God.
1991 French philosopher
Jean-Luc Marion explores the
theme of God as not a being.

N

ikolaus von Kues belongs
to a long tradition of
medieval philosophers
who attempt to describe the nature
of God, stressing how God is unlike
anything that the human mind is
capable of grasping. Von Kues
begins with the idea that we gain
knowledge by using our reason to
deï¬ne things. So in order to know
God, he deduces that we must try
to deï¬ne the basic nature of God.
Plato describes â€œthe Goodâ€ or
â€œthe Oneâ€ as the ultimate source of
all other forms and knowledge, and

some early Christian theologians
talk of God as â€œabove being.â€ Von
Kues, writing around 1440, goes
further, stating that God is what
comes before everything, even
before the possibility of something
existing. Yet reason tells us the
possibility of any phenomenon
existing must come before its
actual existence. It is impossible
for something to come into being
before the possibility of it arises.
The conclusion that von Kues
comes to, therefore, is that
something that is said to do this
must be described as â€œNot-other.â€

Beyond apprehension

Whatever-I-know
is not God and
whatever-I-conceive
is not like God.
Nikolaus von Kues

However, the use of the word
â€œthingâ€ in the line of reasoning that
von Kues adopts is misleading, as
the â€œNot-otherâ€ has no substance.
It is, according to von Kues, â€œbeyond
apprehensionâ€, and is before all
things in such a way that â€œthey
are not subsequent to it, but exist
through it.â€ For this reason too,
von Kues thinks â€œNot-otherâ€ comes
closer to a deï¬nition of God than
any other term. â– 

See also: Plato 50â€“55 â–  Johannes Scotus Eriugena 332
Giovanni Pico della Mirandola 334

â– 

Meister Eckhart 333

â– 

THE MEDIEVAL WORLD 97

TO KNOW NOTHING
IS
THE
HAPPIEST
LIFE
DESIDERIUS ERASMUS (1466â€“1536)
IN CONTEXT
BRANCH
Philosophy of religion
APPROACH
Humanism
BEFORE
354â€“430 CE St. Augustine
of Hippo integrates Platonism
into Christianity.
c.1265â€“1274 Thomas Aquinas
combines Aristotelian and
Christian philosophy in his
Summa Theologica.
AFTER
1517 Theologian Martin
Luther writes The Ninety-Five
Theses, protesting against
clerical abuses. It triggers the
start of the Reformation.
1637 RenÃ© Descartes writes
Discourse on the Method,
putting human beings at the
center of philosophy.
1689 John Locke argues
for separation of government
and religion in A Letter
Concerning Toleration.

T

he treatise In Praise
of Folly, which Erasmus
wrote in 1509, reï¬‚ects
the Humanist ideas that were
beginning to ï¬‚ood across Europe
during the early years of the
Renaissance, and were to play
a key role in the Reformation. It
is a witty satire on the corruption
and doctrinal wranglings of the
Catholic Church. However, it also
has a serious message, stating that
follyâ€”by which Erasmus meant
naive ignoranceâ€”is an essential
part of being human, and is what
ultimately brings us the most
happiness and contentment. He
goes on to claim that knowledge,
on the other hand, can be a burden
and can lead to complications that
may make for a troublesome life.

Faith and folly
Religion is a form of folly too,
Erasmus states, in that true belief
can only ever be based on faith,
never on reason. He dismisses the
mixing of ancient Greek rationalism
with Christian theology by medieval
philosophers, such as St. Augustine

of Hippo and Thomas Aquinas,
as theological intellectualizing,
claiming that it is the root cause
of the corruption of religious faith.
Instead, Erasmus advocates a
return to simple heartfelt beliefs,
with individuals forming a personal
relationship with God, and not one
prescribed by Catholic doctrine.
Erasmus advises us to embrace
what he sees as the true spirit of
the Scripturesâ€”simplicity, naivety,
and humility. These, he says, are
the fundamental human traits that
hold the key to a happy life. â– 

Happiness is
reached when a
person is ready to
be what he is.
Desiderius Erasmus

See also: St. Augustine of Hippo 72â€“73 â–  Thomas Aquinas 88â€“95
RenÃ© Descartes 116â€“23 â–  John Locke 130â€“33

â– 

RENAISS
AND THE
OF
REAS
1500â€“1750

ANCE
AGE
ON

100 INTRODUCTION

NiccolÃ²
Machiavelli
publishes
The Prince.

Nicolaus Copernicus proposes
that Earth orbits the Sun,
in opposition to the Christian
view that Earth lies at the
center of the universe.

Francis Baconâ€™s
New Organon is
published, proposing
a new approach to
investigating nature.

RenÃ© Descartes
writes his Meditations.

1513

1543

1620

1641

T

1517

1593

1633

1644

Martin Luther nails his
95Â Theses to the door of
Castle Church in
Wittenberg, triggering
the Reformation.

The Edict of Nantes
is issued by Henri IV,
granting Protestants rights
within Catholic France.

Galileo Galilei is
excommunicated by the
Church and imprisoned
for life, for upholding
the theory that Earth
revolves around the Sun.

The last ruling
dynasty of
China, the Qing
(Manchu) dynasty,
takes power.

he Renaissanceâ€”a cultural
â€œrebirthâ€ of extraordinary
creativity in Europeâ€”began
in 14th-century Florence. It was to
spread across Europe, lasting until
the 17th century, and it is now
viewed as the bridge between the
medieval and modern periods.
Marked by a renewed interest in the
whole of Greek and Latin Classical
cultureâ€”not just the philosophical
and mathematical texts assimilated
by medieval Scholasticismâ€”it was
a movement that viewed humans,
not God, at its center. This new
humanism was reï¬‚ected ï¬rst in the
art and then the political and social
structure of Italian society; republics
such as Florence and Venice soon
abandoned medieval feudalism
in favor of plutocracies where
commerce ï¬‚ourished alongside
the new scientiï¬c discoveries.

By the end of the 15th century,
Renaissance ideas had spread
across Europe and virtually eclipsed
the Churchâ€™s monopoly of learning.
Although Christian philosophers
such as Erasmus and Thomas More
had contributed to the arguments
within the Church that had sparked
the Reformation, a purely secular
philosophy had yet to emerge.
Unsurprisingly, the ï¬rst truly
Renaissance philosopher was a
Florentine â€“ NiccolÃ² Machiavelli â€“
and his philosophy marked a
deï¬nitive movement from the
theological to the political.

The Age of Reason
The ï¬nal nail in the cofï¬n of the
Churchâ€™s authority came from
science. First Nicolaus Copernicus,
then Johannes Kepler, and ï¬nally
Galileo Galilei showed that the

Ptolemaic model of the universe
with Earth at its center was
mistaken, and their demonstrations
overturned centuries of Christian
teaching. The Church fought back,
ultimately imprisoning Galileo for
heresy, but advances in all the
sciences soon followed those in
astronomy, providing alternative
explanations for the workings of
the universe, and a basis for a new
kind of philosophy.
The victory of rational, scientiï¬c
discovery over Christian dogma
epitomized the thinking of the
17th century. British philosophers,
notably Francis Bacon and Thomas
Hobbes, took the lead in integrating
scientiï¬c and philosophical
reasoning. It was the beginning
of a period that became known as
the Age of Reason, which produced
the ï¬rst great â€œmodernâ€ philosophers

RENAISSANCE AND THE AGE OF REASON 101

The execution of King
Charles I brings an
end to the English
Civil War.

Isaac Newton
begins compiling his
notes on â€œCertain
Philosophical
Questions.â€

John Locke publishes
An Essay concerning
Human Understanding.

George Berkeley
publishes A Treatise
Concerning the Principles
of Human Knowledge.

1649

1664

1690

1710

1651

1670

1704

1721

Thomas Hobbesâ€™ great
political work, Leviathan,
is published.

Blaise Pascalâ€™s
PensÃ©es are published
posthumously.

Gottfried Leibniz
writes New Essays on
Human Understanding.

Britainâ€™s ï¬rst factory
opens, accelerating
the Industrial
Revolution.

and revived the connection between
philosophy and science, especially
mathematics, that dated back to
pre-Socratic Greece.

The birth of rationalism
In the 17th century, many of the
most signiï¬cant philosophers in
Europe were also accomplished
mathematicians. In France, RenÃ©
Descartes and Blaise Pascal made
major contributions to mathematics,
as did Gottfried Leibniz in Germany.
They believed that its reasoning
process provided the best model for
how to acquire all our knowledge of
the world. Descartesâ€™s investigation
of the question â€œWhat can I know?â€
led him to a position of rationalism,
which is the belief that knowledge
comes from reason alone. This
became the predominant belief in
continental Europe for the next

century. At the same time, a very
different philosophical tradition
was being established in Britain.
Following the scientiï¬c reasoning
espoused by Francis Bacon, John
Locke came to the conclusion that
our knowledge of the world comes
not from reason, but experience.
This view, known as empiricism,
characterized British philosophy
during the 17th and 18th centuries.
Despite the division between
continental rationalism and British
empiricism (the same division that
had separated the philosophies of
Plato and Aristotle), both had in
common the placing of the human
at their centers: it is this being
whose reason or experience leads
to knowledge. Philosophers on both
sides of the Channel had moved
from asking questions about the
nature of the universeâ€”which were

being answered by scientists such
as Isaac Newtonâ€”to questioning
how we can know what we know,
and they now began to investigate
the nature of the human mind and
self. But these new philosophical
strands had moral and political
implications. Just as the Churchâ€™s
authority had been undermined by
the ideas of the Renaissance, so the
aristocracies and monarchies were
threatened by the new ideas of the
Enlightenment, as this period came
to be known. If the old rulers were
removed from power, what sort of
society was to replace them?
In Britain, Hobbes and Locke
had laid the foundations for
democratic thinking during the
turbulent 17th century, but it was
another 100 years before a
questioning of the status quo
began in earnest elsewhere. â– 

THE END
JUSTIFIES
THE MEANS

NICCOLO MACHIAVELLI (1469â€“1527)

104 NICCOLO MACHIAVELLI
IN CONTEXT
BRANCH
Political philosophy

The success of a state
or nation is paramount.

APPROACH
Realism
BEFORE
1st century BCE Plato argues
in his Republic that the state
should be governed by a
philosopher-king.
1st century BCE The Roman
writer Cicero argues that the
Roman Republic is the best
form of government.
AFTER
16th century Machiavelliâ€™s
peers begin to use the adjective
â€œMachiavellianâ€ to describe
acts of devious cunning.
1762 Jean-Jacques Rousseau
argues that people should hold
on to their liberty and resist
the rule of princes.
1928 Italian dictator Benito
Mussolini describes The
Prince as â€œthe statesmanâ€™s
supreme guide.â€

I

n order fully to understand
Machiavelliâ€™s views on power,
it is necessary to understand
the background to his political
concerns. Machiavelli was born in
Florence, Italy, during a time of
almost constant upheaval. The
Medici family had been in open but
unofï¬cial control of the city-state
for some 35 years, and the year of
Machiavelliâ€™s birth saw Lorenzo deâ€™
Medici (Lorenzo the Magniï¬cent)
succeed his father as ruler, ushering
in a period of great artistic activity
in Florence. Lorenzo was succeeded
in 1492 by his son Piero (known as

Whoever governs the
state or nation must
strive to secure...

...his or her own glory.

...the success of the state.

In order to do this, they
cannot be bound by morality.

The end justiï¬es
the means.

Piero the Unfortunate), whose
reign was short-lived. The French
under Charles VIII invaded Italy in
considerable force in 1494, and
Piero was forced to surrender and
then ï¬‚ee the city, as the citizens
rebelled against him. Florence was
declared a republic that same year.
The Dominican prior of the
San Marco monastery, Girolamo
Savonarola, then came to dominate
Florentine political life. The citystate entered a democratic period
under his guidance, but after
accusing the pope of corruption
Savonarola was eventually arrested

and burnt as a heretic. This led
to Machiavelliâ€™s ï¬rst known
involvement in Florentine politics,
and he became Secretary to the
second Chancery in 1498.

Career and inï¬‚uences
The invasion by Charles VIII in
1494 had sparked a turbulent period
in the history of Italy, which at the
time was divided into ï¬ve powers:
the papacy, Naples, Venice, Milan,
and Florence. The country was
fought over by various foreign
powers, mainly France, Spain, and
the Holy Roman Empire. Florence

RENAISSANCE AND THE AGE OF REASON 105
See also: Plato 50â€“55

â– 

Francis Bacon 110â€“11

â– 

Jean-Jacques Rousseau 154â€“59

â– 

Karl Marx 196â€“203

Lorenzo the Magniï¬cent (1449â€“1492)
effectively ruled Florence from the
death of his father in 1469 until his
death. Though he ruled as a despot, the
republic ï¬‚ourished under his guidance.

Machiavelli was released from
prison within a month, but his
chances of re-employment were
slim, and his attempts to ï¬nd a new
political position came to nothing.
He decided to present the head of
the deâ€™ Medici family in Florence,
Giuliano, with a book. By the time
it was ready Giuliano had died, so
Machiavelli changed the dedication
to Giulianoâ€™s successor, Lorenzo.
The book was of a type popular at
the time: advice to a prince.

The Prince

was weak in the face of their armies,
and Machiavelli spent 14 years
travelling between various cities
on diplomatic missions, trying to
shore up the struggling republic.
In the course of his diplomatic
activities, Machiavelli met Cesare
Borgia, the illegitimate son of Pope
Alexander VI. The pope was a
powerful ï¬gure in northern Italy,
and a signiï¬cant threat to Florence.
Although Cesare was Florenceâ€™s
enemy, Machiavelliâ€”despite his
republican viewsâ€”was impressed
by his vigor, intelligence, and
ability. Here we see one of the
sources for Machiavelliâ€™s famous
work, The Prince.
Pope Alexander VI died in 1503,
and his successor Pope Julius II
was another strong and successful

man who impressed Machiavelli
with both his military ability and
his cunning. But tension between
France and the papacy led to
Florence ï¬ghting with the French
against the pope and his allies,
the Spanish. The French lost, and
Florence with them. In 1512 the
Spanish dissolved the city-stateâ€™s
government, the Medicis returned,
and what was in effect a tyranny
under Cardinal deâ€™ Medici was
installed. Machiavelli was ï¬red
from his political ofï¬ce and exiled
to his farm in Florence. His political
career might have revived under
the rule of the Medicis, but in
February 1513 he was falsely
implicated in a plot against the
family, and he was tortured,
ï¬ned, and imprisoned.

Machiavelliâ€™s book The Prince was
witty and cynical, and showed a
great understanding of Italy in
general and Florence in particular.
In it, Machiavelli sets out his
argument that the goals of a ruler
justify the means used to obtain
them. The Prince differed markedly
from other books of its type in its
resolute setting aside of Christian
morality. Machiavelli wanted to â¯â¯

How difï¬cult it is
for a people accustomed
to live under a prince to
preserve their liberty!
NiccolÃ² Machiavelli

106 NICCOLO MACHIAVELLI
give ruthlessly practical advice to a
prince and, as his experience with
extremely successful popes and
cardinals had shown him, Christian
values should be cast aside if they
got in the way.
Machiavelliâ€™s approach centers
on the notion of virtÃ¹, but this is not
the modern notion of moral virtue.
It shares more similarities with the
medieval notion of virtues as the
powers or functions of things, such
as the healing powers of plants or
minerals. Machiavelli is writing
about the virtues of princes, and
these were the powers and functions
that concerned rule. The Latin root
of virtÃ¹ also relates it to manliness
(as in â€œvirileâ€), and this feeds into
what Machiavelli has to say in
its application both to the prince
himself and to the stateâ€”where

sometimes virtÃ¹ is used to mean
â€œsuccessâ€, and describes a state
that is to be admired and imitated.
Part of Machiavelliâ€™s point is
that a ruler cannot be bound by
morality, but must do what it takes
to secure his own glory and the
success of the state over which he
rulesâ€”an approach that became
known as realism. But Machiavelli
does not argue that the end justiï¬es
the means in all cases. There are
certain means that a wise prince
must avoid, for though they might
achieve the desired ends, they lay
him open to future dangers.
The main means to be avoided
consist of those that would make
the people hate their prince. They
may love him, they may fear himâ€”
preferably both, Machiavelli says,
though it is more important for a

Prince or republic

A ruler needs to know how to act
like a beast, Machiavelli says in The
Prince, and must imitate the qualities
of the fox as well as the lion.

A ruler must have the
ferocity of the lion to
frighten those who seek
to depose him.

prince to be feared than to be
loved. But the people must not
hate him, for this is likely to lead
to rebellion. Also, a prince who
mistreats his people unnecessarily
will be despisedâ€”a prince should
have a reputation for compassion,
not for cruelty. This might involve
harsh punishment of a few in order
to achieve general social order,
which beneï¬ts more people in
the long run.
In cases where Machiavelli
does think that the end justiï¬es
the means, this rule applies only
to princes. The proper conduct of
citizens of the state is not at all the
same as that of the prince. But even
for ordinary citizens, Machiavelli
generally disdains conventional
Christian morality as being weak
and unsuitable for a strong city.

There are reasons to suspect that
The Prince does not represent
Machiavelliâ€™s own views. Perhaps
the most important is the disparity
between the ideas it contains and
those expressed in his other main
work, Discourses on the Ten Books
of Titus Livy. In the Discourses
Machiavelli argues that a republic
is the ideal regime, and that it

A ruler must have the
cunning of the fox
to recognize snares
and traps.

It must be understood
that a prince cannot
observe all those things
which are considered
good in men.
NiccolÃ² Machiavelli

RENAISSANCE AND THE AGE OF REASON 107
should be instituted whenever a
reasonable degree of equality
exists or can be established. A
princedom is only suitable when
equality does not exist in a state,
and cannot be introduced. However,
it can be argued that The Prince
represents Machiavelliâ€™s genuine
ideas about how the ruler should
rule in such cases; if princedoms
are sometimes a necessary evil, it
is best that they be ruled as well as
possible. Moreover, Machiavelli did
believe that Florence was in such
political turmoil that it needed a
strong ruler to get it into shape.

Ruthlessness has been a virtue of
leadership throughout history. In the
20th century, the fascist dictator Benito
Mussolini used a mixture of fear and
love to hold on to power in Italy.

The world has become more
like that of Machiavelli.
Bertrand Russell

Pleasing the readers
The fact that The Prince was
written by Machiavelli in order to
ingratiate himself with the Medicis
is another reason to treat its
contents with caution. However, he
also dedicated the Discourses to
members of Florenceâ€™s republican
government. Machiavelli, it could
be argued, would have written
what the dedicatee wanted to read.
The Prince, however, contains
much that Machiavelli is thought to
have genuinely believed, such as
the need for a citizensâ€™ militia
rather than reliance on mercenaries.

NiccolÃ² Machiavelli

The problem lies in discerning
which parts are his actual beliefs
and which are not. It is tempting to
divide them according to how well
they ï¬t with the intended readerâ€™s
own beliefs, but that is unlikely to
give an accurate result.
It has also been suggested that
Machiavelli was attempting satire,
and his real intended audience was
the republicans, not the ruling elite.
This idea is supported by the fact
that Machiavelli did not write it in

Latin, the language of the elite, but
in Italian, the language of the people.
Certainly, The Prince at times reads
satirically, as though the audience
is expected to conclude: â€œif that is
how a good prince should behave,
we should at all costs avoid being
ruled by one!â€ If Machiavelli was
also satirizing the idea that â€œthe
end justiï¬es the meansâ€, then the
purpose of this small, deceptively
simple book is far more intriguing
than one might originally assume. â– 

Machiavelli was born in Florence
in 1469. Little is known of the ï¬rst
28 years of his life; apart from a
few inconclusive mentions in his
fatherâ€™s diary, the ï¬rst direct
evidence is a business letter
written in 1497. From his writings,
though, it is clear that he received
a good education, perhaps at the
University of Florence.
By 1498, Machiavelli had
become a politician and diplomat
of the Florentine Republic. After
his enforced retirement on the
return of the Medicis to Florence
in 1512, he devoted himself to
various literary activities, as well

as persistent attempts to return
to the political arena. Eventually
he regained the trust of the
Medicis, and Cardinal Giulio
deâ€™ Medici commissioned him to
write a history of Florence. The
book was ï¬nished in 1525, after
the cardinal had become Pope
Clement VII. Machiavelli died
in 1527, without achieving his
ambition to return to public life.
Key works
1513 The Prince
1517 Discourses on the Ten
Books of Titus Livy

108

FAME AND
TRANQUILLITY
CAN NEVER BE
BEDFELLOWS
MICHEL DE MONTAIGNE (1533â€“1592)
IN CONTEXT
BRANCH
Ethics

Tranquillity depends
upon detachment
from the opinion of others.

APPROACH
Humanism
BEFORE
4th century BCE Aristotle,
in his Nicomachean Ethics,
argues that to be virtuous, a
person must be sociable and
form close relationships with
others; only a bestial man or
a god can ï¬‚ourish alone.
AFTER
Late 18th century Anglican
evangelical clergyman Richard
Cecil states, â€œSolitude shows
us what we should be; society
shows us what we are.â€
Late 19th century Friedrich
Nietzsche describes solitude
as necessary to the task of
self-examination, which he
claims can alone free humans
from the temptation just to
thoughtlessly follow the mob.

If we seek fameâ€”which
is glory in the eyes of
othersâ€”we must seek
their good opinion.

If we seek fame
we cannot
reach detachment.

Fame and
tranquillity can
never be bedfellows.

I

n his essay â€œOn Solitudeâ€
(from the ï¬rst volume of his
Essays), Montaigne takes up a
theme that has been popular since
ancient times: the intellectual and
moral dangers of living among
others, and the value of solitude.
Montaigne is not stressing the
importance of physical solitude, but
rather of developing the ability to
resist the temptation to mindlessly
fall in with the opinion and actions
of the mob. He compares our desire
for the approval of our fellow humans
to being overly attached to material
wealth and possessions. Both
passions diminish us, Montaigne
claims, but he does not conclude
that we should relinquish either,
only that we should cultivate a
detachment from them. By doing so,
we may enjoy themâ€”and even
beneï¬t from themâ€”but we will not
become emotionally enslaved to
them, or devastated if we lose them.
â€œOn Solitudeâ€ then considers
how our desire for mass approval
is linked to the pursuit of glory, or
fame. Contrary to thinkers such
as NiccolÃ² Machiavelli, who see
glory as a worthy goal, Montaigne
believes that constant striving
for fame is the greatest barrier to
peace of mind, or tranquility. He

RENAISSANCE AND THE AGE OF REASON 109
See also: Aristotle 56â€“63 â–  NiccolÃ² Machiavelli 102â€“07
Friedrich Nietzsche 214â€“21

says of those who present glory as a
desirable goal that they â€œonly have
their arms and legs out of the
crowd; their souls, their wills, are
more engaged with it than ever.â€
Montaigne is not concerned
with whether or not we achieve
glory. His point is that we should
shake off the desire for glory in the
eyes of other peopleâ€”that we
should not always think of other
peopleâ€™s approval and admiration
as being valuable. He goes on to
recommend that instead of looking
for the approbation of those around
us, we should imagine that some
truly great and noble being is
constantly with us, able to observe
our most private thoughts, a being
in whose presence even the mad
would hide their failings. By doing
this, we will learn to think clearly
and objectively and behave in a
more thoughtful and rational
manner. Montaigne claims that
caring too much about the opinion

â– 

of those around us will corrupt us,
either because we end up imitating
those who are evil, or become so
consumed by hatred for them that
we lose our reason.

Gloryâ€™s pitfalls
Montaigne returns to his attack
on the pursuit of glory in his later
writings, pointing out that the
acquisition of glory is often so
much a matter of mere chance
that it makes little sense to hold it
in such reverence. â€œMany times Iâ€™ve
seen [fortune] stepping out ahead
of merit, and often a long way
ahead,â€ he writes. He also points
out that encouraging statesmen
and political leaders to value glory
above all things, as Machiavelli
does, merely teaches them never
to attempt any endeavor unless
an approving audience is on hand,
ready and eager to bear witness to
the remarkable nature of their
powers and achievements. â– 

Contagion is very
dangerous in crowds. You
must either imitate the
vicious or hate them.
Michel de Montaigne

Michel de Montaigne
Michel Eyquem de Montaigne
was born and brought up in
his wealthy familyâ€™s chateau
near Bordeaux. However, he
was sent to live with a poor
peasant family until the age
of three, so that he would be
familiar with the life led by
the ordinary workers. He
received all his education at
home, and was allowed to
speak only Latin until the age
of six. French was effectively
his second language.
From 1557, Montaigne
spent 13 years as a member
of his local parliament, but
resigned in 1571, on inheriting
the family estates.
Montaigne published his
ï¬rst volume of Essays in 1580,
going on to write two more
volumes before his death in
1592. In 1580, he also set out
on an extensive tour of Europe,
partly to seek a cure for kidney
stones. He returned to politics
in 1581, when he was elected
Mayor of Bordeaux, an ofï¬ce
he held until 1585.
Key works

Montaigne experienced the results
of mindless mob violence during the
French Wars of Religion (1562â€“98),
including the atrocities of the St.
Bartholomew Day Massacre of 1572.

1569 In Defence of
Raymond Sebond
1580â€“1581 Travel Journal
1580, 1588, 1595 Essays
(3 volumes)

110

KNOWLEDGE
IS POWER
FRANCIS BACON (1561â€“1626)

IN CONTEXT
BRANCH
Philosophy of science
APPROACH
Empiricism
BEFORE
4th century BCE Aristotle
sets observation and inductive
reasoning at the center of
scientiï¬c thinking.
13th century English scholars
Robert Grosseteste and Roger
Bacon add experimentation to
Aristotleâ€™s inductive approach
to scientiï¬c knowledge.
AFTER
1739 David Humeâ€™s Treatise
of Human Nature argues
against the rationality of
inductive thinking.

B

acon is often credited with
being the ï¬rst in a tradition
of thought known as British
empiricism, which is characterized
by the view that all knowledge
must come ultimately from sensory
experience. He was born at a time
when there was a shift from the
Renaissance preoccupation with
the rediscovered achievements of
the ancient world toward a more
scientiï¬c approach to knowledge.
There had already been some
innovative work by Renaissance
scientists such as the astronomer
Nicolaus Copernicus and the
anatomist Andreas Vesalius, but
this new periodâ€”sometimes called

Scientiï¬c knowledge
builds upon itself.

1843 John Stuart Millâ€™s
System of Logic outlines the
ï¬ve inductive principles that
together regulate the sciences.
1934 Karl Popper states that
falsiï¬cation, not induction,
deï¬nes the scientiï¬c method.

Knowledge is
power.

the Scientiï¬c Revolutionâ€”produced
an astonishing number of scientiï¬c
thinkers, including Galileo Galilei,
William Harvey, Robert Boyle,
Robert Hooke, and Isaac Newton.
Although the Church had been
broadly welcoming to science for
much of the medieval period, this
was halted by the rise of opposition
to the Vaticanâ€™s authority during
the Renaissance. Several religious
reformers, such as Martin Luther,
had complained that the Church
had been too lax in countering
scientiï¬c challenges to accounts
of the world based on the Bible.
In response, the Catholic Church,
which had already lost adherents to

It advances steadily and
cumulatively, discovering
new laws and making new
inventions possible.

It enables people to do
things that otherwise
could not be done.

RENAISSANCE AND THE AGE OF REASON 111
See also: Aristotle 56â€“63 â–  Robert Grosseteste 333
John Stuart Mill 190â€“93 â–  Karl Popper 262â€“65

â– 

David Hume 148â€“53

â– 

preconceptions on nature rather
than to see what is really there;
the â€œidols of the marketplaceâ€, our
tendency to let social conventions
distort our experience; and the
â€œidols of the theaterâ€, the distorting
inï¬‚uence of prevailing philosophical
and scientiï¬c dogma. The scientist,
according to Bacon, must battle
against all these handicaps to gain
knowledge of the world.

Scientiï¬c method
Science, not religion, was regarded
increasingly as the key to knowledge
from the 16th century onward. This 1598
print depicts the observatory of Danish
astronomer Tycho Brahe (1546â€“1601).

Lutherâ€™s new form of Christianity,
changed its stance and turned
against scientiï¬c endeavor. This
opposition, from both sides of the
religious divide, hampered the
development of the sciences.
Bacon claims to accept the
teachings of the Christian Church.
But he also argues that science
must be separated from religion,
in order to make the acquisition of
knowledge quicker and easier, so
that it can be used to improve the
quality of peopleâ€™s lives. Bacon
stresses this transforming role for
science. One of his complaints is
that scienceâ€™s ability to enhance
human existence had previously
been ignored, in favor of a focus on
academic and personal glory.
Bacon presents a list of the
psychological barriers to pursuing
scientiï¬c knowledge in terms that
he calls collectively the â€œidols of
the mind.â€ These are the â€œidols of
the tribeâ€, the tendency of human
beings as a species (or â€œtribeâ€) to
generalize; the â€œidols of the caveâ€,
the human tendency to impose

Bacon goes on to argue that the
advancement of science depends on
formulating laws of ever-increasing
generality. He proposes a scientiï¬c
method that includes a variation of
this approach. Instead of making
a series of observations, such as
instances of metals that expand
when heated, and then concluding
that heat must cause all metals to
expand, he stresses the need to
test a new theory by going on to
look for negative instancesâ€”such
as metals not expanding when
they are heated.
Baconâ€™s inï¬‚uence led to a focus
on practical experimentation in
science. He was, however, criticized
for neglecting the importance of
the imaginative leaps that drive all
scientiï¬c progress. â– 

By far the best proof
is experience.
Francis Bacon

Francis Bacon
Born in London, Francis Bacon
was educated privately, before
being sent to Trinity College,
Cambridge, at the age of 12.
After graduation, he started
training as a lawyer, but
abandoned his studies to
take up a diplomatic post in
France. His fatherâ€™s death in
1579 left him impoverished,
forcing him to return to the
legal profession.
Bacon was elected to
parliament in 1584, but his
friendship with the treasonous
Earl of Essex held back his
political career until the
accession of James I in 1603.
In 1618, he was appointed Lord
Chancellor, but was dismissed
two years later, when he was
convicted of accepting bribes.
Bacon spent the rest of his
life writing and carrying out
his scientiï¬c work. He died
from bronchitis, contracted
while stufï¬ng a chicken with
snow, as part of an experiment
in food preservation.
Key works
1597 Essays
1605 The Advancement
of Learning
1620 Novum Organum
1624 Nova Atlantis

112

MAN IS A
MACHINE

THOMAS HOBBES (1588â€“1679)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Physicalism
BEFORE
4th century BCE Aristotle
disagrees with Platoâ€™s theory
of a distinct human soul and
argues that the soul is a form
or function of the body.
1641 RenÃ© Descartes
publishes his Meditations on
First Philosophy, arguing that
mind and body are completely
different and distinct entities.
AFTER
1748 Julien Offray de la
Mettrieâ€™s The Man Machine
presents a mechanistic view
of human beings.
1949 Gilbert Ryle states that
Descartesâ€™ idea that mind and
body are separate â€œsubstancesâ€
is a â€œcategory mistake.â€

A

lthough he is best known
for his political philosophy,
Thomas Hobbes wrote on a
wide range of subjects. Many of his
views are controversial, not least
his defence of physicalismâ€”the
theory that everything in the world
is exclusively physical in nature,
allowing no room for the existence
of other natural entities, such as the
mind, or for supernatural beings.
According to Hobbes, all animals,
including humans, are nothing more
than ï¬‚esh-and-blood machines.
The kind of metaphysical theory
that Hobbes favors was becoming
increasingly popular at the time of
his writing, in the mid-17th century.
Knowledge in the physical sciences

RENAISSANCE AND THE AGE OF REASON 113
See also: Aristotle 56â€“63 â–  Francis Bacon 110â€“11
Julien Offray de la Mettrie 335 â–  Gilbert Ryle 337

Nothing without
substance can exist.

â– 

RenÃ© Descartes 116â€“23

â– 

So everything in the
universe is physical.

Thomas Hobbes
Man is a
machine.

was growing rapidly, bringing
clearer explanations of phenomena
that had long been obscure or
misunderstood. Hobbes had met
the Italian astronomer Galileo,
frequently regarded as the â€œfather
of modern scienceâ€, and had been
closely associated with Francis
Bacon, whose thinking had helped
to revolutionize scientiï¬c practice.
In science and mathematics,
Hobbes saw the perfect counter to
the medieval Scholastic philosophy
that had sought to reconcile the
apparent contradictions between
reason and faith. In common with
many thinkers of his time, he
believed there was no limit to what
science could achieve, taking it as
a matter of fact that any question
about the nature of the world could
be answered with a scientiï¬cally
formulated explanation.

Hobbesâ€™ theory
In Leviathan, his major political
work, Hobbes proclaims: â€œThe
universeâ€”that is, the whole mass
of things that areâ€”is corporeal,

A human
being is therefore
entirely physical.

that is to say, body.â€ He goes on to
say that each of these bodies has
â€œlength, breadth, and depthâ€, and
â€œthat which is not body is no part
of the universe.â€ Although Hobbes
is stating that the nature of
everything is purely physical, he
is not claiming that because of
this physicality everything can be
perceived by us. Some bodies or
objects, Hobbes declares, are
imperceptible, even though they
occupy physical space and have
physical dimensions. These, he
calls â€œspirits.â€ Some of them, â¯â¯

Life is but
a motion of limbs.
Thomas Hobbes

Orphaned in infancy, Thomas
Hobbes was fortunately taken
in by a wealthy uncle, who
offered him a good education.
A degree from the University
of Oxford earned him the post
of tutor to the sons of the Earl
of Devonshire. This job gave
Hobbes the opportunity to
travel widely throughout
Europe, where he met noted
scientists and thinkers, such
as the Italian astronomer
Galileo Galilei as well as the
French philosophers Marin
Mersenne, Pierre Gassendi,
and RenÃ© Descartes.
In 1640, Hobbes ï¬‚ed to
France to escape the English
Civil War, staying there for
11 years. His ï¬rst book, De
Cive, was published in Paris in
1642. But it was his ideas on
morality, politics, and the
functions of society and the
state, set out in Leviathan,
that made him famous.
Also respected as a skilled
translator and mathematician,
Hobbes continued to write until
his death at the age of 91.
Key works
1642 De Cive
1651 Leviathan
1656 De Corpore
1658 De Homine

114 THOMAS HOBBES
Hobbes believed that â€œspiritsâ€ carried
information needed to function around
the body. We now know that this is done
by electrical signals, travelling along
the neurons of the nervous system.

labelled â€œanimal spiritsâ€ (in line
with a common view at the time)
are responsible for most animal,
and especially human, activity.
These animal spirits move around
the body, carrying with them and
passing on information, in much
the same way as we now think of
the nervous system doing.
Sometimes, Hobbes seems to
apply his concept of physical spirits
to God and other entities found in
religion, such as angels. However,
he does state that God himself,
but not other physical spirits, should
be described as â€œincorporeal.â€ For
Hobbes, the divine nature of Godâ€™s
attributes is not something that
the human mind is capable of fully
understanding, therefore the term
â€œincorporealâ€ is the only one that
recognizes and also honors the
unknowable substance of God.
Hobbes does make clear, however,
that he believes the existence and
nature of all religious entities are
matters for faith, not science, and
that God, in particular, will remain

beyond our comprehension. All it is
possible for human beings to know
about God is that he exists, and
that he is the ï¬rst cause, or creator,
of everything in the universe.

What is consciousness?
Because Hobbes considers that
human beings are purely physical,
and are therefore no more than
biological machines, he is then
faced with the problem of how to
account for our mental nature. He
makes no attempt to give an
account of how the mind can be
explained. He simply offers a
general and rather sketchy account
of what he thought science would
eventually reveal to be the case.
Even then, he only covers the
mental activities such as voluntary
motion, appetite, and aversionâ€”all
phenomena that can be studied
and explained from a mechanistic
point of view. Hobbes has nothing
to say about what the modern-day
Australian philosopher David
Chalmers calls â€œthe hard problem of

consciousness.â€ Chalmers points
out that certain functions of
consciousnessâ€”such as the use
of language and the processing
of informationâ€”can be explained
relatively easily in terms of the
mechanisms that perform those
functions, and that physicalist
philosophers have been offering
variants of this approach for
centuries. However, the harder
problem of explaining the nature of
subjective, ï¬rst-person experience
of consciousness remains unsolved
by them. There seems to be a
built-in mismatch between the
objects of the physical sciences
on the one hand and the subjects
of conscious experience on the
otherâ€”something that Hobbes
does not seem to be aware of.
Hobbesâ€™ account of his belief
offers very little argument for his
conviction that everything in the
world, including human beings,
is wholly physical. He appears not
to notice that his grounds for the

For what is the
heart, but a spring; and
the nerves, but so many
strings; and the joints,
but so many wheels,
giving motion to the
whole body.
Thomas Hobbes

RENAISSANCE AND THE AGE OF REASON 115
existence of imperceptible material
spirits could equally be grounds for
a belief in nonmaterial substances.
To most people, something being
imperceptible is more consistent
with a mental than with a physical
concept. In addition, because
Hobbesâ€™ material spirits can only
ever possess the same properties
as other types of physical thing,
they fail to offer any assistance
toward an explanation of the
mental nature of human beings.

Descartesâ€™ dualism
Hobbes also had to contend with
the very different thinking about
mind and body that Descartes set
out in his Meditations of 1641.
Descartes argues for the â€œReal
Distinctionâ€ between mind and
bodyâ€”the notion that they are
utterly distinct sorts of substance.
In objections to Descartesâ€™ ideas
that he expressed at the time,
Hobbes makes no comment on this
distinction. However, 14 years later,
he addressed the problem again in
a passage in his book De Corpore,
presenting and criticizing what
seems to be a muddled form of part
of Descartesâ€™ argument. Here he
rejects the conclusion Descartes
came toâ€”that mind and body are
two distinct substancesâ€”on the
basis that Descartesâ€™ use of the
phrase â€œincorporeal substanceâ€
is an example of insigniï¬cant or
empty language. Hobbes takes it
to mean â€œa body without bodyâ€,
which appears to be nonsense.
However, this deï¬nition must be
based upon his own view that all
substances are bodies; so what
Hobbes appears to present as an
While Hobbes was formulating his
mechanistic ideas, scientists such as
the physician William Harvey were
using empirical techniques to explore
the workings of the human body.

argument for his position that
there can be no incorporeal minds,
in fact depends upon his inaccurate
assumption that the only form of
substance is body, and that there
is no possibility of incorporeal
things existing at all.

A simple prejudice
As Hobbesâ€™ deï¬nition of physical
spirits indicates, it is ultimately
unclear exactly what he took
â€œphysicalâ€ or â€œcorporealâ€ to mean.
If it was meant to be simply
anything that had three spatial
dimensions, then he would be
excluding much of what we, at
the beginning of the 21st century,
might regard as being â€œphysical.â€
For example, his theories about the
nature of the world would rule out
the science of sub-atomic physics.
In the absence of any truly clear
notion of what his key term means,
Hobbesâ€™ insistence that everything
in the world can be explained in
physical terms begins to look less
and less like a statement of scientiï¬c
principle. Instead, it starts to appear
to be merely an unscientiï¬câ€”and

Besides sense, and
thoughts, and the train
of thoughts, the mind
of man has no
other motion.
Thomas Hobbes

unphilosophicalâ€”prejudice against
the mental. But his mechanistic
theories about the nature of our
world were very much in keeping
with the spirit of an age that was
to radically challenge most of the
prevailing views on human nature
and social order, as well as those
concerned with the substance and
workings of the universe that we
inhabit. It was this revolution in
thinking that laid the foundations
of our modern world. â– 

ITHEREFORE
THINK
I AM
RENE DESCARTES (1596â€“1650)

118 RENE DESCARTES
IN CONTEXT
BRANCH
Epistemology
APPROACH
Rationalism
BEFORE
4th century BCE Aristotle
argues that whenever we
perform any action, including
thinking, we are conscious
that we perform it, and in
this way we are conscious
that we exist.
c.420 CE St. Augustine writes
in The City of God that he is
certain he exists, because if he
is mistaken, this itself proves
his existenceâ€”in order to be
mistaken, one must exist.
AFTER
1781 In his Critique of Pure
Reason, Immanuel Kant argues
against Descartes, but adopts
the First Certaintyâ€”â€œI think
therefore I existâ€â€”as the heart
and starting point of his
idealist philosophy.

R

enÃ© Descartes lived in the
early 17th century, during
a period sometimes called
the Scientiï¬c Revolution, an era
of rapid advances in the sciences.
The British scientist and philosopher
Francis Bacon had established a
new method for conducting scientiï¬c
experiments, based on detailed
observations and deductive
reasoning, and his methodologies
had provided a new framework for
investigating the world. Descartes
shared his excitement and optimism,
but for different reasons. Bacon
considered the practical applications
of scientiï¬c discoveries to be their
whole purpose and point, whereas
Descartes was more fascinated by
the project of extending knowledge
and understanding of the world.
During the Renaissanceâ€”the
preceding historical eraâ€”people
had become more skeptical about
science and the possibility of
genuine knowledge in general, and
this view continued to exert an
inï¬‚uence in Descartesâ€™ time. So a
major motivation of his â€œproject of
pure enquiryâ€, as his work has
become known, was the desire to
rid the sciences of the annoyance
of skepticism once and for all.

An evil demon may
be making me believe
things that are false.

I am thinking,
therefore I exist.

There is nothing
of which I can
be certain.

In the Meditations on First
Philosophy, Descartesâ€™ most
accomplished and rigorous work
on metaphysics (the study of being
and reality) and epistemology (the
study of the nature and limits of
knowledge), he seeks to demonstrate
the possibility of knowledge even
from the most skeptical of positions,
and from this, to establish a ï¬rm
foundation for the sciences. The

Descartesâ€™ book De Homine Figuris
takes a biological look at the causes
of knowledge. In it, he suggests that
the pineal gland is the link between
vision and conscious action.

But when I say â€œI am;
I existâ€, I cannot be
wrong about this.

An evil demon could
try to make me believe this
only if I really do exist.

RENAISSANCE AND THE AGE OF REASON 119
See also: Aristotle 56â€“63 â–  St. Augustine of Hippo 72â€“73 â–  Thomas Hobbes 112â€“15 â–  Blaise Pascal 124â€“25
Benedictus Spinoza 126â€“29 â–  John Locke 130â€“33 â–  Gottfried Leibniz 134â€“37 â–  Immanuel Kant 164â€“71

Meditations is written in the ï¬rstperson formâ€”â€œI thinkâ€¦â€â€”because
he is not presenting arguments in
order to prove or disprove certain
statements, but instead wishes to
lead the reader along the path that
he himself has taken. In this way
the reader is forced to adopt the
standpoint of the meditator, thinking
things through and discovering the
truth just as Descartes had done.
This approach is reminiscent of
the Socratic method, in which the
philosopher gradually draws out a
personâ€™s understanding rather than
presenting it already packaged and
ready to take away.

knowledge. Perhaps, he says, we
are dreaming, and the apparently
real world is no more than a dream
world. He notes that this is possible,
as there are no sure signs between
being awake or asleep. But even so,
this situation would leave open the
possibility that some truths, such
as mathematical axioms, could be
known, though not through the
senses. But even these â€œtruthsâ€
might not in fact be true, because
God, who is all-powerful, could
deceive us even at this level. Even
though we believe that God is
good, it is possible that he made â¯â¯

The illusory world

An optical illusion of parallel lines that are made
to look bent can fool our senses. Descartes thinks
we must accept nothing as true or given, but must
instead strip away all preconceptions before we can
proceed to a position of knowledge.

In order to establish that his beliefs
have stability and endurance, which
Descartes takes to be two important
marks of knowledge, he uses what
is known as â€œthe method of doubt.â€
This starts with the meditator
setting aside any belief whose truth
can be doubted, whether slightly
or completely. Descartesâ€™ aim is
to show that, even if we start from
the strongest possible skeptical
position, doubting everything, we
can still reach knowledge. The
doubt is â€œhyperbolicâ€ (exaggerated),
and used only as a philosophical
tool; as Descartes points out: â€œno
sane person has ever seriously
doubted these things.â€
Descartes starts by subjecting
his beliefs to a series of increasingly
rigorous skeptical arguments,
questioning how we can be sure
of the existence of anything at all.
Could it be that the world we know
is just an illusion? We cannot trust
our senses, as we have all been
â€œdeceivedâ€ by them at one time or
another, and so we cannot rely on
them as a sure footing for

â– 

It is necessary that
at least once in your life
you doubt, as far as
possible, all things.
RenÃ© Descartes

120 RENE DESCARTES

I shall suppose that some
malicious demon of the
utmost power and cunning
has employed all his energies
in order to deceive me.
RenÃ© Descartes

An evil demon capable of deceiving
humankind about everything cannot
make me doubt my existence; if he
tries, and I am forced to question my
own existence, this only conï¬rms it.

us in such a way that we are prone
to errors in our reasoning. Or perhaps
there is no Godâ€”in which case we
are even more likely to be imperfect
beings (having arisen only by
chance) that are capable of being
deceived all the time.
Having reached a position in
which there seems to be nothing
at all of which he can be certain,
Descartes then devises a vivid tool
to help him to avoid slipping back
into preconceived opinion: he
supposes that there is a powerful
and evil demon who can deceive
him about anything. When he
ï¬nds himself considering a belief,

he can ask: â€œCould the demon be
making me believe this even
though it was false?â€ and if the
answer is â€œyesâ€ he must set aside
the belief as open to doubt.
At this point, it seems as though
Descartes has put himself into an
impossible positionâ€”nothing
seems beyond doubt, so he has no
solid ground on which to stand.
He describes himself as feeling
helplessly tumbled around by a
whirlpool of universal doubt, unable
to ï¬nd his footing. Skepticism
seems to have made it impossible
for him even to begin his journey
back to knowledge and truth.

The First Certainty
It is at this point that Descartes
realizes that there is one belief that
he surely cannot doubt: his belief in
his own existence. Each of us can

think or say: â€œI am, I existâ€, and
while we are thinking or saying it
we cannot be wrong about it. When
Descartes tries to apply the evil
demon test to this belief, he
realizes that the demon could only
make him believe that he exists if
he does in fact exist; how can he
doubt his existence unless he
exists in order to do the doubting?
This axiomâ€”â€œI am, I existâ€â€”
forms Descartesâ€™ First Certainty.
In his earlier work, the Discourse
on the Method, he presented it
as: â€œI think therefore I amâ€, but he
abandoned this wording when
he wrote the Meditations, as the
inclusion of â€œthereforeâ€ makes the
statement read like a premise and
conclusion. Descartes wants the
readerâ€”the meditating â€œIâ€â€”to
realize that as soon as I consider
the fact that I exist, I know it to be
true. This truth is instantly grasped.
The realization that I exist is a
direct intuition, not the conclusion
of an argument.
Despite Descartesâ€™ move to a
clearer expression of his position,
the earlier formulation was so
catchy that it stuck in peopleâ€™s
minds, and to this day the First
Certainty is generally known as
â€œthe cogitoâ€, from the Latin cogito

RENAISSANCE AND THE AGE OF REASON 121

ergo sum, meaning â€œI think
therefore I am.â€ St. Augustine of
Hippo had used a very similar
argument in The City of God, when
he said: â€œFor if I am mistaken, I
existâ€; meaning that if he did not
exist, he could not be mistaken.
Augustine, however, made little
use of this in his thinking, and
certainly did not reach it in the
way that Descartes did.

Descartes realizes that we might
also be able to gain knowledge
from the certainty itself. This is
because the knowledge that I am
thinking is bound up with the
knowledge of my existence. So
â€œthinkingâ€ is also something that
I cannot rationally doubt, for
doubting is a kind of thinking, so
to doubt that I am thinking is to
be thinking. As Descartes now
knows that he exists and that he
is thinking, then heâ€”and every
other meditatorâ€”also knows
that he is a thinking thing.
Descartes makes clear, though,
that this is as far as he can reason
from the First Certainty. He is
certainly not entitled to say that he
is only a thinking thingâ€”a mindâ€”
as he has no way of knowing what
more he might be. He might be a
physical thing that also has the
ability to think, or he might be
something else, something that he
has not even conceived yet. The
point is that at this stage of his
meditations he knows only that â¯â¯

What is this â€œIâ€?
Despite the fact that the First
Certaintyâ€™s main function is to
provide a ï¬rm footing for knowledge,

The only question that Descartes is deï¬nitely
able to answer using his method of doubt is whether
he is thinking. He cannot prove the existence of his
body or of the external world.

I
DO

H AVE A BO

IS THERE AN OUTSIDE WORLD?

DY

?

AM

This proposition, I am,
I exist, is necessarily true
whenever it is put forward
by me or conceived
in my mind.
RenÃ© Descartes

What use, though, is a single belief?
The simplest logical argument is a
syllogism, which has two premises
and a conclusionâ€”such as: all
birds have wings; a robin is a bird;
therefore all robins have wings. We
surely cannot get anywhere from
the starting point of just one true
belief. But Descartes was not
looking to reach these kinds of
conclusions from his First Certainty.
As he explained: â€œArchimedes
used to demand just one ï¬rm and
immovable point in order to shift
the entire Earth.â€ For Descartes, the
certainty of his own existence gives
him the equivalent; it saves him
from that whirlpool of doubt, gives
him a ï¬rm foothold, and so allows
him to start on the journey back from
skepticism to knowledge. It is crucial
to his project of enquiry, but it is not
the foundation of his epistemology.

IT

HI N

KI N G ?

122 RENE DESCARTES

When someone says
â€˜I am thinking, therefore
I amâ€™, he recognizes
it as something self-evident
by a simple intuition
of the mind.
RenÃ© Descartes

he is a thinking thing; as he puts
it, he knows only that he is, â€œin
the strict sense onlyâ€ a thinking
thing. Later, in the sixth book of the
Meditations, Descartes presents an
argument that mind and body are
different sorts of thingâ€”that they
are distinct substancesâ€”but he is
not yet in a position to do so.

Doubting Descartes
This First Certainty has been the
target of criticism from many
writers who hold that Descartesâ€™
approach to skepticism is doomed

RenÃ© Descartes

from the start. One of the main
arguments against it takes issue
with the very use of the term â€œIâ€ in
â€œI am, I exist.â€ Although Descartes
cannot be wrong in saying that
thinking is occurring, how does he
know that there is â€œa thinkerâ€â€”a
single, uniï¬ed consciousness doing
that thinking? What gives him the
right to assert the existence of
anything beyond the thoughts? On
the other hand, can we make sense
of the notion of thoughts ï¬‚oating
around without a thinker?
It is difï¬cult to imagine detached,
coherent thoughts, and Descartes
argues that it is impossible to
conceive of such a state of affairs.
However, if one were to disagree,
and believe that a world of thoughts
with no thinkers is genuinely
possible, Descartes would not be
entitled to the belief that he exists,
and would thus fail to reach his
First Certainty. The existence of
thoughts would not give him the
solid ground he needed.
The problem with this notion
of thoughts ï¬‚oating around with
no thinker is that reasoning would
be impossible. In order to reason,
it is necessary to relate ideas in
a particular way. For example, if

Patrick has the thought â€œall men
are mortalâ€ and Patricia has the
thought â€œSocrates is a manâ€,
neither can conclude anything.
But if Paula has both thoughts, she
can conclude that â€œSocrates is
mortal.â€ Merely having the thoughts
â€œall men are mortalâ€ and â€œSocrates
is a manâ€ ï¬‚oating around is like
two separate people having them;
in order for reason to be possible
we need to make these thoughts
relative to one another, to link them
in the right way. It turns out that
making thoughts relative to
anything other than a thinker
(for example, to a place or to a
time) fails to do the job. And since
reasoning is possible, Descartes
can conclude that there is a thinker.
Some modern philosophers have
denied that Descartesâ€™ certainty of
his own existence can do the job he
requires of it; they argue that â€œI
existâ€ has no content, as it merely
refers to its subject but says nothing
meaningful or important about it;
it is simply pointing at the subject.
For this reason nothing can follow
from it, and Descartesâ€™ project fails
at the beginning. This seems to
miss Descartesâ€™ point; as we have
seen, he does not use the First

RenÃ© Descartes was born near
Tours, France, and was educated
at the Jesuit CollÃ¨ge Royale, in
La FlÃ¨che. Due to ill-health, he was
allowed to stay in bed until late in
the mornings, and he formed the
habit of meditating. From the age
of 16 he concentrated on studying
mathematics, breaking off his
studies for four years to volunteer
as a soldier in Europeâ€™s Thirty
Years War. During this time he
found his philosophical calling,
and after leaving the army, he
settled ï¬rst in Paris and then in
the Netherlands, where he spent
most of the rest of his life. In 1649

he was invited to Sweden by
Queen Christina to discuss
philosophy; he was expected to
get up very early, much against
his normal practice. He believed
that this new regimeâ€”and the
Swedish climateâ€”caused him
to contract pneumonia, of which
he died a year later.
Key works
1637 Discourse on the Method
1641 Meditations on First
Philosophy
1644 Principles of Philosophy
1662 De Homine Fuguris

RENAISSANCE AND THE AGE OF REASON 123
Certainty as a premise from which
to derive further knowledgeâ€”all
he needs is that there be a self for
him to point to. So even if â€œI existâ€
only succeeds in pointing to the
meditator, then he has an escape
from the whirlpool of doubt.

An unreal thinker
For those who have misunderstood
Descartes to have been offering
an argument from the fact of his
thinking to the fact of his existence,
we can point out that the First
Certainty is a direct intuition, not
a logical argument. Why, though,
would it be a problem if Descartes
had been offering an argument?
As it stands, the apparent
inference â€œI am thinking, therefore I
existâ€ is missing a major premise;
that is, in order for the argument to
work it needs another premise,
such as â€œanything that is thinking
exists.â€ Sometimes an obvious
premise is not actually stated in an
argument, in which case it is
known as a suppressed premise.
But some of Descartesâ€™ critics
complain that this suppressed
premise is not at all obvious. For
example, Hamlet, in Shakespeareâ€™s
play, thought a great deal, but it is

We ought to enquire
as to what sort of
knowledge human reason
is capable of attaining,
before we set about
acquiring knowledge
of things in particular.
RenÃ© Descartes

also clearly true that he did not
exist; so it is not true that anything
that thinks exists.
We might say that in so far as
Hamlet thought, he thought in the
ï¬ctional world of a play, but he also
existed in that ï¬ctional world; in so
far as he did not exist, he did not
exist in the real world. His â€œrealityâ€
and thinking are linked to the same
world. But Descartesâ€™ critics might
respond that that is precisely the
point: knowing that someone called
Hamlet was thinkingâ€”and no more
than thisâ€”does not assure us that
this person exists in the real world;
for that, we should have to know
that he was thinking in the real
world. Knowing that something or
someoneâ€”like Descartesâ€”is
thinking, is not enough to prove
their reality in this world.
The answer to this dilemma lies
in the ï¬rst-person nature of the
Meditations, and the reasons for
Descartesâ€™ use of the â€œIâ€ throughout
now becomes clear. Because while
I might be unsure whether Hamlet
was thinking, and therefore existed,
in a ï¬ctional world or the real world,
I cannot be unsure about myself.

and to establish a ï¬rm, rational
foundation for knowledge. He is
also well known for proposing that
the mind and the body are two
distinct substancesâ€”one material
(the body) and the other immaterial
(the mind)â€”which are nonetheless
capable of interaction. This famous
distinction, which he explains in
the Sixth Meditation, became
known as Cartesian dualism.
However, it is the rigor of
Descartesâ€™ thought and his rejection
of any reliance on authority that are
perhaps his most important legacy.
The centuries after his death were
dominated by philosophers who
either developed his ideas or those
who took as their main task the
refutation of his thoughts, such as
Thomas Hobbes, Benedictus
Spinoza, and Gottfried Leibniz. â– 

Modern philosophy
In the â€œPreface to the Readerâ€ of the
Meditations, Descartes accurately
predicted that many readers would
approach his work in such a way
that most would â€œnot bother to grasp
the proper order of my arguments
and the connection between them,
but merely try to carp at individual
sentences, as is the fashion.â€ On
the other hand, he also wrote that
â€œI do not expect any popular approval,
or indeed any wide audienceâ€, and
in this he was much mistaken. He
is often described as the father of
modern philosophy. He sought to
give philosophy the certainty of
mathematics without recourse to
any kind of dogma or authority,

The separation of mind and body
theorized by Descartes leaves open the
following question: since all we can see
of ourselves is our bodies, how could
we prove that a robot is not conscious?

124

IMAGINATION
DECIDES
EVERYTHING
BLAISE PASCAL (1623â€“1662)

IN CONTEXT
BRANCH
Philosophy of mind

Imagination is a
powerful force in
human beings.

APPROACH
Voluntarism
BEFORE
c.350 BCE Aristotle says that
â€œimagination is the process by
which we say that an image
is presented to us,â€ and that
â€œthe soul never thinks without
a mental image.â€
1641 RenÃ© Descartes claims
that the philosopher must
train his imagination for the
sake of gaining knowledge.
AFTER
1740 In his Treatise of Human
Nature, David Hume argues
that â€œnothing we imagine is
absolutely impossible.â€
1787 Immanuel Kant claims
that we synthesize the
incoherent messages from
our senses into images, and
then into concepts, using
the imagination.

It can override our reason.

But it can lead either to
truths or falsehoods.

We may see beauty, justice, or
happiness where it does not
really exist.

Imagination leads
us astray.

P

ascalâ€™s best-known book,
PensÃ©es, is not primarily a
philosophical work. Rather,
it is a compilation of fragments from
his notes for a projected book on
Christian theology. His ideas were
aimed primarily at what he called
libertinsâ€”ex-Catholics who had
left religion as a result of the sort
of free thinking encouraged by
skeptical writers such as Montaigne.
In one of the longer fragments,
Pascal discusses imagination. He
offers little or no argument for his
claims, being concerned merely to
set down his thoughts on the matter.
Pascalâ€™s point is that imagination
is the most powerful force in human
beings, and one of our chief sources
of error. Imagination, he says,
causes us to trust people despite
what reason tells us. For example,
because lawyers and doctors dress
up in special clothes, we tend to
trust them more. Conversely, we
pay less attention to someone who
looks shabby or odd, even if he is
talking good sense.
What makes things worse is that,
though it usually leads to falsehood,
imagination occasionally leads to
truth; if it were always false, then we
could use it as a source of certainty
by simply accepting its negation.

RENAISSANCE AND THE AGE OF REASON 125
See also: Aristotle 56â€“63
Immanuel Kant 164â€“71

â– 

Michel de Montaigne 108â€“09

After presenting the case against
imagination in some detail, Pascal
suddenly ends his discussion of it
by writing: â€œImagination decides
everything: it produces beauty,
justice, and happiness, which is the
greatest thing in the world.â€ Out of
context, it might seem that he is
praising imagination, but we can
see from what preceded this
passage that his intention is very
different. As imagination usually
leads to error, then the beauty,
justice, and happiness that it
produces will usually be false.

Man is but a reed,
the weakest nature;
yet he is a thinking reed.
Blaise Pascal

Blaise Pascal

â– 

RenÃ© Descartes 116â€“23

â– 

David Hume 148â€“53

â– 

In the wider context of a work of
Christian theology, and especially
in light of Pascalâ€™s emphasis on the
use of reason to bring people to
religious belief, we can see that his
aim is to show the libertins that
the life of pleasure that they have
chosen is not what they think it is.
Although they believe that they
have chosen the path of reason,
they have in fact been misled by
the power of the imagination.

Pascalâ€™s Wager
This view is relevant to one of the
most complete notes in the PensÃ©es,
the famous argument known as
Pascalâ€™s Wager. The wager was
designed to give the libertins a
reason to return to the Church, and
it is a good example of â€œvoluntarismâ€,
the idea that belief is a matter of
decision. Pascal accepts that it is
not possible to give good rational
grounds for religious belief, but
tries to offer rational grounds for
wanting to have such beliefs.
These consist of weighing up
the possible proï¬t and loss of
making a bet on the existence
Blaise Pascal was born in ClermontFerrand, France. He was the son
of a government functionary who
had a keen interest in science and
mathematics and who educated
Pascal and his two sisters. Pascal
published his ï¬rst mathematical
paper at the age of 16, and had
invented the ï¬rst digital calculator
by the time he was 18. He also
corresponded with the famous
mathematician Pierre Fermat, with
whom he laid the foundations of
probability theory.
Pascal underwent two religious
conversions, ï¬rst to Jansenism
(an approach to Christian teaching

According to Pascal, we are
constantly tricked by the imagination
into making the wrong judgmentsâ€”
including judgements about people
based on how they are dressed.

of God. Pascal argues that betting
that God does not exist risks losing
a great deal (inï¬nite happiness in
Heaven), while only gaining a little
(a ï¬nite sense of independence in
this world)â€”but betting that God
exists risks little while gaining a
great deal. It is more rational, on
this basis, to believe in God. â– 
that was later declared heretical),
and then to Christianity proper.
This led him to abandon his
mathematical and scientiï¬c
work in favor of religious
writings, including the PensÃ©es.
In 1660â€“62 he instituted the
worldâ€™s ï¬rst public transport
service, giving all proï¬ts to the
poor, despite suffering from
severe ill health from the 1650s
until his death in 1662.
Key works
1657 Lettres Provinciales
1670 PensÃ©es

126

GOD IS THE CAUSE
OF ALL THINGS,
WHICH ARE IN HIM
BENEDICTUS SPINOZA (1632â€“1677)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Substance monism
BEFORE
c.1190 Jewish philosopher
Moses Maimonides invents
a demythologized version
of religion which later
inspires Spinoza.
16th century Italian scientist
Giordano Bruno develops a
form of pantheism.
1640 RenÃ© Descartes publishes
his Meditations, another of
Spinozaâ€™s inï¬‚uences.
AFTER
Late 20th century
Philosophers Stuart Hampshire,
Donald Davidson, and Thomas
Nagel all develop approaches
to the philosophy of mind that
have similarities to Spinozaâ€™s
monist thought.

L

ike most philosophies of the
17th century, Spinozaâ€™s
philosophical system has the
notion of â€œsubstanceâ€ at its heart.
This concept can be traced back to
Aristotle, who asked â€œWhat is it
about an object that stays the same
when it undergoes change?â€ Wax,
for example, can melt and change
its shape, size, color, smell, and
texture, and yet still remain â€œwaxâ€,
prompting the question: what are
we referring to when we speak of
â€œthe waxâ€? Since it can change in
every way that we can perceive, the
wax must also be something beyond
its perceptible properties, and for
Aristotle this unchanging thing is
the waxâ€™s â€œsubstance.â€ More

RENAISSANCE AND THE AGE OF REASON 127
See also: Aristotle 56â€“63

â– 

Moses Maimonides 84â€“85

â– 

RenÃ© Descartes 116â€“23

â– 

Donald Davidson 338

Everything that
exists is made of this
one substance.

There is only
one substance.

This substance is
â€œGodâ€ or â€œnature.â€

It provides everything
in our universe with itsâ€¦

...process of
formation,

... its purpose,

... its shape,

... and its matter.

In these four ways, God
â€œcausesâ€ everything.

generally, substance is anything
that has propertiesâ€”or that which
underlies the world of appearance.
Spinoza employs â€œsubstanceâ€ in a
similar way, deï¬ning it as that which
is self-explanatoryâ€”or that which
can be understood by knowing its
nature alone, as opposed to all other
things that can be known only by
their relationships with other things.
For example, the concept â€œcartâ€ can
only be understood with reference
to other concepts, such as â€œmotionâ€,
â€œtransportâ€, and so on. Moreover, for
Spinoza, there can only be one such
substance, for if there were two,
understanding one would entail
understanding its relationship with
the other, which contradicts the

deï¬nition of substance. Furthermore,
he argues, since there is only one
such substance, there can, in fact,
be nothing but that substance, and
everything else is in some sense a
part of it. Spinozaâ€™s position is
known as â€œsubstance monismâ€,
which claims that all things are
ultimately aspects of a single thing,
as opposed to â€œsubstance dualismâ€,
which claims that there are
ultimately two kinds of things in
the universe, most commonly
deï¬ned as â€œmindâ€ and â€œmatter.â€

Substance as God or nature
For Spinoza, then, substance
underlies our experience, but it
can also be known by its various

attributes. He does not specify
how many attributes substance
has, but he says that human
beings, at least, can conceive of
twoâ€”namely, the attribute of
extension (physicality) and the
attribute of thought (mentality). For
this reason, Spinoza is also known
as an â€œattribute dualistâ€, and he
claims that these two attributes
cannot be explained by each other,
and so must be included in any
complete account of the world. As
for substance itself, Spinoza says
that we are right to call it â€œGodâ€ or
â€œnatureâ€ (Deus sive natura)â€”that
self-explaining thing which, in
human form, sees itself under the
attributes of body and mind. â¯â¯

128 BENEDICTUS SPINOZA

All changes, from a change of mood
to a change in a candleâ€™s shape, are,
for Spinoza, alterations that occur to
a single substance that has both
mental and physical attributes.

At the level of individual things,
including human beings, Spinozaâ€™s
attribute dualism is intended in
part to deal with the question of
how minds and bodies interact.
The things that we experience as
individual bodies or minds are in
fact modiï¬cations of the single
substance as conceived under
one of the attributes. Each
modiï¬cation is both a physical
thing (in so far as it is conceived
under the attribute of extension)

Benedictus Spinoza

and a mental thing (in so far as it
is conceived under the attribute
of thought). In particular, a human
mind is a modiï¬cation of substance
conceived under the attribute of
thought, and the human brain is
the same modiï¬cation of substance
conceived under the attribute of
extension. In this way, Spinoza
avoids any question about the
interaction between mind and
body: there is no interaction, only
a one-to-one correspondence.
However, Spinozaâ€™s theory
commits him to the view that it is
not only human beings that are
minds as well as bodies, but
everything else too. Tables, rocks,
treesâ€”all of these are modiï¬cations
of the one substance under the
attributes of thought and extension.
So, they are all both physical and
mental things, although their
mentality is very simple and they
are not what we should call minds.
This aspect of Spinozaâ€™s theory is
difï¬cult for many people either to
accept or to understand.

The world is God
Spinozaâ€™s theory, which he explains
fully in Ethics, is often referred to
as a form of pantheismâ€”the belief
Benedictus (or Baruch) Spinoza
was born in Amsterdam, the
Netherlands, in 1632. At the age
of 23 he was excommunicated
by the synagogue of Portuguese
Jews in Amsterdam, who probably
wished to distance themselves
from Spinozaâ€™s teachings. Spinozaâ€™s
Theological-Political Treatise
was later attacked by Christian
theologians and banned in
1674â€”a fate that had already
befallen the work of the French
philosopher RenÃ© Descartes. The
furore caused him to withhold
publication of his greatest work,
the Ethics, until after his death.

Mind and body
are one.
Benedictus Spinoza

that God is the world, and that the
world is God. Pantheism is often
criticized by theists (people who
believe in God), who argue that
it is little more than atheism by
another name. However, Spinozaâ€™s
theory is in fact much closer to
panentheismâ€”the view that the
world is God, but that God is more
than the world. For in Spinozaâ€™s
system, the world is not a mass of
material and mental stuffâ€”rather,
the world of material things is a
form of God as conceived under
the attribute of extension, and the
world of mental things is that same
form of God as conceived under the
attribute of thought. Therefore the
Spinoza was a modest, intensely
moral man who turned down
numerous lucrative teaching
positions for the sake of his
intellectual freedom. Instead
he lived a frugal life in various
places in the Netherlands,
making a living by private
philosophy teaching and as
a lens grinder. He died from
tuberculosis in 1677.
Key works
1670 Theological-Political
Treatise
1677 Ethics

RENAISSANCE AND THE AGE OF REASON 129

The human mind
is part of the inï¬nite
intellect of God.
Benedictus Spinoza

According to Spinoza, all objects, whether animal,
vegetable, or mineral, have a mentality. Both their
bodies and their mentalities are a part of God,
who is greater than all the worldâ€™s physical and
mental attributes. God, for Spinoza, is the
â€œsubstanceâ€ that underlies reality.

Every object in
the universe, even
a rock, has a body
and a mind.

one substance or God is more than
the world, but the world itself is
entirely substance or God.
However, Spinozaâ€™s God is clearly
different from the God of standard
Judaeo-Christian theology. Not
only is it not a person, it cannot be
regarded as being the creator of
the world in the sense found in the
Book of Genesis. Spinozaâ€™s God
does not exist alone before creation,
and then bring it into existence.

Body and mind
are attributes of
substance.

Substance is God,
in whom all is
explained.

God as the cause
What can Spinoza mean, then,
when he says that God is the cause
of everything? The one substance
is â€œGod or natureâ€â€”so even if
there is more to God than those
modiï¬cations of substance that
make up our world, how can the
relationship between God and
nature be causal?
First, we should note that
Spinoza, in common with most
philosophers before him, uses
the word â€œcauseâ€ in a much richer
sense than we do nowâ€”a sense
that originates in Aristotleâ€™s
deï¬nition of four types of cause.
These are (using a statue as an
example): a formal cause, or the
relationship between a thingâ€™s
parts (its shape or form); a material
cause, or the matter a thing is made
of (the bronze, marble, and so on);

an efï¬cient cause, or that which
brings a thing into being (the
sculpting process); and a ï¬nal cause,
or the purpose for which a thing
exists (the creation of a work of art,
the desire for money, and so on).
For Aristotle and Spinoza,
these together deï¬ne â€œcauseâ€, and
provide a complete explanation of a
thingâ€”unlike todayâ€™s usage, which
tends to relate to the â€œefï¬cientâ€
or â€œï¬nalâ€ causes only. Therefore,
when Spinoza speaks of God or
substance being â€œself-causedâ€ he
means that it is self-explanatory,
rather than that it is simply selfgenerating. When he talks of God

being the cause of all things, he
means that all things ï¬nd their
explanation in God.
God, therefore, is not what
Spinoza calls a â€œtransitiveâ€ cause of
the worldâ€”something external that
brings the world into being. Rather,
God is the â€œimmanentâ€ cause of the
world. This means that God is in
the world, that the world is in God,
and that the existence and essence
of the world are explained by Godâ€™s
existence and essence. For Spinoza,
to fully appreciate this fact is to
attain the highest state of freedom
and salvation possibleâ€”a state
he calls â€œblessedness.â€ â– 

130

NO MANâ€™S
KNOWLEDGE HERE
CAN GO BEYOND
HIS EXPERIENCE
JOHN LOCKE (1632â€“1704)

IN CONTEXT
BRANCH
Epistemology
APPROACH
Empiricism
BEFORE
c.380 BCE In his dialogue,
Meno, Plato argues that we
remember knowledge from
previous lives.
Mid-13th century Thomas
Aquinas puts forward the
principle that â€œwhatever is
in our intellect must have
previously been in the senses.â€
AFTER
Late 17th century Gottfried
Leibniz argues that the mind
may seem to be a tabula rasa
at birth, but contains innate,
underlying knowledge, which
experience gradually uncovers.
1966 Noam Chomsky, in
Cartesian Linguistics, sets out
his theory of innate grammar.

J

ohn Locke is traditionally
included in the group of
philosophers known as the
British Empiricists, together with
two later philosophers, George
Berkeley and David Hume. The
empiricists are generally thought
to hold the view that all human
knowledge must come directly or
indirectly from the experience of
the world that we acquire through
the use of our senses alone. This
contrasts with the thinking of the
rationalist philosophers, such
as RenÃ© Descartes, Benedictus
Spinoza, and Gottfried Leibniz,
who hold that in principle, at least,
it is possible to acquire knowledge
solely through the use of reason.

RENAISSANCE AND THE AGE OF REASON 131
See also: Plato 50â€“55 â–  Thomas Aquinas 88â€“95 â–  RenÃ© Descartes 116â€“23
Gottfried Leibniz 134â€“37 â–  George Berkeley 138â€“41 â–  David Hume 148â€“53

â– 
â– 

Benedictus Spinoza 126â€“29
Noam Chomsky 304â€“05

â– 

Rationalists believe that we are
born with some ideas and concepts;
that they are â€œinnate.â€

But this is not borne
out by the fact that...

...there are no truths
that are found in
everyone at birth.

...there are no universal
ideas found in people of
all cultures at all times.

Everything we
know is gained from
experience.

In fact, the division between these
two groups is not as clear-cut as
is often assumed. The rationalists
all accept that in practice our
knowledge of the world ultimately
stems from our experience, and
most notably from scientiï¬c enquiry.
Locke reaches his distinctive views
concerning the nature of the world
by applying a process of reasoning
later known as abduction (inference
to the best explanation from the
available evidence) to the facts of
sensory experience. For example,
Locke sets out to demonstrate that
the best explanation of the world
as we experience it is corpuscular
theory. This is the theory that
everything in the world is made

up of submicroscopic particles, or
corpuscles, which we can have no
direct knowledge of, but which, by
their very existence, make sense of
phenomena that would otherwise
be difï¬cult or impossible to explain.
Corpuscular theory was becoming
popular in 17th-century scientiï¬c
thinking and is fundamental to
Lockeâ€™s view of the physical world.

Innate ideas
The claim that manâ€™s knowledge
cannot go beyond his experience
may therefore seem inappropriate,
or at least an exaggeration, when
attributed to Locke. However,
Locke does argue at some length,
in his Essay Concerning Human

If we attentively consider
newborn children, we
shall have little reason
to think that they bring
many ideas into
the world with them.
John Locke

Understanding, against the theory
proposed by the rationalists to
explain how knowledge could be
accessed without experience. This
is the theory of innate ideas.
The concept that human beings
are born with innate ideas, and that
these can give us knowledge about
the nature of the world around us,
independently of anything we may
experience, dates back to the dawn
of philosophy. Plato had developed
a concept, according to which all
genuine knowledge is essentially
located within us, but that when
we die our souls are reincarnated
into new bodies and the shock of
birth causes us to forget it all.
Education is therefore not about
learning new facts, but about
â€œunforgettingâ€, and the educator
is not a teacher but a midwife.
However, many later thinkers
countered Platoâ€™s theory, proposing
that all knowledge cannot be innate
and that only a limited number of
concepts can be. These include the
concept of God and also that of a
perfect geometric structure, such
as an equilateral triangle. This â¯â¯

132 JOHN LOCKE
type of knowledge, in their view,
can be gained without any direct
sensory experience, in the way
that it is possible to devise a
mathematical formula by using
nothing more than the powers of
reason and logic. RenÃ© Descartes,
for example, declares that although
he believes that we all have an idea
of God imprinted in usâ€”like the
mark that a craftsman makes in
the clay of a potâ€”this knowledge
of Godâ€™s existence can only be
brought into our conscious mind
through a process of reasoning.

Lockeâ€™s objections
Locke was against the idea that
human beings possess any kind
of innate knowledge. He takes
the view that the mind at birth
is a tabula rasaâ€”a blank tablet or
a new sheet of paper upon which
experience writes, in the same
way that light can create images
on photographic ï¬lm. According
to Locke, we bring nothing to the
process except the basic human
ability to apply reason to the
information that we gather through

our senses. He argues that there is
not the slightest empirical evidence
to suggest that the minds of infants
are other than blank at birth, and
adds that this is also true of the
minds of the mentally deï¬cient,
stating that â€œthey have not the least
apprehension or thought of them.â€
Locke, therefore, declares that any
doctrine supporting the existence
of innate ideas must be false.
Locke also goes on to attack
the very notion of innate ideas by
arguing that it is incoherent. In
order for something to be an idea
at all, he states that it has to have
been present at some point in
somebodyâ€™s mind. But, as Locke
points out, any idea that claims
to be truly innate must also be
claiming to precede any form of
human experience. Locke accepts
that it is true, as Gottfried Leibniz
states, that an idea may exist so
deep in a personâ€™s memory that
for a time it is difï¬cult or even
impossible to recall, and so is not
accessible to the conscious mind.
Innate ideas, on the other hand,
are believed to somehow exist

Locke believed the human mind is
like a blank canvas, or tabula rasa, at
birth. He states that all our knowledge
of the world can only come from our
experience, conveyed to us by our
senses. We can then rationalize this
knowledge to formulate new ideas.

Experience

Tabula Rasa

Theory

It seems to me a
near contradiction to
say that there are truths
imprinted on the soul,
which it perceives or
understands not.
John Locke

somewhere, before the presence
of any sort of mechanism that is
capable of conceiving them and
bringing them into consciousness.
The supporters of the existence
of innate ideas often also argue
that as such ideas are present in
all human beings at birth, they
must be by nature universal,
which means that they are found
in all human societies at all points
in history. Plato, for example,
claims that everyone potentially
has access to the same basic
body of knowledge, denying any
difference in that respect between
men and women, or between
slaves and freemen. Similarly,
in Lockeâ€™s time, the theory was
frequently put forward that because
innate ideas can only be placed in
us by God, they must be universal,
as God is not capable of being so
unfair as to hand them out only
to a select group of people.
Locke counters the argument
for universal ideas by once again
bringing to our attention that a
simple examination of the world
around us will readily show that
they do no exist. Even if there
were concepts, or ideas, which
absolutely every human being in

RENAISSANCE AND THE AGE OF REASON 133

Let us then suppose
the mind to be white
paper, void of all
characters, without any
ideas; how comes it
to be furnished?
John Locke

the world held in common, Locke
argues that we would have no ï¬rm
grounds for concluding that they
were also innate. He declares that
it would always be possible to
discover other explanations for
their universality, such as the fact
that they stem from the most basic
ways in which a human being
experiences the world around him,
which is something that we all
must share.
In 1704, Gottfried Leibniz wrote
a rebuttal of Lockeâ€™s empiricist
arguments in his New Essays on

John Locke

the Human Understanding. Leibniz
declares that innate ideas are the
one clear way that we can gain
knowledge that is not based upon
sensory experience, and that Locke
is wrong to deny their possibility.
The debate about whether human
beings can know anything beyond
what they perceive through their
ï¬ve basic senses continues.

Language as innate
Although Locke may reject the
doctrine of innate ideas, he does
not reject the concept that human
beings have innate capacities.
Indeed, the possession of capacities
such as perception and reasoning
are central to his accounts of the
mechanism of human knowledge
and understanding. In the late
20th century, the American
philosophy Noam Chomsky took
this idea further when he put
forward his theory that there is an
innate process of thinking in every
human mind, which is capable
of generating a universal â€œdeep
structureâ€ of language. Chomsky
believes that regardless of their
apparent structural differences,
all human languages have been
generated from this common basis.

As the mind is a blank canvas, or
tabula rasa, at birth, Locke believes
that anybody can be transformed by
a good education, one that encourages
rational thought and individual talents.

John Locke was born in 1632, the
son of an English country lawyer.
Thanks to wealthy patrons, he
received a good education, ï¬rst
at Westminster School in London,
then at Oxford. He was impressed
with the empirical approach to
science adopted by the pioneering
chemist Robert Boyle, and he
both promoted Boyleâ€™s ideas and
assisted in his experimental work.
Though Lockeâ€™s empiricist ideas
are important, it was his political
writing that made him famous. He
proposed a social-contract theory of
the legitimacy of government and
the idea of natural rights to private

property. Locke ï¬‚ed England
twice, as a political exile, but
returned in 1688, after the
accession to the throne of
William and Mary. He remained
in England, writing as well as
holding various government
positions, until his death in 1704.

Locke played an important role in
questioning how human beings
acquire knowledge, at a time when
manâ€™s understanding of the world
was expanding at an unprecedented
rate. Earlier philosophersâ€”notably
the medieval Scholastic thinkers
such as Thomas Aquinasâ€”had
concluded that some aspects of
reality were beyond the grasp of
the human mind. But Locke took
this a stage further. By detailed
analysis of manâ€™s mental faculties,
he sought to set down the exact
limits of what is knowable. â– 

Key works
1689 A Letter Concerning
Toleration
1690 An Essay Concerning
Human Understanding
1690 Two Treatises of
Government

134

THERE ARE TWO
KINDS OF TRUTHS:
TRUTHS OF
REASONING AND
TRUTHS OF FACT
GOTTFRIED LEIBNIZ (1646â€“1716)

IN CONTEXT
BRANCH
Epistemology
APPROACH
Rationalism
BEFORE
1340 Nicolaus of Autrecourt
argues that there are no
necessary truths about the
world, only contingent truths.
1600s RenÃ© Descartes claims
that ideas come to us in three
ways; they can be derived from
experience, drawn from reason,
or known innately (being
created in the mind by God).
AFTER
1748 David Hume explores the
distinction between necessary
and contingent truths.
1927 Alfred North Whitehead
postulates â€œactual entitiesâ€,
similar to Leibnizâ€™s monads,
which reï¬‚ect the whole
universe in themselves.

E

arly modern philosophy
is often presented as being
divided into two schoolsâ€”
that of the rationalists (including
RenÃ© Descartes, Benedictus
Spinoza, and Immanuel Kant) and
that of the empiricists (including
John Locke, George Berkeley, and
David Hume). In fact, the various
philosophers did not easily fall into
two clear groups, each being like
and unlike each of the others in
complex and overlapping ways.
The essential difference between
the two schools, however, was
epistemologicalâ€”that is, they
differed in their opinions about
what we can know, and how we
know what we know. Put simply,

RENAISSANCE AND THE AGE OF REASON 135
See also: Nicolaus of Autrecourt 334 â–  RenÃ© Descartes 116â€“23 â–  David Hume
148â€“53 â–  Immanuel Kant 164â€“71 â–  Alfred North Whitehead 336

Every thing in the world
has a distinct notion.

This notion contains every
truth about that thing,
including its connections
to other things.

We can analyze these
connections through
rational reï¬‚ection.

When the analysis is
ï¬nite, we can reach
the ï¬nal truth.

When the analysis is
inï¬nite, we cannot reach the
ï¬nal truth through reasoningâ€”
only through experience.

These are truths
of reasoning.

These are truths
of fact.

the empiricists held that knowledge
is derived from experience, while
the rationalists claimed that
knowledge can be gained through
rational reï¬‚ection alone.
Leibniz was a rationalist, and
his distinction between truths
of reasoning and truths of fact
marks an interesting twist in the
debate between rationalism and
empiricism. His claim, which he
makes in most famous work, the
Monadology, is that in principle
all knowledge can be accessed by
rational reï¬‚ection. However, due
to shortcomings in our rational â¯â¯

We know hardly anything
adequately, few things
a priori, and most things
through experience.
Gottfried Wilhelm
Leibniz

Gottfried Leibniz
Gottfried Leibniz was a
German philosopher and
mathematician. He was born
in Leipzig, and after university
he took public service with
the Elector of Mainz for ï¬ve
years, during which time he
concentrated mainly on
political writings. After a
period spent travelling, he
took up the post of librarian
to the Duke of Brunswick, in
Hanover, and remained there
until his death. It was during
this last period of his life that
he did most of the work on
the development of his unique
philosophical system.
Leibniz is famous in
mathematics for his invention
of the so-called â€œinï¬nitesimal
calculusâ€ and the argument
that followed this, as both
Leibniz and Newton claimed
the discovery as their own. It
seems clear that they had in
fact reached it independently,
but Leibniz developed a much
more usable notation which
is still used today.
Key works
1673 A Philosopherâ€™s Creed
1685 Discourse on Metaphysics
1695 The New System
1710 Theodicy
1714 Monadology

136 GOTTFRIED LEIBNIZ
A map of the internet shows the
innumerable connections between
internet users. Leibnizâ€™s theory of
monads suggests that all our minds
are similarly connected.

faculties, human beings must
also rely on experience as a
means of acquiring knowledge.

A universe in our minds
To see how Leibniz arrives at this
conclusion, we need to understand
a little of his metaphysicsâ€”his
view of how the universe is
constructed. He holds that every
part of the world, every individual
thing, has a distinct concept or
â€œnotionâ€ associated with it, and that
every such notion contains within
it everything that is true about
itself, including its relations to other
things. Because everything in the
universe is connected, he argues,
it follows that every notion is
connected to every other notion,
and so it is possibleâ€”at least
in principleâ€”to follow these
connections and to discover truths
about the entire universe through

rational reï¬‚ection alone. Such
reï¬‚ection leads to Leibnizâ€™s â€œtruths
of reasoning.â€ However, the human
mind can grasp only a small number
of such truths (such as those of
mathematics), and so it has to
rely on experience, which yields
â€œtruths of fact.â€
So how is it possible to progress
from knowing that it is snowing,
for example, to knowing what will
happen tomorrow somewhere on the
other side of the world? For Leibniz,
the answer lies in the fact that the
universe is composed of individual,
simple substances called â€œmonads.â€
Each monad is isolated from other
monads, and each contains a
complete representation of the
whole universe in its past,
present, and future states. This
representation is synchronized
between all the monads, so that
each one has the same content.

According to Leibniz, this is how
God created thingsâ€”in a state of
â€œpre-established harmony.â€
Leibniz claims that every
human mind is a monad, and so
contains a complete representation
of the universe. It is therefore
possible in principle for us to learn
everything that there is to know
about our world and beyond simply
by exploring our own minds.
Simply by analyzing my notion of
the star Betelgeuse, for example, I
will eventually be able to determine
the temperature on the surface
of the actual star Betelgeuse.
However, in practice, the analysis
that is required for me reach this
information is impossibly
complexâ€”Leibniz calls it â€œinï¬niteâ€
â€”and because I cannot complete
it, the only way that I can discover
the temperature of Betelgeuse is by
measuring it empirically using
astronomical equipment.
Is the temperature of the surface
of Betelgeuse a truth of reasoning
or a truth of fact? It may be true
that I had to resort to empirical

Each singular substance
expresses the whole
universe in its own way.
Gottfried Wilhelm
Leibniz

RENAISSANCE AND THE AGE OF REASON 137
methods to discover the answer,
but had my rational faculties been
better I could also have discovered it
through rational reï¬‚ection. Whether
it is a truth of reasoning or a truth
of fact, therefore, seems to depend
on how I arrive at the answerâ€”but
is this what Leibniz is claiming?

Necessary truths
The trouble for Leibniz is that he
holds that truths of reasoning are
â€œnecessaryâ€, meaning that it is
impossible to contradict them,
while truths of fact are â€œcontingentâ€;
they can be denied without logical
contradiction. A mathematical
truth is a necessary truth, because
denying its conclusions contradicts
the meanings of its own terms.
But the proposition â€œit is raining
in Spainâ€ is contingent, because
denying it does not involve a
contradiction in termsâ€”although
it may still be factually incorrect.
Leibnizâ€™s distinction between
truths of reasoning and truths of
fact is not simply an epistemological
one (about the limits of knowledge),
but also a metaphysical one (about
the nature of the world), and it is
not clear that his arguments
support his metaphysical claim.
Leibnizâ€™s theory of monads seems
to suggest that all truths are truths

of reasoning, which we would have
access to if we could ï¬nish our
rational analysis. But as a truth of
reasoning is a necessary truth, in
what way is it impossible for the
temperature on Betelgeuse to be
2,401 Kelvin rather than 2,400
Kelvin? Certainly not impossible
in the sense that the proposition
2 + 2 = 5 is impossible, for the latter
is simply a logical contradiction.
Likewise, if we follow Leibniz
and separate neccesary and
contingent truths, we end up with
the following problem: I can
discover Pythagorasâ€™s theorem
simply by reï¬‚ecting on the idea of
triangles, so Pythagorasâ€™s theorem
must be a truth of reasoning. But
Betelgeuseâ€™s temperature and
Pythagorasâ€™s theorem are both just
as true, and just as much part of
the monad that is my mindâ€”so
why should one be considered
contingent and the other necessary?
Moreover, Leibniz tells us that
whereas no-one can reach the end of
an inï¬nite analysis, God can grasp
the whole universe at once, and so
for him all truths are neccessary
truths. The difference between a
truth of reasoning and a truth of fact,
therefore, does seem to be a matter
of how one comes to know itâ€”and
in that case it is difï¬cult to see why
the former should always be seen
to be necessarily true, while the
latter may or may not be true.

An uncertain future

God understands
everything through eternal
truth, since he does not
need experience.
Gottfried Wilhelm
Leibniz

In setting out a scheme in which an
omnipotent, omniscient God creates
the universe, Leibniz inevitably
faces the problem of accounting for
the notion of freedom of will. How
can I choose to act in a certain way
if God already knows how I am
going to act? But the problem runs
deeperâ€”there seems to be no room
for genuine contingency at all.
Leibnizâ€™s theory only allows for a

The mechanical calculator was
one of Leibnizâ€™s many inventions. Its
creation is a testament to his interest
in mathematics and logicâ€”ï¬elds in
which he was a great innovator.

distinction between truths whose
necessity we can discover, and
truths whose necessity only God
can see. We know (if we accept
Leibnizâ€™s theory) that the future of
the world is set by an omniscient
and benevolent god, who therefore
has created the best of all possible
worlds. But we call the future
contingent, or undetermined,
because as limited human beings
we cannot see its content.

Leibnizâ€™s legacy
In spite of the difï¬culties inherent
in Leibnizâ€™s theory, his ideas went
on to shape the work of numerous
philosophers, including David Hume
and Immanuel Kant. Kant reï¬ned
Leibnizâ€™s truths of reasoning and
truths of fact into the distinction
between â€œanalyticâ€ and â€œsyntheticâ€
statementsâ€”a division that has
remained central to European
philosophy ever since.
Liebnizâ€™s theory of monads
fared less well, and was criticized
for its metaphysical extravagance.
In the 20th century, however, the
idea was rediscovered by scientists
who were intrigued by Leibnizâ€™s
description of space and time as
a system of relationships, rather
than the absolutes of traditional
Newtonian physics. â– 

138

TO BE IS TO
BE PERCEIVED
GEORGE BERKELEY (1685â€“1753)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Idealism
BEFORE
c.380 BCE In The Republic,
Plato presents his theory of
Forms, which states that the
world of our experience is an
imperfect shadow of reality.
AFTER
1781 Immanuel Kant develops
Berkeleyâ€™s theory into
â€œtranscendental idealismâ€,
according to which the
world that we experience
is only appearance.
1807 Georg Hegel replaces
Kantâ€™s idealism with â€œabsolute
idealismâ€â€”the theory that
absolute reality is Spirit.
1982 In his book The Case
for Idealism, the British
philosopher John Foster
argues for a version of
Berkeleyâ€™s idealism.

L

ike John Locke before him,
George Berkeley was an
empiricist, meaning that
he saw experience as the primary
source of knowledge. This view,
which can be traced back to
Aristotle, stands in contrast to the
rationalist view that, in principle, all
knowledge can be gained through
rational reï¬‚ection alone. Berkeley
shared the same assumptions as
Locke, but reached very different
conclusions. According to Berkeley,
Lockeâ€™s empiricism was moderate;
it still allowed for the existence of
a world independent of the senses,
and followed RenÃ© Descartes in

RENAISSANCE AND THE AGE OF REASON 139
See also: Plato 50â€“55 â–  Aristotle 56â€“63 â–  RenÃ© Descartes 116â€“23 â– 
John Locke 130â€“33 â–  Immanuel Kant 164â€“71 â–  Georg Hegel 178â€“85

from perception.

What we perceive
are ideas, not things
in themselves.

George Berkeley
So the world
consists only
of ideas...

... and minds that
perceive those ideas.

seeing humans as being made
up of two distinct substances,
namely mind and body.
Berkeleyâ€™s empiricism, on the
other hand, was far more extreme,
and led him to a position known
as â€œimmaterialist idealism.â€ This
means that he was a monist,
believing that there is only one
kind of substance in the universe,
and an idealist, believing that
this single substance is mind,
or thought, rather than matter.
Berkeleyâ€™s position is often
summarized by the Latin phrase
esse est percipi (â€œto be is to be
perceivedâ€), but it is perhaps â¯â¯

A thing in
itself must lie
outside experience.

A thing only exists in
so far as it perceives
or is perceived.

There is no
such thing as
what philosophers call
material substance.
George Berkeley

George Berkeley was born and
brought up at Dysart Castle,
near the town of Kilkenny,
Ireland. He was educated ï¬rst
at Kilkenny College, then at
Trinity College, Dublin. In
1707 he was elected a Fellow
of Trinity, and was ordained
an Anglican priest. In 1714,
having written all his major
philosophical works, he left
Ireland to travel around
Europe, spending most
of his time in London.
When he returned to
Ireland he became Dean of
Derry. His main concern,
however, had become a
project to found a seminary
college in Bermuda. In 1728
he sailed to Newport, Rhode
Island, with his wife, Anne
Foster, and spent three years
trying to raise money for the
seminary. In 1731, when it
became clear that funds were
not forthcoming, he returned
to London. Three years later
he became Bishop of Cloyne,
Dublin, where he lived for
the rest of his life.
Key works
1710 Treatise Concerning the
Principles of Human Knowledge
1713 Three Dialogues Between
Hylas and Philonous

140 GEORGE BERKELEY

If there were
external bodies, it is
impossible we should
ever come to know it.
George Berkeley

An idea can be like nothing
but an idea; a color or
ï¬gure can be like nothing
but another color or ï¬gure.
George Berkeley

mistake it for a physical thing itself.
Ideas, then, can only resemble
other ideas. And as our only
experience of the world comes
through our ideas, any claim that
we can even understand the notion
of â€œphysical thingsâ€ is mistaken.
What we are really understanding
are mental things. The world is
constructed purely of thought, and
whatever is not itself perceiving,
exists only as one of our perceptions.

The cause of perception

better represented by esse est aut
perciperi aut percipi (â€œto be is to
perceive or to be perceivedâ€). For
according to Berkeley, the world
consists only of perceiving minds
and their ideas. This is not to say
that he denies the existence of
the external world, or claims that
it is in any way different from what
we perceive. His claim is rather
that all knowledge must come
from experience, and that all we
ever have access to are our
perceptions. And since these
perceptions are simply â€œideasâ€
(or mental representations), we
have no grounds for believing that
anything exists other than ideas
and the perceivers of ideas.

Causation and volition
Berkeleyâ€™s target was Descartesâ€™
view of the world as elaborated
by Locke and the scientist Robert
Boyle. In this view, the physical
world is made up of a vast number
of physical particles, or â€œcorpusclesâ€,
whose nature and interactions give
rise to the world as we understand
it. More controversially, for Berkeley,
this view also maintains that the
world causes the perceptual
ideas we have of it by the way
it interacts with our senses.

Berkeley has two main objections to
this view. First, he argues that our
understanding of causality (the fact
that certain events cause other
events) is based entirely on our
experience of our own volitions (the
way we cause events to happen
through the action of our wills).
His point is not simply that it is
wrong for us to project our own
experience of volitional action onto
the worldâ€”which we do when we
say that the world causes us to
have ideas about the world. His
point is that there is in fact no
such thing as a â€œphysical causeâ€,
because there is no such thing as
a physical world beyond the world
of ideas that could possibly be the
cause of our ideas. The only type
of cause that there is in the world,
according to Berkeley, is precisely
the volitional kind of cause that is
the exercise of the will.
Berkeleyâ€™s second objection is
that because ideas are mental
entities, they cannot resemble
physical entities, because the two
types of thing have completely
different properties. A painting or a
photograph can resemble a physical
object because it is itself a physical
thing, but to think of an idea as
resembling a physical object is to

If things that are not perceivers
only exist in so far as they are
perceived, however, this seems to
mean that when I leave the room,
my desk, computer, books, and so
on all cease to exist, for they are no
longer being perceived. Berkeleyâ€™s
response to this is that nothing is
ever unperceived, for when I am
not in my room, it is still perceived
by God. His theory, therefore, not
only depends on the existence
of God, but of a particular type of
Godâ€”one who is constantly
involved in the world.
For Berkeley, Godâ€™s involvement
in the world runs deeper than this.
As we have seen, he claims that
there are no physical causes, but

Optical illusions are impossible, for
Berkeley, since an object is always as
it appears to be. A straw submerged
in water, for example, really is bent,
and a magniï¬ed object really is larger.

RENAISSANCE AND THE AGE OF REASON 141
only â€œvolitionsâ€, or acts of will, and
it follows that only an act of will can
produce the ideas that we have
about the world. However, I am not
in control of my experience of the
world, and cannot choose what I
experienceâ€”the world simply
presents itself to me the way it does,
whether I like it or not. Therefore,
the volitions that cause my ideas
about the world are not mine; they
are Godâ€™s. So for Berkeley, God not
only creates us as perceivers, he is
the cause and constant generator
of all our perceptions. This raises
a number of questions, the most
urgent being: how is it that we
sometimes perceive things
incorrectly? Why would God
want to deceive us?
Berkeley tries to answer this
question by claiming that our
perceptions are never, in fact, in
error, and that where we go wrong is
in the judgements we make about
what we perceive. For example, if
an oar half-submerged in water
looks bent to me, then it really is
bentâ€”where I go wrong is thinking
that it only appears to be bent.
However, what happens if I reach
into the water and feel the oar? It
certainly feels straight. And since

Can a tree fall over if there is nobody
present to observe it? Objects only exist
while they are perceived, according
to Berkeley. However, the tree
can fall overâ€”because the
tree, and the rest of the
world, is always
perceived by God.

the oar cannot be both straight and
bent at the same time, there must
in fact be two oarsâ€”one that I
see and one that I feel. Even more
problematic for Berkeley, however,
is the fact that two different people
seeing the same oar must in fact be
seeing two different oars, for there
is no single, â€œrealâ€ oar â€œout thereâ€
that their perceptions converge on.

The problem of solipsism

All the choir of heaven and
furniture of earthâ€”in a word,
all those bodies which
compose the frame of the
worldâ€”have not any
subsistence without a mind.
George Berkeley

An inescapable fact of Berkeleyâ€™s
system, therefore, seems to be that
we never perceive the same things.
Each of us is locked in his own
world, cut off from the worlds of
other people. The fact that God has
an idea of an oar cannot help us
here, for that is a third idea, and
therefore a third oar. God caused
my idea and your idea, but unless
we share a single mind with each
other and with God, there are still
three different ideas, so there are
three different oars. This leads us
to the problem of solipsismâ€”the

possibility that the only thing I
can be certain of existingâ€”or
that may in fact existâ€”is myself.
One possible solution to
solipsism runs as follows: since I
can cause changes in the world,
such as raising my own hand, and
since I notice similar changes in
the bodies of other people, I can
infer that those bodies are also
changed by a â€œconsciousnessâ€
inside them. The problem for
Berkeley, though, is that there is no
â€œrealâ€ hand being liftedâ€”the most
a person can do is be the cause of
the idea of his own hand risingâ€”
and only their idea, not another
personâ€™s. I, in other words, must
still rely on God to supply me with
my idea of another personâ€™s hand
rising. Far from supplying us with
empirical certainty, therefore,
Berkeley leaves us depending
for our knowledge of the world,
and of the existence of other
minds, upon our faith in a God
that would never deceive us. â– 

THE AGE
REVOLU
1750â€“1900

OF
TION

144 INTRODUCTION

Volume one of
Denis Diderotâ€™s
EncyclopÃ©die
is published.

Jean-Jacques Rousseauâ€™s
groundbreaking political
work, The Social Contract,
is published.

The American
Declaration of
Independence
is signed.

Immanuel Kant
publishes his Critique
of Pure Reason.

1751

1762

1776

1781

1759

1763

1780

1789

Voltaire publishes Candide,
a novel that satirizes Liebnizâ€™s
notion that â€œall is for the best in
the best of all possible worlds.â€

The Treaty of Paris
makes Britain the
main colonial power
in North America.

Jeremy Bentham develops
the theory of utilitarianism
in his Introduction to the
Principles of Morals and
Legislation, eventually
published in 1789.

The storming of
the Bastille in Paris
marks the start
of the French
Revolution.

D

uring the Renaissance,
Europe had evolved into
a collection of separate
nation states, having previously
been a continent uniï¬ed under the
control of the Church. As power
devolved to separate countries,
distinctive national cultures formed,
which were most obvious in arts
and literature, but could also be
seen in the philosophical styles that
emerged during the 17th century.
During the Age of Reason there
was a very clear difference between
the rationalism of continental
Europe and the empiricism of
British philosophers, and in the
18th century philosophy continued
to center on France and Britain, as
the Enlightenment period unfolded.
Old values and feudal systems
crumbled as the new nations
founded on trade gave rise to a

growing urban middle-class with
unprecedented prosperity. The
richest nations, such as Britain,
France, Spain, Portugal, and the
Netherlands, established colonies
and empires around the world.

France and Britain
Philosophy increasingly focused on
social and political issues, also along
national lines. In Britain, where a
revolution had already come and
gone, empiricism reached a peak
in the works of David Hume, while
the new utilitarianism dominated
political philosophy. This evolved
alongside the Industrial Revolution
that had started in the 1730s, as
thinkers such as John Stuart Mill
reï¬ned the utilitarianism of Jeremy
Bentham and helped to establish
both a liberal democracy and a
framework for modern civil rights.

The situation in France, however,
was less stable. The rationalism
of RenÃ© Descartes gave way to a
generation of philosophes, radical
political philosophers who were to
popularize the new scientiï¬c way
of thinking. They included the
literary satirist Voltaire and the
encyclopedist Denis Diderot, but
the most revolutionary was JeanJacques Rousseau. His vision of a
society governed on the principles
of libertÃ©, egalitÃ©, and fraternitÃ©
(liberty, equality, and fraternity)
provided the battle cry of the
French Revolution in 1789, and has
inspired radical thinkers ever since.
Rousseau believed that civilization
was a corrupting inï¬‚uence on
people, who are instinctively good,
and it was this part of his thinking
set the tone for Romanticism, the
movement that followed.

THE AGE OF REVOLUTION 145

Napoleon
Bonaparte
proclaims himself
Emperor of France.

SÃ¸ren Kierkegaard
writes Either/Or and
Fear and Trembling.

Charles Darwin
publishes the Origin of
Species, explaining his
theory of evolution.

European powers
begin large-scale
colonization of the
African continent.

1802

1843â€“46

1859

188OS

1807

1848

1861

1890

Georg Hegel publishes
Phenomenology of Spirit.

Karl Marx publishes his
Communist Manifesto.
Revolutionary movements
sweep across Europe.

John Stuart Mill
publishes
Utilitarianism.

The leading
pragmatist
William James
publishes The
Principles of
Psychology.

In the Romantic period, European
literature, painting, and music
became preoccupied with an
idealized view of nature, in marked
contrast to the sophisticated urban
elegance of the Enlightenment.
Perhaps the key difference was the
way in which the Romantics valued
feeling and intuition above reason.
The movement took hold throughout
Europe, continuing until the end of
the 19th century.

German Idealism
German philosophy came to
dominate the 19th century, largely
due to the work of Immanuel Kant.
His idealist philosophy, which
claimed that we can never know
anything about things that exist
beyond our selves, radically altered
the course of philosophical thought.
Although only a few years younger

than Hume and Rousseau, Kant
belonged to the next generation:
his major philosophical works were
written after their deaths, and his
new explanation of the universe
and our knowledge of it managed
to integrate the approaches of
rationalism and empiricism in a way
more suited both to Romanticism
and to Germanic culture.
Kantâ€™s followers included Fichte,
Schelling, and Hegel, who together
became known as the German
Idealists, but also Schopenhauer,
whose idiosyncratic interpretation
of Kantâ€™s philosophy incorporated
ideas from Eastern philosophy.
Among the followers of Hegelâ€™s
rigid Idealism was Karl Marx, who
brilliantly brought together German
philosophical methods, French
revolutionary political philosophy,
and British economic theory. After

writing the Communist Manifesto
with Friedrich Engels, he wrote Das
Kapital, arguably one of the most
inï¬‚uential philosophical works of all
time. Within decades of his death,
countries across the world had set
up revolutionary states on the
principles that he had proposed.
Meanwhile in the US, which
had overthrown British colonial rule
and established a republic based
on Enlightenment values, an
American culture independent
of its European roots began to
develop. At ï¬rst Romantic, by the
end of the 19th century it had
produced a homegrown strand
of philosophy, pragmatism, which
examines the nature of truth.
This was in keeping with the
countryâ€™s democratic roots and
well suited to the culture of
the new century. â– 

146

DOUBT IS NOT A
PLEASANT CONDITION,
BUT CERTAINTY
IS
ABSURD
VOLTAIRE (1694â€“1778)
IN CONTEXT
BRANCH
Epistemology
APPROACH
Scepticism
BEFORE
350 BCE Aristotle makes
the ï¬rst reference to a childâ€™s
mind as a â€œblank slateâ€,
which later became known
as a tabula rasa.
1690S John Locke argues that
sense experience allows both
children and adults to acquire
reliable knowledge about the
external world.
AFTER
1859 John Stuart Mill argues
against assuming our own
infallibility in On Liberty.
1900S Hans-Georg Gadamer
and the postmodernists apply
sceptical reasoning to all
forms of knowledge, even that
gained through empirical
(sense-based) information.

V

oltaire was a French
intellectual who lived in
the Age of Enlightenment.
This period was characterized by
an intense questioning of the world
and how people live in it. European
philosophers and writers turned
their attention to the acknowledged
authoritiesâ€”such as the Church
and stateâ€”to question their validity
and their ideas, while also searching
for new perspectives. Until the 17th
century, Europeans had largely
accepted the Churchâ€™s explanations

of what, why, and how things
existed, but both scientists and
philosophers had begun to
demonstrate different approaches
to establishing the truth. In 1690
the philosopher John Locke had
argued that no ideas were innate
(known at birth), and that all ideas
arise from experience alone. His
argument was given further weight
by scientist Isaac Newton whose
experiments provided new ways of
discovering truths about the world.
It was against this background of

Every fact and theory
in history has been
revised at some point.

We are not born with
ideas and concepts
already in our heads.

Every idea and theory
can be challenged.

Doubt is not a
pleasant condition, but
certainty is absurd.

THE AGE OF REVOLUTION 147
See also: Aristotle 56â€“63 â–  John Locke 130â€“33 â–  David Hume 148â€“53
Hans-Georg Gadamer 260â€“61 â–  Karl Popper 262â€“65

â– 

John Stuart Mill 190â€“93

â– 

Scientiï¬c experiments during the
Age of Enlightenment seemed to
Voltaire to lead the way toward a
better world, based on empirical
evidence and unabashed curiosity.

rebellion against the accepted
traditions that Voltaire pronounced
that certainty is absurd.
Voltaire refutes the idea of
certainty in two ways. First, he
points out that apart from a few
necessary truths of mathematics
and logic, nearly every fact and
theory in history has been revised
at some point in time. So what
appears to be â€œfactâ€ is actually little
more than a working hypothesis.
Second, he agrees with Locke that
there is no such thing as an innate
idea, and points out that ideas we
seem to know as true from birth
may be only cultural, as these
change from country to country.

Revolutionary doubt
Voltaire does not assert that there
are no absolute truths, but he sees
no means of reaching them. For
this reason he thinks doubt is the

Voltaire

only logical standpoint. Given that
endless disagreement is therefore
inevitable, Voltaire says that it is
important to develop a system, such
as science, to establish agreement.
In claiming that certainty is
more pleasant than doubt, Voltaire
hints at how much easier it is
simply to accept authoritative
statementsâ€”such as those issued
by the monarchy or Churchâ€”than
it is to challenge them and think

for yourself. But Voltaire believes
it is vitally important to doubt
every â€œfactâ€ and to challenge all
authority. He holds that government
should be limited but speech
uncensored, and that science and
education lead to material and
moral progress. These were
fundamental ideals of both the
Enlightenment and the French
Revolution, which took place
11 years after Voltaireâ€™s death. â– 

Voltaire was the pseudonym of
the French writer and thinker,
FranÃ§ois Marie Arouet. He was
born into a middle-class family in
Paris, and was the youngest of
three children. He studied law
at university, but always preferred
writing, and by 1715 was famous
as a great literary wit. His satirical
writing often landed him in trouble:
he was imprisoned several times
for insulting nobility, and was
once exiled from France. This led
to a stay in England, where he fell
under the inï¬‚uence of English
philosophy and science. After
returning to France he became

wealthy through speculation,
and was thereafter able to
devote himself to writing. He
had several long and scandalous
affairs, and travelled widely
throughout Europe. In later life
Voltaire campaigned vigorously
for legal reform and against
religious intolerance, in France
and further aï¬eld.
Key works
1733 Philosophical Letters
1734 Treatise on Metaphysics
1759 Candide
1764 Philosophical Dictionary

CUSTOM

LIFE
IS THE GREAT GUIDE OF HUMAN

DAVID HUME (1711â€“1776)

150 DAVID HUME
IN CONTEXT
BRANCH
Epistemology
APPROACH
Empiricism
BEFORE
1637 RenÃ© Descartes
espouses rationalism in his
Discourse on the Method.
1690 John Locke sets out the
case for empiricism in An
Essay Concerning Human
Understanding.
AFTER
1781 Immanuel Kant is
inspired by Hume to write
his Critique of Pure Reason.
1844 Arthur Schopenhauer
acknowledges his debt to
Hume in The World as Will
and Representation.

D

avid Hume was born at
a time when European
philosophy was dominated
by a debate about the nature of
knowledge. RenÃ© Descartes had
in effect set the stage for modern
philosophy in his Discourse on the
Method, instigating a movement
of rationalism in Europe, which
claimed that knowledge can be
arrived at by rational reï¬‚ection
alone. In Britain, John Locke had
countered this with his empiricist
argument that knowledge can only
be derived from experience. George
Berkeley had followed, formulating
his own version of empiricism,
according to which the world only
exists in so far as it is perceived.
But it was Hume, the third of the
major British empiricists, who dealt
the biggest blow to rationalism in
an argument presented in his
Treatise of Human Nature.

Humeâ€™s fork

1934 Karl Popper proposes
falsiï¬cation as the basis for the
scientiï¬c method, as opposed
to observation and induction.

With a remarkable clarity of
language, Hume turns a sceptical
eye to the problem of knowledge,
and argues forcibly against the
notion that we are born with
â€œinnate ideasâ€ (a central tenet of
rationalism). He does so by ï¬rst

David Hume

Born in Edinburgh, Scotland, in
1711, Hume was a precocious
child who entered the University
of Edinburgh at the age of 12.
Around 1729 he devoted his time
to ï¬nding â€œsome medium by
which truth might be establishedâ€,
and after working himself into a
nervous breakdown he moved to
La FlÃ¨che in Anjou, France. Here
he wrote A Treatise of Human
Nature, setting out virtually all
his philosophical ideas before
returning to Edinburgh.
In 1763 he was appointed to
the Embassy in Paris, where he
befriended the philosopher

dividing the contents of our minds
into two kinds of phenomena, and
then asking how these relate to
each other. The two phenomena
are â€œimpressionsâ€â€”or direct
perceptions, which Hume calls
the â€œsensations, passions, and
emotionsâ€â€”and â€œideasâ€, which
are faint copies of our impressions,
such as thoughts, reï¬‚ections,
and imaginings. And it is while
analyzing this distinction that
Hume draws an unsettling
conclusionâ€”one that calls into
question our most cherished

In our reasonings
concerning fact, there are
all imaginable degrees
of assurance. A wise man
therefore proportions his
belief to the evidence.
David Hume

Jean-Jacques Rousseau and
became more widely known as
a philosopher. The controversial
Dialogues Concerning Natural
Religion occupied Humeâ€™s ï¬nal
years and, because of what he
called his â€œabundant cautionâ€,
were only published after his
death in Edinburgh in 1776.
Key works
1739 A Treatise of Human Nature
1748 An Enquiry Concerning
Human Understanding
1779 Dialogues Concerning
Natural Religion

THE AGE OF REVOLUTION 151
See also: Plato 50â€“55
Immanuel Kant 164â€“71

â– 
â– 

Aristotle 56â€“63 â–  RenÃ© Descartes 116â€“23 â–  John Locke 130â€“33
Ludwig Wittgenstein 246â€“51 â–  Karl Popper 262â€“65

beliefs, not only about logic and
science, but about the nature of
the world around us.
The problem, for Hume, is that
very often we have ideas that cannot
be supported by our impressions,
and Hume concerns himself with
ï¬nding the extent to which this is
the case. To understand what
he means, we need to note that for
Hume there are only two kinds of
statementâ€”namely â€œdemonstrativeâ€
and â€œprobableâ€ statementsâ€”and he
claims that in everyday experience
we somehow confuse the two types
of knowledge that these express.
A demonstrative statement is
one whose truth or falsity is selfevident. Take, for example, the
statement 2 + 2 = 4. Denying this
statement involves a logical
contradictionâ€”in other words, to
claim that 2 + 2 does not equal 4
is to fail to grasp the meanings of
the terms â€œ2â€ or â€œ4â€ (or â€œ+â€ or â€œ=â€).
Demonstrative statements in logic,
mathematics, and deductive
reasoning are known to be true or
false a priori, meaning â€œprior to
experience.â€ The truth of a â¯â¯

Mathematics and logic yield what
Hume calls â€œdemonstrativeâ€ truths,
which cannot be denied without
contradiction. These are the only
certainties in Humeâ€™s philosophy.

â– 

George Berkeley 138â€“41

â– 

I get into
a habit of expecting
the sun to rise
every morning.

I see the sun rise
every morning.

I reï¬ne this into the
judgment â€œthe sun rises
every morning.â€

This judgment cannot
be a truth of logic, because
the sun not rising (however
unlikely that seems to us)
is conceivable.

This judgment cannot
be empirical, because
I cannot observe future
risings of the sun.

I have no rational
grounds for my belief,
but custom tells me
that it is probable.

Custom is the great
guide of life.

152 DAVID HUME
probable statement, however, is not
self-evident, for it is concerned with
matters of empirical fact. For
example, any statement about the
world such as â€œJim is upstairsâ€, is
a probable statement because it
requires empirical evidence for it
to be known to be true or false. In
other words, its truth or falsity can
only be known through some kind
of experimentâ€”such as by going
upstairs to see if Jim is there.
In light of this, we can ask of
any statement whether it is probable
or demonstrative. If it is neither of
these, then we cannot know it to
be true or false, and so, for Hume,
it is a meaningless statement. This
division of all statements into two

possible kinds, as if forming the
horns of a dilemma, is often referred
to as â€œHumeâ€™s fork.â€

Inductive reasoning
There are no surprises in Humeâ€™s
reasoning so far, but things take
a strange turn when he applies
this line of argument to inductive
inferenceâ€”our ability to infer things
from past evidence.Â We observe an
unchanging pattern, and infer that
it will continue in the future, tacitly
assuming that nature will continue
to behave in a uniform way. For
example, we see the sun rise every
morning, and infer that it will rise
again tomorrow. But is our claim
that nature follows this uniform
The grounds for our belief that
the sun will rise tomorrow, or that
water rather than fruit will ï¬‚ow from
a faucet, are not logical, according to
Hume. They are simply the result of
our conditioning, which teaches us
that tomorrow the world will be
the same as it is today.

pattern really justiï¬able? Claiming
that the sun will rise tomorrow is
not a demonstrative statement, as
claiming the opposite involves no
logical contradiction. Nor is it a
probable statement, as we cannot
experience the sunâ€™s future risings.
The same problem occurs if we
apply Humeâ€™s fork to the evidence
for causality. The statement â€œevent
A causes event Bâ€ seems on the
face of it to be one that we can
verify, but again, this does not
stand up to scrutiny. There is no
logical contradiction involved in
denying that A causes B (as there
would be in denying that 2 + 2 = 4),
so it cannot be a demonstrative
statement. Nor can it be proved
empirically, since we cannot observe
every event A to see if it is followed
by B, so it is not a probable
statement either. The fact that, in
our limited experience, B invariably
follows A is no rational ground for
believing that A will always be
followed by B, or that A causes B.
If there is never any rational
basis for inferring cause and effect,
then what justiï¬cation do we have
for making that connection? Hume
explains this simply as â€œhuman
natureâ€â€”a mental habit that reads
uniformity into regular repetition,
and a causal connection into what

Nature, by an absolute and
uncontrollable necessity,
has determined us to judge
as well as to breathe and feel.
David Hume

THE AGE OF REVOLUTION 153
Science supplies us with ever more
detailed information about the world.
However, according to Hume, science
deals with theories only, and can never
yield a â€œlaw of nature.â€

he calls the â€œconstant conjunctionâ€
of events. Indeed, it is this kind of
inductive reasoning that is the
basis of science, and tempts us to
interpret our inferences as â€œlawsâ€
of natureâ€”but despite what we
may think, this practice cannot
be justiï¬ed by rational argument.
In saying this, Hume makes his
strongest case against rationalism,
for he is saying that it is belief (which
he deï¬nes as â€œa lively idea related
to or associated with a present
impressionâ€), guided by custom,
that lies at the heart of our claims
to knowledge rather than reason.

Custom as our guide
Hume goes on to acknowledge that
although inductive inferences are
not provable, this does not mean
that they are not useful. After all,
we still have a reasonable claim
to expect something to happen,
judging from past observation and
experience. In the absence of a
rational justiï¬cation for inductive
inference, custom is a good guide.
Hume adds, however, that this
â€œmental habitâ€ should be applied
with caution. Before inferring cause
and effect between two events,
we should have evidence both that
this succession of events has been
invariable in the past, and that there
is a necessary connection between
them. We can reasonably predict
that when we let go of an object it
will fall to the ground, because this
is what has always happened in
the past, and there is an obvious
connection between letting go of
the object and its falling. On the
other hand, two clocks set a few
seconds apart will chime one after

anotherâ€”but since there is no
obvious connection between them,
we should not infer that one clockâ€™s
chiming is the cause of the otherâ€™s.
Humeâ€™s treatment of the â€œproblem
of inductionâ€, as this became known,
both undermines the claims of
rationalism and elevates the role of
belief and custom in our lives. As he
says, the conclusions drawn by our
beliefs are â€œas satisfactory to the
mind... as the demonstrative kind.â€

remained a signiï¬cant inï¬‚uence
on German philosophers of the 19th
century and the logical positivists of
the 20th century, who believed that
only meaningful statements could
be veriï¬able. Humeâ€™s account of
the problem of induction remained
unchallenged throughout this period,
and resurfaced in the work of Karl
Popper, who used it to back up his
claim that a theory can only be
deemed scientiï¬c if it is falsiï¬able. â– 

A revolutionary idea
The brilliantly argued and innovative
ideas in the Treatise of Human
Nature were virtually ignored when
they were published in 1739, despite
being the high-point of British
empiricism. Hume was better
known in his own country for being
the author of a History of Great
Britain than for his philosophy; in
Germany, however, the signiï¬cance
of his epistemology had more
impact. Immanuel Kant admitted
to being woken from his â€œdogmatic
slumbersâ€ by reading Hume, who

Hume was perfectly
right in pointing out
that induction cannot be
logically justiï¬ed.
Karl Popper

MAN WAS BORN

FREE
YET EVERYWHERE HE

IS IN CHAINS
JEAN-JACQUES ROUSSEAU (1712â€“1778)

156 JEAN-JACQUES ROUSSEAU
IN CONTEXT
BRANCH
Political philosophy

Man in a
â€œstate of natureâ€ is
fundamentally good.

When the idea of
private property developed,
society had to develop
a system to protect it.

APPROACH
Social contract theory
BEFORE
1651 Thomas Hobbes puts
forward the idea of a social
contract in his book Leviathan.
1689 John Lockeâ€™s Two
Treatises of Government
asserts a humanâ€™s natural right
to defend â€œlife, health, liberty,
or possessions.â€

This system evolved
as laws imposed by
those with property onto
those without property

These laws bind
people in unjust ways.

AFTER
1791 Thomas Paineâ€™s Rights of
Man argues that governmentâ€™s
only purpose is to safeguard
the rights of the individual.
1848 Karl Marx and Friedrich
Engels publish The
Communist Manifesto.
1971 John Rawls develops the
idea of â€œJustice as Fairnessâ€ in
his book A Theory of Justice.

R

ousseau was very much a
product of the mid- to late18th-century period known
as the Enlightenment, and an
embodiment of the continental
European philosophy of the time.
As a young man he tried to make
his name as both a musician and
composer, but in 1740 he met Denis
Diderot and Jean dâ€™Alembert, the
philosopher compilers of the new
EncyclopÃ©die, and became
interested in philosophy. The
political mood in France at this
time was uneasy. Enlightenment
thinkers in France and England had

Man is born free,
yet everywhere
he is in chains.

begun to question the status quo,
undermining the authority of both
the Church and the aristocracy,
and advocates of social reform such
as Voltaire continually fell foul of
the overbearing censorship of the
establishment. Unsurprisingly in
this context, Rousseauâ€™s main
area of interest became political
philosophy. His thinking was
inï¬‚uenced not only by his French
contemporaries, but also by the
work of English philosophersâ€”and
in particular the idea of a social
contract as proposed by Thomas
Hobbes and reï¬ned by John Locke.

Like them, Rousseau compared an
idea of humanity in a hypothetical
â€œnatural stateâ€ with how people
actually live in a civil society.
But he took such a radically
different view of this natural
state and the way it is affected
by society, that it could be
considered a form of â€œcounterEnlightenmentâ€ thinking. It held
within it the seeds of the next
great movement, Romanticism.

Science and art corrupt
Hobbes had envisaged life in the
natural state as â€œsolitary, poor,

THE AGE OF REVOLUTION 157
See also: Thomas Hobbes 112â€“15 â–  John Locke 130â€“33 â–  Edmund Burke 172â€“73
John Stuart Mill 190â€“93 â–  Karl Marx 196â€“203 â–  John Rawls 294â€“95

nasty, brutish, and short.â€ In his
view humanity is instinctively selfinterested and self-serving, and
that civilization is necessary to place
restrictions on these instincts.
Rousseau, however, looks more
kindly on human nature, and sees
civil society as a much less
benevolent force.
The idea that society might be
a harmful inï¬‚uence ï¬rst occurred
to Rousseau when he wrote an essay
for a competition organized by the
Academy of Dijon, answering the
question: â€œHas the restoration of the
sciences and the arts contributed
to reï¬ning moral practices?â€ The
expected answer from thinkers of
the time, and especially from a
musician such as Rousseau, was an
enthusiastic afï¬rmative, but in fact
Rousseau argued the opposite case.
His Discourse on the Sciences and
Arts, which won him ï¬rst prize,

â– 

controversially puts forward the idea
that the arts and sciences corrupt
and erode morals. He argues that far
from improving minds and lives, the
arts and sciences decrease human
virtue and happiness.

The inequality of laws
Having broken with established
thinking in his prize-winning and
publicly acclaimed essay, Rousseau
took the idea a stage further in a
second essay, the Discourse on the
Origin and Foundations of Inequality
among Men. The subject matter
chimed with the mood of the time,
echoing the calls for social reform
from writers such as Voltaire, but â¯â¯
The Romantic movement in art
and literature that dominated the late
18th and early 19th centuries reï¬‚ected
Rousseauâ€™s vision of the state of nature
as one of beauty, innocence, and virtue.

Jean-Jacques Rousseau
Jean-Jacques Rousseau was
born to a Calvinist family in
Geneva. His mother died only
a few days after his birth, and
his father ï¬‚ed home following
a duel a few years later, leaving
him in the care of an uncle.
Aged 16, he left for France
and converted to Catholicism.
While trying to make his name
as a composer, he worked as a
civil servant and was posted to
Venice for two years, but on
his return he began to write
philosophy. His controversial
views led to his books being
banned in Switzerland and
France, and warrants being
issued for his arrest. He was
forced to accept David Humeâ€™s
invitation to live in England for
a short time, but after they
quarrelled he returned to
France under a false name. He
was later allowed to return to
Paris, where he lived until his
death at the age of 66.
Key works
1750 Discourse on the Sciences
and Arts
1755 Discourse on the Origin
and Foundations of Inequality
among Men
1755 Discourse on Political
Economy
1762 The Social Contract

158 JEAN-JACQUES ROUSSEAU
once again Rousseau contradicted
conventional thinking with his
analysis. The selï¬sh, savage, and
unjust state of nature depicted
by Hobbes is, for Rousseau, a
description not of â€œnatural manâ€,
but of â€œcivilized manâ€. In fact
he claims that it is civil society
that induces this savage state.
Humanityâ€™s natural state, he
argues, is innocent, happy, and
independent: man is born free.

Society corrupts
The state of nature that Rousseau
describes is a pastoral idyll, where
people in their natural state are
fundamentally good. (The English
wrongly interpreted Rousseauâ€™s idea
of natural man as a â€œnoble savageâ€,
but this was due to a mistranslation
of the French sauvage, which means
simply â€œnaturalâ€, not brutish.) People
are endowed with innate virtue
and, more importantly, the attributes
of compassion and empathy. But

once this state of innocence is
disrupted, and the power of reason
begins to separate humankind from
the rest of nature, people become
detached from their natural virtues.
The imposition of civil society on
the state of nature therefore entails
a move away from virtue toward
vice, and from idyllic happiness
toward misery.
Rousseau sees the fall from a
state of nature and the establishment
of civil society as regrettable but
inevitable, because it resulted from
the human capacity for reason. The
process began, he thought, the ï¬rst
time that a man enclosed a piece
of land for himself, so introducing
the notion of property. As groups
of people began to live side by side
like this, they formed societies,
which could only be maintained
though a system of laws. But
Rousseau claims that every society
loses touch with humanityâ€™s natural
virtues, including empathy, and so
imposes laws that are not just,
but selï¬sh. They are designed to
protect property, and they are
inï¬‚icted on the poor by the rich.
The move from a natural to a
civilized state therefore brought
about a move not only from virtue
to vice, Rousseau points out, but
also from innocence and freedom
to injustice and enslavement.
Although humanity is naturally
virtuous, it is corrupted by society;
and although man is born free, the
laws imposed by society condemn
him to a life â€œin chains.â€

The Social Contract

Adam and Eve represent the kind of
perfect â€œnaturalâ€ humans that Rousseau
thought predated society. He said that we,
like them, are corrupted by knowledge,
becoming ever more selï¬sh and unhappy.

Rousseauâ€™s second Discourse rufï¬‚ed
even more feathers than his ï¬rst,
but it gained him a reputation and
quite a following. His portrayal of
the state of nature as desirable and
not brutal formed a vital part of the
emerging Romantic movement in
literature. Rousseauâ€™s rallying cry of

Tranquility is found also
in dungeons; but is that
enough to make them
desirable places to live in?
Jean-Jacques
Rousseau

â€œback to nature!â€ and his pessimistic
analysis of modern society as full of
inequalities and injustices sat well
with the growing social unrest of
the 1750s, especially in France.
Not content with merely stating
the problem, Rousseau went on to
offer a solution, in what is seen as
perhaps his most inï¬‚uential work,
The Social Contract.
Rousseau opens his book with
the challenging declaration â€œMan is
born free, yet everywhere he is in
chainsâ€, which was considered such
a call for radical change that it was
adopted as a slogan during the
French Revolution 27Â years later.
Having issued his challenge,
Rousseau then sets out his vision of
an alternative civil society, run not
by aristocrats, the monarchy, and
the Church, but by all citizens, who
participate in the business of
legislation. Modelled on Classical
republican ideas of democracy,
Rousseau imagines the citizen
body operating as a unit,
prescribing laws according to the
volontÃ© gÃ©nÃ©rale, or general will.
The laws would arise from all and
apply to allâ€”everyone would be
considered equal. In contrast with
the social contract envisaged by
Locke, which was designed to

THE AGE OF REVOLUTION 159

The general will
should come from all
to apply to all.
Jean-Jacques
Rousseau

protect the rights and property of
individuals, Rousseau advocates
giving legislative power to the
people as a whole, for the beneï¬t
of all, administered by the general
will. He believes that the freedom to
take part in the legislative process
would lead to an elimination of
inequality and injustice, and that
it would promote a feeling of
belonging to societyâ€”that it would
inevitably lead to the libertÃ©,
Ã©galitÃ©, fraternitÃ© (liberty, equality,
fraternity) that became the motto
of the new French Republic.

The evils of education
In another book written in the same
year, entitled Emile, or On Education,
Rousseau expanded on his theme,
explaining that education was
responsible for corrupting the state
of nature and perpetuating the evils
of modern society. In other books
and essays he concentrated on the
adverse effects of both conventional
religion and atheism. At the center
of all his works lay the idea that
The French Revolution, which
began 11Â years after Rousseauâ€™s death,
was inspired by his claim that it was
unjust for the rich few to rule over the
effectively voiceless, powerless poor.

reason threatens human innocence
and, in turn, freedom and happiness.
Instead of the education of the
intellect, he proposes an education
of the senses, and he suggests that
our religious faith should be guided
by the heart, not the head.

Political inï¬‚uence
Most of Rousseauâ€™s writings were
immediately banned in France,
gaining him both notoriety and a
large following. By the time of his
death in 1778, revolution in France
and elsewhere was imminent, and
his idea of a social contract in which
the general will of the citizen body
controlled the legislative process
offered the revolutionaries a viable
alternative to the corrupt system as
it stood. But his philosophy was at
odds with contemporary thinking,
and his insistence that a state of
nature was superior to civilization
led him to fall out with fellow
reformers such as Voltaire and Hume.

Rousseauâ€™s political inï¬‚uence was
felt most strongly during the period
of revolution immediately after
his death, but his inï¬‚uence on
philosophy, and political philosophy
in particular, emerged to a greater
extent in the 19th century. Georg
Hegel integrated Rousseauâ€™s ideas
of social contract into his own
philosophical system. Later and
more importantly, Karl Marx was
particularly struck by some of
Rousseauâ€™s work on inequality and
injustice. Unlike Robespierre, one of
the leaders of the French Revolution,
who had appropriated Rousseauâ€™s
philosophy for his own ends during
the Reign of Terror, Marx fully
understood and developed
Rousseauâ€™s analysis of capitalist
society and the revolutionary
means of replacing it. Marxâ€™s
Communist Manifesto ends with
a nod to Rousseau, encouraging
the proletarians (workers) have
â€œnothing to lose but their chainsâ€. â– 

160

MAN IS AN
ANIMAL THAT
MAKES
BARGAINS
ADAM SMITH (1723â€“1790)

IN CONTEXT
BRANCH
Political philosophy
APPROACH
Classical economics
BEFORE
c.350 BCE Aristotle emphasizes
the importance of domestic
production (â€œeconomyâ€) and
explains the role of money.
Early 1700s Dutch thinker
Bernard Mandeville argues
that selï¬sh actions can
lead indirectly to socially
desirable consequences.
AFTER
1850s British writer John
Ruskin argues that Smithâ€™s
views are too materialistic
and therefore anti-Christian.
1940s onward Philosophers
apply the idea of bargaining
throughout the social sciences
as a model for explaining
human behavior.

S

cottish writer Adam Smith
is often considered the most
important economist the
world has ever known. The concepts
of bargaining and self-interest that
he explored, and the possibility of
different types of agreements and
interestsâ€”such as â€œthe common
interestâ€â€”are of recurring appeal
to philosophers. His writings are
also important because they give
a more general and abstract form
to the idea of the â€œcommercialâ€
society that was developed by
his friend David Hume.
Like his Swiss contemporary,
Jean-Jacques Rousseau, Smith
assumes that the motives of human
beings are partly benevolent and

THE AGE OF REVOLUTION 161
See also: David Hume 148â€“53 â–  Jean-Jacques Rousseau 154â€“59 â– 
Edmund Burke 172â€“73 â–  Karl Marx 196â€“203 â–  Noam Chomsky 304â€“05

People act out
of self-interest.

We often require
goods and services
that others provide.

Adam Smith
We must therefore
agree to exchange
goods or money between
us in a way that beneï¬ts
both parties.

Man is
an animal
that makes
bargains.

partly self-interested, but that
self-interest is the stronger trait
and so is a better guide to human
behavior. He believes that this can
be conï¬rmed by social observation,
and so, broadly speaking, his
approach is an empirical one. In one
of his most famous discussions of
the psychology of bargaining, he
contends that the most frequent
opening gambit in a bargain is for
one party to urge the otherâ€”â€œthe
best way for you to get what you
want is for you to give me what I
want.â€ In other words, â€œwe address
ourselves, not to [anotherâ€™s]
humanity, but to their self-love.â€
Smith goes on to claim that
the exchange of useful objects is a
distinctively human characteristic.
He notes that dogs are never
observed exchanging bones, and
that should an animal wish to
obtain something, the only way it
can do so is to â€œgain the favor of
those whose service it requiresâ€.
Humans may also depend on this
sort of â€œfawning or servile attentionâ€,
but they cannot resort to it whenever

they need help, because life requires
â€œthe cooperation and assistance of
great multitudes.â€ For example, to
stay comfortably at an inn for a
night we require the input of many
peopleâ€”to cook and serve the food,
to prepare the room and so onâ€”
none of whose services can be
depended on through good will
alone. For this reason, â€œman is an
animal that makes bargainsâ€â€”and
the bargain is struck by proposing
a deal that appears to be in the
self-interest of both parties.

The division of labor
In his account of the emergence of
market economies, Smith argues
that our ability to make bargains
put an end to the once universal
requirement that every person,
or at least every family, be
economically self-sufï¬cient. Thanks
to bargaining, it became possible
for us to concentrate on producing
fewer and fewer goods, and
ultimately to produce just a single
good, or offer a single service, and
to exchange this for everything â¯â¯

The â€œfather of modern
economicsâ€ was born in
Kirkcaldy, Fife, in 1723. An
academic prodigy, Smith
became a professor ï¬rst at
Edinburgh University, then at
Glasgow University where he
became a professor in 1750. In
the 1760s, he took a lucrative
job as a personal tutor to a
young Scottish aristocrat,
Henry Scott, with whom he
visited France and Switzerland.
Already acquainted with
David Hume and other Scottish
Enlightenment thinkers, he
seized the chance to meet
leading ï¬gures of the European
Enlightenment as well. On his
return to Scotland, he spent a
decade writing The Wealth of
Nations, before returning to
public service as Commissioner
of Customs, a position that
allowed him to advise the
British government on various
economic policies. In 1787, he
rejoined Glasgow University,
and spent the last three years
of his life as its rector.
Key works
1759 The Theory of Moral
Sentiments
1776 The Wealth of Nations
1795 Essays on Philosophical
Subjects

162 ADAM SMITH
else we required. This process was
revolutionized by the invention of
money, which abolished the need
to barter. From then on, in Smithâ€™s
view, only those who were unable
to work had to depend on charity.
Everyone else could come to the
marketplace to exchange their
laborâ€”or the money they earned
through laborâ€”for the products
of other peopleâ€™s labor.
This elimination of the need to
provide everything for ourselves led
to the emergence of people with
particular sets of skills (such as
the baker and the carpenter), and
then to what Smith calls a â€œdivision
of laborâ€ among workers. This is
Smithâ€™s phrase for specialization,
whereby an individual not only
pursues a single type of work, but
performs only a single task in a job
that is shared by several people.
The market is the key to establishing
an equitable society, in Smithâ€™s view.
With the freedom provided by the
buying and selling of goods, individuals
can enjoy lives of â€œnatural liberty.â€

The greatest improvement
in the productive
powers of labor seem
to have been the effects
of the division of labor.
Adam Smith

Civilized society stands
at all times in need of
the cooperation
and assistance
of great multitudes.
Adam Smith

Smith illustrates the importance of
specialization at the beginning of
his masterpiece, The Wealth of
Nations, by showing how the
making of a humble metal pin is
radically improved by adopting the
factory system. Where one man
working alone would ï¬nd it hard
to produce 20 perfect pins in a day,
a group of 10 men, charged with
different tasksâ€”from drawing out
the wire, straightening it, cutting

it, pointing it, and grinding it, to
joining it to a pinheadâ€”were able,
in Smithâ€™s time, to produce over
48,000 pins a day.
Smith was impressed by
the great improvements in the
productivity of labor that took place
during the Industrial Revolutionâ€”
improvements that saw workers
provided with much better
equipment, and often saw
machines replacing workers.

THE AGE OF REVOLUTION 163
The jack-of-all-trades could not
survive in such a system, and even
philosophers began to specialize
in the various branches of their
subject, such as logic, ethics,
epistemology, and metaphysics.

The free market
Because the division of labor
increases productivity and makes it
possible for everyone to be eligible
for some kind of work (since it frees
us from training in a craft), Smith
argues that it can lead to universal
wealth in a well-ordered society.
Indeed, he says that in conditions
of perfect liberty, the market can
lead to a state of perfect equalityâ€”
one in which everyone is free to
pursue his own interests in his own
way, so long as it accords with the
laws of justice. And by equality
Smith is not referring to equality
of opportunity, but to equality of
condition. In other words, his goal
is the creation of a society not
divided by competitiveness, but
drawn together by bargaining
based on mutual self-interest.
Smithâ€™s point, therefore, is not
that people should have freedom just
because they deserve it. His point is
that society as a whole beneï¬ts from
individuals pursuing their own
interests. For the â€œinvisible handâ€ of
the market, with its laws of supply
and demand, regulates the amount
of goods that are available, and
prices them far more efï¬ciently than
any government could. Put simply,
the pursuit of self-interest, far
from being incompatible with an
equitable society, is, in Smithâ€™s view,
the only way of guaranteeing it.
In such a society, a government
can limit itself to performing just a
few essential functions, such as
providing defense, criminal justice,
and education, and taxes and duties
can be reduced accordingly. And
just as bargaining can ï¬‚ourish

within national boundaries, so it
can ï¬‚ourish across them, leading to
international tradeâ€”a phenomenon
that was spreading across the
world in Smithâ€™s time.
Smith recognized that there
were problems with the notion of
a free marketâ€”in particular with
the increasingly common bargain
of wages for working time. He also
acknowledged that while the
division of labor had huge
economic beneï¬ts, repetitive work
is not only boring for the worker, it
can destroy a human beingâ€”and
for this reason he proposed that
governments should restrict the
extent to which the production
line is used. Nevertheless, when
The Wealth of Nations was ï¬rst
published, its doctrine of free and
unregulated trade was seen as
revolutionary, not only because of
its attack on established commercial
and agricultural privileges and
monopolies, but also because of its
argument that a nationâ€™s wealth
depends not on its gold reserves,
but on its laborâ€”a view that went
against all economic thinking in
Europe at the time.
Smithâ€™s reputation for being a
revolutionary was bolstered during
the long debate about the nature
of society that followed the French
Revolution of 1789, prompting the
mid-Victorian historian H.T. Buckle
to describe The Wealth of Nations
as â€œprobably the most important
book that has ever been written.â€

Smithâ€™s legacy
Critics have argued that Smith was
wrong to assume that the â€œgeneral
interestâ€ and â€œconsumer interestâ€
are the same, and that the free
market is beneï¬cial to all. What is
true is that even though Smith was
sympathetic toward the victims of
poverty, he never fully succeeded in
balancing the interests of producers

The production line is an incredible
money-creating machine, but Smith
warns against the dehumanizing
effects it can have on workers if it
is used without regulation.

and consumers within his social
model, or integrating into it the
domestic labor, performed mainly
by women, that helped to keep
society running efï¬ciently.
For these reasons, and with the
rise of socialism in the 19th century,
Smithâ€™s reputation declined, but
renewed interest in free market
economics in the late 20th century
saw a revival of Smithâ€™s ideas.
Indeed, only today can we fully
appreciate his most visionary
claimâ€”that a market is more than
just a place. A market is a concept,
and as such can exist anywhereâ€”
not only in a designated place such
as a town square. This foreshadows
the kind of â€œvirtualâ€ marketplace
that only became possible with the
advent of telecommunications
technology. Todayâ€™s ï¬nancial
markets and online trading bear
witness to Smithâ€™s great vision. â– 

THERE ARE TWO WORLDS:

OUR BODIES
AND THE EXTERNAL

WORLD
IMMANUEL KANT (1724â€“1804)

166 IMMANUEL KANT
IN CONTEXT
BRANCH
Metaphysics
APPROACH
Transcendental idealism
BEFORE
1641 RenÃ© Descartes
publishes his Meditations, in
which he doubts all knowledge
apart from the knowledge of
his own consciousness.
1739 David Hume publishes
his Treatise of Human Nature,
which suggests limitations
on how the human mind
perceives reality.
AFTER
19th century The German
idealist movement develops in
response to Kantâ€™s philosophy.
1900s Edmund Husserl
develops phenomenology, the
study of objects of experience,
using Kantâ€™s understanding
of consciousness.

I

mmanuel Kant thought it was
â€œscandalousâ€ that in more than
2,000 years of philosophical
thought, nobody had been able to
produce an argument to prove that
there really is a world out there,
external to us. He particularly had
in mind the theories of RenÃ©
Descartes and George Berkeley,
who both entertained doubts about
the existence of an external world.
At the start of his Meditations,
Descartes argued that we must
doubt all knowledge except that
of our own existence as thinking
beingsâ€”even the knowledge that
there is an external world. He then

went on to counter this sceptical
point of view with an argument
that claims to prove the existence
of God, and therefore the reality of
an outside world. However, many
philosophers (including Kant) have
not found Descartesâ€™ proof of God
to be valid in its reasoning.
Berkeley, on the other hand,
argued that knowledge is indeed
possibleâ€”but that it comes from
experiences our consciousness
perceives. We have no justiï¬cation
for believing that these experiences
have any external existence outside
our own minds.

Time and consciousness
Kant wants to demonstrate that
there is an external, material world,
and that its existence cannot be
doubted. His argument begins as
follows: in order for something to
exist, it must be determinable in
timeâ€”that is, we must be able to
say when it exists and for how long.
But how does this work in the case
of my own consciousness?
Although consciousness seems
to be constantly changing with a
continuous ï¬‚ow of sensations and
thoughts, we can use the word
â€œnowâ€ to refer to what is currently
happening in our consciousness.
But â€œnowâ€ is not a determinate time
or date. Every time I say â€œnowâ€,
consciousness is different.
Here lies the problem: what
makes it possible to specify the
â€œwhenâ€ of my own existence? We
cannot experience time itself,
directly; rather, we experience time
through things that move, change,
or stay the same. Consider the
hands of a clock, constantly moving
slowly around. The moving hands
are useless for determining time on
their ownâ€”they need something
against which they change, such as
the numbers on a clock face. Every
resource I have for measuring my

According to Kant, we can only
experience time through things in the
world that move or change, such as
the hands of a clock. So time is only
ever experienced by us indirectly.

constantly changing â€œnowâ€ is found
in material objects outside me in
space (including my own physical
body). Saying that I exist requires
a determinate point in time, and
this, in turn, requires an actually
existing outside world in which
time takes place. My level of
certainty about the existence of the
external world is thus precisely the
same as my level of certainty about
the existence of consciousness,
which Descartes believed was
absolutely certain.

The problem of science
Kant also looked at how science
understood the exterior world. He
admired the awesome progress
that the natural sciences had made
over the previous two centuries,
compared with the relative
stagnation in the subject from
ancient times until that point. Kant,
along with other philosophers,
wondered what was suddenly being
done correctly in scientiï¬c research.
The answer given by many thinkers
of the period was empiricism. The
empiricists, such as John Locke
and David Hume, argued that there
is no knowledge except that which

THE AGE OF REVOLUTION 167
See also: RenÃ© Descartes 116â€“23 â–  John Locke 130â€“33 â–  George Berkeley 138â€“41 â–  David Hume 148â€“53 â– 
Johann Gottlieb Fichte 176 â–  Georg Hegel 178â€“85 â–  Friedrich Schelling 335 â–  Arthur Schopenhauer 186â€“88

comes to us through our experience
of the world. They opposed the
views of rationalist philosophers,
such as Descartes or Gottfried
Leibniz, who argued that the
mindâ€™s ability to reason and deal
with concepts is more important
for knowledge than experience.
The empiricists claimed that
the recent success of science
was due to scientists being much
more careful in their observations
of the world than they had been
previously, and making fewer
unjustiï¬ed assumptions based
on reason alone. Kant argues that
although this is no doubt partly
true, it could not be the whole
answer, as it is simply false to say
that there was no detailed and
careful empirical observation in
science before the 16th century.
The real issue, Kant argues, is
that a new scientiï¬c method arose
that made empirical observations
valuable. This method involves
two elements. First, it asserts that
concepts such as force or movement
can be perfectly described by
mathematics. Second, it tests its
own conceptions of the world by
asking speciï¬c questions about
nature and observing the answers. â¯â¯

Our sensibility is the
ability to sense things
in the world.

Our understanding
is the ability to think
about things.

Space and time
cannot be learned
about through experience;
they are intuitions
of the mind.

So a thing appears
in space and time only
insofar as it is sensed
by our minds.

Concepts only apply
to things insofar
as they are sensed
by our minds.

A â€œthing-in-itselfâ€
(something considered exterior
to our minds) may have nothing
to do with space,time,
or any of our concepts.

It is precisely in
knowing its limits that
philosophy exists.
Immanuel Kant
â€œThings-in-themselvesâ€
are unknowable.

There are two
worlds: the world of
experience sensed
by our bodies and
the world as it
is in itself.

168 IMMANUEL KANT

Thoughts without content
are empty; intuitions
without concepts are
blindâ€¦ only from their
union can cognition arise.
Immanuel Kant

For example, the experimental
physicist Galileo Galilei wanted to
test the hypothesis that two things
of different weights will nevertheless
fall through the air at the same rate.
He designed an experiment to test
this in such a way that the only
possible explanation of the observed
result would be the truth or falsity
of the hypothesis.
Kant identiï¬es the nature and
importance of the scientiï¬c method.
He believes that this method had
put physics and other subjects on
the â€œsecure road of a science.â€
However, his investigations do not
stop there. His next question is:
â€œWhy is our experience of the world
such that the scientiï¬c method
works?â€ In other words, why is our
experience of the world always
mathematical in nature, and how
is it always possible for human
reason to put questions to nature?

These direct acquaintances he
calls â€œintuitions.â€ Second is what
Kant calls the â€œunderstandingâ€, our
ability to have and use concepts.
For Kant, a concept is an indirect
acquaintance with things as
examples of a type of thing, such
as the concept of â€œbookâ€ in general.
Without concepts we would not
know our intuition was of a book;
without intuitions we would never
know that there were books at all.
Each of these elements has, in
turn, two sides. In sensibility, there
is my intuition of a particular thing
in space and time (like the book)
and my intuition of space and time
as such (my acquaintance with
what space and time are like in
general). In understanding, there is

my concept of some type of thing
(books) and my concept of a â€œthingâ€
as such (substance). A concept
such as substance deï¬nes what
it means to be a thing in general
rather than deï¬ning some type
of thing like a book. My intuition
of a book and the concept of a book
are empirical, for how could I know
anything about books unless I had
come across them in the world?
But my intuition of space and time
and the concept of substance are
a priori, meaning that they are
known before or independently
of any experience.
A true empiricist would argue
against Kant that all acquaintances
come from experienceâ€”in other
words, nothing is a priori. They

Kant split knowledge into intuitions, gained
from direct sensibility of the world, and concepts,
which come indirectly from our understanding.
Some of our knowledgeâ€”both of sensibility and
understandingâ€”comes from empirical evidence,
while some is known a priori.
the concept â€œbookâ€

intuition of a
particular book

Intuitions and concepts
In his most famous work, Critique
of Pure Reason, Kant argues that
our experience of the world involves
two elements. The ï¬rst is what he
calls â€œsensibilityâ€â€”our ability to be
directly acquainted with particular
things in space and time, such as
this book you are reading now.

intuition of
space and time

the concept of substance

Key
Empirical
knowledge
A priori
knowledge

THE AGE OF REVOLUTION 169

Our understanding that entities such
as trees undergo change presupposes an
a priori grasp of the concept â€œsubstanceâ€,
according to Kant. Such concepts are
the preconditions of our experience.

might say that we learn what space
is by observing things in space; and
we learn what substance is from
our observation that the features
of things change without the
underlying thing itself changing.
For instance, though a treeâ€™s leaves
turn from green to brown, and
eventually fall from the tree, it is
still the same tree.

Space and substance
Kantâ€™s arguments show that, on
the contrary, space is an a priori
intuition. In order to learn about
things outside of me, I need to
know that they are outside of me.
But that shows that I could not
learn about space in this way: how
can I locate something outside of
me without already knowing what
â€œoutside of meâ€ means? Some
knowledge of space has to be
assumed before I can ever study
space empirically. We must be
familiar with space a priori.
This argument has an
extraordinary consequence.
Because space itself is a priori, it
does not belong to things in the
world. But our experience of things
in space is a feature of our

sensibility. A thing-in-itselfâ€”Kantâ€™s
term for a thing that is considered
separately from sensibility, and
therefore exterior to our mindsâ€”
may have nothing to do with space.
Kant used similar arguments to
prove the same thing of time.
Kant then turns to proving the
existence of a priori concepts, such
as substance. He asks us ï¬rst to
distinguish between two types of
alteration: variation and change.
Variation concerns the properties
that things have: for instance, a
treeâ€™s leaves may be green or
brown. Change is what the tree
does: the same tree changes its
leaves from green to brown. To
make this distinction is already to
use the notion of substance: the
tree (as substance) changes, but
the leaves (as the properties of
substance) vary. If we do not accept
this distinction, then we cannot
accept the validity of the concept
of substance. We would be saying
that any time there is alteration,
something â€œpopsâ€ into or out of
existence; the tree with green
leaves is annihilated at the same
time that the tree with brown
leaves begins to exist from nothing.
Kant needs to prove that such a
view is impossible. The key to this
is time determination. Time cannot
be directly experienced (it is not a
thing); rather, we experience time
through things that alter or do not
alter, as Kant has already shown. If
we experienced time through the
tree with green leaves and also
experienced time through the tree
with brown leaves without there
being any connection between the
two, then we would be experiencing
two separate real times. Since this
is absurd, Kant believes he has
demonstrated that the concept of
substance is absolutely essential
before we can gain any experience
of the world. And, since it is

through that experience that we
learn anything empirical, the
concept of substance could not
be empirical: it is rather a priori.

The limits of knowledge
A philosophical position that
asserts that some state or activity
of the mind is prior to and more
fundamental than things we
experience is called idealism,
and Kant calls his own position
â€œtranscendental idealism.â€ He
insists that space, time, and
certain concepts are features of
the world we experience (what
Kant called the phenomenal world)
rather than features of the world
itself considered separately from
experience (what Kant called the
noumenal world).
Kantâ€™s claims about a priori
knowledge have both positive
and negative consequences. The
positive consequence is that the
a priori nature of space, time, and
certain concepts is what makes our
experience of the world possible
and reliable. Space and time make
our experience mathematical in
nature; we can measure it against
known values. A priori concepts
such as substance make it possible
to address questions about nature
such as â€œIs that a substance?â€ and
â€œWhat properties does it exhibit â¯â¯

Only from the
human standpoint can
we speak of space.
Immanuel Kant

170 IMMANUEL KANT
and according to what laws?â€ In
other words, Kantâ€™s transcendental
idealism is what makes it possible
for our experience to be considered
useful to science.
On the negative side, certain
types of thinking call themselves
science and even resemble science,
but fail utterly. This is because
they apply to things-in-themselves
intuitions about space and time,
or concepts such as substanceâ€”
which according to Kant must be
valid for experience, but have no
validity with respect to things-inthemselves. Because they resemble
science, these types of thinking are
a constant temptation to us, and
are a trap that many fall into
without realizing it. For example,
we might wish to claim that God is
the cause of the world, but cause
and effect is another of the a priori
concepts, like substance, that Kant
believes is entirely valid for our
experienced world, but not for

Human reason is
troubled by questions that
it cannot dismiss, but
also cannot answer.
Immanuel Kant

things-in-themselves. So the
existence of God (considered, as it
usually is, as a being independent
of the experienced world) is not
something that could be known.
The negative consequence of
Kant's philosophy, then, is to place
quite severe restrictions on the
limits of knowledge.

Reason only has insight
into that which it
produces after a plan
of its own.
Immanuel Kant

Transcendental idealism gives
us a much more radical way of
understanding the distinction
between ourselves and the external
world. What is external to me is
interpreted as not just external to
me in space, but external to space
itself (and to time, and to all the
a priori concepts that make my
experience of the world possible).
And there are two worlds: the
â€œworldâ€ of experience, which
includes both my thoughts and
feelings, and also includes
experience of material things such
as my body, or books; and the
â€œworldâ€ of things-in-themselves,
which is precisely not experienced
and so not in any sense known, and
which we must constantly strive to
avoid fooling ourselves about.
Our bodies have a curious role
to play in all this. On the one hand,
my body as a material thing is a
part of the external world. On the
other hand, the body is a part of us,
and indeed is the medium through
which we encounter other things
The Flammarion woodcut depicts a
man looking outside of space and time.
For Kant, what is external to us is
external to space and time also, and
can never be known as a thing-in-itself.

THE AGE OF REVOLUTION 171
Rationalism
The rationalists believed that
the use of reason, rather than
experience, leads to knowledge
of objects in the world.

Empiricism
The empiricists believed that
knowledge comes from our
experience of objects in the
world, rather than our reason.

Immanuel Kant
Transcendental Idealism
Kantâ€™s theory of transcendental
idealism stated that both reason
and experience were necessary
to understand the world.

(using our skin, nerves, eyes, ears,
and so on). This provides us with
one way of understanding the
distinction between bodies and
the external world: the body as
the medium of my sensations is
different from other external and
material things.

Lasting inï¬‚uence
Kantâ€™s book Critique of Pure Reason
is arguably the most signiï¬cant
single work in the history of
modern philosophy. Indeed, the
whole subject of philosophy is often
divided by many modern thinkers
into everything that happened
before Kant, and everything that
has happened since.
Before Kant, empiricists such as
John Locke emphasized what Kant
termed sensibility, but rationalists
such as Descartes tended to
emphasize understanding. Kant
argues that our experience of the
world always involves both, so it is
frequently said that Kant combined
rationalism and empiricism.

After Kant, German philosophy in
particular progressed rapidly. The
idealists Johann Fichte, Friedrich
Schelling, and Georg Hegel all took
Kantâ€™s thought in new directions
and, in their turn, inï¬‚uenced the
whole of 19th-century thought,
from romanticism to Marxism.
Kant's sophisticated critique of
metaphysical thought was also
important in positivism, which
held that every justiï¬able assertion
is capable of being scientiï¬cally
or logically veriï¬ed.
The fact that Kant locates the
a priori even within our intuitions
of the world was important for
20th-century phenomenologists
such as Edmund Husserl and
Martin Heidegger, who sought to
examine objects of experience
independently of any assumptions
we may have about them. Kantâ€™s
work also remains an important
reference point for contemporary
philosophers today, especially
in the branches of metaphysics
and epistemology. â– 

Immanuel Kant was born into a
family of ï¬nancially struggling
artisans in 1724, and he lived
and worked his whole life in
the cosmopolitan Baltic port
city of Konigsberg, then part
of Prussia. Though he never
left his native province, he
became an internationally
famous philosopher within
his own lifetime.
Kant studied philosophy,
physics, and mathematics at
the University of Konigsberg,
and taught at the same
institution for the next 27
years. In 1792 his unorthodox
views led King Friedrich
Wilhelm II to ban him from
teaching, to which he returned
after the kingâ€™s death ï¬ve
years later. Kant published
throughout his career, but is
best known for the series of
ground-breaking works he
produced in his 50s and 60s.
Though a bright and sociable
man, he never married, and
died at the age of 80.
Key works
1781 Critique of Pure Reason
1785 Foundations of the
Metaphysics of Morals
1788 Critique of Practical
Reason
1790 Critique of Judgement

172

SOCIETY
IS INDEED
A CONTRACT
EDMUND BURKE (1729â€“1797)

IN CONTEXT
BRANCH
Political philosophy
APPROACH
Conservatism
BEFORE
c.350 BCE Aristotle argues that
society is like an organism,
and that man is by nature a
political animal.
5th century St. Augustine of
Hippo argues that government
is a form of punishment for
â€œoriginal sin.â€
17th century Thomas Hobbes
and John Locke develop the
idea of the â€œsocial contract.â€
AFTER
19th century French
philosopher Joseph de Maistre
points out the antidemocratic
legacy of Burke since the
French Revolution.
20th century British
philosopher Michael Oakeshott
develops a more liberal form
of conservatism.

M

any a disaffected person
cries â€œItâ€™s not my fault...
blame society!â€ But the
meaning of the word â€œsocietyâ€ is
not entirely clear, and it has changed
over time. During the 18th century,
when the Irish philosopher and
statesman Edmund Burke was
writing, Europe was becoming
increasingly commercialized, and

the idea that society is a mutual
agreement between its membersâ€”
like a commercial companyâ€”was
readily understood. However, this
point of view also implies that it
is only the material things in life
that matter. Burke attempts to
redress the balance by reminding
us that human beings also enrich
their lives through science, art,

Human beings have
material, scientiï¬c,
artistic, and moral needs.

They cannot meet all these
needs through their own efforts.

They refer to the
customs and religion
of their ancestors
wherever possible.

They agree to help
each other since this
is the best way to meet
their mutual needs.

Society is indeed
a contract.

THE AGE OF REVOLUTION 173
See also: John Locke 130â€“33 â–  David Hume 148â€“53
154â€“59 â–  Adam Smith 160â€“63 â–  John Rawls 294â€“95

and virtue, and that while society
is indeed a contract or partnership,
it is not simply concerned with
economics, or, as he puts it, â€œgross
animal existence.â€ Society embodies
the common good (our agreement
on customs, norms, and values), but
for Burke â€œsocietyâ€ means more
than just the people living nowâ€”
it also includes our ancestors and
descendants. Moreover, because
every political constitution is part
of â€œthe great primeval contract of
eternal societyâ€, God himself is
societyâ€™s ultimate guarantor.
Burkeâ€™s view has the doctrine
of original sin (the idea that we are
born sinful) at its core, so he has
little sympathy for anyone seeking
to blame society for their conduct.
Likewise, he dismisses the idea,
proposed by John Locke, that we
can be perfected by educationâ€”as
though we are born innocent and
merely need to be given the correct
inï¬‚uences. For Burke, the fallibility
of individual judgment is why we
need tradition, to give us the moral
bearings we needâ€”an argument
that echoes David Hume, who
claimed that â€œcustom is the great
guide to human life.â€

â– 

Jean-Jacques Rousseau

Jacques Rousseau, whose book,
The Social Contract, argued that
the contract between citizens and
the state can be broken at any time,
depending on the will of the people.
Another regular target for Burke
was the English philosopher and
scientist Joseph Priestley, who
applauded the French Revolution
and pilloried the idea of original sin.
Despite his scepticism about
modern commercial society, Burke
was a great defender of private
property, and was optimistic about
the free market. For this reason, he is
often hailed as the â€œfather of modern
conservatismâ€â€”a philosophy that
values both economic freedom and
tradition. Today, even socialists
would agree with Burke that private
property is a fundamental social
institution, but would disagree
with him about its value. Likewise,
ecologically-minded philosophers
share his belief in the duties of one
generation to the next, but with
the new agenda of creating a
â€œsustainable society.â€ â– 

Tradition and change
Because society is an organic
structure with roots stretching
deep into the past, Burke believed
its political organization should
develop naturally over time. He
opposed the idea of sweeping or
abrupt political changes that cut
through this natural process. For
this reason he opposed the French
Revolution of 1789, foreseeing its
dangers long before the execution
of the king and the year-long Reign
of Terror. It also prompted him on
several occasions to criticize Jean-

Edmund Burke
The Anglo-Irish politician
Edmund Burke was born
and educated in Dublin. From
his youth onward, he was
convinced that philosophy
was useful training for
politics, and in the 1750s
he wrote notable essays on
aesthetics and the origins
of society. He served as an
English MP from 1766 until
1794, and he was a prominent
member of the Whig partyâ€”
the more liberal of the two
aristocratic parties of the day.
Burke was sympathetic
toward the cause of American
independenceâ€”which sparked
a revolution that was entirely
justiï¬ed, in his viewâ€”and
later became involved in the
impeachment trial of Warren
Hastings, the GovernorGeneral of India. He remained
a scathing critic of colonial
malpractice for the rest of his
life, and earned a reputation
for being the conscience of
the British Empire.
Key works

Burke condemned the French
Revolution for its wholesale rejection
of the past. He believed that change
should occur graduallyâ€”an idea that
became central to modern conservatism.

1756 A Vindication of Natural
Society
1770 Thoughts on the Present
Discontents
1790 Reï¬‚ections on the
Revolution in France

174

THE GREATEST
HAPPINESS FOR THE
GREATEST
NUMBER
JEREMY BENTHAM (1748â€“1832)
IN CONTEXT
BRANCH
Ethics
APPROACH
Utilitarianism
BEFORE
Late 4th century BCE
Epicurus states that the main
goal of life should be the
pursuit of happiness.
Early 17th century Thomas
Hobbes argues that a strong
legal system, with severe
penalties for criminals, leads
to a stable and happier society.
Mid-18th century David
Hume claims that emotion
governs our moral judgement.
AFTER
Mid-19th century John
Stuart Mill advocates education
for all, arguing that it would
improve general happiness.
Late 19th century Henry
Sidgwick says that how moral
an action is equates directly to
the degree of pleasure it brings.

J

eremy Bentham, a legal
reformer and philosopher,
was convinced that all
human activity was driven by
only two motivating forcesâ€”the
avoidance of pain and the pursuit
of pleasure. In The Principles of
Morals and Legislation (1789), he
argues that all social and political
decisions should be made with
the aim of achieving the greatest
happiness for the greatest number
of people. Bentham believes that
the moral worth of such decisions
relates directly to their utility, or
efï¬ciency, in generating happiness
or pleasure. In a society driven by
this â€œutilitarianâ€ approach, he
claims that conï¬‚icts of interest
between individuals can be settled
by legislators, guided solely by the
principle of creating the broadest
possible spread of contentment. If
everyone can be made happy, so
much the better, but if a choice is
necessary, it is always preferable
to favor the many over the few.
One of the main beneï¬ts of his
proposed system, Bentham states,
is its simplicity. By adopting his

ideas, you avoid the confusions and
misinterpretations of more complex
political systems that can often
lead to injustices and grievances.

Calculating pleasure
More controversially, Bentham
proposes a â€œfeliciï¬c calculusâ€ that
can express mathematically the
degree of happiness experienced
by each individual. Using this
precise method, he states, provides
an objective platform for resolving
ethical disputes, with decisions
being made in favor of the view
that is calculated to produce the
highest measure of pleasure.
Bentham also insists that all
sources of pleasure are of equal
value, so that the happiness derived
from a good meal or close friendship
is equal to that derived from an
activity that may require effort or
education, such as engaging in
philosophical debate or reading
poetry. This means that Bentham
assumes a fundamental human
equality, with complete happiness
being accessible to all, regardless
of social class or ability. â– 

See also: Epicurus 64â€“65 â–  Thomas Hobbes 112â€“15
John Stuart Mill 190â€“93 â–  Henry Sidgwick 336

â– 

David Hume 148â€“53

â– 

THE AGE OF REVOLUTION 175

MIND HAS
NO GENDER

MARY WOLLSTONECRAFT (1759â€“1797)

IN CONTEXT
BRANCH
Political philosophy
APPROACH
Feminism
BEFORE
4th century BCE Plato advises
that girls should have a similar
education to boys.
4th century CE Hypatia, a
noted female mathematician
and philosopher, teaches in
Alexandria, Egypt.
1790 In Letters on Education,
British historian Catherine
Macaulay claims the apparent
weakness of women is caused
by their miseducation.
AFTER
1869 John Stuart Millâ€™s The
Subjection of Women argues
for equality of the sexes.
Late 20th century A surge of
feminist activism begins to
overturn most of the social and
political inequalities between
the sexes in Western society.

F

or most of recorded history,
women have been seen as
subordinate to men. But
during the 18th century, the justice
of this arrangement began to be
openly challenged. Among the
most prominent voices of dissent
was that of the English radical
Mary Wollstonecraft.
Many previous thinkers had
cited the physical differences
between the sexes to justify the
social inequality between women
and men. However, in the light of
new ideas that had been formulated
during the 17th century, such as
John Lockeâ€™s view that nearly all
knowledge was acquired through
experience and education, the
validity of such reasoning was
being called into question.

Equal education
Wollstonecraft argues that if men
and women are given the same
education they will acquire the
same good character and rational
approach to life, because they have
fundamentally similar brains and
minds. Her book, A Vindication of

the Rights of Woman, published
in 1792, was partly a response to
Jean-Jacques Rousseauâ€™s Emile
(1762), which recommends that girls
be educated differently to boys, and
that they learn deference to them.
Wollstonecraftâ€™s demand that
women be treated as equal citizens
to menâ€”with equal legal, social,
and political rightsâ€”was still
largely treated with derision in the
late 18th century. But it did sow the
seeds of the suffragette and feminist
movements that were to ï¬‚ourish in
the 19th and 20th centuries. â– 

Let woman share
the rights and she will
emulate the virtues of man.
Mary Wollstonecraft

See also: Plato 50â€“55 â–  Hypatia of Alexandria 331 â–  John Stuart Mill 190â€“93
Simone de Beauvoir 276â€“77 â–  Luce Irigaray 320 â–  HÃ©lÃ¨ne Cixous 322

â– 

176

WHAT SORT OF PHILOSOPHY
ONE CHOOSES DEPENDS
ON WHAT SORT OF
PERSON ONE IS
JOHANN GOTTLIEB FICHTE (1762â€“1814)

IN CONTEXT
BRANCH
Epistemology
APPROACH
Idealism
BEFORE
1641 RenÃ© Descartes discovers
that it is impossible to doubt
that â€œI exist.â€ The self is
therefore the one and only
thing of which we can be sure.
18th century Immanuel
Kant develops a philosophy of
idealism and the transcendental
ego, the â€œIâ€ that synthesizes
information. This forms the
basis of Fichteâ€™s idealism and
notion of the self.
AFTER
20th century Fichteâ€™s
nationalist ideas become
associated with Martin
Heidegger and the Nazi
regime in Germany.
1950S Isaiah Berlin holds
Fichteâ€™s idea of true freedom
of the self as responsible for
modern authoritarianism.

J

ohann Gottlieb Fichte was
an 18th-century German
philosopher and student of
Immanuel Kant. He examined how
it is possible for us to exist as
ethical beings with free will, while
living in a world that appears to be
causally determined; that is to say,
in a world where every event follows
on necessarily from previous events
and conditions, according to
unvarying laws of nature.
The idea that there is a world
like this â€œout thereâ€, beyond our
selves and independent of us, is
known as dogmatism. This is an
idea that gained ground in the
Enlightenment period, but Fichte
thinks that it leaves no room for
moral values or choice. How can
people be considered to have free
will, he asks, if everything is
determined by something else
that exists outside of ourselves?
Fichte argues instead for a
version of idealism similar to Kantâ€™s,
in which our own minds create all
that we think of as reality. In this
idealist world, the self is an active
entity or essence that exists

outside of causal inï¬‚uences, and
is able to think and choose freely,
independently, and spontaneously.
Fichte understands idealism and
dogmatism to be entirely different
starting points. They can never be
â€œmixedâ€ into one philosophical
system, he says; there is no way of
proving philosophically which is
correct, and neither can be used to
refute the other. For this reason one
can only â€œchooseâ€ which philosophy
one believes in, not for objective,
rational reasons, but depending
upon â€œwhat sort of person one is.â€ â– 

Think the I,
and observe what is
involved in doing this.
Johann Gottlieb Fichte

See also: RenÃ© Descartes 116â€“23 â–  Benedictus Spinoza 126â€“29 â– 
Immanuel Kant 164â€“71 â–  Martin Heidegger 252â€“55 â–  Isaiah Berlin 280â€“81

THE AGE OF REVOLUTION 177

ABOUT NO SUBJECT
IS THERE LESS
PHILOSOPHIZING THAN
ABOUT PHILOSOPHY
FRIEDRICH SCHLEGEL (1772-1829)

IN CONTEXT
BRANCH
Metaphilosophy
APPROACH
Reï¬‚exivity
BEFORE
C.450 BCE

Protagoras says that
there are no ï¬rst principles
or absolute truths; â€œman is
the measure of all things.â€
1641 RenÃ© Descartes claims
to have found a ï¬rst principle
on which to build beliefs about
existence when he states that
â€œI think, therefore I am.â€
AFTER
1830 Georg Hegel says that
â€œthe whole of philosophy
resembles a circle of circles.â€
1920S Martin Heidegger
argues that philosophy is a
matter of our relationship with
our own existence.
1967 Jacques Derrida claims
that philosophical analysis can
only be made at the level of
language and texts.

T

he German historian and
poet, Friedrich Schlegel,
is generally credited with
introducing the use of aphorisms
(short, ambiguous sayings) into
later modern philosophy. In 1798
he observed that there was little
philosophizing about philosophy
(metaphilosophy), implying that we
should question both how Western
philosophy functions and its
assumption that a linear type of
argument is the best approach.
Schlegel disagrees with the
approaches of Aristotle and RenÃ©
Descartes, saying they are wrong
to assume that there are solid â€œï¬rst
principlesâ€ that can form a starting
point. He also thinks that it is not
possible to reach any ï¬nal answers,
because every conclusion of an
argument can be endlessly perfected.
Describing his own approach,
Schlegel says philosophy must
always â€œstart in the middleâ€¦ it is a
whole, and the path to recognizing
it is no straight line but a circle.â€
Schlegelâ€™s holistic viewâ€”seeing
philosophy as a wholeâ€”ï¬ts within
the broader context of his Romantic

theories about art and life. These
value individual human emotion
above rational thought, in contrast
to most Enlightenment thinking.
While his charge against earlier
philosophy was not necessarily
correct his contemporary, Georg
Hegel, took up the cause for
reï¬‚exivityâ€”the modern name for
applying philosophical methods to
the subject of philosophy itself. â– 

Philosophy is the art of thinking, and
Schlegel points out that its methods
affect the kind of answers it can ï¬nd.
Western and Eastern philosophies use
very different approaches.

See also: Protagoras 42â€“43 â–  Aristotle 56â€“63 â–  RenÃ© Descartes 116â€“23 â– 
Georg Hegel 178â€“85 â–  Martin Heidegger 252â€“55 â–  Jacques Derrida 308â€“13

REALITY
IS A HISTORICAL

PROCESS
GEORG HEGEL (1770â€“1831)

180 GEORG HEGEL
IN CONTEXT
BRANCH
Metaphysics
APPROACH
Idealism
BEFORE
6th century BCE Heraclitus
claims that all things pass into
their opposites, an important
factor in Hegelâ€™s dialectic.
1781 Immanuel Kant publishes
his Critique of Pure Reason,
which shows the limits of
human knowledge.
1790s The works of Johann
Fichte and Friedrich Schelling
lay the foundations for the
school of German Idealism.
AFTER
1846 Karl Marx writes The
German Ideology, which uses
Hegelâ€™s dialectical method.
1943 Jean-Paul Sartreâ€™s
existentialist work Being and
Nothingness relies upon
Hegelâ€™s notion of the dialectic.

Georg Hegel

H

egel was the single most
famous philosopher in
Germany during the ï¬rst
half of the 19th century. His central
idea was that all phenomena,
from consciousness to political
institutions, are aspects of a single
Spirit (by which he means â€œmindâ€ or
â€œideaâ€) that over the course of time
is reintegrating these aspects into
itself. This process of reintegration
is what Hegel calls the â€œdialecticâ€,
and it is one that we (who are all
aspects of Spirit) understand as
â€œhistory.â€ Hegel is therefore a
monist, for he believes that all
things are aspects of a single thing,
and an idealist, for he believes that
reality is ultimately something
that is not material (in this case
Spirit). Hegelâ€™s idea radically
altered the philosophical landscape,
and to fully grasp its implications
we need to take a look at the
background to his thought.

History and consciousness
Few philosophers would deny that
human beings are, to a great
extent, historicalâ€”that we inherit
things from the past, change them,
and then pass them on to future
generations. Language, for example,
Georg Hegel was born in 1770 in
Stuttgart, Germany, and studied
theology at TÃ¼bingen where he
met and became friends with the
poet Friedrich HÃ¶lderlin and the
philosopher Friedrich Schelling.
He spent several years working
as a tutor before an inheritance
allowed him to join Schelling at
the University of Jena. Hegel
was forced to leave Jena when
Napoleonâ€™s troops occupied the
town, and just managed to rescue
his major work, Phenomenology
of Spirit, which catapulted him to
a dominant position in German
philosophy. In need of funds, he

Certain changes, such those brought
about by the American Revolution,
are explained by Hegel as the progress
of Spirit from a lesser stage of its
development to a higher stage.

is something that we learn and
change as we use it, and the same
is true of scienceâ€”scientists start
with a body of theory, and then go
on either to conï¬rm or to disconï¬rm
it. The same is also true of social
institutions, such as the family, the
state, banks, churches, and so onâ€”
most of which are modiï¬ed forms
of earlier practices or institutions.
became a newspaper editor and
then a school headmaster before
being appointed to the chair of
philosophy ï¬rst in Heidelberg
and then at the prestigious
University of Berlin. At the age
of 41 he married Marie von
Tucher, with whom he had
three children. Hegel died in
1831 during a cholera epidemic.
Key works
1807 Phenomenology of Spirit
1812â€“16 Science of Logic
1817 Encyclopedia of the
Philosophical Sciences

THE AGE OF REVOLUTION 181
See also: Heraclitus 40 â–  Johann Gottlieb Fichte 176
Karl Marx 196â€“203 â–  Jean-Paul Sartre 268â€“71

Human beings, therefore, never
begin their existence from scratch,
but always within some kind of
contextâ€”a context that changes,
sometimes radically within a single
generation. Some things, however,
do not immediately appear to be
historical, or subject to change.
An example of such a thing is
consciousness. We know for certain
that what we are conscious of will
change, but what it means to be
consciousâ€”what kind of a thing it
is to be awake, to be aware, to be
capable of thinking and making
decisionsâ€”is something that we
tend to believe has always been
the same for everyone. Likewise,
it seems plausible to claim that
the structures of thought are not
historicalâ€”that the kind of activity
that thinking is, and what mental
faculties it relies on (memory,
perception, understanding, and so
on), has always been the same for
everyone throughout history. This
was certainly what Hegelâ€™s great
idealist predecessor, Immanuel
Kant, believedâ€”and to understand
Hegel, we need to know what he
thought about Kantâ€™s work.

Kantâ€™s categories
For Kant, the basic ways in which
thought works, and the basic
structures of consciousness, are a
prioriâ€”that is, they exist prior to
(and so are not are not derived from)
experience. This means that they
are independent not only of what we
are thinking about, or are conscious
of, but are independent of any
historical inï¬‚uence or development.
Kant calls these structures
of thought â€œcategoriesâ€, and these
include the concepts â€œcauseâ€,
â€œsubstanceâ€, â€œexistenceâ€, and
â€œreality.â€ For example, experience

â– 

Friedrich Schelling 335

â– 

may give us knowledge about the
outside world, but nothing within
experience itself teaches us that
the outside world actually contains,
for example, causes and effects.
For Kant, knowledge of the basic
structure of the outside world is a
priori knowledge. It is only possible
because we are all born with
categories that supply us with a
framework for experienceâ€”part of
which is the assumption that there
is an external world. However, Kant
continues, this a priori framework
means that the world as it appears â¯â¯

Arthur Schopenhauer 186â€“88

â– 

To comprehend what is
is the task of philosophy,
for what is, is reason.
Georg Hegel

Philosophy must begin
by making no assumptions.

We must not assume
that the structures of
thought and consciousness
never change.

We must not assume that
the whole of reality is divided
into thoughts and the
objects of thought.

These structures themselves
are aspects of spirit.

Thoughts and objects are
both aspects of spirit.

All reality is spirit, and
all spirit undergoes
historical development.

All reality is a
historical process.

182 GEORG HEGEL
Hegelâ€™s dialectic shows how opposites ï¬nd resolution.
A state of tyranny, for example, generates a need for
freedomâ€”but once freedom has been achieved there
can only be anarchy until an element of tyranny is
combined with freedom, creating the synthesis â€œlaw.â€

THESIS

ANTITHESIS

TYRANNY

FREEDOM

they are â€œdialecticalâ€â€”meaning
that they are always subject to
change. Where Kant believes in
an unchanging framework of
experience, Hegel believes that
the framework of experience itself
is subject to changeâ€”as much,
indeed, as the world that we
experience. Consciousness,
therefore, and not merely what
we are conscious of, is part of an
evolving process. This process is
â€œdialecticalâ€â€”a concept that has a
very speciï¬c meaning in Hegelâ€™s
philosophical thought.

Hegelâ€™s dialectic

LAW

SYNTHESIS
is dependent upon the nature of
the human mind, and does not
represent the world as it really isâ€”
in other words, the world as it is â€œin
itself.â€ This â€œworld as it is in itselfâ€
is what Kant calls the noumenal
world, and he claims that it is
unknowable. All that we can
know, according to Kant, is the
world as it appears to us through
the framework of the categoriesâ€”
and this is what Kant calls the
â€œphenomenalâ€ world, or the world
of our everyday experience.

Hegelâ€™s critique of Kant
Hegel believes that Kant made
great strides forward in eliminating
naivety in philosophy, but that his
accounts of the â€œworld in itselfâ€
and the categories still betray
uncritical assumptions. Hegel
argues that Kant fails in at least

two respects to be sufï¬ciently
thorough in his analysis. First of
all, Hegel regards Kantâ€™s notion of
the â€œworld in itselfâ€ as an empty
abstraction that means nothing.
For Hegel, what exists is whatever
comes to be manifested in
consciousnessâ€”for example, as
something sensed or as something
thought. Kantâ€™s second failure, Hegel
argues, is that he makes too many
assumptions about the nature and
origin of the categories.
Hegelâ€™s task is to understand
these categories without making
any assumptions whatsoever,
and the worst assumption that
Hegel sees in Kant concerns the
relationships of the categories to
each other. Kant assumes that the
categories are original and distinct,
and that they are totally separate
from each otherâ€”but for Hegel

The notion of dialectic is central
to what Hegel calls his immanent
(internal) account of the development
of things. He declares that his
account will guarantee four things.
First, that no assumptions are made.
Second, that only the broadest
notions possible are employed, the
better to avoid asserting anything
without justiï¬cation. Third, that it
shows how a general notion gives
rise to other, more speciï¬c, notions.
Fourth, that this process happens
entirely from â€œwithinâ€ the notion
itself. This fourth requirement
reveals the core of Hegelâ€™s logicâ€”
namely that every notion, or
â€œthesisâ€, contains within itself a
contradiction, or â€œantithesisâ€, which
is only resolved by the emergence
of a newer, richer notion, called a
â€œsynthesisâ€, from the original notion
itself. One consequence of this
immanent process is that when we
become aware of the synthesis,
we realize that what we saw as the
earlier contradiction in the thesis
was only an apparent contradiction,
one that was caused by some
limitation in our understanding
of the original notion.
An example of this logical
progression appears at the
beginning of Hegelâ€™s Science of

THE AGE OF REVOLUTION 183

Each of the parts of
philosophy is a philosophical
whole, a circle rounded and
complete in itself.
Georg Hegel

Logic, where he introduces the
most general and all-inclusive
notion of â€œpure beingâ€â€”meaning
anything that in any sense could be
said to be. He then shows that this
concept contains a contradictionâ€”
namely, that it requires the opposite
concept of â€œnothingnessâ€ or â€œnotbeingâ€ for it to be fully understood.
Hegel then shows that this
contradiction is simply a conï¬‚ict

between two aspects of a single,
higher concept in which they ï¬nd
resolution. In the case of â€œbeingâ€
and â€œnot-beingâ€, the concept that
resolves them is â€œbecoming.â€ When
we say that something â€œbecomesâ€,
we mean that it moves from a state
of not-being to a state of beingâ€”so
it turns out that the concept of
â€œbeingâ€ that we started off with
was not really a single concept at
all, but merely one aspect of the
three-part notion of â€œbecoming.â€
The vital point here is that the
concept of â€œbecomingâ€ is not
introduced from â€œoutsideâ€, as it
were, to resolve the contradiction
between â€œbeingâ€ and â€œnot-being.â€
On the contrary, Hegelâ€™s analysis
shows that â€œbecomingâ€ was always
the meaning of â€œbeingâ€ and â€œnotbeingâ€, and that all we had to do
was analyze these concepts to see
their underlying logic.
This resolution of a thesis (being)
with its antithesis (not-being) in a
synthesis (becoming) is just the

beginning of the dialectical process,
which goes on to repeat itself at
a higher level. That is, any new
synthesis turns out, on further
analysis, to involve its own
contradiction, and this in turn
is overcome by a still richer or
â€œhigherâ€ notion. All ideas, according
to Hegel, are interconnected in this
way, and the process of revealing
those connections is what Hegel
calls his â€œdialectical method.â€
In saying that the structures of
thought are dialectical, therefore,
Hegel means that they are not
distinct and irreducible, as Kant
maintained, but that they emerge
from the broadest, emptiest notions
by means of this movement of selfcontradiction and resolution.

Dialectic and the world
The discussion of Hegelâ€™s dialectic
above uses terms such as â€œemergeâ€,
â€œdevelopmentâ€, and â€œmovement.â€
On the one hand, these terms
reï¬‚ect something important â¯â¯

In Hegelâ€™s view, a synthesis emerging from
an antagonism of thesis and antithesis itself
becomes a new thesis, which generates its
own antithesisâ€”which ï¬nally gives birth
to another synthesis. This dialectical
process is one in which Spirit comes to
ever more accurate understandings
of itselfâ€”culminating in
the philosophy of Hegel,
in which it achieves
complete understanding.

T1

A1

S1 / T2
KEY
T = THESIS
A = ANTITHESIS
S = SYNTHESIS

A2

A3

S2 / T3

S3 / T4

184 GEORG HEGEL
about this method of philosophyâ€”
that it starts without assumptions
and from the least controversial
point, and allows ever richer and
truer concepts to reveal themselves
through the process of dialectical
unfolding. On the other hand,
however, Hegel clearly argues that
these developments are not simply
interesting facts of logic, but are real
developments that can be seen at
work in history. For example, a man
from ancient Greece and a man
living in the modern world will
obviously think about different
things, but Hegel claims that their
very ways of thinking are different,
and represent different kinds of
consciousnessâ€”or different stages
in the historical development of
thought and consciousness.
Hegelâ€™s ï¬rst major work,
Phenomenology of Spirit, gives
an account of the dialectical

development of these forms of
consciousness. He starts with the
types of consciousness that an
individual human being might
possess, and works up to collective
forms of consciousness. He does so
in such a way as to show that these
types of consciousness are to be
found externalized in particular
historical periods or eventsâ€”most
famously, for example, in the
American and French revolutions.
Indeed, Hegel even argues that
at certain times in history, Spiritâ€™s
next revolutionary change may
manifest itself as an individual
(such as Napoleon Bonaparte) who,
as an individual consciousness, is
completely unaware of his or her
role in the history of Spirit. And the
progress that these individuals
make is always characterized by
the freeing of aspects of Spirit (in
human form) from recurring states

Each stage of
world-history is a necessary
moment in the Idea of
the World Spirit.
Georg Hegel

of oppression â€”of overcoming
tyrannies that may themselves be
the result of the overcoming of
previous tyrannies.
This extraordinary ideaâ€”that
the nature of consciousness has
changed through time, and changed
in accordance with a pattern that is
visible in historyâ€”means that
there is nothing about human
beings that is not historical in
character. Moreover, this historical
development of consciousness
cannot simply have happened at
random. Since it is a dialectical
process, it must in some sense
contain both a particular sense of
direction and an end point. Hegel
calls this end point â€œAbsolute
Spiritâ€â€”and by this he means a
future stage of consciousness
which no longer even belongs to
individuals, but which instead
belongs to reality as a whole.
At this point in its development,
knowledge is completeâ€”as it must
be, according to Hegel, since Spirit
encompasses, through dialectical
Napoleon Bonaparte, according to
Hegel, perfectly embodied the zeitgeist
(spirit of the age) and was able, through
his actions, to move history into the
next stage of its development.

THE AGE OF REVOLUTION 185

Of the Absolute it must
be said that it is essentially
a result, that only in the
end is it what it truly is.
Georg Hegel

synthesis, both the knower and
what is known. Furthermore, Spirit
grasps this knowledge as nothing
other than its own completed
essenceâ€”the full assimilation of
all forms of â€œothernessâ€ that were
always parts of itself, however
unknowingly. In other words, Spirit
does not simply come to encompass
realityâ€”it comes to be aware of
itself as having always been nothing
other than the movement toward
this encompassing of reality. As
Hegel writes in The Phenomenology
of Spirit, â€œHistory is a conscious,
self-mediating processâ€”[it is]
Spirit emptied out into time.â€

Spirit and nature
But what about the world in which
we live, and which seems to go its
way quite separately from human
history? What does it mean to say
that reality itself is historical?
According to Hegel, what we
ordinarily call â€œnatureâ€ or â€œthe worldâ€
is also Spirit. â€œNature is to be
regarded as a system of stages,â€ he
writes, â€œone arising necessarily from
the other and being the proximate
truth of the stage from which it
results.â€ He goes on to claim that
one of the stages of nature is the
progression from that which is

â€œonly Lifeâ€ (nature as a living whole)
to that which has â€œexistence as
Spiritâ€ (the whole of nature now
revealed as always having been,
when properly understood, Spirit).
At this stage of nature, a different
dialectic begins, namely that of
consciousness itselfâ€”of the forms
that Absolute Spirit takes in its
dialectical progression toward selfrealization. Hegelâ€™s account
of this progression begins with
consciousness ï¬rst thinking of
itself as an individual thing among
other individuals, and occupying a
separate space to that of matter or
the natural world. Later stages of
consciousness, however, are no
longer those of individuals, but are
those of social or political groupsâ€”
and so the dialectic continues,
reï¬ning itself until it reaches the
stage of Absolute Spirit.

only one realityâ€”that of Spirit,
which knows and reï¬‚ects on
itself, and is both thought and
what is thought about.
The â€œWhole of Spiritâ€, or
â€œAbsolute Spiritâ€, is the end point
of Hegelâ€™s dialectic. However,
the preceding stages are not left
behind, as it were, but are revealed
as insufï¬ciently analyzed aspects
of Spirit as a whole. Indeed, what
we think of as an individual person
is not a separate constituent of
reality, but is an aspect of how
Spirit developsâ€”or how it â€œempties
itself out into time.â€ Thus, Hegel
writes, â€œThe True is the Whole.
But the Whole is nothing other
than the essence consummating
itself through its development.â€
Reality is Spiritâ€”both thought
and what is known by thoughtâ€”
and undergoes a process of
historical development. â– 

Spirit and mind
At the time Hegel was writing,
there was a dominant philosophical
view that there are two kinds of
entities in the worldâ€”things that
exist in the physical world and
thoughts about those thingsâ€”
these latter being something like
pictures or images of the things.
Hegel argues that all versions of
this distinction are mistakes, and
involve committing ourselves to the
ridiculous scenario in which two
things are both absolutely different
(things and thoughts), but also
somehow similar (because the
thoughts are images of things).
Hegel argues that it only seems
as though the objects of thought are
different from thought itself. For
Hegel, the illusion of difference and
separation between these two
apparent â€œworldsâ€ is shown as such
when both thought and nature are
revealed as aspects of Spirit. This
illusion is overcome in Absolute
Spirit, when we see that there is

German history had reached its end
point in the Prussian state, according
to Hegel. However, there was a strong
feeling in favor of a united Germany, as
personiï¬ed by the ï¬gure of Germania.

186

EVERY MAN TAKES
THE LIMITS OF
HIS OWN FIELD
OF VISION FOR
THE LIMITS OF
THE WORLD

ARTHUR SCHOPENHAUER (1788â€“1860)

IN CONTEXT
BRANCH
Metaphysics
APPROACH
Idealism
BEFORE
1690 John Locke publishes
An Essay Concerning Human
Understanding, explaining
how all our knowledge comes
from experience.
1781 Immanuel Kantâ€™s Critique
of Pure Reason introduces the
concept of a â€œthing in itselfâ€,
which Schopenhauer used as
a starting point for his ideas.
AFTER
Late 19th century Friedrich
Nietzsche puts forward the
notion of a â€œWill to powerâ€ to
explain human motivations.
Early 20th century Austrian
psychoanalyst Sigmund Freud
explores what lies behind our
basic human urges.

A

rthur Schopenhauer was not
part of the mainstream of
early 19th-century German
philosophy. He acknowledged
Immanuel Kant, whom he idolized,
as a major inï¬‚uence, but dismissed
the idealists of his own generation,
who held that reality ultimately
consists of something nonmaterial.
Most of all he detested the idealist
Georg Hegel for his dry writing
style and optimistic philosophy.
Using Kantâ€™s metaphysics as
his starting point, Schopenhauer
developed his own view of the
world, which he expressed in clear,
literary language. He took Kantâ€™s
view that the world is divided into
what we perceive through our

THE AGE OF REVOLUTION 187
See also: Empedocles 330 â–  John Locke 130â€“33 â–  Immanuel Kant 164â€“71
Georg Hegel 178â€“85 â–  Friedrich Nietzsche 214â€“21

â– 

My version of the world
is limited byâ€¦

â€¦the limited
observations I can
make of a vast universe.

â€¦my limited
experience of a vast
universal Will, of which
my will is just a part.

My version of the world does
not include things I have
not perceived, nor the universal
Will I have not experienced.

I take the limits of my own ï¬eld of
vision for the limits of the world.

senses (phenomena), and â€œthings in
themselvesâ€ (noumena), but he
wanted to explain the nature of the
phenomenal and noumenal worlds.

Interpreting Kant
According to Kant, we each
construct a version of the world
from our perceptionsâ€”the
phenomenal worldâ€”but we can
never experience the noumenal
world as it is â€œin itself.â€ So we each
have a limited vision of the world,
as our perceptions are built from
information acquired through a
limited set of senses. Schopenhauer
adds to this that â€œevery man takes
the limits of his own ï¬eld of vision
for the limits of the world.â€

The idea of knowledge being
limited to our experience was not
an entirely new one; the ancient
philosopher Empedocles had said
that â€œeach man believes only his
experienceâ€, and in the 17th
century John Locke had asserted
that â€œno manâ€™s knowledge here can
go beyond his experience.â€ But the
reason Schopenhauer gives for this
limitation is quite new, and it
comes from his interpretation of
Kantâ€™s phenomenal and noumenal
worlds. The important difference
between Kant and Schopenhauer
is that for Schopenhauer, the
phenomenal and noumenal are
not two different realities or worlds,
but the same world, experienced â¯â¯

Arthur Schopenhauer
Born into a wealthy and
cosmopolitan family in Danzig
(now Gdansk), Schopenhauer
was expected to become a
merchant like his father. He
travelled through France and
England before his family
settled in Hamburg in 1793. In
1805, after his fatherâ€™s deathâ€”
possibly by suicideâ€”he felt
able to stop working and go to
university, where he studied
philosophy and psychology.
He maintained an uneasy
relationship with his mother,
who constantly criticized
his achievements.
After completing his
studies, Schopenhauer taught
at Berlin University. He attained
a reputation as a philanderer
and misogynist; he had several
affairs and avoided marriage,
and was once convicted of
assaulting a woman. In 1831
he moved to Frankfurt, where
he lived until his death with a
succession of poodles called
either Atman (â€œsoulâ€ in
Hinduism and Buddhism) or
Butz (German for hobgoblin).
Key works
1818 and 1844 The World as
Will and Representation
1851 Parerga and
Paralipomena

188 ARTHUR SCHOPENHAUER
differently. It is one world, with two
aspects: Will and Representation.
This is most easily evidenced by our
bodies, which we experience in two
ways: we perceive them as objects
(Representations), and experience
them from within (as Will).
Schopenhauer says that an act
of will, such as wishing to raise my
arm, and the resulting movement,
are not in two different worldsâ€”the
noumenal and phenomenalâ€”but
the same event experienced in two
different ways. One is experienced
from inside, the other observed
from outside. When we look at
things outside ourselves, although
we see only their objective
Representation, not their inner
reality or Will, the world as a whole
still has the same simultaneous
outer and inner existences.

A universal Will
Schopenhauer uses the word â€œwillâ€
to express a pure energy that has
no driving direction, and yet is
responsible for everything that
manifests itself in the phenomenal
world. He believes, like Kant, that
space and time belong in the
phenomenal worldâ€”they are

Schopenhauer studied the Hindu
Bhagavad Gita, in which Krishna the
charioteer tells Arjuna that a man is
a slave to his desires unless he can
free himself from his cravings.

concepts within our minds, not
things outside of themâ€”so the
Will of the world does not mark
time, or follow causal or spatial
laws. This means it must be
timeless and indivisible, and so
must our individual wills. It follows,
then, that the Will of the universe
and individual will are one and the
same thing, and the phenomenal
world is controlled by this vast,
timeless, motiveless Will.

The fundament upon
which all our knowledge
and learning rests
is the inexplicable.
Arthur Schopenhauer

Eastern inï¬‚uence
At this point in his argument,
Schopenhauerâ€™s pessimism shows
through. Where contemporaries
such as Hegel saw will as a positive
force, Schopenhauer sees humanity
at the mercy of a mindless, aimless
universal Will. It lies behind our
most basic urges, he insists, and
is what causes us to live lives of
constant disappointment and
frustration as we attempt to relieve
our cravings. For Schopenhauer, the
world is neither good nor bad, but
meaningless, and humans who
struggle to ï¬nd happiness achieve
at best gratiï¬cation and at worst
pain and suffering.
The only escape from this
miserable condition, according to
Schopenhauer, is nonexistence or
at least a loss of will for gratiï¬cation.
He proposes that relief can be found
through aesthetic contemplation,
especially in music, which is the
one art that does not attempt to
represent the phenomenal world.
Schopenhauerâ€™s philosophy here
echoes the Buddhist concept of
nirvana (a transcendent state free
from desire or suffering). He had
studied Eastern thinkers and
religions in great detail.
From his idea of one universal
Will, Schopenhauer develops a
moral philosophy that may be
somewhat surprising, considering
his otherwise misanthropic and

pessimistic character. He realizes
that if we can recognize that our
separateness from the universe is
essentially an illusionâ€”because all
our individual wills and the Will of
the universe are one and the same
thingâ€”we can learn empathy with
everyone and everything else, and
moral goodness can arise from a
universal compassion. Here, again,
Schopenhauerâ€™s thinking reï¬‚ects
the ideals of Eastern philosophy.

Lasting legacy
Schopenhauer was largely ignored
by other German philosophers in
his lifetime, and his ideas were
overshadowed by those of Hegel,
though he did have an inï¬‚uence
on writers and musicians. Toward
the end of the 19th century, the
primacy he gave to Will became
a theme in philosophy once more.
Friedrich Nietzsche in particular
acknowledged his inï¬‚uence, and
Henri Bergson and the American
pragmatists also owe something
to his analysis of the world as Will.
Perhaps Schopenhauerâ€™s greatest
inï¬‚uence, however, was in the ï¬eld
of psychology, where his ideas
about our basic urges and their
frustration inï¬‚uenced the
psychoanalytic theories of both
Sigmund Freud and Carl Jung. â– 

THE AGE OF REVOLUTION 189

THEOLOGY IS
ANTHROPOLOGY

LUDWIG ANDREAS FEUERBACH (1804â€“1872)

IN CONTEXT
BRANCH
Philosophy of religion
APPROACH
Atheism
BEFORE
C.600 BCE Thales is the ï¬rst
Western philosopher to deny
that the universe owes its
existence to a god.
C.500 BCE

The Indian school
of atheistic philosophy known
as Carvaka is established.

C.400 BCE

The ancient Greek
philosopher Diagoras of Melos
puts forward arguments in
defense of atheism.

AFTER
Mid-19th century Karl
Marx uses Feuerbachâ€™s
reasoning in his philosophy
of political revolution.
Late 19th century The
psychoanalyst Sigmund
Freud argues that religion is
a projection of human wishes.

T

he 19th-century German
philosopher Ludwig
Feuerbach is best known
for his book The Essence of
Christianity (1841), which inspired
revolutionary thinkers such as Karl
Marx and Friedrich Engels. The
book incorporates much of the
philosophical thinking of Georg
Hegel, but where Hegel saw an
Absolute Spirit as the guiding force
in nature, Feuerbach sees no reason
to look beyond our experience to
explain existence. For Feuerbach,
humans are not an externalized
form of an Absolute Spirit, but the
opposite: we have created the idea
of a great spirit, a god, from our
own longings and desires.

than anthropology (the study of
humanity). Not only have we
deceived ourselves into thinking
that a divine being exists, we have
also forgotten or forsaken what we
are ourselves. We have lost sight of
the fact that these virtues actually
exist in humans, not gods. For this
reason we should focus less on
heavenly righteousness and more
on human justiceâ€”it is people
in this life, on this Earth, that
deserve our attention. â– 

Imagining God
Feuerbach suggests that in our
yearning for all that is best in
humankindâ€”love, compassion,
kindness, and so onâ€”we have
imagined a being that incorporates
all of these qualities in the highest
possible degree, and then called
it â€œGod.â€ Theology (the study of
God) is therefore nothing more
See also: Thales of Miletus 22â€“23

â– 

The Israelites of the Bible, in their
need for certainty and reassurance,
created a false godâ€”the golden calfâ€”
to worship. Feuerbach argues that all
gods are created in the same way.

Georg Hegel 178â€“85

â– 

Karl Marx 196â€“203

190

OVER HIS OWN
BODY AND MIND,
THE INDIVIDUAL
IS SOVEREIGN

JOHN STUART MILL (1806â€“1873)

IN CONTEXT
BRANCH
Political philosophy
APPROACH
Utilitarianism
BEFORE
1651 In Leviathan, Thomas
Hobbes says that people
are â€œbrutishâ€ and must be
controlled by a social contract.
1689 John Lockeâ€™s book, Two
Treatises of Government, looks
at social contract theory in the
context of empiricism.
1789 Jeremy Bentham
advocates the â€œgreatest
happiness principle.â€
AFTER
1930s Economist J.M. Keynes,
inï¬‚uenced by Mill, develops
liberal economic theories.
1971 John Rawls publishes
A Theory of Justice, based on
the idea that laws should be
those everyone would accept.

J

ohn Stuart Mill was born into
an intellectually privileged
family, and he was aware
from an early age of the British
traditions of philosophy that had
emerged during the Enlightenment
of the 18th century. John Locke and
David Hume had established a
philosophy whose new empiricism
stood in stark contrast to the
rationalism of continental European
philosophers. But during the late
18th century, Romantic ideas from
Europe began to inï¬‚uence British
moral and political philosophy. The
most obvious product of this
inï¬‚uence was utilitarianism, which
was a very British interpretation of
the political philosophy that had

THE AGE OF REVOLUTION 191
See also: Thomas Hobbes 112â€“15 â–  John Locke 130â€“33 â–  Jeremy Bentham 174
Bertrand Russell 236â€“39 â–  Karl Popper 262â€“65 â–  John Rawls 294â€“95

Decisions should be
made on the principle
of the greatest good for
the greatest number.

â– 

Individuals should be
free to do whatever gives
them pleasure, even if
it could harm themâ€¦

John Stuart Mill

Individuals can choose
to do things that affect
their own body, but not
that of someone else.

â€¦but they are
not entitled to do
things that could
harm others.

Over his own
body and mind,
the individual
is sovereign.

shaped the 18th-century revolutions
of both Europe and America. Its
originator, Jeremy Bentham, was
a friend of the Mill family, and he
inï¬‚uenced Johnâ€™s home education.

Victorian liberalism
As a philosopher Mill sets himself
the task of synthesizing a valuable
intellectual heritage with the new
19th-century Romanticism. His
approach is less sceptical than
that of Hume (who argued that all
knowledge comes from sense

experience, and nothing is certain)
and less dogmatic than Bentham
(who insisted that everything be
judged on its usefulness), but their
empiricism and utilitarianism
informed his thinking. Millâ€™s moral
and political philosophy is less
extreme than his predecessorsâ€™,
aiming for reform rather than
revolution, and it formed the basis
of British Victorian liberalism.
After completing his ï¬rst
philosophical work, the exhaustive
six-volume System of Logic, Mill â¯â¯

John Stuart Mill was born in
London in 1806. His father
was the Scottish philosopher
and historian James Mill, who
founded the movement of
â€œphilosophical radicalsâ€ with
Jeremy Bentham. John was
educated at home by his
father, whose demanding
program began with teaching
Greek to John when he was
only three years old.
After years of intense study,
Mill suffered a breakdown
at the age of 20. He left
university to work for the East
India Company, where he
stayed until his retirement in
1857, as it gave him a living
and time to write. During this
period he met Harriet Taylor,
advocate of womenâ€™s rights,
whoâ€”after a relationship of 20
yearsâ€”eventually became his
wife. Mill served as a Member
of Parliament from 1865 to
1868, putting into practice his
moral and political philosophy.
Key works
1843 System of Logic
1848 Principles of Political
Economy
1859 On Liberty
1861 Utilitarianism
1869 The Subjection of Women
1874 On Nature

192 JOHN STUART MILL
turned his attention to moral
philosophy, particularly Benthamâ€™s
theories of utilitarianism. He had
been struck by the elegant simplicity
of Benthamâ€™s principle of â€œthe
greatest happiness for the greatest
numberâ€, and was a ï¬rm believer in
its usefulness. He describes his
interpretation of how utilitarianism
might be applied as similar to Jesus
of Nazarethâ€™s â€œgolden ruleâ€: do as
you would be done by, and love your
neighbor as yourself. This, he says,
constitutes â€œthe ideal perfection of
utilitarian morality.â€

Legislating for liberty
Mill supports Benthamâ€™s happiness
principle, but he thinks it lacks
practicality. Bentham had seen the
idea as depending upon an abstract
â€œfeliciï¬c calculusâ€ (an algorithm for
calculating happiness), but Mill

wants to ï¬nd out how it might
be implemented in the real world.
He is interested in the social and
political implications of the principle,
rather than merely its use in
making moral decisions. How
would legislation promoting the
â€greatest happiness of the greatest
numberâ€ actually affect the
individual? Might laws that sought
to do this, enacting a kind of majority
rule, actually prevent some people
from achieving happiness?
Mill thinks that the solution
is for education and public opinion
to work together to establish an
â€œindissoluble associationâ€ between
an individualâ€™s happiness and the
good of society. As a result, people
would always be motivated to act
not only for their own good or
happiness, but toward that of
everyone. He concludes that society

It is better
to be Socrates
dissatisï¬ed than
a fool satisï¬ed.
John Stuart Mill

should therefore allow all individuals
the freedom to pursue happiness.
Furthermore, he says that this
right should be protected by the
government, and that legislation
should be drawn up to protect the
individualâ€™s freedom to pursue
personal goals. There is, however,
one situation in which this freedom
should be curtailed, Mill says, and
that is where one personâ€™s action
impinges on the happiness of
others. This is known as the â€œharm
principle.â€ He underlines this by
pointing out that in these cases, a
personâ€™s â€œown good, either physical
or moral, is not a sufï¬cient warrant.â€

Quantifying happiness
Mill then turns his attention to how
best to measure happiness. Bentham
had considered the duration and
intensity of pleasures in his feliciï¬c
calculus, but Mill thinks it is also
important to consider the quality
of pleasure. By this, he is referring
to the difference between a simple
satisfaction of desires and sensual
pleasures, and happiness gained
The good samaritan helps his enemy
in a biblical parable that demonstrates
Millâ€™s golden rule: do as you would be
done by. He believed this would raise
societyâ€™s overall level of happiness.

THE AGE OF REVOLUTION 193
through intellectual and cultural
pursuits. In the â€œhappiness
equationâ€ he gives more weight
to higher, intellectual pleasures
than to baser, physical ones.
In line with his empiricist
background, Mill then tries to pin
down the essence of happiness.
What is it, he asks, that each
individual is striving to achieve?
What causes happiness? He
decides that â€œthe sole evidence it is
possible to produce that anything
is desirable, is that people do
actually desire it.â€ This seems a
rather unsatisfactory explanation,
but he goes on to distinguish
between two different desires:
unmotivated desires (the things we
want that will give us pleasure) and
conscientious actions (the things
we do out a sense of duty or charity,
often against our immediate
inclination, that ultimately bring
us pleasure). In the ï¬rst case, we
desire something as a part of our
happiness, but in the second we
desire it as a means to our
happiness, which is felt only when
the act reaches its virtuous end.

Practical utilitarianism
Mill was not a purely academic
philosopher, and he believed his
ideas should be put into practice,
so he considered what this might
mean in terms of government and
legislation. He saw any restriction
of the individualâ€™s freedom to pursue
happiness as a tyranny, whether
this was the collective tyranny of
the majority (through democratic
election) or the singular rule of a
despot. He therefore suggested
practical measures to restrict the
power of society over the individual,
and to protect the rights of the
individual to free expression.
In his time as a Member of
Parliament, Mill proposed many
reforms which were not to come

The National Society for Womenâ€™s
Suffrage was set up in Britain in 1868,
a year after Mill tried to secure their
legal right to vote by arguing for an
amendment to the 1867 Reform Act.

about until much later, but his
speeches brought the liberal
applications of his utilitarian
philosophy to the attention of a
wide public. As a philosopher and
politician, he argued strongly in
defense of free speech, for the
promotion of basic human rights,
and against slaveryâ€”all of which
were obvious practical applications
of his utilitarianism. Strongly
inï¬‚uenced by his wife Harriet
Taylor-Mill, he was the ï¬rst British
parliamentarian to propose votes
for women as part of his government
reforms. His liberalist philosophy
also encompassed economics, and
contrary to his fatherâ€™s economic
theories, he advocated a freemarket economy where government
intervention is kept to a minimum.

A softer revolution
Mill places the individual, rather than
society, at the center of his utilitarian
philosophy. What is important is
that individuals are free to think
and act as they please, without
interference, even if what they do is
harmful to them. Every individual,
says Mill in his essay On Liberty,
is â€œsovereign over his own body

and mind.â€ His ideas came to
embody Victorian liberalism,
softening the radical ideas that had
led to revolutions in Europe and
America, and combining them with
the idea of freedom from interference
by authority. This, for Mill, is the
basis for just governance and the
means to social progress, which
was an important Victorian ideal.
He believes that if society leaves
individuals to live in a way that
makes them happy, it enables them
to achieve their potential. This
in turn beneï¬ts society, as the
achievements of individual talents
contribute to the good of all.
In his own lifetime Mill was
regarded as a signiï¬cant philosopher,
and he is now considered by many
to be the architect of Victorian
liberalism. His utilitarian-inspired
philosophy had a direct inï¬‚uence on
social, political, philosophical, and
economic thinking well into the
20th century. Modern economics
has been shaped from various
interpretations of his application
of utilitarianism to the free market,
notably by the British economist
John Maynard Keynes. In the ï¬eld
of ethics, philosophers such as
Bertrand Russell, Karl Popper,
William James, and John Rawls all
took Mill as their starting point. â– 

One person with a belief
is a social power
equal to 99 who have
only interests.
John Stuart Mill

194

ANXIETY IS THE
DIZZINESS OF
FREEDOM
SÃ˜REN KIERKEGAARD (1813â€“1855)

IN CONTEXT
BRANCH
Metaphysics

When making decisions,
we have absolute
freedom of choice.
We realize that we can
choose to do nothing,
or anything.

APPROACH
Existentialism
BEFORE
1788 Immanuel Kant stresses
the importance of freedom
in moral philosophy in his
Critique of Practical Reason.
1807â€“22 Georg Hegel suggests
a historical consciousness,
or Geist, establishing a
relationship between human
consciousness and the world
in which it lives.
AFTER
1927 Martin Heidegger
explores the concepts of Angst
and existential guilt in his
book Being and Time.
1938 Jean-Paul Sartre lays
down the foundations of his
existentialist philosophy.
1946 Ludwig Wittgenstein
acknowledges Kierkegaardâ€™s
work in Culture and Value.

Our minds reel at
the thought of this
absolute freedom.
A feeling of dread
or anxiety accompanies
the thought.

Anxiety is the
dizziness of freedom.

S

Ã¸ren Kierkegaardâ€™s philosophy
developed in reaction to the
German idealist thinking
that dominated continental Europe
in the mid-19th century, particularly
that of Georg Hegel. Kierkegaard
wanted to refute Hegelâ€™s idea of
a complete philosophical system,
which deï¬ned humankind as
part of an inevitable historical

development, by arguing for a more
subjective approach. He wants to
examine what â€œit means to be a
human beingâ€, not as part of some
great philosophical system, but as
a self-determining individual.
Kierkegaard believes that our
lives are determined by our actions,
which are themselves determined
by our choices, so how we make

THE AGE OF REVOLUTION 195
See also: Immanuel Kant 164â€“71 â–  Georg Hegel 178â€“85 â–  Friedrich Nietzsche 214â€“21
Jean-Paul Sartre 268â€“71 â–  Simone De Beauvoir 276â€“77 â–  Albert Camus 284â€“85

those choices is critical to our lives.
Like Hegel, he sees moral decisions
as a choice between the hedonistic
(self-gratifying) and the ethical. But
where Hegel thought this choice was
largely determined by the historical
and environmental conditions of our
times, Kierkegaard believes that
moral choices are absolutely free,
and above all subjective. It is our will
alone that determines our judgement,
he says. However, far from being a
reason for happiness, this complete
freedom of choice provokes in us a
feeling of anxiety or dread.
Kierkegaard explains this feeling
in his book, The Concept of Anxiety.
As an example, he asks us to
consider a man standing on a cliff
or tall building. If this man looks
over the edge, he experiences two
different kinds of fear: the fear of
falling, and fear brought on by the
impulse to throw himself off the
edge. This second type of fear, or
anxiety, arises from the realization
that he has absolute freedom to
choose whether to jump or not,
and this fear is as dizzying as his
vertigo. Kierkegaard suggests that

SÃ¸ren Kierkegaard

â– 

Martin Heidegger 252â€“55

â– 

we experience the same anxiety
in all our moral choices, when we
realize that we have the freedom
to make even the most terrifying
decisions. He describes this anxiety
as â€œthe dizziness of freedomâ€, and
goes on to explain that although it
induces despair, it can also shake
us from our unthinking responses
by making us more aware of the
available choices. In this way it
increases our self-awareness and
sense of personal responsibility.

The father of existentialism
Kierkegaardâ€™s ideas were largely
rejected by his contemporaries, but
proved highly inï¬‚uential to later
generations. His insistence on the
importance and freedom of our
choices, and our continual search
for meaning and purpose, was
to provide the framework for
existentialism. This philosophy,
developed by Friedrich Nietzsche
and Martin Heidegger, was later
fully deï¬ned by Jean-Paul Sartre.
It explores the ways in which we
can live meaningfully in a godless
universe, where every act is a
SÃ¸ren Kierkegaard was born in
Copenhagen in 1813, in what
became known as the Danish
Golden Age of culture. His father,
a wealthy tradesman, was both
pious and melancholic, and his
son inherited these traits, which
were to greatly inï¬‚uence his
philosophy. Kierkegaard studied
theology at the University of
Copenhagen, but attended
lectures in philosophy. When he
came into a sizeable inheritance,
he decided to devote his life to
philosophy. In 1837 he met and fell
in love with Regine Olsen, and
three years later they became

Hamlet is caught on the edge of a
terrible choice: whether to kill his uncle
or leave his fatherâ€™s death unavenged.
Shakespeareâ€™s play demonstrates the
anxiety of true freedom of choice.

choice, except the act of our own
birth. Unlike these later thinkers,
Kierkegaard did not abandon his
faith in God, but he was the ï¬rst to
acknowledge the realization of selfconsciousness and the â€œdizzinessâ€
or fear of absolute freedom. â– 
engaged, but Kierkegaard broke
off the engagement the following
year, saying that his melancholy
made him unsuitable for married
life. Though he never lost his
faith in God, he continually
criticized the Danish national
church for hypocrisy. In 1855 he
fell unconscious in the street,
and died just over a month later.
Key works
1843 Fear and Trembling
1843 Either/Or
1844 The Concept of Anxiety
1847 Works of Love

THE
HISTORY
OF ALL HITHERTO EXISTING

SOCIETY
IS THE HISTORY OF

CLASS
STRUGGLES
KARL MARX (1818â€“1883)

198 KARL MARX
IN CONTEXT
BRANCH
Political philosophy
APPROACH
Communism
BEFORE
c.1513 NiccolÃ² Machiavelli
discusses class struggles in
ancient Rome and Renaissance
Italy in Discourses on Livy.
1789 The French Revolution
provides the template for most
19th-century philosophical
arguments about revolution.
1800s Georg Hegel develops
a theory of historical change
through intellectual conï¬‚ict.
AFTER
1880s Friedrich Engels tries
to develop Marxâ€™s theories into
a fully-ï¬‚edged philosophy of
historical materialism.
1930s Marxism becomes
the ofï¬cial philosophy of the
Soviet Union and many other
communist countries.

C

an the complex history
of the human species be
reduced to a single formula?
One of the greatest thinkers of the
19th century, Karl Marx, believed
that it could. He opened the ï¬rst
chapter of his most famous work,
The Communist Manifesto, with
the claim that all historical change
comes about as the result of an
ongoing conï¬‚ict between dominant
(upper) and subordinate (lower)
social classes, and that the roots
of this conï¬‚ict lie in economics.
Marx believed that he had
gained a uniquely important
insight into the nature of society

through the ages. Earlier approaches
to history had emphasized the role
of individual heroes and leaders, or
stressed the role played by ideas,
but Marx focused on a long
succession of group conï¬‚icts,
including those between ancient
masters and slaves, medieval lords
and serfs, and modern employers
and their employees. It was conï¬‚icts
between these classes, he claimed,
that caused revolutionary change.

The Communist Manifesto
Marx wrote the Manifesto with
the German philosopher Friedrich
Engels, whom he had met when
they were both studying academic
philosophy in Germany during the
late 1830s. Engels offered ï¬nancial
support, ideas, and superior writing
skills, but Marx was acknowledged
as the real genius behind their
combined publications.
In their private manuscripts
from the early and mid-1840s, Marx
and Engels emphasized that while
previous philosophers had only
sought to interpret the world, the
whole point of their activities was
to change it. During the 1850s and
60s Marx reï¬ned his ideas in many
short documents, including The
Communist Manifesto, a pamphlet
of about 40 pages.
The Manifesto seeks to explain
the values and political plans of
communismâ€”a new belief system
put forward by a small and relatively
new group of radical German
socialists. The Manifesto claims
that society had simpliï¬ed into
two classes in direct conï¬‚ict: the
bourgeoisie (the capital-owning
class) and the proletariat (the
working class).
The word â€œbourgeoisieâ€ is
derived from the French word
burgeis, or burgher: a propertyowning tradesman who had risen
above the general populace to own

Intellectual debate was widespread in
Germany at the time Marx was writing,
though he himself believed that it was
the task of philosophy not to discuss
ideas, but to bring about real change.

and run his own business. Marx
describes how the discovery and
colonization of America, the opening
of the Indian and Chinese markets,
and the increase in the commodities
that could be exchanged had, by
the mid-19th century, led to the
rapid development of commerce
and industry. Craftsmen no longer
produced enough goods for the
growing needs of new markets, and
so the manufacturing system had
taken their place. As the Manifesto
relates, â€œthe markets kept growing,
demand ever rising.â€

Values of the bourgeoisie
Marx claims that the bourgeoisie,
who controlled all this trade, had left
no link between people other â€œthan
naked self-interest, than callous
â€˜cash payment.â€™â€ People were once
valued for who they were, but the
bourgeoisie â€œhas resolved personal
worth into exchange value.â€ Moral,
religious, and even sentimental
values had been cast aside, as

THE AGE OF REVOLUTION 199
See also: NiccolÃ² Machiavelli 102â€“07 â–  Jean-Jacques Rousseau 154â€“59
Ludwig Andreas Feuerbach 189 â–  Friedrich Nietzsche 214â€“21

everyoneâ€”from scientists and
lawyers to priests and poetsâ€”had
been transformed into nothing but
a paid laborer. In place of religious
and political â€œillusionsâ€, Marx writes,
the bourgeoisie had â€œsubstituted
naked, shameless, direct, brutal
exploitation.â€ Charters that had once
protected peopleâ€™s freedom had been
cast aside for one â€œunconscionable
freedomâ€”Free Trade.â€
The only solution, according to
Marx, was for all the instruments of
economic production (such as land,
raw materials, tools, and factories)
to become common property, so
that every member of society could
work according to their capacities,
and consume according to their
needs. This was the only way to
prevent the rich from living at the
expense of the poor.

â– 

Adam Smith 160â€“63

â– 

Georg Hegel 178â€“85

People align into groups...

...with others who
share their social
and economic interests.

...against those in
conï¬‚ict with their social
and economic interests.

The socio-economic status
of each group is deï¬ned by its
relationship to property and
the means of production.

Dialectical change
The philosophy behind Marxâ€™s
reasoning on the process of change
came largely from his predecessor,
Georg Hegel, who had described
reality not as a state of affairs, but
as a process of continual change.
The change was caused, he said,
by the fact that every idea or state
of affairs (known as the â€œthesisâ€) â¯â¯

The proletariat
owns little property
or business.

The bourgeois or
ruling class owns most
of a countryâ€™s property
and businesses.

When the means of production
changes, such as from agricultural
to industrial, there are
revolutions and wars.

From each according to
his abilities, to each
according to his needs.
Karl Marx

The ruling class is
displaced and a new
one is created.

History is a
record of these
class struggles and
displacements

â– 

200 KARL MARX
contains within it an internal
conï¬‚ict (the â€œantithesisâ€) that
eventually forces a change to occur,
leading to a new idea or state of
affairs (the â€œsynthesisâ€). This
process is known as the dialectic.
Hegel believed that we can never
experience things in the world as
they are, but only as they appear to
us. For him, existence primarily
consists of mind or spirit, so the
journey of history, through countless
cycles of the dialectic, is essentially
the progress of spirit, or Geist,
toward a state of absolute harmony.
But it is here that Hegel and Marx
part company. Marx insists that the
process is not a journey of spiritual
development, but of real historical
change. Marx claims that the ï¬nal,
conï¬‚ict-free state that lies at the end
of the process is not the spiritual
bliss that Hegel predicted, but the
perfect society, where everyone
works harmoniously toward the
good of a greater whole.

The formation of classes
In earlier ages, humans had been
entirely responsible for producing
everything they neededâ€”such as

clothing, food, and habitationâ€”
for themselves, but as the early
societies began to form, people
came to rely more on one another.
This led to the form of â€œbargain
makingâ€ described by the Scottish
economist and philosopher Adam
Smith, as people exchanged goods
or labor. Marx agrees with Smith
that this system of exchange led
people to specialize in their labor,
but he points out that this new
specialization (or â€œjobâ€) had also
come to deï¬ne them. Whatever a
personâ€™s specialization or job, be it
agricultural laborer or hereditary
landowner, it had come to dictate
where he or she lived, what they
ate, and what they wore; it also
dictated with whom in society they
shared interests, and with whom
their interests lay in conï¬‚ict. Over
time, this led to the formation of
distinct socio-economic classes,
locked into conï¬‚ict.
According to Marx, there have
been four major stages in human
history, which he sees as based on
four different forms of property
ownership: the original tribal
system of common property; the

The ruling ideas of each
age have ever been the
ideas of its ruling class.
Karl Marx

ancient communal and state
system of ownership (where both
slavery and private property began);
the feudal or estate system of
property; and the modern system of
capitalist production. Each of these
stages represents a different form
of economic system, or â€œmode of
productionâ€, and the transitions
between them are marked in
history by stormy political events,
such as wars and revolutions, as
one ruling class is displaced by
another. The Communist Manifesto
popularized the idea that through
understanding the system of
property ownership in any one
society, in any particular era, we can
acquire the key to understanding
its social relations.

Rise of cultural institutions
Marx also believes that an analysis
of the economic basis of any society
allows us to see that as its system
of property alters, so too do its
â€œsuperstructuresâ€â€”such as its
politics, laws, art, religions, and
The wealthy bourgeoisie enjoyed
the luxuries of life in the late 18th and
19th centuries, while the workers in
their companies and on their estates
endured terrible poverty.

THE AGE OF REVOLUTION 201

The abolition of religion
as the illusory happiness
of the people is required
for real happiness.
Karl Marx

philosophies. These develop to
serve the interests of the ruling
class, promoting its values and
interests, and diverting attention
away from political realities.
However, even this ruling class is
not, in fact, determining events or
institutions. Hegel had said that
every age is held in the sway of the
Zeitgeist, or spirit of the age, and
Marx agrees. But where Hegel saw
the Zeitgeist as determined by an
Absolute Spirit developing over
time, Marx sees it as deï¬ned by
the social and economic relations
of an era. These deï¬ne the ideas or
â€œconsciousnessâ€ of individuals and
societies. In Marxâ€™s view, people
do not make a stamp on their era,
molding it into a particular shape;
the era deï¬nes the people.
Marxâ€™s revision of Hegelâ€™s
philosophy from a journey of spirit
to one of social and economic
modes of production was also
inï¬‚uenced by another German
philosopher, Ludwig Feuerbach.
Feuerbach believed that traditional
The Industrial Revolution saw the
formalization of specialized skills into
paid employment. People then formed
into groups, or classes, made up of those
with similar socio-economic status.

religion is intellectually falseâ€”it is
not corroborated in any way by
reasoningâ€”and that it contributes
to the general sum of human misery.
He claimed that people make
gods in their own image from
an amalgamation of humanityâ€™s
greatest virtues, and then cling to
these gods and invented religions,
preferring their â€œdreamsâ€ to the real
world. People become alienated
from themselves, through an
unfavorable comparison of their
selves to a god that they have
forgotten they created.
Marx agrees that people cling
to religion because they long for
a place in which the self is not
despised or alienated, but he says
that this is not due to some
authoritarian god, but to material
facts in their actual, daily lives.
The answer for Marx lies not only
in the end of religion, but in total
social and political change.

A Marxist utopia
In addition to its general account
of human history leading to the rise
of the bourgeois and proletarian
classes, The Communist Manifesto

makes a variety of other claims
about politics, society, and
economics. For example, it argues
that the capitalist system is not
merely exploitative, but also
inherently ï¬nancially unstable,
leading to the recurrence of
increasingly severe commercial
crises, the growing poverty of the
workforce, and the emergence of
the proletariat as the one genuinely
revolutionary class. For the ï¬rst
time in history, this revolutionary
class would represent the vast
majority of humanity.
These developments are seen
as underpinned by the increasingly
complex nature of the process of
production. Marx predicted that
as technology improved, it would
lead to increasing unemployment,
alienating more and more people
from the means of production. This
would split society in two, between
the large numbers of impoverished
people and the few who owned and
controlled the means of production.
Following the rules of the dialectic,
this conï¬‚ict would result in a violent
revolution to establish a new,
classless society. This would â¯â¯

202 KARL MARX
Socialist-inspired revolutions
swept through Europe just after
the publication of The Communist
Manifesto. These included the
February Revolution of 1848 in Paris.

found expression. Marx saw political
interests and parties as merely
vehicles for the economic interests
of the ruling classes, which were
forced to appear as though they
were acting in the general interest
in order to gain and maintain power.

The road to revolution

be the utopian, conï¬‚ict-free society
that marked the end of the dialectic.
Marx thought this perfect society
would not require government, but
only administration, and this would
be carried out by the leaders of the
revolution: the communist â€œpartyâ€
(by which he means those who
adhered to the cause, rather than
any speciï¬c organization). Within
this new kind of state (which Marx
called the â€œdictatorship of the
proletariatâ€) people would enjoy
genuine democracy and social
ownership of wealth. Shortly after
this ï¬nal change in the mode of
production to a perfect society,
Marx predicted, political power as
it had previously been understood
would come to an end, because
there would be no good reason for
political dissent or criminality.

Political power
Marx predicted that the outcome
of the intense class struggles in
Europe between the bourgeoisie

and the wage-earning working
class would become evident only
when the great mass of people had
become property-less and were
obliged to sell their labor for wages.
The juxtaposition of poverty with
the great wealth of the few would
become increasingly obvious, he
thought, and communism would
become increasingly attractive.
However, Marx did not expect
the opponents of communism to
give up their privileges easily. In
every period of history, the ruling
class has enjoyed the advantage of
controlling both the government
and the law as a way of reinforcing
their economic dominance. The
modern state, he said, was actually
a â€œcommittee for managing the
affairs of the bourgeois classâ€, and
struggles by excluded groups to
have their own interests taken into
accountâ€”such as the battle to
extend the right to voteâ€”were
simply short-term ways in which the
more fundamental economic conï¬‚ict

Marxâ€™s originality lies in his
combination of pre-existing ideas
rather than the creation of new
ones. His system uses insights from
German idealist philosophers,
especially Georg Hegel and Ludwig
Feuerbach; from French political
theorists, such as Jean-Jacques
Rousseau; and from British political
economists, particularly Adam
Smith. Socialism had become a
recognized political doctrine in the
ï¬rst half of the 19th century, and
from this Marx derives several
insights about property, class,
exploitation, and commercial crises.
Class conï¬‚ict was certainly in
the air when Marx composed the
Manifesto. It was written just
before a succession of revolutions

A specter is haunting
Europeâ€”the specter
of communism.
Karl Marx

THE AGE OF REVOLUTION 203
against the monarchies of many
continental European countries
broke out in 1848 and 1849. In the
preceding decades, a signiï¬cant
number of people had migrated
from the countryside to the towns
in search of work, although
continental Europe had not yet
seen the industrial development
that had taken place in Britain.
A wave of discontent felt by the
poor against the status quo was
exploited by a variety of liberal
and nationalist politicians, and
revolutions rippled across Europe,
although ultimately these uprisings
were defeated and led to little
permanent change.
However, the Manifesto acquired
an iconic status during the 20th
century, inspiring revolutions in
Russia, China, and many other
countries. The brilliance of Marxâ€™s
theories has been proved wrong in
practice: the extent of repression in
Stalinist Russia, in Mao Zedongâ€™s
China, and in Pol Potâ€™s Cambodia,
has widely discredited his political
and historical theories.

Criticism of Marxism
Although Marx did not foresee
communism being implemented
in such a barbaric manner in these
primarily agricultural societies, his

ideas are nevertheless still open to
a variety of criticisms. First, Marx
always argued for the inevitability
of revolution. This was the essential
part of the dialectic, but it is clearly
too simplistic, as human creativity
is always able to produce a variety
of choices, and the dialectic fails
to allow for the possibility of
improvement by gradual reform.
Second, Marx tended to invest
the proletariat with wholly good
attributes, and to suggest that a
communist society would give rise
somehow to a new type of human
being. He never explained how
the dictatorship of this perfect
proletariat would be different from
earlier, brutal forms of dictatorship,
nor how it would avoid the
corrupting effects of power.
Third, Marx rarely discussed
the possibility that new threats
to liberty might emerge after a
successful revolution; he assumed
that poverty was the only real cause
of criminality. His critics have also
alleged that he did not sufï¬ciently
understand the forces of nationalism,
and that he gave no proper account
of the role of personal leadership in
politics. In fact, the 20th-century
communist movement was to
produce immensely powerful
personality cults in virtually every
country in which communists
came to power.

Lasting inï¬‚uence

Marxist states of the 20th century
promoted themselves as utopias. They
produced a proliferation of paintings
and statues glorifying the achievements
of their happy, newly liberated citizens.

Despite the criticism and crises that
Marxâ€™s theories have provoked, his
ideas have been hugely inï¬‚uential.
As a powerful critic of commercial
capitalism, and as an economic
and socialist theorist, Marx is still
considered relevant to politics and
economics today. Many would
agree with the 20th-century
Russian-British philosopher, Isaiah
Berlin, that the The Communist
Manifesto is â€œa work of genius.â€ â– 

Karl Marx
The most famous revolutionary
thinker of the 19th century
was born in the German city
of Trier. The son of a Jewish
lawyer who had converted to
Christianity, Marx studied law
at Bonn University, where he
met his future wife, Jenny von
Westphalen. He then studied
at the University of Berlin,
before working as a journalist.
The favor he bestowed on
democracy in his writing led
to censorship by the Prussian
royal family, and he was forced
into exile in France and
Belgium. During this time he
developed a unique theory of
communism in collaboration
with his German compatriot
Friedrich Engels.
Marx returned to Germany
during the 1848â€“49 revolutions,
but after they were quashed
he lived in exile in London for
the rest of his life. He and his
wife lived in extreme poverty,
and when Marx died stateless
at the age of 64, there were
only 11 mourners at his funeral.
Key works
1846 The German Ideology
1847 The Poverty of Philosophy
1848 The Communist
Manifesto
1867 Das Kapital: Volume 1

204

MUST THE CITIZEN
EVER RESIGN HIS
CONSCIENCE TO
THE
LEGISLATOR?
HENRY DAVID THOREAU (1817â€“1862)
IN CONTEXT
BRANCH
Political philosophy
APPROACH
Non-conformism
BEFORE
c.340 BCE Aristotle claims that
the city-state is more important
than the individual.
1651 Thomas Hobbes says
that society without strong
government reverts to anarchy.
1762 In The Social Contract,
Jean-Jacques Rousseau
proposes government by
the will of the people.
AFTER
1907 Mahatma Gandhi cites
Thoreau as an inï¬‚uence on
his campaign of passive
resistance in South Africa.
1964 Martin Luther King is
awarded the Nobel Peace
Prize for his campaign to
end racial discrimination
through civil disobedience
and noncooperation.

A

lmost a century after JeanJacques Rousseau claimed
that nature was essentially
benign, American philosopher
Henry Thoreau developed the idea
further, arguing that â€œall good
things are wild and freeâ€, and that
the laws of man suppress rather
than protect civil liberties. He
saw that political parties were
necessarily one-sided, and that
their policies often ran contrary to
our moral beliefs. For this reason,
he believed it was the individualâ€™s
duty to protest against unjust laws,
and argued that passively allowing
such laws to be enacted effectively
gave them justiï¬cation. â€œAny fool
can make a rule, and any fool will
mind it,â€ as he said about English
grammar, but the principle runs
through his political philosophy too.
In his essay Civil Disobedience,
written in 1849, Thoreau proposes
a citizenâ€™s right to conscientious
objection through non-cooperation
and non-violent resistanceâ€”which
he put into practice by refusing to
pay taxes that supported the war in
Mexico and perpetuated slavery.

Thoreauâ€™s ideas contrasted sharply
with those of his contemporary Karl
Marx, and with the revolutionary
spirit in Europe at the time, which
called for violent action. But they
were later adopted by numerous
leaders of resistance movements,
such as Mahatma Gandhi and
Martin Luther King. â– 

Mahatma Gandhiâ€™s campaign of civil
disobedience against British rule in
India included the Salt March of 1930,
undertaken in protest against unjust
laws controlling salt production.

See also: Jean-Jacques Rousseau 154â€“59 â–  Adam Smith 160â€“63 â–  Edmund
Burke 172â€“73 â–  Karl Marx 196â€“203 â–  Isaiah Berlin 280â€“81 â–  John Rawls 294â€“95

THE AGE OF REVOLUTION 205

CONSIDER WHAT
EFFECTS
THINGS
HAVE
CHARLES SANDERS PEIRCE (1839â€“1914)
IN CONTEXT
BRANCH
Epistemology
APPROACH
Pragmatism
BEFORE
17th century John Locke
challenges rationalism by
tracing the origin of our
ideas to sense impressions.
18th century Immanuel
Kant argues that speculation
about what lies beyond our
experience is meaningless.
AFTER
1890 S William James and
John Dewey take up the
philosophy of pragmatism.
1920 S Logical positivists in
Vienna formulate the theory of
veriï¬cationâ€”that the meaning
of a statement is the method
by which it is veriï¬ed.
1980 S Richard Rortyâ€™s version
of pragmatism argues that the
very notion of truth can be
dispensed with.

C

harles Sanders Peirce was
the scientist, logician, and
philosopher of science
who pioneered the philosophical
movement known as pragmatism.
Deeply sceptical of metaphysical
ideasâ€”such as the idea that there
is a â€œrealâ€ world beyond the world
we experienceâ€”he once asked his
readers to consider what is wrong
with the following theory: a
diamond is actually soft, and only
becomes hard when it is touched.
Peirce argued that there is â€œno
falsityâ€ in such thinking, for there
is no way of disproving it. However,
he claimed that the meaning of a
concept (such as â€œdiamondâ€ or
â€œhardâ€) is derived from the object
or quality that the concept relates
toâ€”and the effects it has on our
senses. Whether we think of the
diamond as â€œsoft until touchedâ€ or
â€œalways hardâ€ before our experience,
therefore, is irrelevant. Under both
theories the diamond feels the
same, and can be used in exactly
the same way. However, the ï¬rst
theory is far more difï¬cult to work
with, and so is of less value to us.

This idea, that the meaning of a
concept is the sensory effect of its
object, is known as the pragmatic
maxim, and it became the founding
principle of pragmatismâ€”the belief
that the â€œtruthâ€ is the account of
reality that works best for us.
One of the key things Peirce
was trying to accomplish was to
show that many debates in science,
philosophy, and theology are
meaningless. He claimed that they
are often debates about words,
rather than reality, because they
are debates in which no effect on
the senses can be speciï¬ed. â– 

Nothing is vital for
science; nothing can be.
Charles Sanders Peirce

See also: John Locke 130â€“33 â–  Immanuel Kant 164â€“71
John Dewey 228â€“31 â–  Richard Rorty 314â€“19

â– 

William James 206â€“09

â– 

206

ACT AS IF
WHAT YOU
DO MAKES A
DIFFERENCE
WILLIAM JAMES (1842â€“1910)

IN CONTEXT
BRANCH
Epistemology
APPROACH
Pragmatism
BEFORE
1843 John Stuart Millâ€™s
A System of Logic studies the
ways in which we come to
believe something is true.
1870s Charles Sanders Peirce
describes his new pragmatist
philosophy in How to Make
Our Ideas Clear.
AFTER
1907 Henri Bergsonâ€™s Creative
Evolution describes reality as a
ï¬‚ow rather than a state.
1921 Bertrand Russell explores
reality as pure experience in
The Analysis of Mind.
1925 John Dewey develops a
personal version of pragmatism,
known as â€œinstrumentalismâ€,
in Experience and Nature.

O

ver the course of the 19th
century, as the United
States began to ï¬nd its
feet as an independent nation,
philosophers from New England
such as Henry David Thoreau and
Ralph Waldo Emerson gave a
recognizably American slant to
European Romantic ideas. But it
was the following generation of
philosophers, who lived almost a
century after the Declaration of
Independence, that came up with
something truly original.
The ï¬rst of these, Charles
Sanders Peirce, proposed a theory
of knowledge he called pragmatism,
but his work was hardly noticed at
the time; it fell to his lifelong friend

THE AGE OF REVOLUTION 207
See also: John Stuart Mill 190â€“93 â–  Charles Sanders Peirce 205 â–  Henri Bergson 226â€“27
Bertrand Russell 236â€“39 â–  Ludwig Wittgenstein 246â€“51 â–  Richard Rorty 314â€“19

â€¦that it leads
nowhere.

So I do nothing,
stay lost, starve,
and die.

If I am lost in a
forest and see a path,
I can believeâ€¦

My action has
made my beliefs
come true.

â– 

John Dewey 228â€“31

â– 

...that it leads to
food and shelter.

So I follow it and
ï¬nd a way out
of the forest
to safety.

Act as if what
you do makes
a difference.
It does.

William Jamesâ€”godson to Ralph
Emersonâ€”to champion Peirceâ€™s
ideas and develop them further.

Truth and usefulness
Central to Peirceâ€™s pragmatism was
the theory that we do not acquire
knowledge simply by observing,
but by doing, and that we rely on
that knowledge only so long as it is
useful, in the sense that it adequately
explains things for us. When it no
longer fulï¬ls that function, or better
explanations make it redundant, we
replace it. For example, we can see
by looking back in history how our

ideas about the world have changed
constantly, from thinking that Earth
is ï¬‚at to knowing it to be round; from
assuming that Earth is the center of
the universe, to realizing that it is
just one planet in a vast cosmos. The
older assumptions worked perfectly
adequately as explanations in their
time, yet they are not true, and the
universe itself has not changed.
This demonstrates how knowledge
as an explanatory tool is different
from facts. Peirce examined the
nature of knowledge in this way,
but James was to apply this
reasoning to the notion of truth. â¯â¯

Every way of
classifying a thing
is but a way of
handling it for some
particular purpose.
William James

208 WILLIAM JAMES
The idea of a ï¬‚at Earth served well
as a â€œtruthâ€ for several thousand years,
despite the fact that Earth is a sphere.
James claims that an ideaâ€™s usefulness
determines its truthfulness.

For James, the truth of an idea
depends on how useful it is; that is
to say, whether or not it does what
is required of it. If an idea does not
contradict the known factsâ€”such
as laws of scienceâ€”and it does
provide a means of predicting
things accurately enough for our
purposes, he says there can be no
reason not to consider it true, in the
same way that Peirce considered
knowledge as a useful tool

William James

irrespective of the facts. This
interpretation of truth not only
distinguishes it from fact, but also
leads James to propose that â€œthe
truth of an idea is not a stagnant
property inherent in it. Truth
happens to an idea. It becomes
true, is made true by events. Its
verity is in fact an event, a process.â€
Any idea, if acted upon, is found to
be true by the action we take;
putting the idea into practice is the
Born in New York City, William
James was brought up in a
wealthy and intellectual family;
his father was a famously eccentric
theologian, and his brother Henry
became a well-known author.
During his childhood he lived for
several years in Europe, where he
pursued a love of painting, but at
the age of 19 he abandoned this
to study science. His studies at
Harvard Medical School were
interrupted by the ill health and
depression that were to prevent
him from ever practicing medicine,
but he eventually graduated and
in 1872 took a teaching post in

process by which it becomes true.
James also thinks that belief in
an idea is an important factor in
choosing to act upon it, and in this
way belief is a part of the process
that makes an idea true. If I am
faced with a difï¬cult decision, my
belief in a particular idea will lead
to a particular course of action and
so contribute to its success. It is
because of this that James deï¬nes
â€œtrue beliefsâ€ as those that prove
useful to the believer. Again, he is
careful to distinguish these from
facts, which he says â€œare not true.
They simply are. Truth is the
function of the beliefs that start
and terminate among them.â€

The right to believe
Every time we try to establish a
new belief, it would be useful if we
had all the available evidence and
the time to make a considered
decision. But in much of life we do
not have that luxury; either there
is not enough time to examine the
physiology at Harvard University.
His increasing interest in the
subjects of psychology and
philosophy led him to write
acclaimed publications in these
ï¬elds, and he was awarded
a professorship in philosophy
at Harvard in 1880. He taught
there until his retirement in 1907.
Key works
1890 The Principles of Psychology
1896 The Will to Believe
1902 The Varieties of Religious
Experience
1907 Pragmatism

THE AGE OF REVOLUTION 209
known facts, or there is not enough
evidence, and we are forced to a
decision. We have to rely on our
beliefs to guide our actions, and
James says that we have â€œthe right
to believeâ€ in these cases.
James explains this by taking
the example of a man lost and
starving in a forest. When he sees a
path, it is important for him to believe
that the path will lead him out of the
forest and to habitation, because if
he does not believe it, he will not
take the path, and will remain lost
and starving. But if he does, he will
save himself. By acting on his idea
that the path will lead him to safety,
it becomes true. In this way our
actions and decisions make our
belief in an idea become true. This
is why James asserts â€œact as if what
you do makes a differenceâ€â€”to
which he adds the typically concise
and good-humored rider, â€œit does.â€
We must, however, approach
this idea with caution: a shallow

Religious belief can bring about
extraordinary changes in peopleâ€™s lives,
such as the healing of the sick at places
of pilgrimage. This occurs regardless
of whether or not a god actually exists.

interpretation of what James is
saying could give the impression
that any belief, no matter how
outlandish, could become true by
acting upon itâ€”which of course
is not what he meant. There are
certain conditions that an idea
must fulï¬l before it can be
considered a justiï¬able belief. The
available evidence must weigh in
its favor, and the idea must be
sufï¬cient to withstand criticism.
In the process of acting upon the
belief, it must continually justify
itself by its usefulness in increasing
our understanding or predicting
results. And even then, it is only in
retrospect that we can safely say
that the belief has become true
through our acting upon it.

Reality as a process
James was a psychologist as well
as a philosopher, and he sees the
implications of his ideas in terms
of human psychology as much as
in the theory of knowledge. He
recognized the psychological
necessity for humans to hold certain
beliefs, particularly religious ones.
James thinks that while it is not
justiï¬able as a fact, belief in a god is
useful to its believer if it allows him
or her to lead a more fulï¬lled life, or
to overcome the fear of death. These
thingsâ€”a more fulï¬lled life and a
fearless confrontation of deathâ€”
become true; they happen as the
result of a belief, and the decisions
and actions based upon it.
Along with his pragmatic notion
of truth, James proposes a type of
metaphysics that he calls â€œradical
empiricism.â€ This approach takes
reality to be a dynamic, active
process, in the same way that truth
is a process. Like the traditional
empiricists before him, James
rejected the rationalist notion that
the changing world is in some way
unreal, but he also went further to

The pragmatic method
means looking away from
principles and looking
towards consequences.
William James

state that â€œfor pragmatism, [reality]
is still in the makingâ€, as truth is
constantly being made to happen.
This â€œstreamâ€ of reality, he believes,
is not susceptible to empirical
analysis either, both because it is
in continual ï¬‚ux and because the
act of observing it affects the truth
of the analysis. In Jamesâ€™s radical
empiricism, from which both mind
and matter are formed, the ultimate
stuff of reality is pure experience.

Continuing inï¬‚uence
Pragmatism, proposed by Peirce and
expounded by James, established
America as a signiï¬cant center
for philosophical thought in the
20thÂ century. Jamesâ€™s pragmatic
interpretation of truth inï¬‚uenced
the philosophy of John Dewey, and
spawned a â€œneopragmatistâ€ school
of thought in America that includes
philosophers such as Richard Rorty.
In Europe, Bertrand Russell and
Ludwig Wittgenstein were indebted
to Jamesâ€™s metaphysics. His
work in psychology was equally
inï¬‚uential, and often intimately
connected with his philosophy,
notably his concept of the â€œstream
of consciousnessâ€, which in turn
inï¬‚uenced writers such as Virginia
Woolf and James Joyce. â– 

THE MOD
WORLD
1900â€“1950

ERN

212 INTRODUCTION

Death of Friedrich
Nietzsche, whose
philosophy proposed
that â€œGod is dead.â€

1900

T

Edmund Husserl
publishes The Idea
of Phenomenology.

Bertrand Russell and
Alfred North Whitehead
co-author The Principles of
Mathematics, set philosophers
on a new analytical path.

The October Revolution
takes place in Russia,
leading to the creation
of the Soviet Union.

1907

1910â€“13

1917

1906

1908

1914â€“18

1921

Albert Einstein
introduces his theory
of relativity.

Henry Ford produces
the Model T Fordâ€”
the worldâ€™s ï¬rst
mass-produced car.

World War I leads to the
collapse of the Russian,
German, Ottoman, and
Austro-Hungarian empires.

Ludwig
Wittgenstein
publishes his
Tractatus
LogicoPhilosophicus.

oward the end of the
19thÂ century, philosophy
once again reached a
turning point. Science, and
particularly Charles Darwinâ€™s
theory of evolution (1859), had
thrown into doubt the idea of the
universe as Godâ€™s creation, with
humankind as the peak of his
creative genius. Moral and political
philosophy had become entirely
human-centered, with Karl Marx
declaring religion â€œthe opiate of the
people.â€ Following in the footsteps
of Arthur Schopenhauer, Friedrich
Nietzsche believed that Western
philosophy, with its roots in Greek
and Judaeo-Christian traditions,
was ill-equipped to explain this
modern world view. He proposed
a radical new approach to ï¬nding
meaning in life, one that involved
casting aside old values and

traditions. In doing so, he set the
agenda for much of the philosophy
of the 20th century.

A new analytical tradition
To some extent, the traditional
concerns of philosophyâ€”such as
asking what existsâ€”were answered
by science in the early 20th century.
Albert Einsteinâ€™s theories offered
a more detailed explanation of the
nature of the universe, and Sigmund
Freudâ€™s psychoanalytic theories
gave people a radically new insight
into the workings of the mind.
As a result, philosophers turned
their attention to questions of
moral and political philosophy or,
since philosophy had become the
province of professional academics,
to the more abstract business of
logic and linguistic analysis. At the
vanguard of this movement of logical

analysisâ€”which became known
as analytic philosophyâ€”was the
work of Gottlob Frege, who linked
the philosophical process of logic
with mathematics. His ideas
were enthusiastically received
by a British philosopher and
mathematician, Bertrand Russell.
Russell applied the principles
of logic that Frege had outlined to
a thorough analysis of mathematics
in the Principia Mathematica,
which he wrote with Alfred North
Whitehead, and thenâ€”in a move
that revolutionized philosophical
thinkingâ€”he applied the same
principles to language. The process
of linguistic analysis was to
become the major theme in
20thÂ century British philosophy.
One of Russellâ€™s pupils, Ludwig
Wittgenstein, developed Russellâ€™s
work on logic and language, but

THE MODERN WORLD 213

Josef Stalin is made
General Secretary
of the Communist
Party in Russia.

Martin Heidegger
publishes Being and Time.

Karl Popper publishes
The Logic of Scientiï¬c
Discovery, challenging the
idea that science always
proceeds from repeated
observations to theories.

1922

1927

1934

Jean-Paul Sartre
becomes one of the most
important continental
philosophers with the
existentialist work Being
and Nothingness.

1943

1923

1929

1939â€“45

1949

Psychoanalyst
Sigmund Freud
publishes The Ego
and the Id.

The Wall Street
Crash leads to global
economic depression.

In World War II, the
deadliest war in
history, more than
60 million people die.

The Communists
under Mao
Zedong proclaim
the Peopleâ€™s
Republic
of China.

also made key contributions in
areas as diverse as perception,
ethics, and aesthetics, becoming
one of the greatest thinkers of the
20th century. Another, slightly
younger Viennese philosopher, Karl
Popper, took his cue from Einstein,
and strengthened the link between
scientiï¬c thinking and philosophy.
Meanwhile, in Germany,
philosophers rose to the challenge
posed by Nietzscheâ€™s ideas with a
philosophy based on the experience
of the individual in a godless
universe: existentialism. Edmund
Husserlâ€™s phenomenology (the study
of experience) laid the groundwork,
and this was carried forward by
Martin Heidegger, who was also
greatly inï¬‚uenced by the Danish
philosopher, SÃ¸ren Kierkegaard.
Heideggerâ€™s work, produced in the
1920s and 30s, was largely rejected

in the mid-20th century due to his
connections with the Nazi party
during World War II, but his works
were key to the development of
existentialism, and were important
to late 20th-century culture.

Wars and revolutions
Philosophy was as affected by the
massive political upheavals of the
20th century as any other cultural
activity, but it also contributed to
the ideologies that shaped the
modern world. The revolution that
formed the Soviet Union in the
1920s had its roots in Marxism, a
19th-century political philosophy.
This theory became more prevalent
globally than any single religion,
dominating the policy of Chinaâ€™s
Communist Party until around
1982, and replacing traditional
philosophies across Asia.

Liberal democracies in Europe
during the 1930s were threatened
by fascism, forcing many thinkers
to ï¬‚ee from the continent to Britain
and the US. Philosophers turned
their attention to left-wing or liberal
politics in reaction to the oppression
they experienced under totalitarian
regimes. World War II and the Cold
War that followed it colored the
moral philosophy of the second
half of the 20th century.
In France, existentialism was
made fashionable by Jean-Paul
Sartre, Simone de Beauvoir, and
Albert Camus, who were all
novelists. This trend was in keeping
with the French view of philosophy
as part of an essentially literary
culture. It was also fundamental
to the direction that continental
philosophy was to take in the last
decades of the 20th century. â– 

MAN
IS SOMETHING TO BE

SURPASSED
FRIEDRICH NIETZSCHE (1844â€“1900)

216 FRIEDRICH NIETZSCHE
IN CONTEXT
BRANCH
Ethics

Christianity says
that everything in this
world is less important
than that of the â€œnextâ€
after death.

APPROACH
Existentialism
BEFORE
380 BCE Plato explores the
distinction between reality
and appearance in his
dialogue, The Republic.
1st century CE The Sermon
on the Mount, in Matthewâ€™s
gospel in the Bible, advocates
turning away from this world
to the greater reality of the
world to come.

It says we should
turn away from what
seems important in
this life, and try
to transcend it.

1781 Immanuel Kantâ€™s Critique
of Pure Reason argues that we
can never know how the world
is â€œin itself.â€
AFTER
1930s Nietzscheâ€™s work is
used to help construct the
mythology of Nazism.
1966 Michel Foucaultâ€™s The
Order of Things discusses
the overcoming of â€œman.â€

N

ietzscheâ€™s idea that man
is something to be
surpassed appears in
Thus Spoke Zarathustra, perhaps
his most famous book. It was
written in three parts in 1883â€“84,
with a fourth part added in 1885.
The German philosopher used it
to launch a sustained attack on
the history of Western thought.
He targets three linked ideas in
particular: ï¬rst, the idea we
have of â€œmanâ€ or human nature;
second, the idea we have of God;
and third, the ideas we have about
morality, or ethics.

We must
surpass this
limiting idea.

And besides,
God is dead!

But in doing this
we turn away from
life itself.

Elsewhere Nietzsche writes about
philosophizing â€œwith a hammerâ€,
and here he certainly attempts to
shatter many of the most cherished
views of the Western philosophical
tradition, especially in relation to
these three things. He does so in a
style that is astonishingly hot-headed
and fevered, so that at times the
book seems closer to prophecy than
philosophy. It was written quickly,
with Part I taking him only a few
days to set down on paper. Even so,
while Nietzscheâ€™s book does not
have the calm, analytical tone that
people have perhaps come to

Christianityâ€™s
idea of â€œmanâ€
undermines us.

expect of philosophical works, the
author still succeeds in setting out
a remarkably consistent and hugely
challenging vision.

Zarathustra descends
The name of Nietzscheâ€™s prophet,
Zarathustra, is an alternative name
for the ancient Persian prophet
Zoroaster. The book begins by
telling us that at the age of 30,
Zarathustra goes to live in the
mountains. For ten years he
delights in the solitude, but one
dawn, he wakes to ï¬nd that he is
weary of the wisdom he has

THE MODERN WORLD 217
See also: Plato 50â€“55 â–  Immanuel Kant 164â€“71 â–  SÃ¸ren Kierkegaard 194â€“95
Michel Foucault 302â€“03 â–  Jacques Derrida 308â€“13

accumulated alone on the mountain.
So he decides to descend to the
market place to share this wisdom
with the rest of humankind.
On the way down to the town,
at the foot of the hill, he meets with
an old hermit. The two men have
already met, ten years before, when
Zarathustra ï¬rst ascended the
mountain. The hermit sees that
Zarathustra has changed during
the past decade: when he climbed
the mountain, the hermit says,
Zarathustra carried ashes; but now,
as he descends, he is carrying ï¬re.
Then the hermit asks Zarathustra
a question: why are you going to
the trouble of sharing your wisdom?
He advises Zarathustra to stay in
the mountains, warning him that
nobody will understand his
message. Zarathustra then asks a
question: what does the hermit do
in the mountains? The hermit
replies that he sings, weeps, laughs,
mumbles, and praises God. On
hearing this, Zarathustra himself
laughs. Then he wishes the hermit
well and continues on his way
down the mountain. As he goes,

Friedrich Nietzsche

â– 

Albert Camus 284â€“85

â– 

Zarathustra says to himself, â€œHow
can it be possible! This old hermit
has not yet heard that God is dead.â€

Behold the Superman
The idea of the death of God may
be the most famous of all Nietzscheâ€™s
ideas, and it is closely related both
to the idea that man is something
to be surpassed and to Nietzscheâ€™s
distinctive understanding of
morality. The relationship between
these things becomes clear as the
story continues.
When he reaches the town,
Zarathustra sees that there is a
crowd gathered around a tightrope
walker who is about to perform,
and he joins them. Before the
acrobat has a chance to walk across
his rope, Zarathustra stands up. It
is at this point that he says, â€œBehold!
I teach you the Superman!â€ He
continues by telling the crowd the
real point he wishes to convey:
â€œMan is something to surpassed...â€.
Zarathustra follows this with a long
speech, but when he gets to the
end, the crowd only laughs,
imagining that the prophet is â¯â¯
Nietzsche was born in Prussia in
1844 to a religious family; his
father, uncle, and grandfathers
were all Lutheran ministers. His
father and younger brother died
when he was a young child, and
he was brought up by his mother,
grandmother, and two aunts. At
the age of 24 he became a professor
at Basel University, where he met
the composer Richard Wagner,
who inï¬‚uenced him strongly until
Wagnerâ€™s anti-semitism forced
Nietzsche to end their friendship.
In 1870 he contracted diphtheria
and dysentery, and thereafter
suffered continual ill health. He

The prophet Zoroaster (c.628â€“551 BCE),
also known as Zarathustra, founded a
religion based on the struggle between
good and evil. Nietzscheâ€™s Zarathustra
places himself â€œbeyond good and evil.â€

was forced to resign his
professorship in 1879, and for
the next ten years traveled in
Europe. In 1889 he collapsed in
the street while attempting to
prevent a horse from being
whipped, and suffered some
form of mental breakdown from
which he never recovered. He
died in 1900 aged 56.
Key works
1872 The Birth of Tragedy
1883â€“85 Thus Spoke Zarathustra
1886 Beyond Good and Evil
1888 Twilight of the Idols

218 FRIEDRICH NIETZSCHE
just another showman, or perhaps
even a warm-up performer for the
tightrope-walker.
In opening his book in this
unusual way, Nietzsche seems to
be betraying his own unease with
the reception that his philosophy
will receive, as if he is afraid that he
will be seen as a philosophical
showman without anything real
to say. If we want to avoid making
the same mistake as the crowd
gathered around Zarathustra, and
actually understand what Nietzsche
is saying, it is necessary to explore
some of Nietzscheâ€™s core beliefs.

Overturning old values
Nietzsche believes that certain
concepts have become inextricably
entangled: humankind, morality,
and God. When his character
Zarathustra says that God is dead,
he is not simply launching an
attack upon religion, but doing
something much bolder. â€œGodâ€ here
does not only mean the god that
philosophers talk about or the
religious pray to; it means the sum

total of the higher values that we
might hold. The death of God is not
just the death of a deity; it is also
the death of all the so-called higher
values that we have inherited.
One of the central purposes of
Nietzscheâ€™s philosophy is what he
calls the â€œrevaluation of all valuesâ€,
an attempt to call into question all
of the ways that we are accustomed
to thinking about ethics and the
meanings and purposes of life.
Nietzsche repeatedly maintains
that in doing so he is setting out a
philosophy of cheerfulness, which,
although it overturns everything we
have thought up until now about
good and evil, nevertheless seeks to
afï¬rm life. He claims that many of
the things that we think are â€œgoodâ€
are, in fact, ways of limiting, or of
turning away from, life.
We may think it is not â€œgoodâ€ to
make a fool of ourselves in public,
and so resist the urge to dance
joyfully in the street. We may
believe that the desires of the ï¬‚esh
are sinful, and so punish ourselves
when they arise. We may stay in

mind-numbing jobs, not because
we need to, but because we feel it is
our duty to do so. Nietzsche wants
to put an end to such life-denying
philosophies, so that humankind
can see itself in a different way.

Blaspheming against life
After Zarathustra proclaims the
coming of the Superman, he swiftly
moves to condemn religion. In the
past, he says, the greatest blasphemy
was to blaspheme against God; but
now the greatest blasphemy is to
blaspheme against life itself. This is
the error that Zarathustra believes
he made upon the hillside: in turning
away from the world, and in offering
up prayers to a God who is not
there, he was sinning against life.
The history behind this death
of God, or loss of faith in our higher
values, is told in Nietzscheâ€™s essay,
How the â€œReal Worldâ€ at last Became
a Myth, which was published in
Twilight of the Idols. The essay
carries the subtitle â€œHistory of an
Errorâ€, and it is an extraordinarily
condensed one-page history of

Man is a rope tied
between the animal
and the Supermanâ€”
a rope over an abyss.
Friedrich Nietzsche

Existing between the levels of
animal and Superman, human life,
Nietzsche says, is â€œa dangerous
wayfaring, a dangerous looking-back,
a dangerous trembling and halting.â€

THE MODERN WORLD 219
Western philosophy. The story
begins, Nietzsche says, with the
Greek philosopher Plato.

Some religions and philosophies insist that a more
important â€œreal worldâ€ exists elsewhere. Nietzsche sees
this as a myth that tragically prevents us from living
fully now, in this world.

The real world
Plato divided the world into an
â€œapparentâ€ world that appears to
us through our senses, and a â€œrealâ€
world that we can grasp through
the intellect. For Plato, the world
we perceive through the senses is
not â€œrealâ€ because it is changeable
and subject to decay. Plato suggests
that there is also an unchanging,
permanent â€œreal worldâ€ that can
be attained with the help of the
intellect. This idea comes from
Platoâ€™s study of mathematics.
The form or idea of a triangle, for
example, is eternal and can be
grasped by the intellect. We know
that a triangle is a three-sided,
two-dimensional ï¬gure whose
angles add up to 180Âº, and that
this will always be true, whether
anyone is thinking about it or not
and however many triangles exist
in the world. On the other hand,
the triangular things that do exist
in the world (such as sandwiches,
pyramids, and triangular shapes
drawn on a chalkboard), are
triangular only insofar as they
are reï¬‚ections of this idea or form
of the triangle.
Inï¬‚uenced by mathematics in
this way, Plato proposed that the
intellect can gain access to a
whole world of Ideal Forms, which
is permanent and unchanging,
whereas the senses have access
only to the world of appearances.
So, for example, if we want to know
about goodness, we need to have
an intellectual appreciation of the
Form of Goodness, of which the
various examples of goodness in
the world are only reï¬‚ections.
This is an idea that has had farreaching consequences for our
understanding of the world; not least

this world, even beauty, is
only a â€œshadowâ€ of Forms
in another world.

Christianity sees this
life as merely a forerunner
to the more important
â€œlife after death.â€

because, as Nietzsche points out,
this way of dividing up the world
makes the â€œreal worldâ€ of the
intellect the place where everything
of value resides. In contrast, it
makes the â€œapparent worldâ€ of the
senses a world that is, relatively
speaking, unimportant.

Christian values
Nietzsche traces the fortunes of
this tendency to split the world into
two and ï¬nds that the same idea
appears within Christian thought.
In place of the â€œreal worldâ€ of Platoâ€™s
Forms, Christianity substitutes an
alternative â€œreal worldâ€; a future world
of heaven that is promised to the
virtuous. Nietzsche believes that

Christianity views the world we live
in now as somehow less real than
heaven, but in this version of the
â€œtwo worldsâ€ idea the â€œreal worldâ€ is
attainable, albeit after death and on
condition that we follow Christian
rules in this life. The present world
is devalued, as it is with Plato, except
insofar as it acts as a stepping stone
to the world beyond. Nietzsche
claims that Christianity asks us to
deny the present life in favor of the
promise of a life to come.
Both the Platonic and Christian
versions of the idea that the world
is divided into a â€œrealâ€ and an
â€œapparentâ€ one have profoundly
affected our thoughts about
ourselves. The suggestion that â¯â¯

220 FRIEDRICH NIETZSCHE
The Superman is someone of
enormous strength and independence
in mind and body; Nietzsche denied
any had existed, but named Napoleon,
Shakespeare, and Socrates as models.

absolutely unattainableâ€”even to the
wise or the virtuous, in this world or
the nextâ€”then it is â€œan idea grown
useless, superï¬‚uous.â€ As a result, it
is an idea that we need to do away
with. If God is dead, Nietzsche is
perhaps the person who stumbles
across the corpse; nevertheless, it
is Kant whose ï¬ngerprints are all
over the murder weapon.

Philosophyâ€™s longest error

everything of value in the world is
somehow â€œbeyondâ€ the reach of this
world leads to a way of thinking
that is fundamentally life-denying.
As a result of this Platonic and
Christian heritage, we have come
to see the world we live in as a
world that we should resent and
disdain, a world from which we
should turn away, transcend, and
certainly not enjoy. But in doing so,
we have turned away from life itself
in favor of a myth or an invention,
an imagined â€œreal worldâ€ that is
situated elsewhere. Nietzsche calls
priests of all religions â€œpreachers of
deathâ€, because their teachings
encourage us to turn from this
world, and from life to death. But
why does Nietzsche insist that God
is dead? To answer this, we must

look to the work of the 18th-century
German philosopher Immanuel
Kant, whose ideas are critical to
understanding the philosophy
behind Nietzscheâ€™s work.

A world beyond reach
Kant was interested in the limits
of knowledge. In his book Critique
of Pure Reason, he argued that we
cannot know the world as it is â€œin
itself.â€ We cannot attain it with the
intellect, as Plato believed; nor is it
promised to us after death as in
the Christian view. It exists (we
assume), but it is forever out of
reach. The reasons that Kant uses
to come up with this conclusion are
complex, but what is important
from Nietzscheâ€™s point of view is
that, if the real world is said to be

Once we have dispensed with the
idea of the â€œreal worldâ€, the longheld distinction between the â€œreal
worldâ€ and the â€œapparent worldâ€
begins to break down. In How the
â€œReal Worldâ€ at last Became a Myth,
Nietzsche goes on to explain this
as follows: â€œWe have abolished the
real world; what world is left? The
apparent world, perhaps? â€¦ But
no! With the real world we have
also abolished the apparent world.â€
Nietzsche now sees the beginning
of the end of philosophyâ€™s â€œlongest
errorâ€: its infatuation with the
distinction between â€œappearanceâ€
and â€œrealityâ€, and the idea of two
worlds. The end of this error,
Nietzsche writes, is the zenith of
mankindâ€”the high point of all
humanity. It is at this pointâ€”in an
essay written six years after Thus
Spake Zarathustraâ€”that Nietzsche
writes â€œZarathustra begins.â€
This is a key moment for
Nietzsche because when we grasp
the fact that there is only one world,
we suddenly see the error that had
put all values beyond this world.
We are then forced to reconsider
all our values and even what it
means to be human. And when we
see through these philosophical

THE MODERN WORLD 221
illusions, the old idea of â€œmanâ€ can
be surpassed. The Superman is
Nietzscheâ€™s vision of a fundamentally
life-afï¬rming way of being. It is one
that can become the bearer of
meaning not in the world beyond,
but here; Superman is â€œthe
meaning of the Earth.â€

Creating ourselves
Nietzscheâ€™s writings did not reach
a large audience in his lifetime, so
much so that he had to pay for the
publication of the ï¬nal part of Thus
Spoke Zarathustra himself. But
around 30Â years after his death in
1900, the idea of the Superman fed
into the rhetoric of Nazism through
Hitlerâ€™s readings of Nietzscheâ€™s
work. Nietzscheâ€™s ideas about the
Superman, and particularly his call
for an eradication of the JewishChristian morality that held sway
throughout Europe would have been
attractive to Hitler as validation for
his own aims. But where Nietzsche
seemed to be searching for a return
to the more rustic, life-afï¬rming
values of pagan Europe, Hitler

took his writings as an excuse for
unbridled violence and transgression
on a grand scale. The consensus
amongst scholars is that Nietzsche
himself would have been horriï¬ed
by this turn of events. Writing in
an era of extraordinary nationalism,
patriotism, and colonial expansion,
Nietzsche was one of the few
thinkers to call these assumptions
into question. At one point in Thus
Spoke Zarathustra he makes it
clear that he considers nationalism
a form of alienation or failure. â€œOnly
where the state ends,â€ Zarathustra
says, â€œthere begins the human
being who is not superï¬‚uousâ€.
Nietzscheâ€™s open-ended idea of
human possibility was important
to many philosophers in the period
following World War II. His ideas
about religion and the importance
of self-evaluation can be traced
especially in the work of succeeding
existentialists such as the French
philosopher Jean-Paul Sartre. Like
Nietzscheâ€™s Superman, Sartre says
that we must each deï¬ne the
meaning of our own existence.

The degree of
introspection achieved by
Nietzsche had never been
achieved by anyone.
Sigmund Freud

Nietzscheâ€™s damning criticisms of
the Western philosophical tradition
have had a huge impact not only on
philosophy, but also on European
and world culture, and they went
on to inï¬‚uence countless artists
and writers in the 20th century. â– 
Nietzscheâ€™s writings were edited
and censored by his anti-semitic sister
Elizabeth, who controlled his archive
after he became insane. This allowed
the Nazis to wilfully misinterpret them.

222

MEN WITH
SELF-CONFIDENCE
COME AND
SEE
AND
CONQUER
AHAD HAâ€™AM (1856â€“1927)
IN CONTEXT
BRANCH
Ethics
APPROACH
Cultural Zionism
BEFORE
5th century BCE Socrates
combines both conï¬dence
and an admission of his
own foolishness.
1511 Desiderius Erasmus
writes The Praise of Folly, a
satirical work which appears
to praise foolish behavior.
1711 The English poet
Alexander Pope writes that
â€œFools rush in where angels
fear to tread.â€
1843 In his book Fear and
Trembling, SÃ¸ren Kierkegaard
writes about founding faith
â€œon the strength of the absurd.â€
AFTER
1961 Michel Foucault writes
Madness and Civilization, a
philosophical study of the
history of folly.

A

had Haâ€™am was the pen
name of the Ukrainianborn Jewish philosopher
Asher Ginzberg, a leading Zionist
thinker who advocated a Jewish
spiritual renaissance. In 1890 he
claimed in a semi-satirical essay
that although we worship wisdom,
self-conï¬dence matters more.
In any difï¬cult or dangerous
situation, he says, the wise are
those who hold back, weighing up
the advantages and disadvantages
of any action. Meanwhile (and
greatly to the disapproval of the
wise) it is the self-conï¬dent who
forge ahead, and often win the day.
Haâ€™am wants to suggestâ€”and
when reading him we should
remember that this is a suggestion
that is meant half-seriously and
half-satiricallyâ€”that individual folly
can often yield a result, simply
because of the self-conï¬dence that
goes along with it.

Wisdom and conï¬dence
Although in his original essay
Haâ€™am seemed to celebrate the
potential advantages of foolishness,
See also: Socrates 46â€“49
Luce Irigaray 320

â– 

this was a view from which he
later distanced himself, perhaps
afraid that others might read what
was essentially an exercise in
satire as if it were written with
high-minded seriousness. Selfconï¬dence is only warranted,
he later made clear, when the
difï¬culties of an undertaking are
fully understood and evaluated.
Haâ€™am was fond of quoting an
old Yiddish proverb: â€œan act of folly
which turns out well is still an act
of folly.â€ On some occasions we act
foolishly, without fully understanding
the difï¬culties of the task we are
undertaking, but we win through
because luck is on our side.
However, says Haâ€™am, this does
not make our prior foolishness in
any way commendable.
If we want our actions to bring
results, it may indeed be the case
that we need to develop and use
the kind of self-conï¬dence that can
occasionally be seen in acts of folly.
At the same time, we must always
temper this self-conï¬dence with
wisdom, or our acts will lack true
effectiveness in the world. â– 

SÃ¸ren Kierkegaard 194â€“95

â– 

Michel Foucault 302â€“03 â– 

THE MODERN WORLD 223

EVERY MESSAGE
IS MADE OF
SIGNS
FERDINAND DE SAUSSURE (1857â€“1913)

IN CONTEXT
BRANCH
Philosophy of language
APPROACH
Semiotics
BEFORE
c.400 BCE Plato explores the
relationship between names
and things.
c.250 BCE Stoic philosophers
develop an early theory of
linguistic signs.
1632 Portuguese philosopher
John Poinsot writes his
Treatise on Signs.
AFTER
1950s Saussureâ€™s analysis of
the structures of language
inï¬‚uences Noam Chomskyâ€™s
theory of generative grammar,
which aims to expose the rules
of a language that govern its
possible word combinations.
1960s Roland Barthes explores
the literary implications of
signs and semiotics.

S

aussure was a 19th-century
Swiss philosopher who saw
language as made up of
systems of â€œsignsâ€, with the signs
acting as the basic units of the
language. His studies formed the
basis of a new theory, known as
semiotics. This new theory of signs
was developed by other philosophers
during the 20th century such as
Russiaâ€™s Roman Jakobson, who
summed up the semiotic approach
when he said that â€œevery message
is made of signs.â€
Saussure said that a sign is made
up of two things. Firstly, a â€œsigniï¬erâ€,
which is a sound-image. This is not
the actual sound, but the mental
â€œimageâ€ we have of the sound.
Secondly, the â€œsigniï¬edâ€, or concept.
Here Saussure turns his back on a
long tradition that says language is
about the relationships between
words and things, because he is
saying that both aspects of a sign
are mental (our concept of a â€œdogâ€
for example, and a sound-image of
the sound â€œdogâ€). Saussure claims
that any messageâ€”for example
â€œmy dog is called Fredâ€â€”is a system

of signs. This means that it is a
system of relationships between
sound-images and concepts.
However, Saussure states that the
relationship between the signiï¬ed
and the signiï¬er is arbitraryâ€”so
there is nothing particularly
â€œdoggyâ€ about the sound â€œdogâ€,
which is why the word can be
chien in French, or gou in Chinese.
Saussureâ€™s work on language
became the basis of modern
linguistics, and inï¬‚uenced many
philosophers and literary theorists. â– 

In the lives of individuals
and of societies, language
is a factor of greater
importance than any other.
Ferdinand de Saussure

See also: Plato 50â€“55 â–  Charles Sanders Peirce 205 â–  Ludwig Wittgenstein
246â€“51 â–  Roland Barthes 290â€“91 â–  Julia Kristeva 323

224

EXPERIENCE
BY ITSELF IS
NOT SCIENCE
EDMUND HUSSERL (1859â€“1938)

IN CONTEXT
BRANCH
Ontology

Science aspires to
certainty about the world.

APPROACH
Phenomenology
BEFORE
5th century BCE Socrates
uses argument to try to
answer philosophical
questions with certainty.
17th century RenÃ© Descartes
uses doubt as a starting point
for his philosophical method.

But science is empirical:
it depends upon
experience.

1874 Franz Brentano, Husserlâ€™s
teacher, claims that philosophy
needs a new scientiï¬c method.
AFTER
From 1920s Martin
Heidegger, Husserlâ€™s student,
develops his teacherâ€™s method
of phenomenology, leading to
the birth of existentialism.
From 1930s Husserlâ€™s
phenomenology reaches
France, inï¬‚uencing thinkers
such as Emmanuel Levinas
and Maurice Merleau-Ponty.

H

usserl was a philosopher
haunted by a dream that
has preoccupied thinkers
since the time of the ancient
Greek philosopher Socrates: the
dream of certainty. For Socrates,
the problem was this: although we
easily reach agreement on questions
about things we can measure (for
example, â€œhow many olives are
there in this jar?â€), when it comes
to philosophical questions such
as â€œwhat is justice?â€ or â€œwhat is
beauty?â€, it seems that there is no
clear way of reaching agreement.
And if we cannot know for certain
what justice is, then how can we
say anything about it at all?

The problem of certainty
Experience is subject to
assumptions and biases.

So experience
by itself is
not science.

Husserl was a philosopher who
started life as a mathematician.
He dreamed that problems such as
â€œwhat is justice?â€ might be solved
with the same degree of certainty
with which we are able to solve
mathematical problems such as
â€œhow many olives are in the jar?â€ In
other words, he hoped to put all the
sciencesâ€”by which he meant all
branches of human knowledge and
activity, from math, chemistry,
and physics to ethics and politics â€“
on a completely secure footing.

THE MODERN WORLD 225
See also: RenÃ© Descartes 116â€“23
252â€“55 â–  Emmanuel Levinas 273

â– 
â– 

Franz Brentano 336 â–  Martin Heidegger
Maurice Merleau-Ponty 274â€“75

Scientiï¬c theories are based on
experience. But Husserl believed
that experience alone did not add
up to science, because as any
scientist knows, experience is full
of all kinds of assumptions, biases,
and misconceptions. Husserl
wanted to drive out all of these
uncertainties to give science
absolutely certain foundations.
To do this, Husserl made use of
the philosophy of the 19th-century
philosopher RenÃ© Descartes. Like
Husserl, Descartes wanted to free
philosophy from all assumptions,
biases, and doubts. Descartes wrote
that although almost everything
could be doubted, he could not
doubt that he was doubting.

Phenomenology
Husserl takes up a similar approach
to Descartes, but uses it differently.
He suggests that if we adopt a
scientiï¬c attitude to experience,
laying aside every single assumption
that we have (even including the
assumption that an external world
exists outside of us), then we can

Mathematics does not rely on
empirical evidence, which is full of
assumptions, to reach its conclusions.
Husserl wanted to put all science (and
all knowledge) on a similar foundation.

We entirely lack
a rational science
of man and of the
human community.
Edmund Husserl

start philosophy with a clean slate,
free of all assumptions. Husserl
calls this approach phenomenology:
a philosophical investigation of
the phenomena of experience. We
need to look at experience with a
scientiï¬c attitude, laying to one
side (or â€œbracketing outâ€ as Husserl
calls it) every single one of our
assumptions. And if we look
carefully and patiently enough, we
can build a secure foundation of
knowledge that might help us deal
with the philosophical problems
that have been with us since the
very beginnings of philosophy.
However, different philosophers
following Husserlâ€™s method came
to different results, and there was
little agreement as to what the
method actually was, or how one
carried it out. Toward the end of his
career, Husserl wrote that the
dream of putting the sciences on
ï¬rm foundations was over. But
although Husserlâ€™s phenomenology
failed to provide philosophers with
a scientiï¬c approach to experience,
or to solve philosophyâ€™s most
enduring problems, it nevertheless
gave birth to one of the richest
traditions in 20th-century thought. â– 

Edmund Husserl
Husserl was born in 1859
in Moravia, then a part of
the Austrian empire. He
started his career studying
mathematics and astronomy,
but after ï¬nishing his
doctorate in mathematics he
decided to take up philosophy.
In 1887 Husserl married
Malvine Steinschneider, with
whom he had three children.
He also became Privatdozent
(private lecturer) at Halle,
where he remained until 1901.
He then accepted an associate
professorship at the University
of GÃ¶ttingen, before becoming
a professor of philosophy at
the University of Freiburg in
1916, where Martin Heidegger
was among his students. In
1933, Husserl was suspended
from the university on account
of his Jewish background, a
decision in which Heidegger
was implicated. Husserl
continued to write until his
death in 1938.
Key works
1901 Logical Investigations
1907 The Idea of
Phenomenology
1911 Philosophy as a
Rigorous Science
1913 Ideas toward a Pure
Phenomenology

226

INTUITION GOES
IN THE VERY
DIRECTION
OF
LIFE
HENRI BERGSON (1859â€“1941)
IN CONTEXT
BRANCH
Epistemology

kinds of knowledge.

APPROACH
Vitalism
BEFORE
13th century John Duns
Scotus distinguishes between
intuitive and abstract thought,
and claims that intuitive
thought takes precedence.

Relative knowledge:
knowing objects
in the world from a
particular perspective.

Absolute knowledge:
knowing objects in
the world as
they actually are.

This is gained by using
our intellect and reason;
we are distanced from
the thing itself.

This is acquired
through an intuitive grasp
of the truth; it is a very direct
form of knowledge.

1781 Immanuel Kant publishes
Critique of Pure Reason,
claiming that absolute
knowledge is impossible.
AFTER
1890s William James begins
to explore the philosophy
of everyday experience,
popularizing pragmatism.
1927 Alfred North Whitehead
writes Process philosophy,
suggesting that the existence
of the natural world should be
understood in terms of process
and change, not things or
ï¬xed stabilities.

Intuition goes
in the very
direction of life.

THE MODERN WORLD 227
See also: John Duns Scotus 333
Gilles Deleuze 338

â– 

Immanuel Kant 164â€“71

H

enri Bergsonâ€™s 1910 book
Creative Evolution explored
his vitalism, or theory of
life. In it, Bergson wanted to discover
whether it is possible to really know
somethingâ€”not just to know about
it, but to know it as it actually is.
Ever since the philosopher
Immanuel Kant published The
Critique of Pure Reason in 1781,
many philosophers have claimed
that it is impossible for us to know
things as they actually are. This is
because Kant showed that we can
know how things are relative to we
ourselves, given the kinds of minds
we have; but we can never step
outside of ourselves to achieve an
absolute view of the worldâ€™s actual
â€œthings-in-themselves.â€

Two forms of knowledge
Bergson, however, does not agree
with Kant. He says that there are
two different kinds of knowledge:
relative knowledge, which involves
knowing something from our own
unique particular perspective;
and absolute knowledge, which is
knowing things as they actually

Henri Bergson

â– 

William James 206â€“09

are. Bergson believes that these are
reached by different methods, the
ï¬rst through analysis or intellect,
and the second through intuition.
Kantâ€™s mistake, Bergson believes,
is that he does not recognize the
full importance of our faculty of
intuition, which allows us to grasp
an objectâ€™s uniqueness through
direct connection. Our intuition is
linked to what Bergson called our
Ã©lan vital, a life-force (vitalism) that
interprets the ï¬‚ux of experience in
terms of time rather than space.
Suppose you want to get to know
a city, he says. You could compile a
record of it by taking photographs
of every part, from every possible
perspective, before reconstructing
these images to give some idea of
the city as a whole. But you would
be grasping it at one remove, not as
a living city. If, on the other hand,
you were simply to stroll around the
streets, paying attention in the right
way, you might acquire knowledge
of the city itselfâ€”a direct knowledge
of the city as it actually is. This
direct knowledge, for Bergson, is
knowledge of the essence of the city.
Henri Bergson was one of the most
inï¬‚uential French philosophers
of his time. Born in France in 1859,
he was the son of an English
mother and a Polish father. His
early intellectual interests lay in
mathematics, at which he excelled.
Despite this, he took up philosophy
as a career, initially teaching in
schools. When his book Matter
and Memory was published in
1896, he was elected to the
CollÃ¨ge de France and became
a university lecturer. He also had
a successful political career, and
represented the French government
during the establishment of the

â– 

Alfred North Whitehead 336

â– 

Capturing the essence of a city,
person, or object may only be possible
through direct knowledge gained from
intuition, not analysis. Bergson says we
underestimate the value of our intuition.

But how do we practice intuition?
Essentially, it is a matter of seeing
the world in terms of our sense of
unfolding time. While walking
through the city, we have a sense
of our own inner time, and we also
have an inner sense of the various
unfolding times of the city through
which we are walking. As these
times overlap, Bergson believes that
we can make a direct connection
with the essence of life itself. â– 
League of Nations in 1913. His
work was widely translated
and inï¬‚uenced many other
philosophers and psychologists,
including William James. He
was awarded the Nobel Prize
for Literature in 1928, and died
in 1941 at the age of 81.
Key works
1896 Matter and Memory
1903 An Introduction to
Metaphysics
1910 Creative Evolution
1932 The Two Sources of
Morality and Religion

228

WE ONLY THINK
WHEN WE ARE
CONFRONTED
WITH PROBLEMS
JOHN DEWEY (1859â€“1952)

IN CONTEXT
BRANCH
Epistemology
APPROACH
Pragmatism
BEFORE
1859 Charles Darwinâ€™s On
the Origin of Species puts
human beings in a new,
naturalistic perspective.
1878 Charles Sanders Peirceâ€™s
essay How to Make our Ideas
Clear lays the foundations of
the pragmatist movement.
1907 William James publishes
Pragmatism: A New Name for
Some Old Ways of Thinking,
popularizing the philosophical
term â€œpragmatism.â€
AFTER
From 1970 JÃ¼rgen Habermas
applies pragmatic principles
to social theory.
1979 Richard Rorty combines
pragmatism with analytic
philosophy in Philosophy and
the Mirror of Nature.

J

ohn Dewey belongs to the
philosophical school known
as pragmatism, which arose
in the US in the late 19th century.
The founder is generally considered
to be the philosopher Charles
Sanders Peirce, who wrote a
groundbreaking essay in 1878
called How to Make our Ideas Clear.
Pragmatism starts from the
position that the purpose of
philosophy, or â€œthinkingâ€, is not
to provide us with a true picture
of the world, but to help us to act
more effectively within it. If we are
taking a pragmatic perspective,
we should not be asking â€œis this the

THE MODERN WORLD 229
See also: Heraclitus 40 â–  Charles Sanders Peirce 205
JÃ¼rgen Habermas 306â€“07 â–  Richard Rorty 314â€“19

Problems arise
because we are trying
to make sense ofâ€¦

â– 

William James 206â€“09

â– 

â€¦the challenges
of living in a
changing world.

John Dewey
We only think
when we are
confronted with
problems.

â€¦the traditions
we have inherited.
Philosophy is not about
gaining a true picture of
the world, but about practical
problem solving.

way things are?â€ but rather, â€œwhat
are the practical implications of
adopting this perspective?â€
For Dewey, philosophical
problems are not abstract problems
divorced from peopleâ€™s lives. He
sees them as problems that occur
because humans are living beings
trying to make sense of their
world, struggling to decide how
best to act within it. Philosophy
starts from our everyday human
hopes and aspirations, and from
the problems that arise in the
course of our lives. This being the
case, Dewey thinks that philosophy
should also be a way of ï¬nding

practical responses to these
problems. He believes that
philosophizing is not about being
a â€œspectatorâ€ who looks at the
world from afar, but about actively
engaging in the problems of life.

Evolving creatures
Dewey was strongly inï¬‚uenced
by the evolutionary thought of the
naturalist Charles Darwin, who
published On The Origin of Species
in 1859. Darwin described humans
as living creatures who are a part
of the natural world. Like the other
animals, humans have evolved in
response to their changing â¯â¯

John Dewey was born in
Vermont, USA, in 1859. He
studied at the University of
Vermont, and then worked as
a schoolteacher for three years
before returning to undertake
further study in psychology
and philosophy. He taught at
various leading universities for
the remainder of his life, and
wrote extensively on a broad
range of topics, from education
to democracy, psychology,
and art. In addition to his
work as a scholar, he set up
an educational institutionâ€”
the University of Chicago
Laboratory Schoolsâ€”which
put into practice his
educational philosophy of
learning by doing. This
institution is still running
today. Deweyâ€™s broad range of
interests, and his abilities as a
communicator, allowed his
inï¬‚uence on American public
life to extend far beyond the
Laboratory Schools. He wrote
about philosophy and social
issues until he died in 1952 at
the age of 92.
Key works
1910 How We Think
1925 Experience and Nature
1929 The Quest for Certainty
1934 Art as Experience

230 JOHN DEWEY
environments. For Dewey, one of
the implications of Darwinâ€™s
thought is that it requires us to
think of human beings not as ï¬xed
essences created by God, but
instead as natural beings. We are
not souls who belong in some other,
non-material world, but evolved
organisms who are trying to do our
best to survive in a world of which
we are inescapably a part.

Everything changes
Dewey also takes from Darwin the
idea that nature as a whole is a
system that is in a constant state of
change; an idea that itself echoes
the philosophy of the ancient Greek
philosopher Heraclitus. When
Dewey comes to think about what
philosophical problems are, and
how they arise, he takes this
insight as a starting point.
Dewey discusses the idea that
we only think when confronted
with problems in an essay entitled
Kant and the Philosophic Method
(1884). We are, he says, organisms
that ï¬nd ourselves having to respond
to a world that is subject to constant
change and ï¬‚ux. Existence is a
risk, or a gamble, and the world
is fundamentally unstable. We
depend upon our environment to
be able to survive and thrive, but

We do not solve
philosophical problems,
we get over them.
John Dewey

the many environments in which
we ï¬nd ourselves are themselves
always changing. Not only this, but
these environments do not change
in a predictable fashion. For several
years there may be a good crop of
wheat, for instance, but then the
harvest fails. A sailor may set sail
under ï¬ne weather, only to ï¬nd that
a storm suddenly blows up out of
nowhere. We are healthy for years,
and then disease strikes us when
we least expect it.
In the face of this uncertainty,
Dewey says that there are two
different strategies we can adopt.
We can either appeal to higher
beings and hidden forces in the
universe for help, or we can seek
to understand the world and gain
control of our environment.

Appeasing the gods
The ï¬rst of these strategies involves
attempting to affect the world by
means of magical rites, ceremonies,
and sacriï¬ces. This approach to the
uncertainty of the world, Dewey
believes, forms the basis of both
religion and ethics.
In the story that Dewey tells,
our ancestors worshipped gods and
spirits as a way of trying to ally
themselves with the â€œpowers that
dispense fortune.â€ This scenario is
played out in stories from around the
world, in myths and legends such as
those about unfortunate seafarers
who pray to gods or saints to calm
the storm, and thereby survive. In
the same way, Dewey believes,
ethics arises out of the attempts
our ancestors made to appease
hidden forces; but where they made
sacriï¬ces, we strike bargains with
the gods, promising to be good if
they spare us from harm.
The alternative response to the
uncertainties of our changing world
is to develop various techniques of
mastering the world, so that we

We no longer employ sacriï¬ce as a
way to ask for help from the gods, but
many people ï¬nd themselves offering
up a silent promise to be good in return
for help from some higher being.

can live in it more easily. We can
learn the art of forecasting the
weather, and build houses to
shelter ourselves from its extremes,
and so on. Rather than attempting
to ally ourselves with the hidden
powers of the universe, this
strategy involves ï¬nding ways of
revealing how our environment
works, and then working out how
to transform it to our beneï¬t.
Dewey points out that it is
important to realize that we can
never completely control our
environment or transform it to
such an extent that we can drive
out all uncertainty. At best, he
says, we can modify the risky,
uncertain nature of the world in
which we ï¬nd ourselves. But life
is inescapably risky.

A luminous philosophy
For much of human history, Dewey
writes, these two approaches to
dealing with the riskiness of life
have existed in tension with each
other, and they have given rise to
two different kinds of knowledge:

THE MODERN WORLD 231
Scientiï¬c experiments, such as those
performed by Benjamin Franklin in the
1740s, help us gain control over the
world. Dewey thought philosophical
theories should be equally useful.

He is critical of any philosophical
approaches that ultimately make
our experience more puzzling,
or the world more mysterious.
Second, he thinks we should judge
a philosophical theory by asking
to what extent it succeeds in
addressing the problems of living.
Is it useful to us, in our everyday
lives? Does it, for instance, â€œyield
the enrichment and increase of
powerâ€ that we have come to
expect from new scientiï¬c theories?

A practical inï¬‚uence

on the one hand, ethics and religion;
and on the other hand, arts and
technologies. Or, more simply,
tradition and science. Philosophy,
in Deweyâ€™s view, is the process by
means of which we try to work
through the contradictions between
these two different kinds of
response to the problems in our
lives. These contradictions are not
just theoretical; they are also
practical. For example, I may have
inherited innumerable traditional
beliefs about ethics, meaning, and
what constitutes a â€œgood lifeâ€, but
I may ï¬nd that these beliefs are in
tension with the knowledge and
understanding that I have gained

from studying the sciences. In this
context philosophy can be seen as
the art of ï¬nding both theoretical
and practical responses to these
problems and contradictions.
There are two ways in which to
judge whether a form of philosophy
is successful. First, we should ask
whether it has made the world
more intelligible. Does this
particular philosophical theory
make our experience â€œmore
luminousâ€, Dewey asks, or does it
make it â€œmore opaqueâ€? Here
Dewey is agreeing with Peirce that
philosophyâ€™s purpose is to make our
ideas and our everyday experience
clearer and easier to understand.

A number of philosophers, such as
Bertrand Russell, have criticized
pragmatism by claiming that it
has simply given up on the long
philosophical quest for truth.
Nevertheless, Deweyâ€™s philosophy
has been enormously inï¬‚uential in
America. Given that Dewey places
such an overriding emphasis on
responding to the practical problems
of life, it is perhaps unsurprising
that much of his inï¬‚uence has been
in practical realms, such as in
education and in politics. â– 

Education is not an affair
of telling and being told,
but an active and
constructive process.
John Dewey

232

THOSE WHO CANNOT
REMEMBER THE PAST
ARE CONDEMNED
TO
REPEAT
IT
GEORGE SANTAYANA (1863â€“1952)
IN CONTEXT
BRANCH
Philosophy of history
APPROACH
Naturalism
BEFORE
55 BCE Lucretius, a Roman
poet, explores the origins of
societies and civilizations.

I

n The Life of Reason (1905), the
Spanish-American philosopher
George Santayana wrote that
those who cannot remember the
past are condemned to repeat it.
Santayanaâ€™s naturalistic approach
means that he sees knowledge and
belief as arising not from reasoning,
but through interaction between
our minds and the material
environment. Santayana is often
misquoted as saying that those

1730s The Italian philosopher
Giovanni Vico claims that all
civilizations pass through
three stages: the age of the
gods; the age of artistocrats
and heroes; and democracy.
This is due to â€œan uninterrupted
order of causes and effectsâ€.
1807â€“22 Georg Hegel writes
of history as the continual
progress of mind or spirit.
AFTER
2004 In his book, Memory,
History, Forgetting, French
philosopher Paul Ricoeur
explores the necessity not
only of remembering, but
also of forgetting the past.

who do not remember the past are
condemned to repeat it, and this is
sometimes understood to mean
that we must do our best to
remember past atrocities. But
Santayana is actually making a
point about progress. For progress
to be possible, we must not only
remember past experiences, but
also be able to learn from them; to
see different ways of doing things.
The psyche structures new beliefs
through experiences, and this is
how we prevent ourselves from
repeating mistakes.
Real progress, Santayana
believes, is not so much a matter
of revolution as of adaptation, taking
what we have learned from the past
and using it to build the future.
Civilization is cumulative, always
building on what has gone before,
in the same way that a symphony
builds note by note into a whole. â– 
Progress is only possible through
an understanding of the past coupled
with a sense of possible alternatives.
The AT&T Building, New York, uses
old architectural patterns in new ways.

See also: Georg Hegel 178â€“85
Bertrand Russell 236â€“39

â– 

Karl Marx 196â€“203 â–  William James 206â€“09 â– 

THE MODERN WORLD 233

IT IS ONLY SUFFERING
THAT MAKES US
PERSONS
MIGUEL DE UNAMUNO (1864â€“1936)
IN CONTEXT
BRANCH
Ontology
APPROACH
Existentialism
BEFORE
c.500 BCE The Buddha
claims that all life is marked
by suffering and offers the
Eightfold Path as a route to
release from its causes.
c.400 CE Saint Augustine asks
how there can be suffering in
a world created by a good and
all-powerful God.
AFTER
1940 The Irish author and
scholar C.S. Lewis explores
the question of suffering in
his book The Problem of Pain.
20th century Unamunoâ€™s
philosophy of suffering
inï¬‚uences other Spanish
writers such as Federico
GarcÃ­a Lorca and Juan RamÃ³n
JimÃ©nez, and the British author
Graham Greene.

T

he Spanish philosopher,
novelist, and poet, Miguel
de Unamuno, is perhaps
best known for his book The Tragic
Sense of Life (1913). In this he
writes that all consciousness is
consciousness of death (we are
painfully aware of our lack of
immortality) and of suffering.
What makes us human is the
fact that we suffer.
At ï¬rst glance, it may seem
as if this idea is close to that of
Sidhartha Gautama, the Buddha,
who also said that suffering is an
inescapable part of all human life.
But Unamunoâ€™s response to suffering
is very different. Unlike the Buddha,
Unamuno does not see suffering as
a problem to be overcome through
practicing detachment. Instead he
argues that suffering is an essential
part of what it means to exist as a
human being, and a vital experience.
If all consciousness amounts to
consciousness of human mortality
and suffering, as Unamuno claims,
and if consciousness is what
makes us distinctively human,
then the only way we can lend

our lives a kind of weight and
substance is to embrace this
suffering. If we turn away from
it, we are not only turning away
from what makes us human, we
are also turning away from
consciousness itself.

Love or happiness
There is also an ethical dimension
to Unamunoâ€™s ideas on suffering.
He claims that it is essential to
acknowledge our pain, because
it is only when we face the fact of
our own suffering that we become
capable of truly loving other
suffering beings. This presents us
with a stark choice. On the one
hand, we can choose happiness
and do our best to turn away from
suffering. On the other hand, we
can choose suffering and love.
The ï¬rst choice may be easier,
but it is a choice that ultimately
limits us â€“ indeed, severs us from
an essential part of ourselves. The
second choice is more difï¬cult,
but it is one that opens the way
to the possibility of a life of depth
and signiï¬cance. â– 

See also: Siddhartha Gautama 30â€“33 â–  St Augustine of Hippo 72â€“73 â– 
Martin Heidegger 252â€“55 â–  Albert Camus 284â€“85 â–  Jean-Paul Sartre 268â€“71

234

BELIEVE IN LIFE
WILLIAM DU BOIS (1868â€“1963)

IN CONTEXT
BRANCH
Ethics
APPROACH
Pragmatism
BEFORE
4th century BCE Aristotle
explores the ancient Greek
ethical concept of eudaimonia
or â€œhuman ï¬‚ourishingâ€.
1845 Publication of Narrative
of the Life of Frederick
Douglass, an American Slave
boosts support for the abolition
of slavery in the United States.
Late 19th and early 20th
century Pragmatists, such as
Charles Sanders Peirce and
William James, argue that we
should judge the value of ideas
in terms of their usefulness.
AFTER
1950s and 1960s Martin
Luther King Jr., leader of the
African-American Civil Rights
movement, adopts a policy of
non-violent direct action to
address social segregation.

I

n 1957, close to the end of
his long life, the American
academic, political radical,
and civil rights activist, William
Du Bois, wrote what has become
known as his last message to the
world. Knowing that he did not
have much longer to live, he penned
a short passage to be read at his
funeral. In this message, Du Bois
expresses his hope that any good
he has done will survive long
enough to justify his life, and that
those things he has left undone, or
has done badly, may be taken up by
others to be bettered or completed.

We aspire to
a broader and
fuller life.

To attain this
we need to believe
in the possibility
of progress.

â€œAlways,â€ Du Bois writes, â€œhuman
beings will live and progress to a
greater, broader, and fuller life.â€
This is a statement of belief rather
than a statement of fact. It is as if
Du Bois is saying that we must
believe in the possibility of a fuller
life, or in the possibility of progress,
to be able to progress at all. In this
idea, Du Bois shows the inï¬‚uence
of the American philosophical
movement known as Pragmatism,
which claims that what matters is
not just our thoughts and beliefs,
but also the practical implications
of these thoughts and beliefs.

â€¦believe
in life.

So we must...

If we lose this belief,
we suffer a form
of death: existence
without growth.

THE MODERN WORLD 235
See also: Aristotle 56â€“63

â– 

Charles Sanders Peirce 205

â– 

William James 206â€“09

â– 

John Dewey 228â€“31

The problem of the 20th
century is the problem
of the color line.
William Du Bois

Du Bois goes on to say that the
â€œonly possible deathâ€ is to lose oneâ€™s
belief in the prospects for human
progress. But there are also hints
of deeper philosophical roots here,
going all the way back to the
ancient Greek idea of eudaimonia
or â€œhuman ï¬‚ourishingâ€; for the
philosopher Aristotle, this involved
living a life of excellence based
upon virtue and reason.

Political activist
Du Bois considers two of the major
impediments to a life of excellence
to be racism and social inequality.

William Du Bois

He rejects scientiï¬c racismâ€”the
idea that black people are inferior
genetically to white peopleâ€”that
was prevalent throughout most of
his life. As racial inequality has
no basis in biological science,
he regards it as a purely social
problem, one that can be addressed
only by committed political and
social activism.
Du Bois is tireless in his search
for solutions to the problem of all
forms of social inequality. He
argues that social inequality is
one of the major causes of crime,
claiming that lack of education and

Martin Luther King Jr. cited Du
Boisâ€™ writings as a key inï¬‚uence in his
decision to become actively involved in
the battle to demolish racial divisions
and establish social equality in the US.

Du Bois showed exceptional
academic promise from an early
age. He won a scholarship to Fisk
University, and spent two years in
Germany studying in Berlin before
attending Harvard, where he
wrote a dissertation on the slave
trade. He was the ï¬rst AfricanAmerican to graduate from
Harvard with a doctorate.
Alongside an active career as
a university teacher and writer,
Du Bois was involved in the Civil
Rights movement and in radical
politics. His political judgement
has sometimes been called into
question: he famously wrote a

glowing eulogy on the death of
the Soviet dictator Josef Stalin.
Nevertheless, Du Bois remains
a key ï¬gure in the struggle for
racial equality, thanks to what
Martin Luther King Jr. called his
â€œdivine dissatisfaction with all
forms of injusticeâ€.

employment are correlated with
high levels of criminal activity.
In his ï¬nal message to the world,
Du Bois reminds us that the task
of bringing about a more just society
is still incomplete. He states that it
is up to future generations to believe
in life, so that we can continue to
contribute to the fulï¬lment of
â€œhuman ï¬‚ourishing.â€ â– 

Key works
1903 The Souls of Black Folk
1915 The Negro
1924 The Gift of Black Folk
1940 Dusk of Dawn: An Essay
Toward an Autobiography of
a Race Concept

236

THE ROAD TO
HAPPINESS LIES
IN AN ORGANIZED
DIMINUTION
OF WORK
BERTRAND RUSSELL (1872â€“1970)

IN CONTEXT
BRANCH
Ethics
APPROACH
Analytic philosophy
BEFORE
1867 Karl Marx publishes the
ï¬rst volume of Capital.
1905 In The Protestant Ethic
and the Spirit of Capitalism,
German sociologist Max
Weber argues that the
Protestant work ethic was
partly responsible for the
growth of capitalism.
AFTER
1990s Growth of the trend
of â€œdownshiftingâ€, promoting
fewer working hours.
2005 Tom Hodgkinson,
editor of the British magazine
The Idler, publishes his leisurepraising book How To Be Idle.
2009 British philosopher Alain
de Botton explores our working
lives in The Pleasures and
Sorrows of Work.

T

he British philosopher
Bertrand Russell was no
stranger to hard work. His
collected writings ï¬ll countless
volumes; he was responsible for
some of the most important
developments in 20th-century
philosophy, including the founding
of the school of analytic philosophy;
and throughout his long lifeâ€”he
died aged 97â€”he was a tireless
social activist. So why is this most
active of thinkers suggesting that
we should work less?
Russellâ€™s essay In Praise of
Idleness was ï¬rst published in
1932, in the middle of the Great

THE MODERN WORLD 237
See also: Jean-Jacques Rousseau 154â€“59 â–  Adam Smith 160â€“163 â–  Edmund Burke 172â€“73 â–  Jeremy Bentham 174 â– 
John Stuart Mill 190â€“93 â–  Karl Marx 196â€“203 â–  Henry David Thoreau 204 â–  Isaiah Berlin 280â€“81 â–  John Rawls 294â€“95

Depression, a period of global
economic crisis following the Wall
Street Crash of 1929. It might seem
distasteful to promote the virtues
of idleness at such a time, when
unemployment was rising to a third
of the working population in some
parts of the world. For Russell,
however, the economic chaos of
the time was itself the result of a
set of deep-rooted and mistaken
attitudes about work. Indeed, he
claims that many of our ideas
about work are little more than
superstitions, which should be
swept away by rigorous thinking.

class and the middle class. But to
these Russell adds a third class,
who he claims has a lot to answer
forâ€”that of the leisured landowner
who avoids all work, and who
depends on the labor of others
to support his or her idleness.
According to Russell, history is
littered with examples of people
working hard all their lives and
being allowed to keep just enough
for themselves and their families
to survive, while any surplus they
produce is appropriated by warriors,
priests, and the leisured ruling
classes. And it is always these
beneï¬ciaries of the system, says â¯â¯

The Great Depression was the worst
economic depression of the 20th century.
For Russell, it highlighted the need for a
critique of capitalism and a re-evaluation
of the ethics of work.

What is work?
Russell begins by deï¬ning work,
which he says is of two kinds. First,
there is work aimed at â€œaltering the
position of matter at or near the
earthâ€™s surface relative to other
such matter.â€ This is the most
fundamental sense of workâ€”that
of manual labor. The second kind of
work is â€œtelling other people to alter
the position of matter relative to
other such matter.â€ This second
kind of work, Russell says, can be
extended indeï¬nitelyâ€”not only
can you have people employed to
supervise people who move matter,
but others can be employed to
supervise the supervisors, or give
advice on how to employ other
people, while still more can be
employed to manage the people
who give advice on how to employ
people, and so on. The ï¬rst kind
of work, he says, tends to be
unpleasant and badly paid, while
the second tends to be more
pleasant, and better paid. These
two types of work deï¬ne two types
of workerâ€”the laborer and the
supervisorâ€”and these in turn relate
to two social classesâ€”the working

Our attitudes to work
are irrational.

We value different
types of work
differently.

We assume that work
is good in itself.

These attitudes lead
to unhappiness.

So we should recognize
what work is genuinely
valuable, and only do this.

Working less will
increase human
happiness.

238 BERTRAND RUSSELL
Russell, who are heard extolling
the virtues of â€œhonest toilâ€, giving
a moral gloss to a system that is
manifestly unjust. And this fact
alone, according to Russell, should
prompt us to re-evaluate the ethics
of work, for by embracing â€œhonest
toilâ€ we comply with and even
promote our own oppression.
Russellâ€™s account of society,
with its emphasis on the struggle
between classes, owes something
to the thought of the 19th-century
philosopher Karl Marx, although
Russell was always uneasy with
Marxism, and his essay is as
critical of Marxist states as it is
of capitalist states. His view also
owes much to Max Weberâ€™s book
The Protestant Ethic and the Spirit
of Capitalism, ï¬rst published in 1905,
particularly Weberâ€™s examination
of the moral claims that underlie
our attitudes to workâ€”claims that
Russell insists should be challenged.
For example, not only do we see
work as a duty and an obligation,
we also see different types of work
as occupying a hierarchy of virtue.
Manual work is generally considered
less virtuous than more skilled or
intellectual work, and we tend to
reward people in accordance with

Bertrand Russell

this perceived virtue rather than
for what they produce. And given
that we consider work itself to be
inherently virtuous, we tend to see
the unemployed as lacking in virtue.
The more we think about it, the
more it seems that our attitudes
toward work are both complex and
incoherent. What, then, can be
done? Russellâ€™s suggestion is that
we look at work not in terms of
these curious moral ideas that
are a relic of earlier times, but in
terms of what makes for a full and
satisfying human life. And when
we do this, Russell believes, it is
hard to avoid the conclusion that
we should all simply work less.
What, Russell asks, if the working
day were only four hours long? Our
present system is such that part of
the population can be overworked,
and so miserable, while another
part can be totally unemployed,
and so also miserable. This, it
seems, does not beneï¬t anyone.

The importance of play
Russellâ€™s view is that reducing our
working hours would free us to
pursue more creative interests.
â€œMoving matter about,â€ Russell
writes, â€œis emphatically not one of
Bertrand Russell was born in
Wales in 1872 to an aristocratic
family. He had an early interest
in mathematics, and went on to
study the subject at Cambridge.
There he met the philosopher
Alfred North Whitehead, with
whom he later collaborated on the
Principia Mathematica, a book
that established him as one of the
leading philosophers of his era. It
was also at Cambridge that he
met, and deeply inï¬‚uenced, the
philosopher Ludwig Wittgenstein.
Russell wanted philosophy to
speak to ordinary people. He was
a social activist, a paciï¬st, an

Immense harm
is caused by
the belief that
work is virtuous.
Bertrand Russell

the ends of human life.â€ If we allow
work to occupy every waking hour,
we are not living fully. Russell
believes that leisure, previously
something known only to the
privileged few, is necessary for a
rich and meaningful life. It might
be objected that nobody would
know what to do with their time
if they worked only for four hours
a day, but Russell regrets this.
If this is true, he says, â€œit is a
condemnation of our civilization,â€
suggesting that our capacity for
play and light-heartedness has been
eclipsed by the cult of efï¬ciency.
A society that took leisure seriously,
educationalist, an advocate of
atheism, and a campaigner
against nuclear arms, as well as
the author of numerous popular
works of philosophy. He died of
inï¬‚uenza in February, 1970.
Key works
1903 The Principles of
Mathematics
1910, 1912, and 1913 (3 vols)
Principia Mathematica
1914 Our Knowledge of the
External World
1927 The Analysis of Matter
1956 Logic and Knowledge

THE MODERN WORLD 239

The morality of work is
the morality of slaves, and
the modern world has
no need of slavery.
Bertrand Russell

Russell believes, would be one that
took education seriouslyâ€”because
education is surely about more than
training for the workplace. It would
be one that took the arts seriously,
because there would be time to
produce works of quality without
the struggle that artists have for
economic independence. Moreover,
it would be one that took the need
for enjoyment seriously. Indeed,
Russell believes that such a society
would be one in which we would
lose the taste for war because, if
nothing else, war would involve
â€œlong and severe work for all.â€

The balanced life
Russellâ€™s essay may appear to
present something of a Utopian
vision of a world in which work is
reduced to a minimum. It is not
entirely clear how, even if it were
possible to reduce the working day
to four hours, this change would
lead to the social revolution that
Russell claims. Nor is Russellâ€™s faith
in the idea that industrialization
can ultimately free us from manual
Leisure time, for Russell, should
no longer be spent merely recovering
from work. On the contrary, it should
constitute the largest part of our lives
and be a source of play and creativity.

labor entirely convincing. The raw
materials for industrial production
still need to come from somewhere.
They need to be mined and reï¬ned
and exported to the place of
production, all of which depends
on manual labor. Despite these
problems, Russellâ€™s reminder that
we need to look more closely at
our attitudes to work is one that
remains relevant today. We take as
â€œnaturalâ€ the length of the working
week and the fact that some kinds
of work are rewarded more than
others. For many of us, neither our

work nor our leisure are as fulï¬lling
as we believe they could be, and at
the same time we cannot help
feeling that idleness is a vice.
Russellâ€™s idea reminds us that not
only do we need to scrutinize our
working lives, but that there is a
virtue and a usefulness to lounging,
loaï¬ng, and idling. As Russell
says: â€œHitherto we have continued
to be as energetic as we were
before there were machines; in
this we have been foolish, but
there is no reason to go on being
foolish forever.â€ â– 

240

LOVE IS A BRIDGE
FROM POORER TO
RICHER
KNOWLEDGE
MAX SCHELER (1874â€“1928)
IN CONTEXT
BRANCH
Ethics
APPROACH
Phenomenology
BEFORE
C.380 BCE Plato writes his
Symposium, a philosophical
exploration of the nature of
love and knowledge.
17th century Blaise Pascal
writes of the logic of the
human heart.
Early 20th century Edmund
Husserl develops his new
phenomenological method
for studying the experience
of the human mind.
AFTER
1954 Polish philosopher Karol
Wojtyza (later Pope John Paul
II) writes his PhD thesis on
Scheler, acknowledging the
philosopherâ€™s inï¬‚uence on
Roman Catholicism.

T

he German philosopher
Max Scheler belongs to the
philosophical movement
known as phenomenology. This
attempts to investigate all the
phenomena of our inner experience;
it is the study of our consciousness
and its structures.
Scheler says that phenomenology
has tended to focus too exclusively
on the intellect in examining
the structures of consciousness,
and has overlooked something
fundamental: the experience of
love, or of the human heart. He
introduces the idea that love forms
a bridge from poorer to richer
knowledge in an essay entitled
Love and Knowledge (1923).
Schelerâ€™s starting point, which is
taken from the 17th-century French
philosopher Blaise Pascal, is that
there is a speciï¬c logic to the
human heart. This logic is different
from the logic of the intellect.

knowledge possible. Scheler writes
that love is â€œa kind of spiritual
midwifeâ€ that is capable of
drawing us toward knowledge,
both knowledge of ourselves and
knowledge of the world. It is the
â€œprimary determinantâ€ of a personâ€™s
ethics, possibilities, and fate.
At root, in Schelerâ€™s view, to
be human is not to be a â€œthinking
thingâ€ as the French philosopher
Descartes said in the 17th century,
but a being who loves. â– 

Philosophy is a lovedetermined movement toward
participation in
the essential reality
of all possibles.
Max Scheler

A spiritual midwife
It is love, Scheler believes, that
makes things apparent to our
experience and that makes
See also: Plato 50â€“55

â– 

Blaise Pascal 124â€“25

â– 

Edmund Husserl 224â€“25

THE MODERN WORLD 241

ONLY AS AN INDIVIDUAL
CAN MAN BECOME
AKARLPHILOSOPHER
JASPERS (1883â€“1969)
IN CONTEXT
BRANCH
Epistemology
APPROACH
Existentialism
BEFORE
1800s SÃ¸ren Kierkegaard
writes of philosophy as a
matter of the individualâ€™s
struggle with truth.
1920s Martin Heidegger
claims that philosophy is a
matter of our relationship with
our own existence.
1920s Friedrich Nietzsche
says that â€œGod is deadâ€, there
are no absolute truths, and we
must rethink all our values.
AFTER
From 1940 Hannah Arendtâ€™s
ideas of freedom are inï¬‚uenced
by Jaspersâ€™ philosophy.
From 1950 Hans-Georg
Gadamer explores the idea
that philosophy progresses
through a fusion of individual
perspectives.

F

or some, philosophy is a
way to discover objective
truths about the world.
For German philosopher and
psychiatrist Karl Jaspers, on the
other hand, philosophy is a personal
struggle. Strongly inï¬‚uenced by
the philosophers Kierkegaard and
Nietzsche, Jaspers is an existentialist
who suggests that philosophy is
a matter of our own attempts to
realize truth. Since philosophy is an
individual struggle, he writes in his

1941 book On my Philosophy, we
can philosophize only as individuals.
We cannot turn to anybody else to
tell us the truth; we must discover
it for ourselves, by our own efforts.

A community of individuals
Although in this sense truth is
something that we realize alone,
it is in communication with others
that we realize the fruits of our
efforts and raise our consciousness
beyond its limits. Jaspers considers
his own philosophy â€œtrueâ€ only so
far as it aids communication with
others. And while other people
cannot provide us with a form of
â€œready-made truthâ€, philosophy
remains a collective endeavor. For
Jaspers, each individualâ€™s search
for truth is carried out in community
with all those â€œcompanions in
thoughtâ€ who have undergone
the same personal struggle. â– 
The philosopher lives in the invisible
realm of the spirit, struggling to realize
truth. The thoughts of other, companion,
philosophers act as signposts towards
potential paths to understanding.

See also: SÃ¸ren Kierkegaard 194â€“95 â–  Friedrich Nietzsche 214â€“21 â–  Martin
Heidegger 252â€“55 â–  Hans-Georg Gadamer 260â€“61 â–  Hannah Arendt 272

242

LIFE IS A SERIES
OF COLLISIONS
WITH THE FUTURE
JOSE ORTEGA Y GASSET (1883â€“1955)

IN CONTEXT
BRANCH
Ontology
APPROACH
Existentialism
BEFORE
1641 In his Meditations, RenÃ©
Descartes argues that there
are two worlds: the world of
mind and the world of matter.
Early 1900s Edmund Husserl
establishes phenomenology. He
claims that philosophers must
look at the world anew, putting
all preconceptions aside.
AFTER
1920s Martin Heidegger
explores questions about what
our existence means for us,
citing Ortega as an inï¬‚uence.
1930s onward Ortegaâ€™s
philosophy becomes popular
in Spain and Latin America,
inï¬‚uencing philosophers
Xavier Zubiri, JosÃ© Gaos,
Ignacio EllacurÃ­a, and MarÃ­a
Zambrano, among others.

O

rtega y Gassetâ€™s philosophy
is about life. He is not
interested in analyzing the
world in a cool and detached fashion.
Instead, he wants to explore how
philosophy can engage creatively
with life. Reason, Ortega believes,
is not something passive, but
something activeâ€”something that
allows us to get to grips with the
circumstances in which we ï¬nd
ourselves, and allows us to change
our lives for the better.

In his Meditations on Quixote,
published in 1914, Ortega writes:
â€œI am myself and my circumstances.â€
Descartes said that it was possible
to imagine ourselves as thinking
beings, and yet to doubt the
existence of the external world,
including our own bodies. But
Ortega says that it makes no sense
to see ourselves as separate from
the world. If we want to think
seriously about ourselves, we have
to see that we are always immersed

We are always immersed in
particular circumstances, such
as where we live, what we do,
and things we assume.

We can accept or reject these
circumstances, by imagining
new possibilities.

The new possibilities
collide with our current
circumstances.

Life is a series of
collisions with the future.

THE MODERN WORLD 243
See also: RenÃ© Descartes 116â€“23 â–  Immanuel Kant 164â€“71 â–  Edmund Husserl
224â€“25 â–  Martin Heidegger 252â€“55 â–  Jean-Paul Sartre 268â€“71

in particular circumstancesâ€”
circumstances that are often
oppressive and limiting. These
limitations are not only those of
our physical surroundings, but also
of our thoughts, which contain
prejudices, and our behavior, which
is shaped by habit.
While many people live without
reï¬‚ecting on the nature of their
circumstances, Ortega says that
philosophers should not only strive
to understand their circumstances
better, they should actively seek
to change them. Indeed, he claims
that the philosopherâ€™s duty is to
expose the assumptions that
lie behind all our beliefs.

The energy of life
In order to transform the world and
to engage creatively with our own
existence, Ortega says that we
must look at our lives with fresh
eyes. This means not only looking
anew at our external circumstances,
but also looking inside ourselves to
reconsider our beliefs and prejudices.
Only when we have done this will
we be able to commit ourselves to
creating new possibilities.

Every act of hope, such as celebrating
Christmas on the front line in World
War I, is a testament to our ability
to overcome our circumstances. For
Ortega, this is â€œvital reasonâ€ in action.

I am myself and
my circumstances.
JosÃ© Ortega y Gasset
JosÃ© Ortega y Gasset

However, there is a limit to the
amount that we can change the
world. Our habitual thinking runs
deep, and even if we free ourselves
enough to imagine new possibilities
and new futures, our external
circumstances may stand in the
way of realizing these possibilities.
The futures that we imagine will
always collide with the reality of
the circumstances in which we
ï¬nd ourselves. This is why Ortega
sees life as a series of collisions
with the future.
Ortegaâ€™s idea is challenging
on both a personal and a political
level. It reminds us that we have
a duty to attempt to change our
circumstances, even though we
may encounter difï¬culties in doing
so, and even though our attempts
may not always succeed. In The
Revolt of the Masses, he warns that
democracy carries within it the
threat of tyranny by the majority,
and that to live by majority ruleâ€”to
live â€œlike everyone elseâ€â€”is to live
without a personal vision or moral
code. Unless we engage creatively
with our own lives, we are hardly
living at all. This is why for Ortega,
reason is vitalâ€”it holds the energy
of life itself. â– 

JosÃ© Ortega y Gasset was
born in Madrid, Spain, in 1883.
He studied philosophy ï¬rst
in Madrid, then at various
German universitiesâ€”where
he became inï¬‚uenced by the
philosophy of Immanuel
Kantâ€”before settling in Spain
as a university professor.
Throughout his life, Ortega
earned a living not only as a
philosopher but as a journalist
and essayist. He was also
actively engaged in Spanish
politics in the 1920s and
1930s, but his involvement
came to an end with the
outbreak of the Spanish Civil
War in 1936. Ortega then
went into exile in Argentina,
where he stayed, disillusioned
with politics, until 1945.
After three years in Portugal,
he returned to Madrid in
1948, where he founded the
Institute of Humanities. He
continued working as a
philosopher and journalist
for the remainder of his life.
Key works
1914 Meditations on Quixote
1925 The Dehumanization
of Art
1930 The Revolt of the Masses
1935 History as a System
1957 What is Philosophy?

244

TO PHILOSOPHIZE,
FIRST ONE MUST
CONFESS
HAJIME TANABE (1885â€“1962)

IN CONTEXT
BRANCH
Ethics
APPROACH
Phenomenology
BEFORE
5th century BCE Socrates
claims that he is wise because
he knows he is ignorant.
4th century St. Augustine
of Hippo writes Confessions,
which is both an autobiography
and a work of philosophy.

B

efore you read on, confess!
This may seem like a
strange idea, but it is one
that Japanese philosopher Tanabe
Hajime wants us to take seriously.
If we want to philosophize, Tanabe
believes, we cannot do so without
making a confession. But what is it
that we should confess, and why?

asking
deeper questions about life.

Early 13th century Buddhist
monk Shinran claims that
salvation is only possible
through â€œother power.â€
1920s Martin Heidegger
writes that philosophy is a
matter of our relationship
with our own being.
AFTER
1990s Jacques Derrida,
inï¬‚uenced by phenomenology,
explores themes such as
confession and forgiveness.

To answer these questions, we
need to look at the roots of Tanabeâ€™s
philosophy in both the European
and the Japanese traditions of
philosophy. In terms of its European
roots, Tanabe traces his thought
back to the Greek philosopher
Socrates who lived in the 5th
century BCE. Socrates is important

To do this,
we need to
admit thatâ€¦
...our powers of
reason are limited.

...we do not know
the answers.

In order
to philosophize,
ï¬rst one must
confess.

THE MODERN WORLD 245
See also: Siddharta Gautama 30â€“33 â–  Socrates 46â€“49
Martin Heidegger 252â€“55 â–  Jacques Derrida 308â€“13

The Buddha Amitabha, here shown
between Kannon (Compassion) and
Seishi (Wisdom), is the principal buddha
of the Pure Land school of Buddhism,
to which Shinran belonged.

to Tanabe because of the way he
frankly confessed that he knew
nothing. According to the story, the
oracle at Delphi said that Socrates
was the wisest man in Athens, and
Socrates, who was certain of his
own ignorance, set out to prove the
oracle wrong. After innumerable
conversations with people in Athens,
he came to the conclusion that he
was indeed the wisest person in
the city, because he alone could
accept that he knew nothing.

Hajime Tanabe
Hajime Tanabe was born in
Tokyo, Japan, in 1885. After
studying at Tokyo University,
he was appointed associate
professor of philosophy at Kyoto
University, where he was an
active member of what became
known as the Kyoto School
of philosophy. In the 1920s,
he spent time in Germany
studying with the philosophers
Edmund Husserl and Martin
Heidegger, and after his return

â– 

St. Augustine of Hippo 72â€“73

The Japanese roots of Tanabeâ€™s
idea go back to the thought of the
Buddhist monk Shinran, who
belonged to what is known as the
Pure Land school of Buddhism.
Shinranâ€™s innovation was his claim
that enlightenment is impossible if
we rely on our own power. Instead,
we must confess our own ignorance
and limitations, so that we are open
to what both Shinran and Tanabe
call tariki, or â€œother power.â€ In the
context of Pure Land Buddhism,
this other power is that of the
Buddha Amitabha. In the context
of Tanabeâ€™s philosophy, confession
leads to a recognition of â€œabsolute
nothingnessâ€, and ultimately to
self-awakening and wisdom.

Forsaking ourselves
For Tanabe, then, philosophy is
not about discussing the ï¬ner
points of logic, or about arguing
or debating anythingâ€”it is not,
in fact, an â€œintellectualâ€ discipline.
For Tanabe, it is something much
more fundamentalâ€”a process of
relating, in the deepest possible
sense, to our very own beingâ€”an
idea that is partly shaped by his
reading of Martin Heidegger.
to Japan he was appointed to
the post of full professor. He
was deeply affected by World
War II, and when it ended in
1945 he retired from teaching
philosophy. Tanabeâ€™s book
Philosophy as Metanoetics was
published a year later, in 1946.
After his retirement, Tanabe
dedicated the remainder of his
life to meditation and writing.
Key works
1946 Philosophy as Metanoetics

â– 

Edmund Husserl 224â€“25

â– 

For a problem
to belong to philosophy,
there must be something
inconceivable in it.
Hajime Tanabe

It is only through confessing,
Tanabe believes, that we can
rediscover our true beingâ€”a
process he describes in directly
religious terms as a form of death
and resurrection. This death and
resurrection is the rebirth of the
mind through â€œother powerâ€, and
its passing from the limited view
of the â€œselfâ€ to the perspective of
enlightenment. However, this shift
is not simply a preparation for
philosophyâ€”on the contrary, it is
the very work of philosophy itself,
which is rooted in scepticism and
the â€œforsaking of ourselves to the
grace of other power.â€ Philosophy,
in other words, is not an activity
that we engage in, but something
that happens through us when we
gain access to our true selves by
letting go of the selfâ€”a phenomenon
that Tanabe calls â€œaction without
an acting subject.â€
Continual confession is, Tanabe
writes, â€œthe ultimate conclusionâ€
to which the recognition of our
limitations drives us. In other
words, Tanabe asks us not to ï¬nd
new answers to old philosophical
questions, but to re-evaluate the
very nature of philosophy. â– 

THE LIMITS OF MY

LANGUAGE
ARE THE LIMITS OF MY

WORLD
LUDWIG WITTGENSTEIN (1889â€“1951)

248 LUDWIG WITTGENSTEIN
IN CONTEXT
BRANCH
Philosophy of language
APPROACH
Logic

Language is
made up of propositions:
assertions about things,
which may be true or false.

The world is
made up of facts:
things are a certain way.

BEFORE
4th century BCE Aristotle
sets the foundations of logic.
Late 19th century Gottlob
Frege develops the foundations
of modern logic.

Propositions are â€œpicturesâ€
of facts, in the same way that
maps are pictures of the world.

Early 20th century Bertrand
Russell develops notation that
translates natural language
into logical propositions.
AFTER
1920s Ideas in the Tractatus
are used by philosophers of the
Vienna Circle, such as Moritz
Schlick and Rudolf Carnap, to
develop Logical Positivism.
From 1930 Wittgenstein
rejects the ideas expressed in
the Tractatus, and begins to
explore very different ways
of viewing language.

W

ittgensteinâ€™s Tractatus
Logico-Philosophicus
is perhaps one of the
most forbidding texts in the history
of 20th-century philosophy. Only
around 70 pages long in its English
translation, the book is made up of
a series of highly condensed and
technical numbered remarks.
In order to appreciate the full
signiï¬cance of the Tractatus, it
is important to set it within its
philosophical context. The fact
that Wittgenstein is talking about
the â€œlimitsâ€ of my language and my
world sets him ï¬rmly within the

Any proposition that
does not picture facts
is meaninglessâ€”for
example â€œkilling is bad.â€

My language is
therefore limited
to statements of facts
about the world.

The limits of my language
are the limits of my world.

philosophical tradition that stems
from the 18th-century German
philosopher Immanuel Kant. In
The Critique of Pure Reason, Kant
set out to explore the limits of
knowledge by posing questions
such as â€œWhat can I know?â€ and
â€œWhat things will lie forever outside
of human understanding?â€ One
reason that Kant asked such
questions was that he believed
many problems in philosophy arose
because we fail to recognize the
limitations of human understanding.
By turning our attention back onto
ourselves and asking about the

necessary limits of our knowledge,
we can then either resolve, or even
perhaps dissolve, nearly all of the
philosophical problems of the past.
The Tractatus tackles the same
kind of task that Kant did, but does
so in a far more radical fashion.
Wittgenstein states that he is
setting out to make clear what can
be meaningfully said. In much the
same way that Kant strives to set
the limits of reason, Wittgenstein
wants to set the limits of language
and, by implication, of all thought.
He does this because he suspects
that a great deal of philosophical

THE MODERN WORLD 249
See also: Aristotle 56â€“63

â– 

Immanuel Kant 164â€“71

The solution of
the problem of life is
seen in the vanishing
of the problem.
Ludwig Wittgenstein

discussion and disagreement is
based on some fundamental errors
in how we go about thinking and
talking about the world.

Logical structure
For all of their apparent complexity,
Wittgensteinâ€™s central ideas in
the Tractatus are essentially based
on a fairly simple principle, that
both language and the world are
formally structured, and that
these structures can be broken
down into their component parts.
Wittgenstein attempts to lay bare
the structures both of the world
and of language, and then to show
the way they relate to each other.
Having done this, he attempts to
draw a number of wide-reaching
philosophical conclusions.
If we are to understand what
Wittgenstein means when he says
that limits of my language are the
limits of my world, we need to ask
what he means by the words
The ancient Egyptians arranged
symbols and stylized images of objects
in the world, known as hieroglyphs,
into logically structured sequences
to create a form of written language.

â– 

Gottlob Frege 336

â– 

Bertrand Russell 236â€“39

â€œworldâ€ and â€œlanguageâ€, because
he does not use these words in the
everyday sense we might expect.
When he talks about language,
the debt Wittgenstein owes to
the British philosopher Bertrand
Russell becomes apparent. For
Russell, who was an important
ï¬gure in the development of
philosophical logic, everyday
language was inadequate for
talking clearly and precisely about
the world. He believed that logic
was a â€œperfect languageâ€, which
excluded all traces of ambiguity, so
he developed a way of translating
everyday language into what he
considered a logical form.
Logic is concerned with what are
known in philosophy as propositions.
We can think of propositions as
assertions that it is possible for us
to consider as being either true or
false. For example, the statement
â€œthe elephant is very angryâ€ is a
proposition, but the word â€œelephantâ€
is not. According to Wittgensteinâ€™s

â– 

Rudolf Carnap 257

Tractatus, meaningful language
must consist solely of propositions.
â€œThe totality of propositions,â€ he
writes, â€œis language.â€
Knowing a little about what
Wittgenstein means by language,
we can now explore what he means
by â€œthe world.â€ The Tractatus
begins with the claim that â€œthe
world is all that is the case.â€ This
might appear to be straightforward
and robustly matter-of-fact, but
taken on its own, it is not entirely
clear what Wittgenstein means by
this statement. He goes on to write
that â€œthe world is the totality of
facts, not of things.â€ Here we can
see a parallel between the way that
Wittgenstein treats language and
the way he is treating the world. It
may be a fact, for example, that the
elephant is angry, or that there is
an elephant in the room, but an
elephant just by itself is not a fact.
From this point, it begins to
become clear how the structure
of language and that of the world â¯â¯

250 LUDWIG WITTGENSTEIN

Logic is not
a body of doctrine
but a mirror-image
of the world.
Ludwig Wittgenstein

A digital image, although not the
same sort of object as the one it depicts,
has the same â€œlogical form.â€ Words only
represent reality for Wittgenstein if,
again, both have the same logical form.

might be related. Wittgenstein says
that language â€œpicturesâ€ the world.
He formulated this idea during
World War I, when he read in a
newspaper about a court case in
Paris. The case concerned a
car accident, and the events were
re-enacted for those present in
court using model cars and model
pedestrians to represent the cars
and pedestrians in the real world.
The model cars and the model
pedestrians were able to depict
their counterparts, because they
were related to each other in
exactly the same way as the real
cars and real pedestrians involved
in the accident. Similarly, all the
elements depicted on a map are
related to each other in exactly
the same way as they are in the
landscape that the map represents.
What a picture shares with that
which it is depicting, Wittgenstein
says, is a logical form.
It is important here to realize
that we are talking about logical
pictures, and not about visual
pictures. Wittgenstein presents a
useful example to show what he

means. The sound waves generated
by a performance of a symphony,
the score of that symphony, and the
pattern formed by the grooves on
a gramophone recording of the
symphony all share between them
the same logical form. Wittgenstein
states, â€œA picture is laid against
reality like a measure.â€ In this way
it can depict the world.
Of course, our picture may be
incorrect. It may not agree with
reality, for example, by appearing to
show that the elephant is not angry
when the elephant is, in fact, very
angry. There is no middle ground
here for Wittgenstein. Because he
starts with propositions that are,
by their very nature, true or false,
pictures also are either true or false.
Language and the world, then,
both have a logical form; and
language can speak about the
world by picturing the world, and
picturing it in a fashion that agrees
with reality. It is at this point that
Wittgensteinâ€™s idea gets really
interesting, and it is here that we
can see why Wittgenstein is
interested in the limits of language.

Consider the following idea: â€œYou
should give half of your salary to
charity.â€ This is not picturing
anything in the world in the sense
meant by Wittgenstein. What can
be saidâ€”what Wittgenstein calls
the â€œtotality of true propositionsâ€â€”
is merely the sum of all those
things that are the case, or the
natural sciences.
Discussion about religious and
ethical values is, for Wittgenstein,
strictly meaningless. Because the
things that we are attempting to
talk about when we discuss such
topics are beyond the limits of the
world, they also lie beyond the
limits of our language. Wittgenstein
writes, â€œIt is clear that ethics cannot
be put into language.â€

Beyond words
Some readers of Wittgenstein,
at this point, claim that he is a
champion of the sciences, driving
out vague concepts involved in talk
of ethics, religion, and the like. But
something more complex is going
on. Wittgenstein does not think
that the â€œproblems of lifeâ€ are

THE MODERN WORLD 251

What we cannot
speak about we
must pass
over in silence.
Ludwig Wittgenstein

was fearless in following his
argument to its conclusion,
ultimately recognizing that the
answer to such a question must be
yes. Anybody who understands the
Tractatus properly, he claims, will
eventually see that the propositions
used in it are nonsense, too. They
are like the steps of a philosophical
ladder that helps us to climb
altogether beyond the problems of
philosophy, but which we can kick
away once we have ascended.

Ludwig Wittgenstein
Change of direction
nonsensical. Instead, he believes
that these are the most important
problems of all. It is simply that
they cannot be put into words,
and because of this, they cannot
become a part of philosophy.
Wittgenstein writes that these
things, even though we cannot
speak of them, nevertheless make
themselves manifest, adding that
â€œthey are what is mystical.â€
All of this, however, has serious
repercussions for the propositions
that lie within the Tractatus itself.
After all, these are not propositions
that picture the world. Even logic,
one of Wittgensteinâ€™s major tools,
does not say anything about the
world. Is the Tractatus, therefore,
nonsense? Wittgenstein himself

After completing the Tractatus,
Wittgenstein concluded that
there were no more philosophical
problems left to resolve, and so
abandoned the discipline. However,
over the course of the 1920s and
1930s, he began to question his
earlier thinking, becoming one of
its ï¬ercest critics. In particular, he
questioned his once ï¬rmly held
belief that language consists
solely of propositions, a view that
ignores much of what we do in our
everyday speechâ€”from telling
jokes, to cajoling, to scolding.
Nevertheless, despite all of its
problems, the Tractatus remains
one of the most challenging and
compelling works of Western
philosophyâ€”and ultimately one
of the most mysterious. â– 

Philosophy demands logical, unambiguous
language. Wittgenstein concludes, therefore, that it
can only be made up of propositions, or statements
of fact, such as â€œthe cat sat on the matâ€, which can
be clearly divided into their component parts.

Born into a wealthy Viennese
family in 1889, Wittgenstein
ï¬rst studied engineering and
in 1908 traveled to England
to continue his education in
Manchester. However, he soon
developed an interest in logic,
and by 1911 had moved to
Cambridge to study under the
philosopher Bertrand Russell.
During World War I, he
served on the Russian front
and in Italy, where he was
taken prisoner. Around this
time, he began the Tractatus
Logico-Philosophicus, which
was published in 1921.
Believing that the Tractatus
resolved all the problems of
philosophy, Wittgenstein now
embarked on an itinerant
career as a schoolteacher,
gardener, and architect. But
after developing criticisms of
his earlier ideas, he resumed
his work at Cambridge in
1929, becoming a professor
there in 1939. He died in 1951.
Key works

+

=

1921 Tractatus
Logico-Philosophicus
1953 Philosophical
Investigations
1958 The Blue and
Brown Books
1977 Remarks on Colour

252

WE ARE OURSELVES
THE ENTITIES
TO BE ANALYZED

MARTIN HEIDEGGER (1889â€“1976)

IN CONTEXT
BRANCH
Ontology
APPROACH
Phenomenology
BEFORE
c.350 BCE Diogenes of Sinope
uses a plucked chicken to
parody Platoâ€™s followersâ€™ claim
that a human being is a
â€œfeatherless biped.â€
1900â€“13 Edmund Husserl
proposes his phenomenological
theories and method in Logical
Investigations and Ideas I.
AFTER
1940s Jean-Paul Sartre
publishes Being and
Nothingness, which looks at
the connection between
â€œbeingâ€ and human freedom.
1960 Hans-Georg Gadamerâ€™s
Truth and Method, inspired by
Heidegger, explores the nature
of human understanding.

I

t is said that in ancient
Athens the followers of
Plato gathered one day to ask
themselves the following question:
â€œWhat is a human being?â€ After
a great deal of thought, they came
up with the following answer:
â€œa human being is a featherless
biped.â€ Everybody seemed content
with this deï¬nition until Diogenes
the Cynic burst into the lecture
hall with a live plucked chicken,
shouting, â€œBehold! I present you
with a human being.â€ After the
commotion had died down, the
philosophers reconvened and
reï¬ned their deï¬nition. A human
being, they said, is a featherless
biped with broad nails.

THE MODERN WORLD 253
See also: Plato 50â€“55 â–  Diogenes of Sinope 66 â–  Edmund Husserl 224â€“25 â–  Hans-Georg Gadamer 260â€“61
Ernst Cassirer 337 â–  Jean-Paul Sartre 268â€“71 â–  Hannah Arendt 272 â–  Richard Rorty 314â€“19

â– 

Philosophy has always
asked deep questions
about â€œBeing.â€

The question of existence
never gets straightened out
except through existing itself.
Martin Heidegger
We need to ask these
questions by looking at
the being for whom
Being is an issue.

Us!

We ourselves are
the entities to
be analyzed.

experience of them. For example,
phenomenology would not look
directly at the question â€œwhat is a
human being?â€ but would instead
look at the question â€œwhat is it like
to be human?â€

The human existence

This curious story from the history
of early philosophy shows the kinds
of difï¬culties philosophers have
sometimes been faced with when
attempting to give abstract, general
deï¬nitions of what it is to be human.
Even without the intervention
of Diogenes, it seems clear that
describing ourselves as featherless
bipeds does not really capture much
of what it means to be human.

An insiderâ€™s perspective
It is this questionâ€”how we might
go about analyzing what it is to be
humanâ€”that concerned the
philosopher Martin Heidegger.
When Heidegger came to answer
the question, he did so in a way

that was strikingly different from
many of his predecessors. Instead
of attempting an abstract deï¬nition
that looks at human life from the
outside, he attempts to provide a
much more concrete analysis of
â€œbeingâ€ from what could be called
an insiderâ€™s position. He says that
since we exist in the thick of
thingsâ€”in the midst of lifeâ€”if we
want to understand what it is to be
human, we have to do so by looking
at human life from within this life.
Heidegger was a student of
Husserl, and he followed Husserlâ€™s
method of phenomenology. This
is a philosophical approach that
looks at phenomenaâ€”how things
appearâ€”through examining our

For Heidegger, this constitutes the
fundamental question of philosophy.
He was most interested in the
philosophical subject of ontology
(from the Greek word ontos,
meaning â€œbeingâ€), which looks at
questions about being or existence.
Examples of ontological questions
might be: â€œwhat does it mean to say
that something exists?â€ and â€œwhat
are the different kinds of things
that exist?â€ Heidegger wanted use
the question â€œwhat is it like to be
human?â€ as a way of answering
deeper questions about existence
in general.
In his book, Being and Time,
Heidegger claims that when other
philosophers have asked ontological
questions, they have tended to use
approaches that are too abstract
and shallow. If we want to know
what it means to say that something
exists, we need to start looking â¯â¯

254 MARTIN HEIDEGGER

We should raise anew
the question of the
meaning of being.
Martin Heidegger

We try to make sense of the world
by engaging with projects and tasks
that lend life a unity. Being human,
Heidegger says, means to be immersed
in the day-to-day world.

at the question from the perspective
of those beings for whom being is
an issue. We can assume that
although cats, dogs, and toadstools
are beings, they do not wonder
about their being: they do not fret
over ontological questions; they do
not ask â€œwhat does it mean to say
that something exists?â€ But there
is, Heidegger points out, one being
that does wonder about these

things, and that is the human being.
In saying that we are ourselves the
entities to be analyzed, Heidegger is
saying that if we want to explore
questions of being, we have to start
with ourselves, by looking at what
it means for us to exist.

Being and time
When Heidegger asks about
the meaning of being, he is not
asking about abstract ideas, but
about something very direct and
immediate. In the opening pages of
his book, he says that the meaning
of our being must be tied up with
time; we are essentially temporal

beings. When we are born, we ï¬nd
ourselves in the world as if we had
been thrown here on a trajectory
we have not chosen. We simply ï¬nd
that we have come to exist, in an
ongoing world that pre-existed us,
so that at our birth we are presented
with a particular historical, material,
and spiritual environment. We
attempt to make sense of this world
by engaging in various pastimesâ€”
for example, we might learn Latin, or
attempt to ï¬nd true love, or decide
to build ourselves a house. Through
these time-consuming projects we
literally project ourselves toward
different possible futures; we deï¬ne
our existence. However, sometimes
we become aware that there is an
outermost limit to all our projects, a
point at which everything we plan
will come to an end, whether ï¬nished
or unï¬nished. This point is the
point of our death. Death, Heidegger
says, is the outermost horizon of our
being: everything we can do or see
or think takes place within this
horizon. We cannot see beyond it.
Heideggerâ€™s technical vocabulary
is famously difï¬cult to understand,
but this is largely because he is
attempting to explore complex
philosophical questions in a concrete
or non-abstract way; he wants to
relate to our actual experience. To
say that â€œthe furthest horizon of our

THE MODERN WORLD 255
being is deathâ€ is to say something
about what it is like to live a human
life, and it captures some idea of
what we are in a way that many
philosophical deï¬nitionsâ€”
â€œfeatherless bipedâ€ or â€œpolitical
animalâ€, for exampleâ€”overlook.

Living authentically
It is to Heidegger that we owe the
philosophical distinction between
authentic and inauthentic existence.
Most of the time we are wrapped
up in various ongoing projects, and
forget about death. But in seeing
our life purely in terms of the
projects in which we are engaged,
we miss a more fundamental
dimension of our existence, and to
that extent, Heidegger says, we are
existing inauthentically. When we
become aware of death as the
ultimate limit of our possibilities, we
start to reach a deeper understanding
of what it means to exist.
For example, when a good friend
dies, we may look at our own lives
and realize that the various projects
which absorb us from day to day
feel meaningless, and that there is
a deeper dimension to life that is

All being is a â€œbeing-towards-deathâ€,
but only humans recognize this. Our
lives are temporal, and it is only once
we realize this that we can live a
meaningful and authentic life.

Dying is not an event;
it is a phenomenon to be
understood existentially.
Martin Heidegger

Martin Heidegger
missing. And so we may ï¬nd
ourselves changing our priorities
and projecting ourselves toward
different futures.

A deeper language
Heideggerâ€™s later philosophy
continues to tackle questions of
being, but it turns away from his
earlier, exacting approach to take
a more poetic look at the same
kinds of questions. Philosophy, he
comes to suspect, simply cannot
reï¬‚ect this deeply on our own
being. In order to ask questions
about human existence, we must
use the richer, deeper language of
poetry, which engages us in a way
that goes far beyond the mere
exchange of information.
Heidegger was one of the
20th centuryâ€™s most inï¬‚uential
philosophers. His early attempt to
analyze what it means to be
human, and how one might live an
authentic life, inspired philosophers
such as Sartre, Levinas, and
Gadamer, and contributed to the
birth of existentialism. His later,
more poetic, thinking has also had
a powerful inï¬‚uence on ecological
philosophers, who believe it offers
a way of thinking about what it
means to be a human being
within a world under threat of
environmental destruction. â– 

Heidegger is acknowledged to
be one of the most important
philosophers of the 20th
century. He was born in 1889
in Messkirch, Germany, and
had early aspirations to be a
priest, but after coming across
the writings of Husserl he took
up philosophy instead. He
quickly became well known as
an inspirational lecturer, and
was nicknamed â€œthe magician
of Messkirch.â€ In the 1930s he
became rector of Freiburg
University and a member of
the Nazi party. The extent and
nature of his involvement with
Nazism remains controversial,
as is the question of how far
his philosophy is implicated in
the ideologies of Nazism.
Heidegger spent the last
30 years of his life traveling
and writing, exchanging ideas
with friends such as Hannah
Arendt and the physicist
Werner Heisenberg. He died
in Freiburg in 1976, aged 86.
Key works
1927 Being and Time
1936â€“53 Overcoming
Metaphyics
1955â€“56 The Principle
of Reason
1955â€“57 Identity and
Difference

256

THE INDIVIDUALâ€™S ONLY
TRUE MORAL CHOICE IS
THROUGH SELF-SACRIFICE
FOR THE COMMUNITY
TETSURO WATSUJI (1889â€“1960)

IN CONTEXT
BRANCH
Ethics
APPROACH
Existentialism
BEFORE
13th century Japanese
philosopher DoÌ„gen writes
about â€œforgetting the self.â€
Late 19th century Friedrich
Nietzsche writes about the
inï¬‚uence of â€œclimateâ€ on
philosophy; this idea becomes
important to Watsujiâ€™s thought.
1927 Martin Heidegger
publishes Being and Time.
Watsuji goes on to rethink
Heideggerâ€™s book in the light of
his ideas on â€œclimateâ€.
AFTER
Late 20th century Japanese
philosopher Yuasa Yasuo
further develops Watsujiâ€™s
ethics of community.

T

etsuro Watsuji was one of
the leading philosophers
in Japan in the early part
of the 20th century, and he wrote
on both Eastern and Western
philosophy. He studied in Japan and
Europe, and like many Japanese
philosophers of his time, his work
shows a creative synthesis of these
two very different traditions.

Forgetting the self
Watsujiâ€™s studies of Western
approaches to ethics convinced him
that thinkers in the West tend to
take an individualistic approach to
human nature, and so also to ethics.
But for Watsuji, individuals can only
be understood as expressions of
their particular times, relationships,
and social contexts, which together
constitute a â€œclimateâ€. He explores
the idea of human nature in terms
of our relationships with the wider

community, which form a network
within which we exist; Watsuji calls
this â€œbetweenness.â€ For Watsuji
ethics is a matter not of individual
action, but of the forgetting or
sacriï¬ce of oneâ€™s self, so that the
individual can work for the beneï¬t
of the wider community.
Watsujiâ€™s nationalist ethics and
insistence on the superiority of the
Japanese race led to his fall from
favor following World War II,
although he later distanced himself
from these views. â– 

Samurai warriors often sacriï¬ced
their own lives in battle in order to save
the state, in an act of extreme loyalty
and self-negation that Watsuji called
kenshin, or â€œabsolute self-sacriï¬ce.â€
See also: SÃ¸ren Kierkegaard 194â€“95 â–  Friedrich Nietzsche 214â€“21 â– 
Nishida Kitaro 336â€“37 â–  Hajime Tanabe 244â€“45 â–  Martin Heidegger 252â€“55

THE MODERN WORLD 257

LOGIC IS THE LAST
SCIENTIFIC
INGREDIENT
OF
PHILOSOPHY
RUDOLF CARNAP (1891â€“1970)
IN CONTEXT
BRANCH
Philosophy of science
APPROACH
Logical positivism
BEFORE
1890 Gottlob Frege starts to
explore the logical structures
of language.
1921 Ludwig Wittgenstein
writes that philosophy is the
study of the limits of language.
AFTER
1930s Karl Popper proposes
that science works by means
of falsiï¬ability: no amount of
positive proofs can prove
something to be true, whereas
one negative result conï¬rms
that a theory is incorrect.
1960s Thomas Kuhn explores
the social dimensions of
scientiï¬c progress,
undermining some of the
tenets of logical positivism.

O

ne of the problems for
20th-century philosophy
is determining a role for
philosophy given the success of the
natural sciences. This is one of the
main concerns of German-born
Rudolf Carnap in The Physical
Language as the Universal Language
of Science (1934), which suggests
that philosophyâ€™s proper functionâ€”
and its primary contribution to
scienceâ€”is the analysis and
clariï¬cation of scientiï¬c concepts.
Carnap claims that many
apparently deep philosophical
problemsâ€”such as metaphysical
onesâ€”are meaningless, because
they cannot be proved or disproved
through experience. He adds that
they are also in fact pseudo-problems
caused by logical confusions in the
way we use language.

Logical language
Logical positivism accepts as true
only strictly logical statements that
can be empirically veriï¬ed. For
Carnap, philosophyâ€™s real task is
therefore the logical analysis of
language (in order to discover and

In logic,
there are no morals.
Rudolf Carnap

rule out those questions that are,
strictly speaking, meaningless), and
to ï¬nd ways of talking clearly and
unambiguously about the sciences.
Some philosophers, such as
Willard Quine and Karl Popper, have
argued that Carnapâ€™s standards for
what can be said meaningfully are
too exacting and present an idealized
view of how science operates,
which is not reï¬‚ected in practice.
Nevertheless, Carnapâ€™s reminder
that language can fool us into
seeing problems that are not really
there is an important one. â– 

See also: Gottlob Frege 336 â–  Ludwig Wittgenstein 246â€“51
Willard Van Orman Quine 278â€“79 â–  Thomas Kuhn 293

â– 

Karl Popper 262â€“65

â– 

258

THE ONLY WAY OF
KNOWING A PERSON
IS TO LOVE THEM
WITHOUT
HOPE
WALTER BENJAMIN (1892â€“1940)
IN CONTEXT
BRANCH
Ethics
APPROACH
Frankfurt School
BEFORE
c.380 BCE Plato writes his
Symposium, considered the
ï¬rst sustained philosophical
account of love.
1863 The French writer
Charles Baudelaire explores
the idea of the ï¬‚Ã¢neur, the
â€œperson who walks the city to
experience it.â€
AFTER
1955 Guy Debord establishes
psychogeography, the study
of the effects of geography
on an individualâ€™s emotions
and behavior.
1971 Italian novelist
Italo Calvino explores the
relationships between
cities and signs in his book
Invisible Cities.

T

he German philosopher
Walter Benjamin was an
afï¬liate of the Frankfurt
School, a group of neo-Marxist
social theorists who explored the
signiï¬cance of mass culture and
communication. Benjamin was also
fascinated by the techniques of ï¬lm
and literature, and his 1926 essay
One-Way Street is an experiment in
literary construction. It is a
collection of observationsâ€”
intellectual and empiricalâ€”that
apparently occur to him as he walks
down an imaginary city street.

The construction of life
currently lies far more in
the hands of facts than
of convictions.
Walter Benjamin

In the essay Benjamin does not
set out a grand theory. Instead
he wants to surprise us with ideas,
in the same way that we might be
surprised by something catching
our eye while on a walk. Toward
the end of the essay, he says that
â€œQuotations in my work are like
wayside robbers who leap out,
brandishing weapons, and relieve
the idler of his certainty.â€

Illuminating love
The idea that the only way of
knowing a person is to love them
hopelessly appears in the middle of
the essay, under the heading â€œArc
Lamp.â€ In a ï¬‚are of light, Benjamin
pauses and thinks just this, and no
moreâ€”the essay moves immediately
afterward to a new section. We are
forced to guess what he means. Is
he saying that knowledge arises
out of love? Or that it is only when
we stop hoping for some outcome
that we can clearly see the beloved?
We cannot know. All we can do is
walk down the street alongside
Benjamin, experiencing the ï¬‚are of
light of these passing thoughts. â– 

See also: Plato 50â€“55 â–  Karl Marx 196â€“203
Roland Barthes 290â€“91

â– 

Theodor Adorno 266â€“67

â– 

THE MODERN WORLD 259

THAT WHICH IS
CANNOT BE TRUE
HERBERT MARCUSE (1898â€“1979)

IN CONTEXT
BRANCH
Political philosophy
APPROACH
Frankfurt School
BEFORE
1820 Georg Hegel writes in
his Philosophy of Right that
what is actual is rational and
what is rational is actual.
1867 Karl Marx publishes
the ï¬rst volume of Das Kapital,
setting out his view of the
â€œlaws of motionâ€ within
capitalist societies, and
asserting that capitalism is
guilty of exploiting humans.
1940s Martin Heidegger
begins to explore the problems
of technology.
AFTER
2000 Slavoj Å½iÅ¾ek explores
the relationship between
technology, capitalist society,
and totalitarianism.

t ï¬rst glance, nothing
seems to be more irrational
than Marcuseâ€™s claim that
â€œthat which isâ€ cannot be true,
which appears in his 1941 book,
Reason and Revolution. If that
which is cannot be true, the reader
is tempted to ask, then what is? But
Marcuseâ€™s idea is partly an attempt
to overturn the claim made by the
German philosopher Hegel that
what is rational is actual, and also
that what is actual is rational.
Marcuse believes this is a
dangerous idea because it leads us
to think that what is actually the
caseâ€”such as our existing political
systemâ€”is necessarily rational.
He reminds us that those things
we take as reasonable may be far
more unreasonable than we like to
admit. He also wants to shake us
up into realizing the irrational
nature of many of the things that
we -take for granted.

A

harmony of freedom and oppression,
productivity and destruction,
growth and regression.â€ We assume
that the societies we live in are
based upon reason and justice,
but when we look more closely, we
may ï¬nd that they are neither as
just nor as reasonable as we believe.
Marcuse is not discounting
reason, but trying to point out that
reason is subversive, and that we
can use it to call into question the
society in which we live. The aim
of philosophy, for Marcuse, is a
â€œrationalist theory of society.â€ â– 

Subversive reason

Fast cars are the kind of consumables
that Marcuse accuses us of using to
recognize ourselves; he says we ï¬nd
â€œour soulâ€ in these items, becoming
mere extensions of the things we create.

In particular, Marcuse is deeply
uneasy with capitalist societies and
with what he calls their â€œterrifying
See also: Georg Hegel 178â€“85
252â€“55 â–  Slavoj Å½iÅ¾ek 326

â– 

Karl Marx 196â€“203

â– 

Martin Heidegger

260

HISTORY DOES NOT
BELONG TO US BUT
WE
BELONG
TO
IT
HANS-GEORG GADAMER (1900â€“2002)
IN CONTEXT
BRANCH
Philosophy of history
APPROACH
Hermeneutics
BEFORE
Early 19th century German
philosopher Friedrich
Schleiermacher lays the
groundwork for hermeneutics.
1890s Wilhelm Dilthey, a
German philosopher, describes
interpretation as taking place
in the â€œhermeneutic circle.â€
1927 Martin Heidegger
explores the interpretation
of being, in Being and Time.
AFTER
1979 Richard Rorty uses
a hermeneutic approach in
his book Philosophy and the
Mirror of Nature.
1983â€“85 French philosopher
Paul Ricoeur writes Time
and Narrative, examining
the capacity of narrative to
represent our feeling of time.

G

adamer is associated in
particular with one form of
philosophy: â€œhermeneuticsâ€.
Derived from the Greek word
hermeneuo, meaning â€œinterpretâ€,
this is the study of how humans
interpret the world.
Gadamer studied philosophy
under Martin Heidegger, who said
that the task of philosophy is to
interpret our existence. This
interpretation is always a process
of deepening our understanding by
starting from what we already
know. The process is similar to how
we might interpret a poem. We start

We understand
the world through
interpretation.

History does
not belong
to us, but we
belong to it.

by reading it carefully in the light
of our present understanding. If
we come to a line that seems strange
or particularly striking, we might
need to reach for a deeper level of
understanding. As we interpret
individual lines, our sense of the
poem as a whole might begin to
change; and as our sense of the
poem as a whole changes, so might
our understanding of individual
lines. This is known as the
â€œhermeneutic circle.â€
Heideggerâ€™s approach to
philosophy moved in this circular
fashion, and this was the approach

This always takes place within
a particular historical era,
which gives us particular
prejudices and biases.

We cannot understand
things outside of these
prejudices and biases.

THE MODERN WORLD 261
See also: Immanuel Kant 164â€“71 â–  Georg Hegel 178â€“85
Jacques Derrida 308â€“13 â–  Richard Rorty 314â€“19

When viewing historical objects
we should not view time as a gulf to
be bridged, says Gadamer. Its distance
is ï¬lled with the continuity of tradition,
which sheds light on our understanding.

that Gadamer later explored in his
book Truth and Method. Gadamer
goes on to point out that our
understanding is always from the
point of view of a particular point in
history. Our prejudices and beliefs,
the kinds of questions that we

Hans-Georg Gadamer

â– 

Martin Heidegger 252â€“55

think are worth asking, and the
kinds of answers with which we
are satisï¬ed are all the product
of our history. We cannot stand
outside of history and culture, so
we can never reach an absolutely
objective perspective.
But these prejudices should not
be seen as a bad thing. They are,
after all, our starting point, and our
current understanding and sense
of meaning are based upon these
prejudices and biases. Even if it
were possible to get rid of all our
prejudices, we would not ï¬nd that
we would then see things clearly.
Without any given framework for
interpretation, we would not be
able to see anything at all.

Conversing with history
Gadamer sees the process of
understanding our lives and our
selves as similar to having a
â€œconversation with history.â€ As
we read historical texts that have
existed for centuries, the differences
in their traditions and assumptions
reveal our own cultural norms and
prejudices, leading us to broaden
Gadamer was born in Marburg
in 1900, but grew up in Breslau,
Germany (now Wroclaw, Poland).
He studied philosophy ï¬rst in
Breslau and then in Marburg,
where he wrote a second doctoral
dissertation under the tutelage of
the philosopher Martin Heidegger,
who was an enormous inï¬‚uence
on his work. He became an
associate professor at Marburg,
beginning a long academic career
which eventually included
succeeding the philosopher Karl
Jaspers as Professor of Philosophy
in Heidelberg in 1949. His most
important book, Truth and

â– 

JÃ¼rgen Habermas 306â€“07

â– 

and deepen our understanding
of our own lives in the present.
For instance, if I pick up a book by
Plato, and read it carefully, I might
ï¬nd not only that I am deepening
my understanding of Plato, but also
that my own prejudices and biases
become clear, and perhaps begin to
shift. Not only am I reading Plato,
but Plato is reading me. Through
this dialogue, or what Gadamer
calls â€œthe fusion of horizonsâ€, my
understanding of the world reaches
a deeper, richer level. â– 

Because an experience
is itself within the whole
of life, the whole of life
is present in it too.
Hans-Georg Gadamer

Method, was published when
he was 60. It attacked the idea
that science offered the only
route to truth and its publication
brought him wider international
fame. A sociable and lively man,
Gadamer remained active right
up until his death in Heidelberg
at the age of 102.
Key works
1960 Truth and Method
1976 Philosophical Hermeneutics
1980 Dialogue and Dialectic
1981 Reason in the Age of
Science

262

IN SO FAR AS
A SCIENTIFIC
STATEMENT SPEAKS
ABOUT REALITY,
IT MUST BE
FALSIFIABLE
KARL POPPER (1902â€“1994)

IN CONTEXT
BRANCH
Philosophy of science
APPROACH
Analytic philosophy
BEFORE
4th century BCE Aristotle
stresses the importance of
observation and measurement
to understanding the world.
1620 Francis Bacon sets
out the inductive methods of
science in Novum Organum.

1748 David Humeâ€™s
Enquiry concerning Human
Understanding raises the
problem of induction.
AFTER
1962 Thomas Kuhn criticizes
Popper in The Structure of
Scientiï¬c Revolutions.

1978 Paul Feyerabend, in
Against Method, questions the
very idea of scientiï¬c method.

W

e often think that science
works by â€œprovingâ€
truths about the world.
We might imagine that a good
scientiï¬c theory is one that we
can prove conclusively to be true.
The philosopher Karl Popper,
however, insists that this is not the
case. Instead, he says that what
makes a theory scientiï¬c is that it
is capable of being falsiï¬ed, or being
shown to be wrong by experience.
Popper is interested in the
method by which science ï¬nds out
about the world. Science depends
on experiment and experience, and
if we want to do science well, we
need to pay close attention to what
philosopher David Hume called

THE MODERN WORLD 263
See also: Socrates 46â€“49 â–  Aristotle 56â€“63
Thomas Kuhn 293 â–  Paul Feyerabend 297

Scientiï¬c understanding
S
works by induction.

But these principles
canâ€™t be proved, only
disproved (such as by the
sighting of a black swan).

â– 

Francis Bacon 110â€“11

â– 

David Hume 148â€“53

Rudolf Carnap 257

â– 

This means working from
particular observations
(such as â€œevery swan
I see is whiteâ€)...

... and moving to general
principles (such as â€œall
swans are whiteâ€).

In so far as
a scientiï¬c statement
speaks about reality, it
must be falsiï¬able.

the â€œregularitiesâ€ of natureâ€”the fact
that events unfold in the world in
particular patterns and sequences
that can be systematically explored.
Science, in other words, is empirical,
or based on experience, and to
understand how it works we need
to understand how experience in
general leads to knowledge.
Consider the following statement:
â€œIf you drop a tennis ball from a
second-ï¬‚oor window, it will fall to
the ground.â€ Leaving aside any
chance events (such as the ball
being snatched away by a passing
eagle), we can be fairly sure that
this claim is a reasonable one. It
would be a strange person who
said, â€œHold on, are you sure it will

â– 

fall to the ground?â€ But how do we
know that this is what will happen
when we drop the tennis ball?
What kind of knowledge is this?
The short answer is that we
know it will fall because that is
what it always does. Leaving aside
chance events, no-one has ever
found that a tennis ball hovers or
rises upward when it is released.
We know it falls to the ground
because experience has shown us
that this will happen. And not only
can we be sure that the ball will fall
to the ground, we can also be sure
about how it will fall to the ground.
For example, if we know the force of
gravity, and how high the window
is above the ground, we can

Black swans were ï¬rst encountered
by Europeans in the 17th century.
This falsiï¬ed the idea that all swans
are white, which at the time was held
to be universally true.

calculate the speed at which the
ball will fall. Nothing about the
event is even remotely mysterious.
Nevertheless, the question
remains: can we be certain that the
next time we drop the ball it will
fall to the ground? No matter how
often we conduct the experiment,
and no matter how conï¬dent we
become about its outcome, we can
never prove that the result will be
the same in the future.

Inductive reasoning
This inability to speak with any
certainty about the future is called
the problem of induction, and it
was ï¬rst recognized by Hume
in the 18th century. So what is
inductive reasoning?
Induction is the process of
moving from a set of observed facts
about the world to more general
conclusions about the world. We
expect that if we drop the ball it
will fall to the ground because, at
least according to Hume, we are
generalizing from innumerable â¯â¯

264 KARL POPPER
experiences of similar occasions
on which we have found things
like balls to fall to the ground
when we release them.

Deductive reasoning
Another form of reasoning,
which philosophers contrast with
induction, is deductive reasoning.
While induction moves from the
particular case to the general,
deduction moves from the general
case to the particular. For instance,
a piece of deductive reasoning might
start from two premises, such as:
â€œIf it is an apple, then it is a fruit
(since all apples are fruit)â€ and
â€œThis is an apple.â€ Given the nature
of these premises, the statement
â€œThis is an appleâ€ leads inescapably
to the conclusion â€œIt is a fruit.â€
Philosophers like to simplify
deductive arguments by writing
them out in notation. So the general
form of the argument above would
be â€œIf P then Q; since P,Â therefore
Q.â€ In our example, â€œPâ€ stands for
â€œIt is an appleâ€, and â€œQâ€ stands for
An example of the
problem of induction is
that no matter how reliably
a tennis ball behaves in
the present, we can never
know for certain how it
will behave in the future.

48Â°

â€œIt is a fruit.â€ Given the starting
points â€œIf P then Qâ€ and â€œPâ€, then
the conclusion â€œQâ€ is necessary, or
unavoidably true. Another example
would be: â€œIf it is raining, the cat
will meow (since all cats meow
in the rain). It is raining, therefore
the cat will meow.â€
All arguments of this kind are
considered by philosophers to be
valid arguments, because their
conclusions follow inevitably from
their premises. However, the fact
that an argument is valid does not
mean that its conclusions are true.
For example, the argument â€œIf it is
a cat, then it is banana-ï¬‚avored;
this is a cat, therefore it is bananaï¬‚avoredâ€ is valid, because it follows
a valid form. But most people would
agree that the conclusion is false.
And a closer look shows that there
is a problem, from an empirical
perspective, with the premise â€œIf it
is a cat, then it is banana-ï¬‚avoredâ€,
because cats, in our world at least,
are not banana-ï¬‚avored. In other
words, because the premise is

Every solution to a
problem creates new
unsolved problems.
Karl Popper

untrue, even though the argument
itself is valid, the conclusion is
also untrue. Other worlds can be
imagined in which cats are in fact
banana-ï¬‚avored, and for this reason
the statement that cats
are not banana-ï¬‚avored is said to
be contingently true, rather than
logically or necessarily true, which
would demand that it be true in
all possible worlds. Nevertheless,
arguments that are valid and have
true premises are called â€œsoundâ€

Experiment B

Experiment A

Experiment C

48Â°
66Â°

66Â°

?

?

THE MODERN WORLD 265

Science may be described
as the art of systematic
over-simpliï¬cation.
Karl Popper

arguments. The banana-ï¬‚avored
cat argument, as we have seen,
is valid but not soundâ€”whereas
the argument about apples and
fruit is both valid and sound.

Falsiï¬ability
Deductive arguments could be said
to be like computer programsâ€”the
conclusions they reach are only as
good as the data that is fed into
them. Deductive reasoning has
an important role to play in the
sciences, but on its own, it cannot
say anything about the world. It
can only say â€œIf this is the case,
then that is the case.â€ And if we
want to use such arguments in the

Karl Popper

sciences, we still have to rely on
induction for our premises, and
so science is lumbered with the
problem of induction.
For this reason, according to
Popper, we cannot prove our
theories to be true. Moreover, what
makes a theory scientiï¬c is not that
it can be proved at all, but that it
can be tested against reality and
shown to be potentially false. In
other words, a falsiï¬able theory
is not a theory that is false, but
one that can only be shown to be
false by observation.
Theories that are untestable (for
example, that we each have an
invisible spirit guide, or that God
created the universe) are not part
of the natural sciences. This does
not mean that they are worthless,
only that they are not the kinds of
theories that the sciences deal with.
The idea of falsiï¬ability does not
mean we are unjustiï¬ed in having
a belief in theories that cannot be
falsiï¬ed. Beliefs that stand up to
repeated testing, and that resist
our attempts at falsiï¬cation, can be
taken to be reliable. But even the
best theories are always open to
the possibility that a new result
will show them to be false.
Karl Popper was born in Vienna,
Austria, in 1902. He studied
philosophy at the University of
Vienna, after which he spent six
years as a schoolteacher. It was
during this time that he published
The Logic of Scientiï¬c Discovery,
which established him as one
of the foremost philosophers of
science. In 1937, he emigrated
to New Zealand, where he lived
until the end of World War II,
and where he wrote his study of
totalitarianism, The Open Society
and Its Enemies. In 1946, he moved
to England to teach, ï¬rst at the
London School of Economics, then

Experiments can show that certain
phenomena reliably follow others in
nature. But Popper claims that no
experiment can ever verify a theory,
or even show that it is probable.

Popperâ€™s work has not been without
its critics. Some scientists claim
that he presents an idealized view
of how they go about their work,
and that science is practiced very
differently from how Popper
suggests. Nevertheless, his
idea of falsiï¬ability is still used in
distinguishing between scientiï¬c
and non-scientiï¬c claims, and
Popper remains perhaps the most
important philosopher of science
of the 20th century. â– 
at the University of London.
He was knighted in 1965, and
remained in England for the rest
of his life. Although he retired in
1969, he continued to write and
publish until his death in 1994.
Key works
1934 The Logic of Scientiï¬c
Discovery
1945 The Open Society and Its
Enemies
1957 The Poverty of Historicism
1963 Conjectures and
Refutations: The Growth of
Scientiï¬c Knowledge

266

INTELLIGENCE
IS A MORAL
CATEGORY
THEODOR ADORNO (1903â€“1969)

IN CONTEXT
BRANCH
Ethics
APPROACH
Frankfurt School
BEFORE
1st century CE Saint Paul
writes about being a â€œfool
for Christ.â€
500â€“1450 The idea of the
â€œholy foolâ€, who represents an
alternative view of the world,
becomes popular throughout
Medieval Europe.
20th century The global
rise of differing forms of
mass-media communication
raises new ethical questions.
AFTER
1994 Portuguese neuroscientist
Antonio Damasio publishes
Descartesâ€™ Error: Emotion,
Reason, and the Human Brain.
21st century Slavoj Å½iÅ¾ek
explores the political, social,
and ethical dimensions of
popular culture.

T

he idea of the holy fool has
a long tradition in the West,
dating all the way back to
Saint Paulâ€™s letter to the Corinthians
in which he asks his followers to be
â€œfools for Christâ€™s sake.â€ Throughout
the Middle Ages this idea was
developed into the popular cultural
ï¬gure of the saint or sage who was
foolish or lacked intelligence, but
who was morally good or pure.
In his book Minima Moralia, the
German philosopher Theodor Adorno
calls into question this long tradition.
He is suspicious of attempts to (as
he puts it) â€œabsolve and beatify the

blockheadâ€, and wants to make the
case that goodness involves our
entire being, both our feeling and
our understanding.
The problem with the idea of
the holy fool, Adorno says, is that
it divides us into different parts,
and in doing so makes us incapable
of acting judiciously at all. In reality,
judgement is measured by the
extent to which we manage to
make feeling and understanding
cohere. Adornoâ€™s view implies that
evil acts are not just failures of
feeling, but also failures of
intelligence and understanding.

Emotion

Intelligence

Both are needed for me
to make judgements about
what is right and wrong.

So to act morally I need to
be able to use my intelligence
as well as my emotions.

Intelligence is a
moral category.

THE MODERN WORLD 267
See also: RenÃ© Descartes 116â€“23
Slavoj Å½iÅ¾ek 326

â– 

Georg Hegel 178â€“85

Adorno was a member of the
Frankfurt School, a group of
philosophers who were interested
in the development of capitalism.
He condemned forms of mass
communication such as television
and radio, claiming that these
have led to the erosion of both
intelligence and feeling, and to a
decline in the ability to make moral
choices and judgements. If we
choose to switch off our brains
by watching blockbuster movies
(insofar as we can choose at all,
given the prevailing cultural
conditions in which we live), for
Adorno, this is a moral choice.
Popular culture, he believes, not
only makes us stupid; it also
makes us unable to act morally.

Essential emotions
Adorno believes that the opposite
error to that of imagining that there
might be such a thing as a holy fool
is imagining that we can judge on
intelligence alone, without emotion.
This might happen in a court of
law; judges have been known to
instruct the jury to put all emotion
to one side, so that they can come
to a cool and measured decision.

The power of judgement
is measured by the
cohesion of self.
Theodor Adorno

â– 

Karl Marx 196â€“203 â– 

Theodor Adorno
Lighthearted television is inherently
dangerous, says Adorno, because it
distorts the world and imbues us with
stereotypes and biases that we begin
to take on as our own.

But in Adornoâ€™s view, we can no
more make wise judgements by
abandoning emotion than we can
by abandoning intelligence.
When the last trace of emotion
has been driven out of our thinking,
Adorno writes, we are left with
nothing to think about, and the idea
that intelligence might beneï¬t â€œfrom
the decay of the emotionsâ€ is simply
mistaken. For this reason Adorno
believes that the sciences, which
are a form of knowledge that do not
make reference to our emotions,
have, like popular culture, had a
dehumanizing effect upon us.
Unexpectedly, it may in fact be
the sciences that will ultimately
demonstrate the wisdom of
Adornoâ€™s central concerns about
the severing of intelligence and
feeling. Since the 1990s, scientists
such as Antonio Damasio have
studied emotions and the brain,
providing increasing evidence of
the many mechanisms by which
emotions guide decision-making.
So if we are to judge wisely or even
to judge at all, we must employ both
emotion and intelligence. â– 

Born in 1903 in Frankfurt,
Theodor Adornoâ€™s two
passions from an early age
were philosophy and music;
his mother and aunt were
both accomplished musicians.
At university Adorno studied
musicology and philosophy,
graduating in 1924. He had
ambitions to be a composer,
but setbacks in his musical
career led him increasingly
toward philosophy. One area
in which Adornoâ€™s interests
converged was in his criticism
of the industry surrounding
popular culture, demonstrated
in his notorious essay On
Jazz, published in 1936.
In 1938, during the rise of
Nazism in Germany, Adorno
emigrated to New York, and
then moved to Los Angeles,
where he taught at the
University of California. He
returned to Germany after
the end of World War II, and
took up a professorship at
Frankfurt. Adorno died at the
age of 66 while on holiday in
Switzerland in 1969.
Key works
1949 Philosophy of New Music
1951 Minima Moralia
1966 Negative Dialectics
1970 Aesthetic Theory

268

EXISTENCE
PRECEDES
ESSENCE
JEAN-PAUL SARTRE (1905â€“1980)

IN CONTEXT
BRANCH
Ethics
APPROACH
Existentialism
BEFORE
4th century BCE Aristotle
asks the question â€œHow should
we live?â€
1840S SÃ¸ren Kierkegaard
writes Either/Or, exploring
the role played by choice in
shaping our lives.
1920S Martin Heidegger
says that what is important
is our relationship with our
own existence.
AFTER
1945 Sartreâ€™s friend and
companion, Simone de
Beauvoir, publishes The
Second Sex, which applies
Sartreâ€™s ideas to the question
of the relationship between
men and women.

S

ince ancient times, the
question of what it is to
be human and what makes
us so distinct from all other types
of being has been one of the main
preoccupations of philosophers.
Their approach to the question
assumes that there is such a thing
as human nature, or an essence of
what it is to be human. It also tends
to assume that this human nature
is ï¬xed across time and space. In
other words, it assumes that there
is a universal essence of what it is
to be human, and that this essence
can be found in every single human
that has ever existed, or will ever
exist. According to this view, all
human beings, regardless of their

THE MODERN WORLD 269
See also: Aristotle 56â€“63 â–  SÃ¸ren Kierkegaard 194â€“95 â–  Martin Heidegger
252â€“55 â–  Simone de Beauvoir 276â€“77 â–  Albert Camus 284â€“85

When we make something
we do so for a purpose.

There is no God.

The purpose (or essence)
of a made thing comes
before its existence.

We are not
made by God.

We are not made
for any purposeâ€¦

â€¦so our existence
precedes our
essence.

circumstances, possess the same
fundamental qualities and are
guided by the same basic values.
For Sartre, however, thinking about
human nature in this way risks
missing what is most important
about human beings, and that is
our freedom.
To clarify what he means by
this, Sartre gives the following
illustration. He asks us to imagine
a paper-knifeâ€”the kind of knife
that might be used to open an
envelope. This knife has been made
by a craftsman who has had the
idea of creating such a tool, and
who had a clear understanding of
what is required of a paper-knife. It
needs to be sharp enough to cut

We have to create our
purpose for ourselves.

through paper, but not so sharp as
to be dangerous. It needs to be easy
to wield, made of an appropriate
substanceâ€”metal, bamboo, or
wood, perhaps, but not butter,
wax, or feathersâ€”and fashioned
to function efï¬ciently. Sartre says
that it is inconceivable for a paperknife to exist without its maker
knowing what it is going to be used
for. Therefore the essence of a
paper-knifeâ€”or all of the things
that make it a paper-knife and not
a steak knife or a paper airplaneâ€”
comes before the existence of any
particular paper-knife.
Humans, of course, are not
paper-knives. For Sartre, there is
no preordained plan that makes â¯â¯

Jean-Paul Sartre
Born in Paris, Sartre was just
15 months old when his father
died. Brought up by his mother
and grandfather, he proved a
gifted student, and gained
entry to the prestigious Ã‰cole
Normale SupÃ©rieure. There he
met his lifelong companion
and fellow philosopher Simone
de Beauvoir. After graduation,
he worked as a teacher and
was appointed Professor of
Philosophy at the University
of Le Havre in 1931.
During World War II, Sartre
was drafted into the army and
brieï¬‚y imprisoned. After his
release in 1941, he joined the
resistance movement.
After 1945, Sartreâ€™s writing
became increasingly political
and he founded the literary
and political journal Modern
Times. He was offered, but
declined, the Nobel Prize for
Literature in 1964. Such was
his inï¬‚uence and popularity
that more than 50,000 people
attended his funeral in 1980.
Key works
1938 Nausea
1943 Being and Nothingness
1945 Existentialism and
Humanism
1960 Critique of Dialectical
Reason

270 JEAN-PAUL SARTRE
us the kind of beings that we are.
We are not made for any particular
purpose. We exist, but not because
of our purpose or essence like a
paper-knife does; our existence
precedes our essence.

Deï¬ning ourselves
This is where we begin to see the
connection between Sartreâ€™s claim
that â€œexistence precedes essenceâ€
and his atheism. Sartre points out
that religious approaches to the
question of human nature often
work by means of an analogy with

human craftsmanshipâ€”that
human nature in the mind of God
is analogous to the nature of the
paper-knife in the mind of the
craftsman who makes it. Even
many non-religious theories of
human nature, Sartre claims, still
have their roots in religious ways of
thinking, because they continue to
insist that essence comes before
existence, or that we are made for a
speciï¬c purpose. In claiming that
existence comes before essence,
Sartre is setting out a position that
he believes is more consistent with

The use or purpose of a tool, such
as a pair of scissors, dictates its
form. Humans, according to Sartre,
have no speciï¬c purpose, so are
free to shape themselves.

blades
to slice effortlessly
through any
material.

Ergonomically
designed handles
for a ï¬rm grip.

Precision-made
screw for a smooth
pivoting action.

his atheism. There is no universal,
ï¬xed human nature, he declares,
because no God exists who could
ordain such a nature.
Here Sartre is relying on a very
speciï¬c deï¬nition of human nature,
identifying the nature of something
with its purpose. He is rejecting the
concept of what philosophers call
teleology in human natureâ€”that it
is something that we can think
about in terms of the purpose of
human existence. Nevertheless,
there is a sense in which Sartre is
offering a theory of human nature,
by claiming that we are the kinds
of beings who are compelled to
assign a purpose to our lives. With
no divine power to prescribe that
purpose, we must deï¬ne ourselves.
Deï¬ning ourselves, however, is
not just a matter of being able to
say what we are as human beings.
Instead, it is a matter of shaping
ourselves into whatever kind of
being we choose to become. This
is what makes us, at root, different
from all the other kinds of being
in the worldâ€”we can become
whatever we choose to make of
ourselves. A rock is simply a rock;
a cauliï¬‚ower is simply a cauliï¬‚ower;
and a mouse is simply a mouse. But
human beings possess the ability
to actively shape themselves.

First of all man exists,
turns up, appears on the
scene, and only afterwards
deï¬nes himself.
Jean-Paul Sartre

THE MODERN WORLD 271
Sartreâ€™s idea that we are free to shape
our own lives inï¬‚uenced the students
that took to the streets of Paris in May
1968 to protest against the draconian
powers of the university authorities.

Because Sartreâ€™s philosophy
releases us from the constraint of
a human nature that is preordained,
it is also one of freedom. We are free
to choose how to shape ourselves,
although we do have to accept
some limitations. No amount of
willing myself to grow wings, for
example, will ever cause that to
happen. But even within the range
of realistic choices we have, we
often ï¬nd that we are constrained
and simply make decisions based
upon habit, or because of the
way in which we have become
accustomed to see ourselves.
Sartre wants us to break free
of habitual ways of thinking, telling
us to face up to the implications of
living in a world in which nothing
is preordained. To avoid falling into
unconscious patterns of behavior, he
believes we must continually face
up to choices about how to act.

Responsible freedom
By making choices, we are also
creating a template for how we think
a human life ought to be. If I decide

As far as men go,
it is not what they are that
interests me, but what
they can become.
Jean-Paul Sartre

to become a philosopher, then I am
not just deciding for myself. I am
implicitly saying that being a
philosopher is a worthwhile activity.
This means that freedom is the
greatest responsibility of all. We
are not just responsible for the
impact that our choices have upon
ourselves, but also for their impact
on the whole of mankind. And,
with no external principles or rules
to justify our actions, we have
no excuses to hide behind for the
choices that we make. For this
reason, Sartre declares that we are
â€œcondemned to be free.â€
Sartreâ€™s philosophy of linking
freedom with responsibility has
been labelled as pessimistic, but
he refutes that charge. Indeed, he
states that it is the most optimistic
philosophy possible, because
despite bearing responsibility for
the impact of our actions upon
others, we are able to choose to
exercise sole control over how we
fashion our world and ourselves.

Sartreâ€™s ideas were particularly
inï¬‚uential on the writings of his
companion and fellow philosopher
Simone de Beauvoir, but they also
had a marked impact on French
cultural and daily life. Young people
especially were thrilled by his call
to use their freedom to fashion
their existence. He inspired them
to challenge the traditionalist,
authoritarian attitudes that prevailed
in France in the 1950s and 1960s.
Sartre is cited as a key inï¬‚uence
on the streets protests in Paris in
May 1968, which helped to bring
down the conservative government
and herald a more liberal climate
throughout France.
Engagement with political
issues was an important part
of Sartreâ€™s life. His constantly
changing afï¬liations, as well as
his perpetual movement between
politics, philosophy, and literature,
are themselves perhaps testament
to a life lived in the light of the idea
that existence precedes essence. â– 

272

THE BANALITY
OF EVIL
HANNAH ARENDT (1906â€“1975)

IN CONTEXT
BRANCH
Ethics
APPROACH
Existentialism
BEFORE
c.350 St Augustine of Hippo
writes that evil is not a
force, but comes from a lack
of goodness.
1200s Thomas Aquinas
writes Disputed questions
on evil, exploring the idea of
evil as a lack of something,
rather than a thing in itself.
AFTER
1971 American social scientist
Philip Zimbardo conducts
the notorious â€œStanford Prison
Experimentâ€ in which ordinary
students are persuaded to
participate in â€œevilâ€ acts that
would normally be considered
unthinkable both to themselves
and to others.

I

n 1961, the philosopher
Hannah Arendt witnessed the
trial of Adolph Eichmann, one
of the architects of the Holocaust.
In her book Eichmann in Jerusalem,
Arendt writes of the apparent
â€œeverydaynessâ€ of Eichmann. The
ï¬gure before her in the dock did
not resemble the kind of monster
we might imagine. In fact, he
would not have looked out of place
in a cafÃ© or in the street.

A failure of judgement
After witnessing the trial, Arendt
came to the conclusion that evil
does not come from malevolence or
a delight in doing wrong. Instead,
she suggests, the reasons people
act in such ways is that they fall
victim to failures of thinking and
judgement. Oppressive political
systems are able to take advantage
of our tendencies toward such
failures, and can make acts that
we might usually consider to be
â€œunthinkableâ€ seem normal.
The idea that evil is banal does
not strip evil acts of their horror.
Instead, refusing to see people
See also: St Augustine of Hippo 72â€“73
Theodor Adorno 266â€“67

Eichmann committed atrocities
not through a hatred of the Jewish
community, Arendt suggests, but
because he unthinkingly followed
orders, disengaging from their effects.

who commit terrible acts as
â€œmonstersâ€, brings these acts
closer to our everyday lives,
challenging us to consider how
evil may be something of which
we are all capable. We should
guard against the failures of our
political regimes, says Arendt,
and the possible failures in our
own thinking and judgement. â– 
â– 

Thomas Aquinas 88â€“95

â– 

THE MODERN WORLD 273

REASON LIVES
IN LANGUAGE
EMMANUEL LEVINAS (1906â€“1995)

IN CONTEXT
BRANCH
Ethics
APPROACH
Phenomenology
BEFORE
1920s Edmund Husserl
explores our relationship to
other human beings from a
phenomenological perspective.
1920s Austrian philosopher
Martin Buber claims that
meaning arises out of our
relationship with others.
AFTER
From 1960 Levinasâ€™s work on
relationships inï¬‚uences the
thoughts of French feminist
philosophers such as Luce
Irigaray and Julia Kristeva.
From 1970 Levinasâ€™s ideas
on responsibility inï¬‚uence
psychotherapy.
2001 Jacques Derrida explores
responsibility in relation to
humanitarian questions such
as political asylum.

L

evinasâ€™s ideas are most easily
understood through looking
at an example. Imagine that
you are walking down a street on a
cold winter evening, and you see a
beggar huddled in a doorway. She
may not even be asking for change,
but somehow you canâ€™t help feeling
some obligation to respond to this
strangerâ€™s need. You may choose
to ignore her, but even if you do,
something has already been
communicated to you: the fact that
this is a person who needs your help.

out of the face-to-face relationships
we have with other people. It is
because we are faced by the needs
of other human beings that we must
offer justiï¬cations for our actions.
Even if you do not give your change
to the beggar, you ï¬nd yourself
having to justify your choice. â– 

Inevitable communication
Levinas was a Lithuanian Jew who
lived through the Holocaust. He says
that reason lives in language in
Totality and Inï¬nity (1961), explaining
that â€œlanguageâ€ is the way that we
communicate with others even
before we have started to speak.
Whenever I see the face of another
person, the fact that this is another
human being and that I have a
responsibility for them is instantly
communicated. I can turn away
from this responsibility, but I cannot
escape it. This is why reason arises

Nothing else in our lives so disrupts
our consciousness as an encounter
with another person, who, simply by
being there, calls to us and asks us
to account for ourselves.

See also: Edmund Husserl 224â€“25 â–  Roland Barthes 290â€“91
HÃ©lÃ¨ne Cixous 322 â–  Julia Kristeva 323

â– 

Luce Irigaray 320

â– 

274

IN ORDER TO SEE THE
WORLD, WE MUST BREAK
WITH OUR FAMILIAR
ACCEPTANCE OF IT
MAURICE MERLEAU-PONTY (1908â€“1961)

IN CONTEXT
BRANCH
Epistemology
APPROACH
Phenomenology
BEFORE
4th century BCE Aristotle
claims that philosophy begins
with a sense of wonder.
1641 RenÃ© Descartesâ€™
Meditations on First Philosophy
establishes a form of mindâ€“
body dualism that MerleauPonty will reject.

T

he idea that philosophy
begins with our ability to
wonder at the world goes
back as far as ancient Greece.
Usually we take our everyday lives
for granted, but Aristotle claimed
that if we want to understand the
world more deeply, we have to put
aside our familiar acceptance of
things. And nowhere, perhaps, is
this harder to do than in the realm

Our experience is
ï¬lled with puzzles and
contradictions.

Early 1900s Edmund Husserl
founds phenomenology as a
philosophical school.
1927 Martin Heidegger writes
Being and Time, a major
inï¬‚uence on Merleau-Ponty.
AFTER
1979 Hubert Dreyfus draws
on the works of Heidegger,
Wittgenstein, and MerleauPonty to explore philosophical
problems raised by artiï¬cial
intelligence and robotics.

of our experience. After all, what
could be more reliable than the
facts of direct perception?
French philosopher MerleauPonty was interested in looking
more closely at our experience of
the world, and in questioning our
everyday assumptions. This puts
him in the tradition known as
phenomenology, an approach to
philosophy pioneered by Edmund

Our everyday assumptions
prevent us from seeing these
puzzles and contradictions.

We must...

...put our everyday
assumptions to one side.

...relearn to look at
our experience.

In order to see the world,
we must break with our
familiar acceptance of it.

THE MODERN WORLD 275
See also: Aristotle 56â€“63 â–  Edmund Husserl 224â€“25 â–  Ludwig Wittgenstein
246â€“51 â–  Martin Heidegger 252â€“55 â–  Jean-Paul Sartre 268â€“71

Cognitive science

Man is in the world and
only in the world
does he know himself.
Maurice Merleau-Ponty

Husserl at the beginning of the
20th century. Husserl wanted to
explore ï¬rst-person experience in
a systematic way, while putting all
assumptions about it to one side.

The body-subject
Merleau-Ponty takes up Husserlâ€™s
approach, but with one important
difference. He is concerned that
Husserl ignores what is most
important about our experienceâ€”
the fact that it consists not just
of mental experience, but also of
bodily experience. In his most
important book, The Phenomenology
of Perception, Merleau-Ponty
explores this idea and comes to
the conclusion that the mind and
body are not separate entitiesâ€”
a thought that contradicts a long
philosophical tradition championed
by Descartes. For Merleau-Ponty,
we have to see that thought and
perception are embodied, and
that the world, consciousness, and
the body are all part of a single
system. And his alternative to the
disembodied mind proposed by
Descartes is what he calls the bodysubject. In other words, MerleauPonty rejects the dualistâ€™s view that
the world is made of two separate
entities, called mind and matter.

Because he was interested in seeing
the world anew, Merleau-Ponty took
an interest in cases of abnormal
experience. For example, he believed
that the phantom limb phenomenon
(in which an amuptee â€œfeelsâ€ his
missing limb) shows that the body
cannot simply be a machine. If it
were, the body would no longer
acknowledge the missing partâ€”but
it still exists for the subject because
the limb has always been bound
up with the subjectâ€™s will. In other
words, the body is never â€œjustâ€ a
bodyâ€”it is always a â€œlivedâ€ body.
Merleau-Pontyâ€™s focus on the role
of the body in experience, and his
insights into the nature of the mind
as fundamentally embodied, have
led to a revival of interest in his work
among cognitive scientists. Many
recent developments in cognitive
science seem to bear out his idea
that, once we break with our familiar
acceptance of the world, experience
is very strange indeed. â– 

Maurice MerleauPonty
Maurice Merleau-Ponty was
born in Rochefort-sur-Mer,
France, in 1908. He attended
the Ã‰cole Normale SupÃ©rieure
along with Jean-Paul Sartre
and Simone de Beauvoir, and
graduated in philosophy in
1930. He worked as a teacher
at various schools, until joining
the infantry during World
War II. His major work, The
Phenomenology of Perception,
was published in 1945, after
which he taught philosophy
at the University of Lyon.
Merleau-Pontyâ€™s interests
extended beyond philosophy
to include subjects such as
education and child psychology.
He was also a regular
contributor to the journal Les
Temps modernes. In 1952,
Merleau-Ponty became the
youngest-ever Chair of
Philosophy at the College de
France, and remained in the
post until his death in 1961,
at the age of only 53.
Key works

MRI scans of the brain provide
doctors with life-saving information.
However, in Merleau-Pontyâ€™s view, no
amount of physical information can give
us a complete account of experience.

1942 The Structure of
Behaviour
1945 The Phenomenology
of Perception
1964 The Visible and the
Invisible

276

MAN IS DEFINED AS
A HUMAN BEING AND
WOMAN
AS
A
FEMALE
SIMONE DE BEAUVOIR (1908â€“1986)
IN CONTEXT
BRANCH
Ethics
APPROACH
Feminism
BEFORE
c.350 BCE Aristotle says, â€œThe
female is a female by virtue of
a certain lack of qualities.â€
1792 Mary Wollstonecraft
publishes A Vindication of the
Rights of Woman, illustrating
the equality of the sexes.
1920s Martin Heidegger sets
out a â€œphilosophy of existence,â€
preï¬guring existentialism.

F

rench philosopher Simone
de Beauvoir writes in her
book The Second Sex that
throughout history, the standard
measure of what we take to be
humanâ€”both in philosophy and
in society at largeâ€”has been a
peculiarly male view. Some
philosophers, such as Aristotle,
have been explicit in equating full
humanity with maleness. Others
have not said as much, but have
nevertheless taken maleness as the
standard against which humanity

is to be judged. It is for this reason
that de Beauvoir says that the Self
(or â€œIâ€) of philosophical knowledge
is by default male, and his binary
pairâ€”the femaleâ€”is therefore
something else, which she calls
the Other. The Self is active and
knowing, whereas the Other is all
that the Self rejects: passivity,
voicelessness, and powerlessness.
De Beauvoir is also concerned
with the way that women are
judged to be equal only insofar as
they are like men. Even those who

Most of those who have
written about human
nature have been men.

1940s Jean-Paul Sartre says
â€œexistence precedes essence.â€
AFTER
1970s Luce Irigaray explores
the philosophical implications
of sexual difference.
From 1980 Julia Kristeva
breaks down the notions
of â€œmaleâ€ and â€œfemaleâ€ as
characterized by de Beauvoir.

Men have taken maleness
as the standard against
which they judge
human nature.

Men have deï¬ned
women by how they
differ from this standard.

Man is deï¬ned as
a human being and
woman as a female.

THE MODERN WORLD 277
See also: Hypatia of Alexandria 331 â–  Mary Wollstonecraft 175 â–  Jean-Paul Sartre
268â€“71 â–  Luce Irigaray 320 â–  HÃ©lÃ¨ne Cixous 322 â–  Martha Nussbaum 339

Representation of the
world is the work of men;
they describe it from
their own point of view.
Simone de Beauvoir

have written on behalf of the
equality of women, she says, have
done so by arguing that equality
means that women can be and do
the same as men. She claims that
this idea is mistaken, because it
ignores the fact that women and
men are different. De Beauvoirâ€™s
philosophical background was in
phenomenology, the study of how
things appear to our experience.
This view maintains that each of us
constructs the world from within the
frame of our own consciousness; we
constitute things and meanings
from the stream of our experiences.
Consequently de Beauvoir maintains
that the relationship that we have to
our own bodies, to others, and to the
world, as well as to philosophy itself,
is strongly inï¬‚uenced by whether
we are male or female.

construct. Since any construct is
open to change and interpretion, this
means that there are many ways of
â€œbeing a womanâ€; there is room for
existential choice. In the introduction
to The Second Sex de Beauvoir
notes societyâ€™s awareness of this
ï¬‚uidity: â€œWe are exhorted to be
women, remain women, become
women. It would appear, then, that
every female human being is not
necessarily a woman.â€ She later
states the position explicitly: â€œOne
is not born but becomes a woman.â€
De Beauvoir says that women
must free themselves both from the
idea that they must be like men, and
from the passivity that society has
induced in them. Living a truly
authentic existence carries more
risk than accepting a role handed
down by society, but it is the only
path to equality and freedom. â– 

Existential feminism
Simone De Beauvoir was also an
existentialist, believing that we are
born without purpose and must
carve out an authentic existence for
ourselves, choosing what to become.
In applying this idea to the notion
of â€œwomanâ€, she asks us to separate
the biological entity (the bodily
form which females are born into)
from femininity, which is a social

Simone de Beauvoir
The existentialist philosopher
Simone de Beauvoir was born
in Paris in 1908. She studied
philosophy at the Sorbonne
University, and it was here
that she met Jean-Paul Sartre,
with whom she began a
lifelong relationship. Both a
philosopher and an awardwinning novelist, she often
explored philosophical themes
within ï¬ctional works such as
She Came to Stay and The
Mandarins. Her most famous
work, The Second Sex, brought
an existentialist approach to
feminist ideas. Despite initially
being viliï¬ed by the political
right and left, and being placed
on the Vaticanâ€™s Index of
Forbidden Books, it became
one of the most important
feminist works of the 20th
century. De Beauvoir was a
proliï¬c writer, producing
travel books, memoirs, a
four-volume autobiography,
and political essays over the
course of her life. She died at
the age of 78, and was buried
in Montparnasse cemetery.
Key works

The many myths of woman as mother,
wife, virgin, symbol of nature, and so on
trap women, claimed de Beauvoir, into
impossible ideals, while denying their
individual selves and situations.

1944 Pyrrhus and Cineas
1947 The Ethics of Ambiguity
1949 The Second Sex
1954 The Mandarins

278

LANGUAGE IS
A SOCIAL ART

WILLARD VAN ORMAN QUINE (1908â€“2000)

IN CONTEXT
BRANCH
Philosophy of language

â€¦because we become used
to the ways in which they
are used by othersâ€¦

Words are
meaningful to usâ€¦

APPROACH
Analytic philosophy
BEFORE
c.400 BCE Platoâ€™s Cratylus
investigates the relationship
between words and things.
19th century SÃ¸ren
Kierkegaard stresses the
importance of the study of
language for philosophy.
1950s Ludwig Wittgenstein
writes that there is no such
thing as a private language.
AFTER
1980s Richard Rorty suggests
that knowledge is more like
â€œconversationâ€ than the
representation of reality.
1990s In Consciousness
Explained, Quineâ€™s former
student Daniel Dennett says
that both meaning and inner
experience can only be
understood as social acts.

â€¦not because there is
a link between words
and actual things.

The way that language
is used socially
makes it meaningful.

Language is
a social art.

S

ome philosophers assert
that language is about the
relationship between words
and things. Quine, however,
disagrees. Language is not about
the relationship between objects
and verbal signiï¬ers, but about
knowing what to say and when to
say it. It is, he says in his 1968 essay
Ontological Relativity, a social art.
Quine suggests the following
thought experiment. Imagine that
we come across some peopleâ€”
perhaps natives of another countryâ€”
who speak a language we do not
share. We are sitting with a group

of these people when a rabbit
appears, and one of the natives
says â€œgavagai.â€ We wonder if there
can be a connection between the
eventâ€”the appearance of the
rabbitâ€”and the fact that the native
says â€œgavagai.â€ As time goes on,
we note that every time a rabbit
appears, somebody says â€œgavagaiâ€,
so we conclude that â€œgavagaiâ€ can
be reliably translated as rabbit.
But, Quine insists, we are wrong.
â€œGavagaiâ€ could mean all manner
of things. It could mean â€œoh, look,
dinner!â€ for example, or it could
mean â€œbehold, a ï¬‚uffy creature!â€

THE MODERN WORLD 279
See also: Plato 50â€“55 â–  SÃ¸ren Kierkegaard 194â€“95
Roland Barthes 290â€“91 â–  Daniel Dennett 339

If we wanted to determine the
meaning of â€œgavagaiâ€, we could try
another method. We could point to
other ï¬‚uffy creatures (or other things
on the dinner menu) and see if our
utterance of â€œgavagaiâ€ met with
assent or dissent. But even if we
were to reach a position where, in
each and every occasion on which
â€œgavagaiâ€ was uttered, we ourselves
would utter the word â€œrabbitâ€, we
still could not be sure that this was
an appropriate translation. â€œGavagaiâ€
could mean â€œset of rabbit partsâ€ or
â€œwood-living rabbitâ€ or â€œrabbit or
hareâ€; it might even refer to a short
prayer that must be uttered
whenever a rabbit is seen.

â– 

Ferdinand de Saussure 223

No word has a ï¬xed meaning,
according to Quine. When the
word â€œrabbitâ€ is spoken, it may
mean any one of a number
of things, depending on the
context in which it is said.

Ludwig Wittgenstein 246â€“51

â– 

â– 

Pet
Dinner

Pest

Animal
spirit

Unsettled language
In attempting to establish the
precise meaning of this mysterious
â€œgavagaiâ€, therefore, we might think
that the solution would be to learn
the language of our informants
thoroughly, so that we could be
absolutely sure of the contexts in
which the word was spoken. But
this would only result in multiplying
the problem, because we could not

Willard Van
Orman Quine

be sure that the other words we
found ourselves using to explain
the meaning of â€œgavagaiâ€ were
themselves accurate translations.
Quine refers to this problem as
the â€œindeterminacy of translationâ€,
and it has unsettling implications.
It suggests that ultimately words
do not have meanings. The sense

of somebody uttering â€œgavagaiâ€
(or, for that matter, â€œrabbitâ€), and
of this utterance being meaningful
comes not from some mysterious
link between words and things,
but from the patterns of our
behavior, and the fact that we
have learned to participate in
language as a social art. â– 

Born in 1908 in Ohio, USA, Quine
studied at Harvard with Alfred
North Whitehead, a philosopher
of logic and mathematics. While
there he also met Bertrand
Russell, who was to become a
profound inï¬‚uence on his thought.
After completing his PhD in 1932,
Quine traveled throughout Europe,
meeting many of its most eminent
philosophers, including several of
the Vienna Circle.
Returning to teach at Harvard,
Quineâ€™s philosophical career was
brieï¬‚y interrupted during World
War II when he spent four years
decrypting messages for the US

Navy intelligence. A great
traveler, he was said to be
prouder of the fact that he had
visited 118 countries than of his
many awards and fellowships.
Quine became professor of
philosophy at Harvard in 1956,
and taught there until his death
in 2000, aged 92.
Key works
1952 Methods of Logic
1953 From a Logical Point
of View
1960 Word and Object
1990 The Pursuit of Truth

280

THE FUNDAMENTAL
SENSE OF FREEDOM
IS FREEDOM
FROM
CHAINS
ISAIAH BERLIN (1909â€“1997)
IN CONTEXT

Freedom is both
positive and negative.

BRANCH
Ethics
APPROACH
Analytic philosophy
BEFORE
1651 In his
the book
Leviathan,
Leviathan,
Thomas Hobbes considers
the relationship between
freedom and state power.
1844 SÃ¸ren Kierkegaard
argues that our freedom to
make moral decisions is a
chief cause of unhappiness.

Positive: we are free to
control our own destiny and
choose our own goals.

Negative: we are free from
external obstacles and
domination, or â€œchainsâ€.

But our individual goals
sometimes conï¬‚ict or lead
to the domination of others.

When our own positive
freedom leads to a decrease
in othersâ€™ negative freedom,
it becomes oppression.

1859 In his book On Liberty,
John Stuart Mill distinguishes
between freedom from coercion
and freedom to act.
1941 Psychoanalyst
Psychoanalyst Erich
Erich
Fromm explores
explores positive
positiveand
and
negative liberty
liberty in
in his
hisbook
book
The Fear of Freedom.
Freedom.
AFTER
Present day The development
of new surveillance technology
raises fresh questions about
the nature of freedom.

The fundamental sense
of freedom is freedom
from chains.

W

hat does it mean to be
free? This is the question
explored by the British
philosopher Isaiah Berlin in his
famous essay Two Concepts of
Liberty, written in 1958. Here he
makes a distinction between what
he calls â€œpositiveâ€ and â€œnegativeâ€

freedom. Although he is not the ï¬rst
to draw this distinction, he does so
with great originality, and uses it to
expose apparent inconsistencies in
our everyday notion of freedom.
For Berlin, â€œnegativeâ€ freedom
is what he calls our â€œfundamental
senseâ€ of freedom. This kind of

THE MODERN WORLD 281
See also: Jean-Jacques Rousseau 154â€“59 â–  John Stuart Mill 190â€“93 â– 
SÃ¸ren Kierkegaard 194â€“95 â–  Karl Marx 196â€“203 â–  Jean-Paul Sartre 268â€“71

For Berlin, the problem is that these
two forms of freedom are often in
conï¬‚ict. Think, for example, of
the freedom that comes from the
discipline of learning how to play
the tuba. As a beginner, I can
do little more than struggle with
my own inability to playâ€”but
eventually I can play with a kind
of liberated gusto. Or think of the
fact that people frequently exercise
their â€œpositiveâ€ freedom by voting
for a particular government,
knowing that their â€œnegativeâ€
freedom will be restricted when
that government comes to power.

The goals of life

Soviet propaganda often depicted
workers liberated from capitalism.
From a capitalist view, however, such
images showed a triumph of negative
freedom over positive freedom.

freedom is freedom from external
obstacles: I am free because I am
not chained to a rock, because I am
not in prison, and so on. This is
freedom from something else. But
Berlin points out that when we
talk about freedom, we usually
mean something more subtle than
this. Freedom is also a matter of
self-determination, of being a
person with hopes, and intentions,
and purposes that are oneâ€™s own.
This â€œpositiveâ€ freedom is about
being in control of oneâ€™s own
destiny. After all, I am not free just
because all the doors of my house
are unlocked. And this positive
freedom is not exclusively personal,
because self-determination can
also be desired at the level of the
group or of the state.

Berlin points to another problem.
Who is to say what a suitable goal
of â€œpositiveâ€ freedom should be?
Authoritarian or totalitarian
regimes often have an inï¬‚exible
view of the purpose of human life,
and so restrict â€œnegativeâ€ freedoms
to maximize their idea of human
happiness. Indeed, political
oppression frequently arises from an
abstract idea of what the good life
is, followed by state intervention
to make that idea a reality.
Berlinâ€™s response to this is
twofold. First, it is important to
recognize that the various freedoms
we may desire will always be in
conï¬‚ict, for there is no such thing
as â€œthe goal of lifeâ€â€”only the goals
of particular individuals. This
fact, he claims, is obscured by
philosophers who look for a universal
basis for morality, but confuse â€œright
actionâ€ with the purpose of life
itself. Second, we need to keep
alive the fundamental sense of
freedom as an absence of â€œbullying
and dominationâ€, so that we do not
ï¬nd our ideals turning into chains
for ourselves and for others. â– 

Isaiah Berlin
Isaiah Berlin was born in Riga,
Latvia, in 1909. He spent the
ï¬rst part of his life in Russia,
ï¬rstly under the Russian
empire, and then under the
rule of the new Communist
state. Due to rising antiSemitism, however, and
problems with the Soviet
rÃ©gime, his family emigrated
to Britain in 1921. Berlin was
an outstanding student at
Oxford University, where he
remained as a lecturer. He
was a philosopher with broad
interests, ranging from art and
literature to politics. His essay
Two Concepts of Liberty was
delivered in 1958 at Oxford
University, and it is often
considered one of the classics
of 20th-century political
theory. He is celebrated for
being one of the foremost
scholars of liberalism.
Key works
1953 The Hedgehog and the
Fox: An Essay on Tolstoyâ€™s
View of History
1958 Two Concepts of Liberty
1990 The Crooked Timber of
Humanity: Chapters in the
History of Ideas
2000 The Power of Ideas
2006 Political Ideas in the
Romantic Age

282

THINK LIKE
A MOUNTAIN

ARNE NAESS (1912â€“2009)

IN CONTEXT
BRANCH
Ethics
APPROACH
Environmental philosophy
BEFORE
C.1660 Benedictus Spinoza
develops his philosophy of
nature as an extension of God.
1949 Aldo Leopoldâ€™s The Sand
County Almanac is published.
1960 British scientist James
Lovelock ï¬rst proposes his
â€œGaia hypothesisâ€, exploring
the natural world as a single,
self-regulating system.
1962 American biologist
Rachel Carson publishes
Silent Spring, which becomes
an important inï¬‚uence on
Naessâ€™s thinking.
AFTER
1984 Zen master and teacher
Robert Aitken Roshi combines
deep ecology with the ideas
of the Japanese Buddhist
philosopher DoÌ„gen.

T

he injunction to think like
a mountain has become
closely associated with the
concept of â€œdeep ecologyâ€â€”a term
coined in 1973 by the Norwegian
philosopher and environmental
campaigner, Arne Naess. He uses
the term to stress his belief that we
must ï¬rst recognize we are part of
nature, and not separate from it, if
we are to avoid environmental
catastrophe. But the notion of
thinking like a mountain goes back
to 1949, when it was expressed by
American ecologist Aldo Leopold
in The Sand County Almanac.

Working as a forester in New
Mexico in the early part of the
20th century, Leopold shot a female
wolf on the mountainside. â€œWe
reached the old wolf in time to
watch a ï¬erce green ï¬re dying in
her eyes," he wrote. â€œI realized then,
and have known ever since, that
there was something new to me in
those eyesâ€”something known only
to her and to the mountain.â€ It was
from this experience that Leopold
came to the idea that we should
think like a mountain, recognizing
not just our needs or those of our
fellow humans, but those of the

Thinking like
a mountain isâ€¦

â€¦realizing that we are part
of the biosphere.

â€¦realizing our responsibilities
to all other living things.

We must think about the
long-term interests of the
environment as a whole.

THE MODERN WORLD 283
See also: Laozi 24â€“25

â– 

Benedictus Spinoza 126â€“29

The thinking for
the future has to be
loyal to nature.
Arne Naess

entire natural world. He implies
that often we miss the broader
implications of our actions, only
considering the immediate beneï¬ts
to ourselves. To â€œthink like a
mountainâ€ means identifying with
the wider environment and being
aware of its role in our lives.

Harmonizing with nature
Naess takes up Leopold's idea by
proposing his â€œdeep ecology.â€ He
states that we only protect our
environment by undergoing the
kind of transformation of which
Leopold writes. Naess urges us to

Arne Naess

â– 

Friedrich Schelling 335

move toward seeing ourselves as
part of the whole biosphere. Instead
of viewing the world with a kind of
detachment, we must ï¬nd our place
in nature, by acknowledging the
intrinsic value of all elements of the
world we inhabit.
Naess introduces the â€œecological
selfâ€, a sense of self that is rooted in
an awareness of our relationship to
a â€œlarger community of all living
beings." He claims that broadening
our identiï¬cation with the world to
include wolves, frogs, spiders, and
perhaps even mountains, leads to a
more joyful and meaningful life.
Naessâ€™s "deep ecology" has had
a powerful effect on environmental
philosophy and on the development
of environmental activism. For
those of us who live in cities, it may
seem hard or even impossible to
connect with an "ecological self."
Nevertheless, it may be possible.
As the Zen master Robert Aitken
Roshi wrote in 1984, â€œWhen one
thinks like a mountain, one thinks
also like the black bear, so that
honey dribbles down your fur as
you catch the bus to work.â€ â– 
Widely acknowledged as the
leading Norwegian philosopher
of the 20th century, Arne Naess
became the youngest-ever full
professor at the University of Oslo
at the age of 27. He was also a
noted mountaineer and led a
successful expedition to the
summit of Tirich Mir in northern
Pakistan in 1950.
It was only after Naess retired
from his teaching post in 1970
that he actively developed his
thinking about the natural world
and became involved in direct
action on environmental issues. In
1970, he chained himself to the

The natural world, for Naess, is not
something that we should strive to
control and manipulate for our own gain.
Living well involves living as an equal
with all the elements of our environment.

rocks by the Mardalsfossen
Waterfall in Norway to protest
against the building of a nearby
dam. Elected as chairperson of
Greenpeace Norway in 1988, he
was knighted in 2005.
Key works
1968 Scepticism
1974 Ecology, Society
and Lifestyle
1988 Thinking Like a Mountain
(with John Seed, Pat Fleming
and Joanna Macy)
2002 Lifeâ€™s Philosophy: Reason
and Feeling in a Deeper World

284

LIFE WILL BE LIVED
ALL THE BETTER IF
IT
HAS
NO
MEANING
ALBERT CAMUS (1913â€“1960)
IN CONTEXT
BRANCH
Epistemology

Because we have
consciousness, we feel that
life is meaningful.

But we know that
the universe as a whole
has no meaning.

To live well, we need to
overcome this contradiction.

Our lives are
a contradiction.

APPROACH
Existentialism
BEFORE
1849 SÃ¸ren Kierkegaard
explores the idea of the absurd
in his book, Fear and Trembling.
1864 Russian writer Fyodor
Dostoyevsky publishes Notes
from the Underground, which
has existentialist themes.
1901 Friedrich Nietzsche
writes in Will to Power that
â€œour existence (action,
suffering, willing, feeling)
has no meaning.â€
1927 Martin Heideggerâ€™s
Being and Time lays the
ground for the development
of existential philosophy.
AFTER
1971 Philosopher Thomas
Nagel argues that absurdity
arises out of a contradiction
within us.

We can do this by fully
embracing the
meaninglessness
of existence.

S

ome people believe that
philosophyâ€™s task is to
search for the meaning of
life. But the French philosopher and
novelist Albert Camus thought that
philosophy should recognize instead
that life is inherently meaningless.
While at ï¬rst this seems a depressing
view, Camus believes that only by
embracing this idea are we capable
of living as fully as possible.

Life will be lived
all the better if it
has no meaning.

Camusâ€™ idea appears in his essay
The Myth of Sisyphus. Sisyphus
was a Greek king who fell out of
favor with the gods, and so was
sentenced to a terrible fate in the
Underworld. His task was to roll
an enormous rock to the top of a
hill, only to watch it roll back to
the bottom. Sisyphus then had to
trudge down the hill to begin the
task again, repeating this for all

THE MODERN WORLD 285
See also: SÃ¸ren Kierkegaard 194â€“95

â– 

Friedrich Nietzsche 214â€“21

â– 

Martin Heidegger 252â€“55

Camus recognizes that much of what
we do certainly seems meaningful,
but what he is suggesting is quite
subtle. On the one hand, we are
conscious beings who cannot
help living our lives as if they are
meaningful. On the other hand,
these meanings do not reside out
there in the universe; they reside
only in our minds. The universe as
a whole has no meaning and no
purpose; it just is. But because,
unlike other living things, we have
consciousness, we are the kinds of
beings who ï¬nd meaning and
purpose everywhere.
Sisyphus was condemned eternally
to push a rock up a hill, but Camus
thought he might ï¬nd freedom even
in this grim situation if he accepted
the meaninglessness of his eternal task.

eternity. Camus was fascinated by
this myth, because it seemed to
him to encapsulate something of the
meaninglessness and absurdity of
our lives. He sees life as an endless
struggle to perform tasks that are
essentially meaningless.

Albert Camus

Recognizing the absurd
The absurd, for Camus, is the feeling
that we have when we recognize
that the meanings we give to life
do not exist beyond our own
consciousness. It is the result of
a contradiction between our own
sense of lifeâ€™s meaning, and our
knowledge that nevertheless the
universe as a whole is meaningless.
Camus explores what it might
mean to live in the light of this
contradiction. He claims that it is
Camus was born in Algeria in
1913. His father was killed a year
later in World War I, and Camus
was brought up by his mother in
extreme poverty. He studied
philosophy at the University of
Algiers, where he suffered the
ï¬rst attack of the tuberculosis
which was to recur throughout his
life. At the age of 25 he went to
live in France, where he became
involved in politics. He joined the
French Communist Party in 1935
but was expelled in 1937. During
World War II he worked for the
French Resistance, editing an
underground newspaper and

â– 

Jean-Paul Sartre 268â€“71

The struggle towards
the heights is enough
to ï¬ll a manâ€™s heart.
Albert Camus

only once we can accept the fact
that life is meaningless and absurd
that we are in a position to live fully.
In embracing the absurd, our lives
become a constant revolt against the
meaninglessness of the universe,
and we can live freely.
This idea was further developed
by the philosopher Thomas Nagel,
who said that the absurdity of life
lies in the nature of consciousness,
because however seriously we take
life, we always know that there is
some perspective from which this
seriousness can be questioned. â– 
writing many of his best-known
novels, including The Stranger.
He wrote many plays, novels,
and essays, and was awarded
the Nobel Prize for Literature in
1957. Camus died in a car crash
aged 46, having discarded
a train ticket to accept a lift
back to Paris with a friend.
Key works
1942 The Myth of Sisyphus
1942 The Stranger
1947 The Plague
1951 The Rebel
1956 The Fall

CONTEM
PHILOSO
1950â€“PRESENT

PORARY
PHY

288 INTRODUCTION

Frantz Fanon
publishes Black
Skin, White Masks.

The Vietnam War begins.
The USSR and China
support communist North
Vietnam, while the US
supports South Vietnam.

Thomas Kuhn
publishes The
Structure of Scientiï¬c
Revolutions.

Chinaâ€™s Great Proletarian
Cultural Revolution
â€œpurgesâ€ China of everything
Western, capitalist,
traditionalist, or religious.

1952

1955

1962

1966

T

1953

1961

1964

1967

Simone de Beauvoir
publishes her
groundbreaking
feminist work,
The Second Sex.

The Berlin Wall is
constructed, dividing
East and West Germany
until its fall in 1989.

The Civil Rights Act
1964 becomes law in
the US, prohibiting
discrimination by race.

Jacques Derrida,
the founder of
deconstruction,
publishes Writing
and Difference.

he closing decades of the
20th century were notable
for accelerating advances
in technology and the subsequent
improvement in communications
of all kinds. The increasing power
of the mass media, especially
television, since the end of World
War II had fuelled a rise in popular
culture with its associated
antiestablishment ideals, and this
in turn was prompting political and
social change. From the 1960s
onward, the old order was being
questioned in Europe and the US,
and dissent gathered momentum
in Eastern Europe.
By the 1980s, relations between
the East and West were thawing,
and the Cold War was coming to a
close; the fall of the Berlin Wall in
1989 offered hope for the new
decade. But the 1990s was a period

of ethnic and religious unrest,
culminating in the US declaring
a â€œWar on Terrorâ€ at the start of the
new millennium.

Elitist philosophies
Culture in the West went through
similarly signiï¬cant changes. The
gap between popular and â€œhighâ€
culture widened after the 1960s, as
the intellectual avant-garde often
decided to disregard public taste.
Philosophy followed a similarly
elitist path, particularly after the
death of Jean-Paul Sartre, whose
Marxist existentialismâ€”beloved of
1960s intellectualsâ€”now had less
of an audience.
Continental philosophy was
dominated in the 1970s and 80s
by structuralism, a movement
that grew from literature-based
French philosophy. Central to

this movement was the notion
of â€œdeconstructingâ€ texts and
revealing them to be inherently
unstable, with many contradictory
meanings. The theoryâ€™s principal
proponentsâ€”French theorists Louis
Althusser, Jacques Derrida, and
Michel Foucaultâ€”linked their
textual analyses with left-wing
politics, while the analyst Jacques
Lacan gave structuralism a
psychoanalytic perspective. Their
ideas were soon taken up by a
generation of writers and artists
working under the banner of
â€œpostmodernismâ€, which rejected
all possibility of a single, objective
truth, viewpoint, or narrative.
Structuralismâ€™s contribution to
philosophy was not enthusiastically
received by philosophers in the
English-speaking world, who
viewed the work at best with

CONTEMPORARY PHILOSOPHY 289

Apollo 11 becomes
the ï¬rst successful
manned mission
to the moon.

Jean-FranÃ§ois Lyotard
publishes The Postmodern
Condition: A Report
on Knowledge.

The World Wide Web
opens up to home and
personal use.

Al-Qaeda terrorist
attacks on New York
and Washington,
D.C., US, lead to the
â€œWar on Terror.â€

1969

1979

1992

2001

1971

1989

1994

2009

The non-government
environment agency
Greenpeace is formed
in Canada, evolving
from peace movements
and antinuclear groups.

Many European states
overthrow their communist
regimes, including Poland,
Hungary, East Germany,
Bulgaria, Romania,
and Czechoslovakia.

Henry David
Oruka publishes
Sage Philosophy.

Barack Obama
becomes the ï¬rst
African-American
president of the
United States.

suspicion, and largely with derision.
Within a philosophical tradition of
linguistic analysis, continental
structuralism seemed ultimately
simplisticâ€”although it was often
written in impenetrable prose that
belied its literary roots.
The squabbles of philosophers
did not inspire the popular culture
of the time. This may have been
because postmodernism was largely
incomprehensible to the general
public. Their most common
experience of it was postmodern art,
which was highly conceptual and
accompanied by knowing references
by an intellectual elite. It seemed to
deliberately exclude any possibility
of mass appreciation, and became
seen as an abstract philosophy only
enjoyed by professional academics
and artists, and out of touch with
the world most people lived in. The

public, as well as businesses and
governments, wanted more downto-earth guidance from philosophy.

A more practical approach
Though postmodern philosophy
may not have found favor with the
majority of the general public, some
philosophers of the period chose to
focus on more pressing social,
political, and ethical questions
that had more relevance to peopleâ€™s
everyday lives. Thinkers in
postcolonial Africa such as Frantz
Fanon began to examine race,
identity, and the problems that
were inherent in any struggle for
liberation. Later thinkers, such as
Henry David Oruka, would begin
to amass a new history of African
philosophy, questioning the rules
governing philosophy itself, and
what it should include.

Continuing in the tradition of
Simone de Beauvoirâ€™s existential
feminist philosophy, French
philosophers such as HÃ©lÃ¨ne
Cixous and Luce Irigaray added
a postmodern perspective to
feminism, but other thinkers on
both sides of the Atlantic left
postmodernism completely to
one side. Some, such as American
philosopher John Rawls and
Germanyâ€™s JÃ¼rgen Habermas,
returned to examining important
everyday concepts in depth, such
as justice and communication.
The more practical approach to
philosophy in the 21st century has
led to a renewed public interest in
the subject. There is no way of
predicting what direction it will
take, but philosophy is certain to
continue to provide the world with
thought-provoking ideas. â– 

290

LANGUAGE
IS A SKIN

ROLAND BARTHES (1915â€“1980)

IN CONTEXT
BRANCH
Philosophy of language

The loverâ€™s language
â€œtrembles with desire.â€

All philosophy about love
is addressed toward a
particular object of desire.

APPROACH
Semiotics
BEFORE
380 BCE Platoâ€™s Symposium
is the ï¬rst sustained
philosophical discussion
of love in the West.
4th century CE St Augustine
of Hippo writes extensively on
the nature of love.
1916 Ferdinand de Saussureâ€™s
Course in General Linguistics
establishes modern semiotics
and the study of language as
a series of signs.
1966 French psychoanalyst
Jacques Lacan looks at
the relationship between
Alcibiades, Socrates, and
Agathon in his Ã‰crits.
AFTER
1990s Julia Kristeva explores
the relationship between love,
semiotics, and psychoanalysis.

When I write or speak
about love, my language
â€œrubs againstâ€ the secret
object of my desire.

Language affects the other
like skin-on-skin contact.

T

he strangest, but most
popular, book written by
philosopher and literary
critic Roland Barthes is A Loverâ€™s
Discourse. As the French title,
Fragments dâ€™un discours amoureux,
suggests, this is a book told in
fragments and snapshots, somewhat
like the essay One-Way Street by
the German philosopher Walter
Benjamin. A Loverâ€™s Discourse is
not so much a book of philosophy
as it is a love story; but it is a love
story without any real story. There

Language
is a skin.

are no characters, and there
is nothing in the way of a plot.
There are only the reï¬‚ections
of a lover in what Barthes calls
â€œextreme solitude.â€
At the very beginning of the
book, Barthes makes clear that a
plot is not possible, because the
solitary thoughts of a lover come in
outbursts that are often contradictory
and lack any clear order. As a lover,
Barthes suggests, I might even ï¬nd
myself plotting against myself. The
lover is somebody who might be

CONTEMPORARY PHILOSOPHY 291
See also: Plato 50â€“55 â–  St Augustine of Hippo 72â€“73
Jacques Derrida 308â€“13 â–  Julia Kristeva 323

Every lover
is mad.
Roland Barthes

affectionately described as having
â€œlost the plot.â€ So instead of using
a plot, or narrative, Barthes arranges
his book like an extraordinary
encyclopaedia of contradictory and
disordered outbursts, any of which
might serve as the point the reader
might suddenly exclaim, â€œThatâ€™s so
true! I recognize that scene...â€

The language of love
It is in this context that Barthes
suggests â€œlanguage is a skin.â€
Language, at least the language
of the lover, is not something that

Roland Barthes

â– 

Ferdinand de Saussure 223

â– 

Walter Benjamin 258

â– 

simply talks about the world in a
neutral fashion, but it is something
that, as Barthes says, â€œtrembles
with desire.â€ Barthes writes of how
â€œI rub my language against the
other. It is as if I had words instead
of ï¬ngers, or ï¬ngers at the tip of my
words.â€ Even if I write cool, detached
philosophy about love, Barthes
claims, there is buried in my
philosophical coolness a secret
address to a particular person, an
object of my desire, even if this
somebody is â€œa phantom or a
creature still to come.â€
Barthes gives an example of this
secret address (although not, it
should be said, in the context of a
particularly detached philosophical
discussion) from Platoâ€™s dialogue,
The Symposium. This is an account
of a discussion on the subject of
love that takes place in the house
of the poet Agathon. A statesman
called Alcibiades turns up to the
discussion both late and drunk, and
sits down on a couch with Agathon
and the philosopher Socrates. The
drunken speech he gives is full
of praise for Socrates, but it is

Agathon that Alcibiades desires; it
is against Agathon, so to speak, that
Alcibiadesâ€™ language is rubbing.
But what of the language that
we use when talking of other
things? Is only the loverâ€™s language
a skin that trembles with hidden
desire, or is this also true of other
types of language? Barthes does
not tell us, leaving us to consider
the idea for ourselves. â– 

Barthes was born in Cherbourg,
France, in 1915. He attended the
University of Sorbonne in Paris
from 1935, graduating in 1939,
but by this time he had already
contracted the tuberculosis that
would afï¬‚ict him for the
remainder of his life. His illness
made it difï¬cult to acquire
teaching qualiï¬cations, but it
exempted him from military
service during World War II. After
the war, having ï¬nally qualiï¬ed
as a teacher, he taught in France,
Romania, and Egypt. He returned
to live in France full time in 1952,
and there started to write the

pieces that were collected
together and published under
the title Mythologies in 1957.
Barthesâ€™ reputation grew
steadily through the 1960s, in
France and internationally, and
he taught both at home and
abroad. He died at the age of 64,
when he was run over by
a laundry van after lunching
with President Mitterrand.

The loverâ€™s language is like a skin,
says Barthes, which inhabits the lover.
Its words are able to move the
belovedâ€”and only the belovedâ€”in
an almost physical or tactile way.

Key works
1957 Mythologies
1973 The Pleasure of the Text
1977 A Loverâ€™s Discourse

292

HOW WOULD WE
MANAGE WITHOUT
AMARYCULTURE?
MIDGLEY (1919â€“)
IN CONTEXT
BRANCH
Philosophy of science
APPROACH
Analytic philosophy
BEFORE
4th century BCE Aristotle
deï¬nes human beings as
â€œpolitical animalsâ€, suggesting
that not only are we natural
beings, but that the creation of
culture is a part of our nature.
1st century BCE Roman poet
Titus Lucretius Carus writes
On the Nature of the Universe,
exploring the natural roots of
human culture.
1859 Naturalist Charles
Darwin publishes On the
Origin of Species, arguing that
all life has evolved through a
process of natural selection.
AFTER
1980s onward Richard
Dawkins and Mary Midgley
debate the implications of
Darwinism for our view of
human nature.

I

n her book Beast and Man,
published in 1978, the British
philosopher Mary Midgley
assesses the impact the natural
sciences have on our understanding
of human nature. It is often claimed
that the ï¬ndings of the sciences,
particularly those of palaeontology
and evolutionary biology, undermine
our views of what it is to be human.
Midgley wants to address these
fears, and she does so by stressing
both the things that set us apart

We mistakenly cut
ourselves off from other
animals, trying not
to believe we have
an animal nature.
Mary Midgley

See also: Plato 50â€“55

â– 

Aristotle 56â€“63

from other animals and the things
that we share with the rest of
the animal kingdom.
One of the questions that she
tackles is that of the relationship
between nature and culture in
human life. Her concern is to
address the fact that many people
see nature and culture as somehow
opposed, as if culture is something
non-natural that is added onto
our animal natures.
Midgley disagrees with the
idea that culture is something of
a wholly different order to nature.
Instead, she wants to argue that
culture is a natural phenomenon.
In other words, we have evolved to
be the kinds of creatures who have
cultures. It could be said that we
spin culture as naturally as spiders
spin webs. If this is so, then we
can no more do without culture
than a spider can do without its
web: our need for culture is both
innate and natural. In this way,
Midgley hopes both to account
for human uniqueness, and also
to put us in the larger context of
our evolutionary past. â– 
â– 

Ludwig Wittgenstein 246â€“51

CONTEMPORARY PHILOSOPHY 293

NORMAL SCIENCE
DOES NOT AIM AT
NOVELTIES OF FACT
OR
THEORY
THOMAS KUHN (1922â€“1996)
IN CONTEXT
BRANCH
Philosophy of science
APPROACH
History of science
BEFORE
1543 Nicolaus Copernicus
publishes On the Revolutions
of the Heavenly Spheres,
leading to a paradigm shift in
our view of the solar system.
1934 In The Logic of Scientiï¬c
Discovery, Karl Popper deï¬nes
â€œfalsiï¬abilityâ€ as a criterion
for science.
AFTER
1975 Paul Feyerabend writes
Against Method, advocating
â€œepistemological anarchismâ€.
1976 In Proofs and Refutations,
Imre Lakatos brings together
Karl Popperâ€™s â€œfalsiï¬cationismâ€
and the work of Kuhn.
Today Rival interpretations
of quantum phenomena yield
rival paradigms of the
subatomic world.

A

merican physicist and
historian of science
Thomas Kuhn is best
known for his book The Structure
of Scientiï¬c Revolutions, published
in 1962. The book is both an
exploration of turning points in
the history of science and an
attempt to set out a theory of how
revolutions in science take place.

Paradigm shifts
Science, in Kuhnâ€™s view, alternates
between periods of â€œnormal scienceâ€
and periods of â€œcrisis.â€ Normal
science is the routine process by
which scientists working within
a theoretical frameworkâ€”or
â€œparadigmâ€â€”accumulate results
that do not call the theoretical
underpinnings of their framework
into question. Sometimes, of
course, anomalous, or unfamiliar,
results are encountered, but these
are usually considered to be errors
on the part of the scientists
concernedâ€”proof, according to
Kuhn, that normal science does
not aim at novelties. Over time,
however, anomalous results can

accumulate until a crisis point is
reached. Following the crisis, if a
new theory has been formulated,
there is a shift in the paradigm,
and the new theoretical framework
replaces the old. Eventually this
framework is taken for granted,
and normal science resumesâ€”until
further anomalies arise. An example
of such a shift was the shattering
of the classical view of space and
time following the conï¬rmation of
Einsteinâ€™s theories of relativity. â– 

Nicolaus Copernicusâ€™s claim that
Earth orbits the Sun was a revolution
in scientiï¬c thinking. It led to scientists
abandoning the belief that our planet
is at the center of the universe.

See also: Francis Bacon 110â€“11 â–  Rudolf Carnap 257
Paul Feyerabend 297 â–  Richard Rorty 314â€“19

â– 

Karl Popper 262â€“65

â– 

294

THE PRINCIPLES OF
JUSTICE ARE CHOSEN
BEHIND A VEIL
OF
IGNORANCE
JOHN RAWLS (1921â€“2002)
IN CONTEXT
BRANCH
Political philosophy
APPROACH
Social contract theory

Rules that are fair and just
must apply equally to all,
ignoring social status.

BEFORE
c.380 BCE Plato discusses the
nature of justice and the just
society in The Republic.

1762 Jean-Jacques Rousseau
writes The Social Contract.
His views are later adopted
by French revolutionaries.
AFTER
1974 Robert Nozick criticizes
Rawlsâ€™ â€œoriginal positionâ€ in
his inï¬‚uential book Anarchy,
State, and Utopia.
2001 Rawls defends his views
in his last book, Justice as
Fairness: A Restatement.

This requires
rules.

The principles of justice
should be chosen behind
a veil of ignorance.

1651 Thomas Hobbes sets out
a theory of social contract in
his book Leviathan.
1689 John Locke develops
Hobbesâ€™s theory in his Second
Treatise of Government.

To do this we need
to work together.

We all want to further
our own interests.

I

n his book A Theory of Justice,
ï¬rst published in 1971, political
philosopher John Rawls argues
for a re-evaluation of justice in
terms of what he calls â€œjustice as
fairness.â€ His approach falls into
the tradition known as social
contract theory, which sees the rule
of law as a form of contract that
individuals enter into because it
yields beneï¬ts that exceed what
they can attain individually. Rawlsâ€™
version of this theory involves a
thought experiment in which people
are made ignorant of their place in
society, or placed in what he calls
the â€œoriginal positionâ€ in which the

social contract is made. From this
Rawls establishes principles of
justice on which, he claims, all
rational beings should agree.

The original position
Imagine that a group of strangers is
marooned on a desert island, and
that, after giving up hope of being
rescued, they decide to start a new
society from scratch. Each of the
survivors wants to further their
own interests, but each also sees
that they can only do so by working
together in some wayâ€”in other
words, by forming a social contract.
The question is: how do they go

CONTEMPORARY PHILOSOPHY 295
See also: Plato 50â€“55 â–  Thomas Hobbes 112â€“15 â–  John Locke 130â€“33
Jean-Jacques Rousseau 154â€“59 â–  Noam Chomsky 304â€“05

about establishing the principles
of justice? What rules do they lay
down? If they are interested in a
truly rational and impartial justice,
then there are countless rules that
have to be discounted immediately.
For example, the rule â€œIf your name
is John, you must always eat lastâ€,
is neither rational nor impartial,
even if it may be to your advantage
if your name is â€œJohnâ€.
In such a position, says Rawls,
what we need to do is cast a â€œveil
of ignoranceâ€ over all the facts of
our lives, such as who we are, and
where we were born, and then ask
what kind of rules it would be best
for us to live by. Rawlsâ€™ point
is that the only rules that could
rationally be agreed on by all

â– 

parties are ones that genuinely
honor impartiality, and donâ€™t, for
example, take race, class, creed,
natural talent, or disability into
account. In other words, if I donâ€™t
know what my place in society will
be, rational self-interest compels
me to vote for a world in which
everyone is treated fairly.

Rationality versus charity
It is important to note that for
Rawls this is not a story about how
justice has actually arisen in the
world. Instead, he gives us a way
of testing our theories of justice
against an impartial benchmark. If
they fail to measure up, his point is
that it is our reason, and not simply
our charity, that has failed. â– 

The representation of
justice as a blindfolded
lady with a set of scales
expresses the idea that
no-one is above the law.

Lady Justice is
blind, and therefore
impartial.

John Rawls
John Rawls was born in 1921
in Maryland, USA. He studied
at Princeton University, then
joined the army and served in
the Paciï¬c during World War II.
After the war, in which he
saw the ruins of Hiroshima,
he resigned from the army
and returned to studying
philosophy, earning his PhD
from Princeton in 1950.
Rawls undertook further
study at Oxford University,
where he met philosopher
Isaiah Berlin, before returning
to the US to teach. After a
period at Cornell and MIT, he
moved to Harvard, where he
wrote A Theory of Justice.
While at Harvard, he also
taught up-and-coming
philosophers Thomas Nagel
and Martha Nussbaum.
In 1995 Rawls suffered
the ï¬rst of several strokes,
but continued working until
his death in 2002.
Key works

The scales of
justice represent
equality.
Punishment is
the same for all.

1971 A Theory of Justice
1993 Political Liberalism
1999 The Law of Peoples
2000 Lectures on the History
of Moral Philosophy
2001 Justice as Fairness:
A Restatement

296

ART IS A
FORM OF LIFE
RICHARD WOLLHEIM (1923â€“2003)

IN CONTEXT
BRANCH
Aesthetics
APPROACH
Analytic philosophy
BEFORE
c.380 BCE Platoâ€™s Republic
explores the relationship
between art forms and
political institutions.
1953 Ludwig Wittgensteinâ€™s
Philosophical Investigations
introduces and explores his
concept of â€œforms of life.â€
1964 Arthur Danto publishes
his philosophical essay
The Artworld, which analyzes
artistic endeavor from an
institutional viewpoint.
AFTER
1969 American philosopher
George Dickie develops further
the institutional theory of
artistic creativity in his
essay Deï¬ning Art.

T

he British philosopher of
art, Richard Wollheim,
believes that we should
resist the tendency to see art as
an abstract idea that needs to be
analyzed and explained. If we are
to fully understand art, he believes,
we must always deï¬ne it in relation
to its social context. By describing
art as a â€œform of lifeâ€, in Art and
its Objects (1968), he uses a term
coined by the Austrian-born
philosopher Ludwig Wittgenstein
to describe the nature of language.
For Wittgenstein, language is a
â€œform of lifeâ€, because the way we
use it is always a reï¬‚ection of our
individual experiences, habits, and
skills. He is attempting to resist the
tendency of philosophy to make
simplistic generalizations about
language and instead is pointing to
the many different roles language
plays in our lives.

Social setting
Wollheim is making the same point
as Wittgenstein, but in relation to
works of art. Artists, he states, are
conditioned by their contextâ€”their
See also: Plato 50â€“55

â– 

What we consider art may depend
on the context in which we view it.
Andy Warholâ€™s 32 Campbellâ€™s Soup
Cans creates ï¬ne art from images
usually associated with commerce.

beliefs, histories, emotional
dispositions, physical needs,
and communitiesâ€”and the world
that they interpret is a world of
constant change. For Wollheim, one
implication of this is that there can
be no general â€œartistic impulseâ€ or
instinct for the creation of art
that is totally independent of the
institutions in which it operates. â– 

Ludwig Wittgenstein 246â€“51

CONTEMPORARY PHILOSOPHY 297

ANYTHING GOES
PAUL FEYERABEND (1924â€“1994)

IN CONTEXT
BRANCH
Philosophy of science
APPROACH
Analytic philosophy
BEFORE
1934 In The Logic of Scientiï¬c
Discovery, Karl Popper deï¬nes
â€œfalsiï¬abilityâ€ as a criterion for
any scientiï¬c theory.
1962 Thomas Kuhn introduces
the idea of â€œparadigm shiftsâ€
in science in The Structure of
Scientiï¬c Revolutions.
1960s and early 1970s
Feyerabend develops his ideas
in discussion with his friend
and fellow philosopher of
science, Imre Lakatos.
AFTER
From 1980s Feyerabendâ€™s
ideas contribute to the theories
of the mind proposed by
American philosophers
Patricia and Paul Churchland.

B

orn in Austria, Feyerabend
became a student of Karl
Popper at the London
School of Economics, but he went
on to depart signiï¬cantly from
Popperâ€™s rational model of science.
During his time at the University of
California in the 1960s and 1970s,
Feyerabend became friendly with
the German-born philosopher
Thomas Kuhn, who argued that
scientiï¬c progress is not gradual,
but always jumps in â€œparadigm
shiftsâ€ or revolutions that lead to
whole new frameworks for scientiï¬c
thinking. Feyerabend goes even
further, suggesting that when this
occurs, all the scientiï¬c concepts
and terms are altered, so there is no
permanent framework of meaning.

Anarchy in science
Feyerabendâ€™s most famous book
Against Method: Outline of an
Anarchistic Theory of Knowledge,
was ï¬rst published in 1975. Here
he sets out his vision of what he
calls â€œepistemological anarchismâ€.
Epistemology is the branch of
philosophy that deals with
See also: Karl Popper 262â€“65

â– 

questions and theories about
knowledge, and Feyerabendâ€™s
â€œanarchismâ€ is rooted in the idea
that all of the methodologies used
in the sciences are limited in
scope. As a result, there is no such
thing as â€œscientiï¬c method.â€ If we
look at how science has developed
and progressed in practice, the only
â€œmethodâ€ that we can discern is
that â€œanything goes.â€ Science,
Feyerabend maintains, has never
progressed according to strict
rules, and if the philosophy of
science demands such rules, it
will limit scientiï¬c progress. â– 

Science and myth
overlap in many ways.
Paul Feyerabend

Thomas Kuhn 293

298

KNOWLEDGE
IS PRODUCED
TO BE SOLD

JEAN-FRANCOIS LYOTARD (1924â€“1998)

IN CONTEXT
BRANCH
Epistemology
APPROACH
Postmodernism
BEFORE
1870s The term â€œpostmodernâ€
is ï¬rst used in the context of
art criticism.

T

he idea that knowledge
is produced to be sold
appears in Jean-FranÃ§ois
Lyotardâ€™s book The Postmodern
Condition: A Report on Knowledge.
The book was originally written
for the Council of Universities in
Quebec, Canada, and the use of
the term â€œpostmodernâ€ in its title
is signiï¬cant. Although Lyotard
did not invent the term, which had

1939â€“45 Technological
advances in World War II lay
the ground for the computer
revolution of the 20th century.
1953 Ludwig Wittgenstein
writes in his Philosophical
Investigations about â€œlanguage
gamesâ€â€”an idea that Lyotard
uses to develop his idea of
meta-narratives.
AFTER
1984 American literary critic
Fredric Jameson writes
Postmodernism, or the Cultural
Logic of Late Capitalism.
From 1990s The World Wide
Web offers unprecedented
access to information.

been used by various art critics
since the 1870s, his book was
responsible for broadening its range
and increasing its popularity. His
use of the word in the title of this
book is often said to mark the
beginning of postmodern thought.
The term â€œpostmodernismâ€
has since been used in so many
different ways that it is now hard
to know exactly what it means,

Computer technology has
changed knowledge into
information that isâ€¦

â€¦stored in vast
databases.

â€¦owned by large
corporations.

This information is judged
by its commercial value,
not by its truth.

Knowledge is
produced to be sold.

CONTEMPORARY PHILOSOPHY 299
See also: Immanuel Kant 164â€“71 â–  Georg Hegel 178â€“85
Martin Heidegger 252â€“55 â–  Gilles Deleuze 338

â– 

Friedrich Nietzsche 214â€“21

â– 

Ludwig Wittgenstein 246â€“51

â– 

When knowledge becomes data it is
no longer the indeï¬nable matter of
minds, but a commodity that can be
transferred, stored, bought, or sold.

but Lyotardâ€™s deï¬nition is very
clear. Postmodernism, he writes,
is a matter of â€œincredulity towards
meta-narratives.â€ Meta-narratives
are overarching, single stories that
attempt to sum up the whole of
human history, or that attempt to
put all of our knowledge into a
single framework. Marxism (the
view that history can be seen as a
series of struggles between social
classes) is an example of a metanarrative. Another is the idea that
humanityâ€™s story is one of progress
toward deeper knowledge and
social justice, brought about by
greater scientiï¬c understanding.

Externalized knowledge
Our incredulity toward these metanarratives implies a new scepticism.
Lyotard suggests that this is due to
a shift in the way we have related
to knowledge since World War II,
and to the huge change in the

Jean-FranÃ§ois Lyotard

technologies we use to deal with
it. Computers have fundamentally
transformed our attitudes, as
knowledge has become information
that can be stored in databases,
moved to and fro, and bought and
sold. This is what Lyotard calls the
â€œmercantilizationâ€ of knowledge.
This has several implications.
The ï¬rst, Lyotard points out, is that
knowledge is becoming externalized.
It is no longer something that helps
toward the development of minds;
something that might be able to
transform us. Knowledge is also

becoming disconnected from
questions of truth. It is being judged
not in terms of how true it is, but in
terms of how well it serves certain
ends. When we cease to ask
questions about knowledge such
as â€œis it true?â€ and start asking
questions such as â€œhow can this
be sold?â€, knowledge becomes a
commodity. Lyotard is concerned
that once this happens, private
corporations may begin to seek to
control the ï¬‚ow of knowledge, and
decide who can access what types
of knowledge, and when. â– 

Jean-FranÃ§ois Lyotard was born
in Versailles, France in 1924. He
studied philosophy and literature
at the Sorbonne, Paris, becoming
friends with Gilles Deleuze. After
graduating, he taught philosophy
in schools for several years in
France and Algeria.
Lyotard became involved
in radical left-wing politics in the
1950s, and was a well-known
defender of the 1954â€“62 Algerian
revolution, but his philosophical
development ultimately led him
to become disillusioned with the
meta-narratives of Marxism. In
the 1970s he began working as

a university professor, teaching
philosophy ï¬rst at the Sorbonne
and then in many other
countries around the world,
including the US, Canada,
Brazil, and France. Lyotard
retired as Professor Emeritus at
the University of Paris VIII, and
died of leukemia in 1998.
Key works
1971 Discourse, Figure
1974 Libidinal Economy
1979 The Postmodern Condition:
A Report on Knowledge
1983 The Differend

300

FOR THE BLACK MAN,
THERE IS ONLY
ONE DESTINY
AND
IT
IS
WHITE
FRANTZ FANON (1925â€“1961)
IN CONTEXT
BRANCH
Political philosophy
APPROACH
Existentialism
BEFORE
4th century BCE Aristotle
argues in the Nicomachean
Ethics that slavery is a
natural state.
19th century Africa is
partitioned and colonized
by European countries.
1930s The French nÃ©gritude
movement calls for a uniï¬ed
black consciousness.
AFTER
1977 Steve Biko, an antiapartheid activist inspired
by Fanon, dies in police
custody in South Africa.
1978 Edward Said, inï¬‚uenced
by Fanonâ€™s work, writes
Orientalism, a post-colonial
study of Western perspectives
on the Middle East in the
19th century.

P

hilosopher and psychiatrist
Frantz Fanon ï¬rst published
his psychoanalytic study of
colonialism and racism, Black Skin,
White Masks, in 1952. In the book
Fanon attempts to explore the
psychological and social legacy
of colonialism among non-white
communities around the world.
In saying that â€œfor the black
man, there is only one destinyâ€,
and this destiny is white, Fanon is
saying at least two things. First,
he says that â€œthe black man wants
to be like the white manâ€; that is,
the aspirations of many colonized

White colonial cultures
equate â€œblacknessâ€
with inferiority.

The only escape is
to reject â€œblackness.â€

peoples have been formed by
the dominant colonial culture.
European colonial cultures tended
to equate â€œblacknessâ€ with
impurity, which shaped the selfview of those who were subject
to colonial rule, so that they came
to see the color of their skin as
a sign of inferiority.
The only way out of this
predicament seems to be an
aspiration to achieve a â€œwhite
existenceâ€; but this will always fail,
because the fact of having dark
skin will always mean that one will
fail to be accepted as white. For

Colonized people want
to escape from this
â€œinferiorâ€ position.

Colonized people start
to take on the assumed
superiority of colonial cultures.

For the black man
there is only one
destiny. And it is white.

CONTEMPORARY PHILOSOPHY 301
See also: Aristotle 56â€“63

â– 

Jean-Paul Sartre 268â€“71

There is a fact:
White men consider
themselves superior
to black men.
Frantz Fanon

Fanon, this aspiration to achieve
â€œa white existenceâ€ not only fails
to address racism and inequality,
but it also masks or even condones
these things by implying that there
is an â€œunarguable superiorityâ€ to
white existence.
At the same time, Fanon is
saying something more complex.
It might be thought that, given this
tendency to aspire to a kind of
â€œwhite existenceâ€, the solution would
be to argue for an independent view
of what it means to be black. Yet
this, too, is subject to all kinds of
problems. Elsewhere in his book,

Frantz Fanon
Frantz Fanon was born in 1925
in Martinique, a Caribbean
island that was at that time a
French colony. He left Martinique
to ï¬ght with the Free French
Forces in World War II, after
which he studied both medicine
and psychiatry in Lyon, France.
He also attended lectures on
literature and philosophy,
including those given by the
philosopher Merleau-Ponty. The
young Fanon had thought of
himself as French, and the
racism he encountered on ï¬rst

â– 

Maurice Merleau-Ponty 274â€“75

Fanon writes that â€œthe black manâ€™s
soul is a white manâ€™s artefact.â€ In
other words, the idea of what it
means to be black is the creation
of patterns of fundamentally racist
European thought.
Here Fanon is, in part, responding
to what was known in France as
the nÃ©gritude (or â€œblacknessâ€)
movement. This was a movement of
French and French-speaking black
writers from the 1930s who wanted
to reject the racism and colonialism
of mainstream French culture, and
argued for an independent, shared
black culture. But Fanon believes
that this idea of nÃ©gritude is one
that fails to truly address the
problems of racism that it seeks to
overcome, because the way that it
thinks about â€œblacknessâ€ simply
repeats the fantasies of mainstream
white culture.

â– 

Edward Said 321

address these injustices. â€œI ï¬nd
myself in the world and I recognize
that I have one right alone,â€ Fanon
writes at the end of his book; â€œthat
of demanding human behavior from
the other.â€ Fanonâ€™s thought has been
of widespread importance in anticolonial and anti-racist movements,
and has inï¬‚uenced social activists
such as anti-apartheid campaigner
Steve Biko and scholars such as
Edward Said. â– 

Human rights
In one sense, Fanon believes that
the solution can only come when
we move beyond racial thinking;
that if we remain trapped within
the idea of race we cannot ever
entering France surprised him. It
played a huge role in shaping his
philosophy, and one year after
qualifying as a psychiatrist in
1951, he published his book Black
Skin, White Masks.
In 1953 Fanon moved to
Algeria where he worked as a
hospital psychiatrist. After two
years of hearing his patientsâ€™
tales of the torture they endured
during the 1954â€“62 Algerian War
of Independence, he resigned his
government-funded post, moved
to Tunisia, and began working
for the Algerian independence
movement. In the late 1950s, he

The inferiority associated with being
black led many colonized people to
adopt the â€œmother countryâ€™s cultural
standardsâ€, says Fanon, and even to
aspire to a â€œwhite existence.â€

developed leukemia. During his
illness, he wrote his ï¬nal book,
The Wretched of the Earth,
arguing for a different world. It
was published in the year of his
death with a preface by JeanPaul Sartre, a friend who had
ï¬rst inï¬‚uenced Fanon, then
been inï¬‚uenced by him.
Key works
1952 Black Skin, White Masks
1959 A Dying Colonialism
1961 The Wretched of the Earth
1969 Toward the African
Revolution (collected short works)

302

MAN IS AN
INVENTION OF
RECENT DATE
MICHEL FOUCAULT (1926â€“1984)

IN CONTEXT
BRANCH
Epistemology
APPROACH
Discursive archaeology
BEFORE
Late 18th century Immanuel
Kant lays the foundation for the
19th-century model of â€œman.â€
1859 Charles Darwinâ€™s On
the Origin of Species causes
a revolution in how we
understand ourselves.
1883 Friedrich Nietzsche,
in Thus Spoke Zarathustra,
announces that man is
something to be surpassed.
AFTER
1991 Daniel Dennettâ€™s
Consciousness Explained
calls into question many of
our most cherished notions
about consciousness.
1991 American philosopher
Donna Harawayâ€™s A Cyborg
Manifesto attempts to imagine
a post-human future.

We treat the idea
of â€œmanâ€ or humankind
as if it is a natural
and eternal idea.

Man is an
invention of
recent date.

T

he idea that man is an
invention of recent date
appears in The Order of
Things: An Archaeology of the
Human Sciences by French
philosopher Michel Foucault. To
understand what Foucault means
by this, we need to know what he
means by archaeology, and why he
thinks that we should apply it
to the history of thought.
Foucault is interested in how
our discourseâ€”the way in which
we talk and think about thingsâ€”
is formed by a set of largely
unconscious rules that arise out of
the historical conditions in which

But an archaeology of our
thinking shows that the idea
of â€œmanâ€ arose as an object of
study at the beginning
of the 19th century.

we ï¬nd ourselves. What we take to
be the â€œcommon senseâ€ background
to how we think and talk about the
world is in fact shaped by these rules
and these conditions. However, the
rules and conditions change over
time, and consequently so do our
discourses. For this reason, an
â€œarchaeologyâ€ is needed to unearth
both the limits and the conditions
of how people thought and talked
about the world in previous ages.
We cannot take concepts that we
use in our present context (for
example, the concept of â€œhuman
natureâ€) and assume that they are
somehow eternal, and that all we

CONTEMPORARY PHILOSOPHY 303
See also: Immanuel Kant 164â€“71 â–  Friedrich Nietzsche 214â€“21
Maurice Merleau-Ponty 274â€“75 â–  Daniel Dennett 339

â– 

Martin Heidegger 252â€“55

The 19th century saw a revolution in
anatomy, as shown in this illustration
from a medical text book. Foucault
believes that our modern concept of
man dates from this period.

need is a â€œhistory of ideasâ€ to trace
their genealogy. For Foucault, it is
simply wrong to assume that our
current ideas can be usefully
applied to any previous point in
history. The ways in which we use
the words â€œmanâ€, â€œmankindâ€, and
â€œhuman natureâ€, Foucault believes,
are examples of this.
The roots of this idea lie ï¬rmly
in the philosophy of Immanuel Kant,
who turned philosophy on its head

Michel Foucault

by abandoning the old question
â€œWhy is the world the way it is?â€
and asking â€œWhy do we see the
world the way we do?â€ We take our
idea of what it is to be human as
fundamental and unchanging, but
it is in fact only a recent invention.
Foucault locates the beginning of
our particular idea of â€œmanâ€ at the
beginning of the 19th century,
around the time of the birth of the
natural sciences. This idea of â€œmanâ€
is, Foucault considers, paradoxical:
we see ourselves both as objects in
the world, and so as objects of study,
and as subjects who experience and
study the worldâ€”strange creatures
that look in two directions at once.

â– 

endâ€”one that may soon be erased
â€œlike a face drawn in the sand at
the edge of the sea.â€
Is Foucault right? In a time of
rapid advances in computing and
human-machine interfaces, and
when philosophers informed by
cognitive science, such as Daniel
Dennett and Dan Wegner, are
questioning the very nature of
subjectivity, it is hard not to feel
that, even if the face in the sand is
not about to be erased, the tide is
lapping alarmingly at its edges. â– 

Man is neither the oldest nor
the most constant problem
that has been posed for
human knowledge.
Michel Foucault

The human self-image
Foucault suggests that not only is
this idea of â€œmanâ€ an invention of
recent date, it is also an invention
that may be close to coming to its
Foucault was born in Poitiers,
France, in 1926 to a family of
doctors. After World War II, he
entered the Ã‰cole Normale
SupÃ©rieure, where he studied
philosophy under Maurice
Merleau-Ponty. In 1954 he spent
time in Uppsala, Sweden, and
then lived for a time both in
Poland and Germany, only
returning to France in 1960.
He received a PhD in 1961 for
his study A History of Madness,
which argued that the distinction
between madness and sanity is
not real, but a social construct.
After the month-long student

strikes in Paris of 1968, he
became involved in political
activism, and continued to
work both as a lecturer and an
activist for the rest of his life.
Key works
1961 A History of Madness
1963 The Birth of the Clinic:
An Archaeology of Medical
Perception
1966 The Order of Things:
An Archaeology of the
Human Sciences
1975 Discipline and Punish:
The Birth of the Prison

304

IF WE CHOOSE, WE CAN
LIVE IN A WORLD OF
COMFORTING
ILLUSION
NOAM CHOMSKY (1928â€“)
IN CONTEXT
BRANCH
Ethics
APPROACH
Universalism
BEFORE
c.380 BCE In The Republic,
Plato claims that many of
us live in a world of illusion.
1739 David Hume publishes
A Treatise of Human Nature.
Though an empiricist, he
claims that there must be
some ï¬xed principles from
which morality derives.
1785 Immanuel Kant, in
his Groundwork of the
Metaphysic of Morals,
argues that morality should
be based on universality.

A

lthough originally famous
for his work in linguistics,
Noam Chomsky is today
best known for his analyses of
political power. Since the publication
of his ï¬rst political book, American
Power and the New Mandarins, in
1969, he has claimed that there is
often a mismatch between the way
that states exert power and the
rhetorical claims that they make. He
maintains that rhetorical claims by

If we assume that
our own government is
naturally more ethical
than other governments...

... we are choosing
to live in a world of
c mforting illusion..
co

To break with this
illusion we need toâ€¦

Early 20th century John
Dewey argues that politics
is the shadow cast on society
by big business.
1971 John Rawls revives
Kantâ€™s notion of universality
in his A Theory of Justice.

governments are not by themselves
sufï¬cient for us to reach the truth
about political power. Governments
may speak the language of â€œfactsâ€
as a way of justifying their actions,
but unless their claims are
supported by evidence, then they
are only illusions, and the actions
to which they lead lack justiï¬cation.
If we are to understand more clearly
how states operate, it is necessary
to move beyond the battle between

... look at the evidence
for what our government
actually does.

... apply the same ethical
principles that we apply
to other governments
to our own.

CONTEMPORARY PHILOSOPHY 305
See also: Plato 50â€“55 â–  David Hume 148â€“53
John Dewey 228â€“31 â–  John Rawls 294â€“95

States are not moral agents;
people are.
Noam Chomsky

rival forms of rhetoric, and instead
to look at history, at institutional
structures, at ofï¬cial policy
documents, and so forth.

Ethics and universality
Chomskyâ€™s ethical analyses are
based on what he calls the
â€œprinciple of universality.â€ At root,
this principle is relatively simple.
It says that at the very least we
should apply to ourselves the same
standards that we apply to others.
This is a principle that Chomsky
claims has always been central to
any responsible system of ethics.
The central psychological insight
here is that we are fond of using
ethical language as a way of
protesting about others, but that we
are less inclined to pass judgment
on ourselves. Nevertheless, if we
claim to uphold any set of ethical or
moral standards, and if we wish to
be consistent, then we must apply
to others the standards we apply to
ourselves. In terms of government,
this means that we must analyze
our political actions rigorously,
instead of allowing ourselves to
be blinded by rhetoric.
This is both a moral and an
intellectual imperative. For Chomsky,
these are closely related. He points

â– 

Immanuel Kant 164â€“71

â– 

out that if anyone making a moral
claim is also violating universality,
then their claim cannot be taken
seriously and should be rejected.
If we are to cut through the
rhetoric and examine political
morality in a rigorous fashion,
it seems that universality is a
necessary starting point. Some of
Chomskyâ€™s speciï¬c claims about
the nature of global power have
caused considerable controversy,
but this does not invalidate his
central insight. For if we wish
to call his speciï¬c claims into
question, then we should do so in
the light of universality and of all
the available evidence. If his claims
turn out to be false, then they should
be rejected or modiï¬ed; but if they
turn out to be true, then they
should be acted upon. â– 

Noam Chomsky
Chomsky was born in 1928
in Pennsylvania, USA, and
was raised in a multilingual
Jewish household. He studied
mathematics, philosophy, and
linguistics at the University
of Pennsylvania, where he
wrote a groundbreaking thesis
on philosophical linguistics.
In 1957, his book Syntactic
Structures secured his
reputation as one of the
leading ï¬gures in linguistics,
and revolutionized the ï¬eld.
Although continuing to
teach and publish in linguistics,
Chomsky became increasingly
involved in politics. He was a
prominent opponent of the
Vietnam War, which prompted
him to publish his critique of
US intellectual culture, The
Responsibility of Intellectuals,
in 1967. Today, he continues to
write and lecture on linguistics,
philosophy, politics, and
international affairs.
Key works

Uncle Sam, the personiï¬cation of the
United States, is one of countless props
used by governments to foster public
support. Chomsky warns that such
images can distract us from the truth.

1967 The Responsibility of
Intellectuals
1969 American Power and the
New Mandarins
2001 9-11
2006 Failed States: The Abuse
of Power and the Assault
on Democracy

306

SOCIETY IS
DEPENDENT UPON
A CRITICISM OF ITS
OWN
TRADITIONS
JURGEN HABERMAS (1929â€“ )
IN CONTEXT
BRANCH
Political philosophy
APPROACH
Social theory
BEFORE
1789 The French Revolution
begins, marking the end of
a â€œrepresentationalâ€ power
structure in France.
1791 Jeremy Bentham
writes Of Publicity, an early
exploration of the idea of
the â€œpublic.â€

A

ccording to the German
philosopher JÃ¼rgen
Habermas, modern society
depends not only on technological
advances, but also upon our ability
to criticize and reason collectively
about our own traditions. Reason,
says Habermas, lies at the heart
of our everyday communications.
Somebody says or does something,
and we say, â€œWhy did you do that?â€
or â€œWhy did you say that?â€ We
continually ask for justiï¬cations,
which is why Habermas talks
about â€œcommunicativeâ€ reason.

1842 Karl Marx writes his
essay On Freedom of the Press.
AFTER
1986 Edward Said criticizes
Habermas and the Frankfurt
School for their Eurocentric
views and their silence on
racist theory and imperialism.
1999 Canadian author Naomi
Kleinâ€™s No Logo explores the
fate of the public sphere in an
era dominated by advertising
and the mass media.

Coffee houses became a focus of
social and political life in the major cities
of 18th-century Europe. Noted as places
where â€œthe dissaffected metâ€, attempts
were frequently made to close them.

Reason, for him, is not about
discovering abstract truths, but
about the need we have to justify
ourselves to others.

Creating a public sphere
In the 1960s and 1970s, Habermas
concluded that there was a link
between communicative reason
and what he calls the â€œpublic
sphere.â€ Up until the 18th century,
he states, European culture was
largely â€œrepresentationalâ€, meaning
that the ruling classes sought to
â€œrepresentâ€ themselves to their
subjects with displays of power
that required no justiï¬cation, such
as impressive pageants or grand
architectural projects. But in the
18th century, a variety of public
spaces emerged that were outside
state control, including literary
salons and coffee houses. These
were places where individuals could
gather to engage in conversation or
reasoned debate. This growth of
the public sphere led to increased
opportunities to question the
authority of representational state
culture. The public sphere became
a â€œthird spaceâ€, a buffer between
the private space of our immediate
friends and family, and the space
occupied by state control.

CONTEMPORARY PHILOSOPHY 307
See also: Jeremy Bentham 174 â–  Karl Marx 196â€“203 â–  Theodor Adorno 266â€“67
Niklas Luhmann 339 â–  Noam Chomsky 304â€“05 â–  Edward Said 321

By establishing a public sphere, we
also open up more opportunities for
recognizing that we have interests
in common with other private
individualsâ€”interests that the
state may fail to serve. This can
lead to questioning the actions of
the state. Habermas believes that
the growth of the public sphere
helped to trigger the French
Revolution in 1789.
The expansion of the public
sphere, from the 18th century
onward, has led to a growth of
democratically elected political
institutions, independent courts,
and bills of rights. But Habermas
believes that many of these brakes
on the arbitrary use of power are
now under threat. Newspapers, for
example, can offer opportunities
for reasoned dialogue between
private individuals, but if the press
is controlled by large corporations,
such opportunities may diminish.
Informed debate on issues of
substance is replaced with
celebrity gossip, and we are
transformed from critical, rational
agents into mindless consumers. â– 

JÃ¼rgen Habermas

â– 

A societyâ€™s traditions are
not necessarily in the best
interests of individuals.

Edgar Morin 338

â– 

Individuals need to be
able to question and
change these traditions.

They can do this by reasoning together
in the public sphere, whichâ€¦

â€¦builds
consensus.

â€¦brings about
change.

â€¦strengthens
society.

Society is dependent upon a
criticism of its own traditions.

JÃ¼rgen Habermas grew up in
Germany under the Nazi regime.
His realization that â€œwe had been
living in a criminal systemâ€,
following the Nuremburg trials
(1945â€“46), was to have a lasting
effect on his philosophy.
On completing his doctorate in
1954, he studied with members of
the Frankfurt School, including
Max Horkheimer and Theodor
Adorno. During the 1960s and
1970s, he lectured at universities
in Bonn and Gottingen. In 1982, he
became Professor of Philosophy at
the University at Frankfurt, where
he taught until his retirement in

1993. More recently, Habermas
has himself taken an active role
in the public sphere, entering
into debates on Holocaust denial
and global terrorism.
Key works
1962 The Structural
Transformation of the
Public Sphere
1981 The Theory of
Communicative Action
1985 The Philosophical
Discourse of Modernity
2005 Between Naturalism
and Religion

THERE IS
NOTHING
OUTSIDE OF THE TEXT
JACQUES DERRIDA (1930â€“2004)

310 JACQUES DERRIDA
IN CONTEXT
BRANCH
Epistemology
APPROACH
Deconstruction
BEFORE
4th century BCE Platoâ€™s Meno
explores the idea of â€œaporia.â€
Early 20th century Charles
Sanders Peirce and Ferdinand
de Saussure begin the study of
signs and symbols (semiotics),
which would become a key
inï¬‚uence on Of Grammatology.
1961 Emmanuel Levinas
publishes Totality and Inï¬nity,
which Derrida would respond
to in Writing and Difference.
Levinas becomes a growing
inï¬‚uence in Derridaâ€™s later
explorations of ethics.
AFTER
1992 English philosopher
Simon Critchleyâ€™s Ethics of
Deconstruction explores
aspects of Derridaâ€™s work.

J

acques Derrida remains one
of the most controversial
20th-century philosophers.
His name is associated, ï¬rst and
foremost, with â€œdeconstructionâ€,
a complex and nuanced approach
to how we read and understand the
nature of written texts. If we are to
understand what Derrida means
when he says in his famous book
Of Grammatology that there is
nothing outside of the text (the
original French is â€œil nâ€™y a pas de
hors-texteâ€, also translated as
â€œthere is no outside-textâ€), we need
to take a closer look at Derridaâ€™s
deconstructive approach in general.

We are all mediators,
translators.
Jacques Derrida

Often when we pick up a book,
whether a philosophy book or a
novel, we imagine that what we
have in our hands is something
that we can understand or interpret
as a relatively self-contained whole.
When it comes to philosophical
texts, we might be expected to
imagine that these are especially
systematic and logical. Imagine
that you go into a bookshop and
pick up a copy of Of Grammatology.
You would think that, if you were to
read the book, by the end of it you
would have a reasonable grasp of
what â€œgrammatologyâ€ itself might
be, what Derridaâ€™s main ideas were
on the subject, and what this said
about the world. But, for Derrida,
texts do not work in this way.

Derrida aims to broaden our
understanding of what texts are
and what they do, and to show the
complexity that lies behind even
the most apparently simple works.
Deconstruction is a way of reading
texts to bring these hidden
paradoxes and contradictions out
into the open. This is not, however,
just a matter of how we read
philosophy and literature; there are
much broader implications to
Derridaâ€™s approach that bring into
question the relationship between
language, thought, and even ethics.
At this point, it would help to
introduce an important technical
term from Derridaâ€™s vocabulary:
â€œdiffÃ©rance.â€ This may look like a
typographical errorâ€”and indeed,
when the term diffÃ©rance ï¬rst
entered the French dictionary, the
story goes that even Derridaâ€™s
mother sternly said to him, â€œBut
Jacques, that is not how you spell
it!â€ But in fact diffÃ©rance is a word
that Derrida coined himself to point
to a curious aspect of language.
â€œDiffÃ©ranceâ€ (with an â€œaâ€) is a
play both on the French â€œdiffÃ©renceâ€
(with an â€œeâ€), meaning â€œto differâ€,
and the French â€œdefÃ©rrerâ€ meaning
â€œto defer.â€ To understand how this

Aporia and diffÃ©rance
Even the most straightforward
texts (and Of Grammatology is not
one such text) are riddled with
what Derrida calls â€œaporiasâ€. The
word â€œaporiaâ€ comes from the
Ancient Greek, where it means
something like â€œcontradictionâ€,
â€œpuzzleâ€, or â€œimpasse.â€ For Derrida,
all written texts have such gaps,
holes, and contradictions and his
method of deconstruction is a way
of reading texts while looking out
for these puzzles and impasses. In
exploring these contradictions as
they appear in different texts,

A typesetter can check plates of type
closely before they are printed, but the
ideas they express are full of â€œaporiasâ€,
or contradictions, says Derrida, which
no amount of analysis can eliminate.

CONTEMPORARY PHILOSOPHY 311
See also: Plato 50â€“55 â–  Charles Sanders Peirce 205 â–  Ferdinand de Saussure 223
Louis Althusser 338 â–  RenÃ© Girard 338 â–  Michel Foucault 302â€“03

â– 

Emmanuel Levinas 273

â– 

I try to explain what Derrida
means when he says that â€œthere
is nothing outside of the text.â€

But I can never completely
explain the idea becauseâ€¦

The meaning of what we write is,
for Derrida, changed by what we write
next. Even the deceptively simple act
of writing a letter can lead to a deferral
of meaning in the text itself.

word works, it would be useful to
consider how this deferring and
differing might actually take place
in practice. Let us start with
deferring. Imagine that I say â€œThe
catâ€¦â€, then I add, â€œthat my friend
sawâ€¦â€. After a pause, I say, â€œin the
garden was black and whiteâ€¦â€,
and so on. The precise meaning of
the word â€œcatâ€ as I am using it is
continually deferred, or put off, as
more information is given. If I had
been cut off after saying â€œThe catâ€¦â€
and had not mentioned my friend
or the garden, the meaning of â€œcatâ€
would have been different. The
more I add to what I say, in other
words, the more the meaning of
what I have already said is revised.
Meaning is deferred in language.
But there is something else
going on as well. The meaning of
â€œcatâ€, Derrida believes, cannot be
considered as something that rests
in the relationship between my
words and actual things in the
world. The word takes its â¯â¯

â€¦the meaning of
what I say depends on
what I (or others) go
on to say later.

â€¦the meaning of the
words I use depends
on their relationship
to the words I am
not using.

So meaning is
always incomplete.

So I say more to
clarify things.

In this way, my explanation
of Derridaâ€™s idea can
grow until it is inï¬nitely
large, and I realize...

â€¦there is
nothing outside
of the text.

312 JACQUES DERRIDA

We think only in signs.
Jacques Derrida

meaning from its position in a whole
system of language. So when I say
â€œcatâ€, this is meaningful not because
of some mysterious link between
the word and an actual cat, but
because this term differs from, for
example, â€œdogâ€ or â€œlionâ€ or â€œzebra.â€
Taken together, these two
ideas of deferring and differing
say something quite strange about
language in general. On the one
hand, the meaning of anything we
say is ultimately always deferred,
because it depends on what else
we say; and the meaning of that,
in turn, depends on what else we
say, and so on. And on the other
hand, the meaning of any particular
term we use depends on all the
things that we donâ€™t mean. So
meaning is not self-contained
within the text itself.

The written word
For Derrida, diffÃ©rance is an aspect
of language that we become aware
of thanks to writing. Since ancient
Greek times, philosophers have
been suspicious of written
language. In Platoâ€™s dialogue, the
Phaedrus, Socrates tells a legend
about the invention of writing, and
says that writing provides only â€œthe
appearance of wisdomâ€ and not its
reality. Writing, when philosophers
have thought about it at all, has
tended to be seen simply as a pale
reï¬‚ection of the spoken word; the

latter has been taken as the primary
means of communication. Derrida
wants to reverse this; according to
him, the written word shows us
something about language that the
spoken word does not.
The traditional emphasis on
speech as a means of transmitting
philosophical ideas has fooled us
all, Derrida believes, into thinking
that we have immediate access to
meaning. We think that meaning is
about â€œpresenceâ€â€”when we speak
with another person, we imagine
that they make their thoughts
â€œpresentâ€ for us, and that we are
doing the same for them. If there is
any confusion, we ask the other
person to clarify. And if there are
any puzzles, or aporias, we either
ask for clariï¬cation, or these simply
slide past us without our noticing.
This leads us to think that meaning
in general is about presenceâ€”to
think, for example, that the real
meaning of â€œcatâ€ can be found in
the presence of a cat on my lap.
But when we deal with a
written text, we are freed from this
naÃ¯ve belief in presence. Without
the author there to make their
excuses and explain for us, we start
to notice the complexities and the

puzzles and the impasses. All of a
sudden, language begins to look
a little more complicated.

Questioning meaning
When Derrida says that there is
nothing outside of the text, he does
not mean that all that matters is the
world of books, that somehow the
world â€œof ï¬‚esh and boneâ€ does not
matter. Nor is he trying to play
down the importance of any social
concerns that might lie behind the
text. So what exactly is he saying?
First, Derrida is suggesting that
if we take seriously the idea that
meaning is a matter of diffÃ©rance,
of differing and of deferring, then
if we want to engage with the
question of how we ought think
about the world, we must always
keep alive to the fact that meaning
is never as straightforward as we
think it is, and that this meaning
is always open to being examined
by deconstruction.
Second, Derrida is suggesting
that in our thinking, our writing,
and our speaking, we are always
implicated in all manner of political,
historical, and ethical questions
that we may not even recognize or
acknowledge. For this reason, some

Derridaâ€™s own thesis that there is nothing
outside of the text is open to be analyzed
using his own deconstructive methods.
Even the idea as explained in this book
is subject to diffÃ©rance.

CONTEMPORARY PHILOSOPHY 313

Jacques Derrida

Derrida registered his opposition
to the Vietnam War in a lecture given
in the US in 1968. His involvement in
numerous political issues and debates
informed much of his later work.

philosophers have suggested that
deconstruction is essentially an
ethical practice. In reading a text
deconstructively, we call into
question the claims that it is
making, and we open up difï¬cult
ethical issues that may have
remained hidden. Certainly in
his later life, Derrida turned his
attention to some of the very real
ethical puzzles and contradictions
that are raised by ideas such as
â€œhospitalityâ€ and â€œforgiveness.â€

latterâ€™s response to this, perhaps,
might be to say that the idea of
having a thesis is itself based on
the idea of â€œpresenceâ€ that he is
attempting to call into question.
This may seem like dodging the
issue; but if we take Derridaâ€™s idea
seriously, then we have to admit
that the idea that there is nothing
outside of the text is itself not
outside of the text. To take this
idea seriously, then, is to treat it
sceptically, to deconstruct it, and
to explore the puzzles, impasses,
and contradictions thatâ€”according
to Derrida himselfâ€”lurk within it. â– 

Critics of Derrida
Given that Derridaâ€™s idea is based
on the notion that meaning can
never be completely present in the
text, it is perhaps not surprising
that Derridaâ€™s work can often be
difï¬cult. Michel Foucault, one of
Derridaâ€™s contemporaries, attacked
Derridaâ€™s thinking for being wilfully
obscure; he protested that often it
was impossible to say exactly what
Derridaâ€™s thesis actually was. The

I never give in
to the temptation to
be difï¬cult just for the
sake of being difï¬cult.
Jacques Derrida

Jacques Derrida was born to
Jewish parents in the then
French colony of Algeria. He
was interested in philosophy
from an early age, but also
nurtured dreams of becoming
a professional soccer player.
Eventually it was philosophy
that won out and, in 1951, he
entered the Ã‰cole Normale
SupÃ©rieure in Paris. There he
formed a friendship with Louis
Althusser, also of Algerian
origin, who, like Derrida, went
on to become one of the most
prominent thinkers of his day.
The publication in 1967 of
Of Grammatology, Writing and
Difference, and Speech and
Phenomena sealed Derridaâ€™s
international reputation. A
regular visiting lecturer at a
number of European and
American universities, he took
up the post of Professor of
Humanities at the University
of California, Irvine, in 1986.
His later work increasingly
focused on issues of ethics,
partly due to the inï¬‚uence
of Emmanuel Levinas.
Key works
1967 Of Grammatology
1967 Writing and Difference
1967 Speech and Phenomena
1994 The Politics of Friendship

THERE IS NOTHING

DEEP DOWN INSIDE US

EXCEPT

WHAT WE HAVE PUT THERE

OURSELVES
RICHARD RORTY (1931â€“2007)

316 RICHARD RORTY
IN CONTEXT
BRANCH
Ethics
APPROACH
Pragmatism
BEFORE
5th century BCE Socrates
disputes the nature of justice,
goodness, and other concepts
with the citizens of Athens.
4th century BCE Aristotle
writes a treatise on the nature
of the soul.
1878 Charles Sanders Peirce
coins the term â€œpragmatism.â€
1956 American philosopher
Wilfrid Sellars publishes
Empiricism and the Philosophy
of Mind, calling into question
the â€œmyth of the given.â€
AFTER
1994 South-African-born
philosopher John McDowell
publishes Mind and World, a
book strongly inï¬‚uenced by
Rortyâ€™s work.

T

he soul is a curious thing.
Even if we cannot say
much about our souls or
describe what a soul is like, many
of us nonetheless hold ï¬rmly to
the belief that, somewhere deep
down, we each have such a thing.
Not only this, we might claim that
this thing is the fundamental self
(â€œmeâ€) and, at the same time, is
somehow connected directly with
the truth or reality.
The tendency to picture
ourselves as possessing a kind of
â€œdoubleâ€â€”a soul or a deep self that
â€œuses Realityâ€™s own languageâ€â€”is
explored by American philosopher
Richard Rorty in the introduction
to his book, The Consequences of
Pragmatism (1982). Rorty argues
that, to the extent that we have
such a thing at all, a soul is a
human invention; it is something
that we have put there ourselves.

Knowledge as a mirror
Rorty was a philosopher who worked
within the American tradition of
pragmatism. In considering a
statement, most philosophical
traditions ask â€œis this true?â€ , in
the sense of: â€œdoes this correctly
represent the way things are?â€. But

Some theories of knowledge claim that we gain
knowledge by processing â€œraw dataâ€ like a camera
captures light, but Rorty says our perceptions
are tangled up with our beliefs, which we
impose on things in the world.

Philosophy makes
progress not by becoming
more rigorous but by
becoming more imaginative.
Richard Rorty

pragmatists consider statements in
quite a different way, asking instead:
â€œwhat are the practical implications
of accepting this as true?â€
Rortyâ€™s ï¬rst major book,
Philosophy and the Mirror of
Nature, published in 1979, was an
attempt to argue against the idea
that knowledge is a matter of
correctly representing the world,
like some kind of mental mirror.
Rorty argues that this view of
knowledge cannot be upheld, for
two reasons. First, we assume that
our experience of the world is
directly â€œgivenâ€ to usâ€”we assume
that what we experience is the raw

CONTEMPORARY PHILOSOPHY 317
See also: Socrates 46â€“49
JÃ¼rgen Habermas 306â€“07

â– 

Aristotle 56â€“63

data of how the world is. Second,
we assume that once this raw data
has been collected, our reason (or
some other faculty of mind) then
starts to work on it, reconstructing
how this knowledge ï¬ts together
as a whole, and mirroring what is
in the world.
Rorty follows the philosopher
Wilfrid Sellars in claiming that
the idea of experience as â€œgivenâ€
is a myth. We cannot ever access
anything like raw dataâ€”it is not
possible for us to experience a dog,
for instance, outside of thought or
language. We only become aware of
something through conceptualizing
it, and our concepts are learned
through language. Our perceptions
are therefore inextricably tangled up
with the habitual ways that we use
language to divide up the world.
Rorty suggests that knowledge
is not so much a way of mirroring
nature as â€œa matter of conversation
and social practice.â€ When we
decide what counts as knowledge,
our judgement rests not on how
strongly a â€œfactâ€ correlates to the
world, so much as whether it is
something â€œthat society lets us
say.â€ What we can and cannot
count as knowledge is therefore
limited by the social contexts that
we live in, by our histories, and by
what those around us will allow us
to claim. â€œTruth,â€ said Rorty, â€œis
what your contemporaries let you
get away with saying.â€

â– 

Charles Sanders Peirce 205

â– 

William James 206â€“09

John Dewey 228â€“31

When we say â€˜â€˜I know in
my heart it is wrongâ€¦â€™â€™

â€¦we assume there
is an eternal truth
about â€˜â€˜wrongness.â€™â€™

â€¦we assume that
the knowledge we have
is certain knowledge.

But we cannot ï¬nd
any eternal truths
about ethics.

But absolutely certain
knowledge of how things
are is not possible.

is a matter of conversation
and social practice.

Reasons for judgement
But does truth really reduce down to
a matter of what we can get away
with? Rorty is aware that there are
some disturbing implications here,
especially in questions of ethics.
Imagine, for instance, that I kidnap
my neighborâ€™s pet hamster and â¯â¯

â– 

There is nothing
deep down inside us
except what we have
put there ourselves.

â– 

318 RICHARD RORTY

What sort of a world can
we prepare for our
great-grandchildren?
Richard Rorty

Using children as soldiers may seem
intrinsically wrong, but Rorty says there
are no ethical absolutes. Ethics is a
matter of doing our best, in solidarity
with others, to realize a better world.

subject it to all manner of cruel
tortures, simply for the fun of
hearing it squeak. We might all
agree that doing such a thing to the
poor hamster (or, for that matter,
doing such a thing to my neighbor)
is a morally blameable act. We
might claim that there is something
absolutely and fundamentally
wrong about doing such a thing to
another living being; and we might
all agree that we ought not let other
people get away with such things.
But when we look at the reasons
that we give for saying that this is
a morally blameable act, things
become interesting. For example,
imagine that you are asked by
a particularly awkward moral
philosopher why it is wrong to treat
hamsters (or horses, or humans)
in this way. At ï¬rst you might
suggest all manner of reasons. But
philosophy being what it is, and

moral philosophers being the kinds
of beings they are, you might ï¬nd
that for every reason you can think
of, your philosopher friend has a
counter-reason or leads you into
some kind of contradiction.
This is, in fact, precisely what
the philosopher Socrates did in
ancient Athens. Socrates wanted
to ï¬nd out what concepts such as
â€œgoodnessâ€ and â€œjusticeâ€ really
were, so he questioned people who
used these concepts, to ï¬nd out
whether they really knew what
these things were. As the dialogues
of Plato show, most of the people
Socrates talked to were surprisingly
unclear about what it was they
were actually talking about, despite
their earlier conviction that they
fully grasped the relevant concepts.
In the same way, after an hour or
two of being interrogated by a
modern-day Socrates about how
to treat hamsters, you might blurt
out in frustration the following
sentence: â€œBut I just know, in my
heart of hearts, that it is wrong!â€

My heart of hearts
We say or think this kind of thing
relatively frequently, but it is not
immediately clear what exactly we
mean. To examine the idea more
closely, we can break it down into
three parts. First, it seems that

when we say â€œI know, in my heart
of hearts, that it is wrongâ€, we are
speaking as if there is something
out there in the world that is
â€œwrongnessâ€, and that this thing is
knowable. Or, as some philosophers
put it, we are speaking as if there
is an essence of â€œwrongnessâ€ to
which this particular instance
of wrongness corresponds.
Second, by saying that we just
â€œknowâ€ in our heart of hearts, we
imply that this mysterious entity
â€”our â€œheart of heartsâ€â€”is a thing
that, for reasons unknown, has a
particular grasp of truth.
Third, we seem to be speaking
as if there is a straightforward
relationship between our â€œheart
of heartsâ€ and this â€œwrongnessâ€
that lies out there in the world,
such that if we know something
in our heart of hearts, we can have
access to an absolutely certain kind
of knowledge. In other words, this
is just another version of the idea
that knowledge is a way of mirroring
the world. And this, Rorty believes,
is unacceptable.

A world without absolutes
In order for his beliefs to be
consistent, Rorty has to give up
on the idea of fundamental moral
truths. There can be no absolute
right or wrong if knowledge is

If we can rely on
one another, we need
not rely on anything else.
Richard Rorty

CONTEMPORARY PHILOSOPHY 319
â€œwhat society lets us say.â€ Rorty
recognizes that this is a difï¬cult
thing to accept. But is it necessary
to believe that on doing something
morally wrong you are betraying
something deep within you? Must
you believe that there is â€œsome
truth about life, or some absolute
moral law, that you are violatingâ€
in order to maintain even a shred of
human decency? Rorty thinks not.
He maintains that we are ï¬nite
beings, whose existence is limited
to a short time on Earth, and none
of us have a hotline to some deeper,
more fundamental moral truth.
However, this does not imply that
the problems of life have either
We do not need to believe in an
absolute moral law in order to live as
ethical beings. Conversation, social hope,
and solidarity with others allow us to
form a working deï¬nition of â€œthe good.â€

gone away or ceased to matter.
These problems are still with us,
and in the absence of absolute
moral laws we are thrown back
upon our own resources. We are
left, Rorty writes, with â€œour loyalty
to other human beings clinging
together against the dark.â€ There
is no absolute sense of rightness
and wrongness to be discovered.
So we simply have to hold on to
our hopes and loyalties, and
continue to participate in involved
conversations in which we talk
about these difï¬cult issues.
Perhaps, Rorty is saying, these
things are enough: the humility
that comes from recognizing that
there is no absolute standard of
truth; the solidarity we have with
others; and our hopes that we may
be able to contribute to, and to
bequeath to those who come after
us, a world that is worth living in. â– 

Richard Rorty
Richard Rorty was born in
New York, USA in 1931. His
parents were political activists,
and Rorty describes his early
years as being spent reading
about Leon Trotsky, the
Russian revolutionary. He said
that he knew by the age of 12
that â€œthe point of being human
was to spend oneâ€™s life ï¬ghting
social injustice.â€ He began
attending the University of
Chicago early, at the age of 15,
going on to take a PhD at Yale
in 1956. He was then drafted
into the army for two years,
before becoming a professor.
He wrote his most important
book, Philosophy and the
Mirror of Nature, while
professor of philosophy at
Princeton. He wrote widely
on philosophy, literature, and
politics and, unusually for a
20th-century philosopher, drew
on both the so-called analytic
and the continental traditions.
Rorty died of cancer aged 75.
Key works
1979 Philosophy and the
Mirror of Nature
1989 Contingency, Irony,
and Solidarity
1998 Achieving Our Country
2001 Philosophy and Social
Hope

320

EVERY DESIRE
HAS A RELATION
TO
MADNESS
LUCE IRIGARAY (1932â€“ )
IN CONTEXT
BRANCH
Political philosophy
APPROACH
Feminism
BEFORE
1792 Mary Wollstonecraftâ€™s
A Vindication of the Rights of
Woman ï¬rst initiates serious
debate about the place of
women in society.
1890s Austrian psychologist
Sigmund Freud establishes
his psychoanalytic method,
which will greatly inï¬‚uence
Irigarayâ€™s work.
1949 Simone de Beauvoirâ€™s
The Second Sex explores
the implications of sexual
difference.
AFTER
1993 Luce Irigaray turns to
non-Western modes of thought
about sexual difference in
An Ethics of Sexual Difference.

T

he Belgian philosopher and
analyst Luce Irigaray is
concerned above all else
with the idea of sexual difference.
A former student of Jacques Lacan,
a psychoanalyst who famously
explored the linguistic structure
of the unconscious, Irigaray claims
that all language is essentially
masculine in nature.
In Sex and Genealogies (1993)
she writes: â€œEverywhere, in
everything, menâ€™s speech, menâ€™s
values, dreams, and desires are
law.â€ Irigarayâ€™s feminist work can
be seen as a struggle to ï¬nd

One must assume the
feminine role deliberately.
Luce Irigaray

authentically female ways of
speaking and desiring that are
free from male-centeredness.

Wisdom and desire
To address this problem, Irigaray
suggests that all thinkingâ€”even
the most apparently sober and
objective-sounding philosophy,
with its talk of wisdom, certainty,
rectitude, and moderationâ€”is
underpinned by desire. In failing
to acknowledge the desire that
underpins it, traditional malecentered philosophy has also failed
to acknowledge that beneath its
apparent rationality simmer all
manner of irrational impulses.
Irigaray suggests that each sex
has its own relationship to desire,
and as a result each sex has a
relation to madness. This calls
into question the long tradition
of equating maleness with this
rationality, and femaleness with
irrationality. It also opens the
way to the possibility of new
ways of writing and thinking
about philosophy, for both men
and women. â– 

See also: Mary Wollstonecraft 175 â–  Ludwig Wittgenstein 246â€“51 â– 
Simone de Beauvoir 276â€“77 â–  HÃ©lÃ¨ne Cixous 322 â–  Julia Kristeva 323

CONTEMPORARY PHILOSOPHY 321

EVERY EMPIRE TELLS
ITSELF AND THE WORLD
THAT IT IS UNLIKE ALL
OTHER EMPIRES
EDWARD SAID (1935â€“2003)

IN CONTEXT
BRANCH
Political philosophy
APPROACH
Post-colonialism
BEFORE
19th century European
scholars research the histories
of their colonial subjects.
1940S In the aftermath of
World War II, the European
colonial empires begin to
fragment and collapse.

T

he Palestinian writer
Edward Said was one of
the 20th centuryâ€™s foremost
critics of imperialism. In 1978 he
published Orientalism, which
explored how the depictions of
Islamic societies by 19th-century
European scholars were closely
related to the imperialist ideologies
of European states.
In his later work, Said remained
critical of all forms of imperialism,
past and present. He points out that
although we may be critical of
empires of the past, these empires
saw themselves as bringing

civilization to the worldâ€”a view
not shared by the people they
claimed to be helping. Empires
plunder and control, while masking
their abuses of power by talking
about their â€œcivilizingâ€ missions.
If this is the case, Said warns, we
should be wary of present-day
claims by any state undertaking
foreign interventions. â– 
The British Empire was one of many
19th-century empires that claimed
to believe it was bringing the beneï¬ts
of civilization to the countries it
colonized, such as India.

1952 Frantz Fanon writes
Black Skin, White Masks, an
early study of the damage
caused by colonialism.
AFTER
1988 Indian philosopher
Gayatri Spivak publishes
Can the Subaltern Speak?
examining post-colonialism.
From 2000 Scholars such as
Noam Chomsky increasingly
interpret American global
power according to a model
of imperialism.

See also: Frantz Fanon 300â€“01

â– 

Michel Foucault 302â€“03

â– 

Noam Chomsky 304â€“05

322

THOUGHT HAS
ALWAYS WORKED
BY
OPPOSITION
HELENE CIXOUS (1937â€“ )
IN CONTEXT
BRANCH
Epistemology
APPROACH
Feminism
BEFORE
1949 Simone de Beauvoirâ€™s
The Second Sex explores the
philosophical implications of
sexual difference.
1962 French anthropologist
Claude LÃ©vi-Strauss writes
The Savage Mind, a study of
binary oppositions in culture.
1967 Controversial French
philosopher Jacques Derrida
publishes Of Grammatology,
introducing the concept of
deconstruction, which Cixous
uses in her study of gender.
AFTER
1970s The French literary
movement of Ã©criture fÃ©minine
(â€œwomenâ€™s writingâ€) explores
appropriate use of language in
feminist thinking, taking its
inspiration from Cixous.

I

n 1975, the French poet,
novelist, playwright, and
philosopher HÃ©lÃ¨ne Cixous
wrote Sorties, her inï¬‚uential
exploration of the oppositions that
often deï¬ne the way we think
about the world. For Cixous, a
thread that runs through centuries
of thought is our tendency to group
elements of our world into opposing
pairs, such as culture/nature, day/
night, and head/heart. Cixous
claims that these pairs of elements
are always by implication ranked
hierarchically, underpinned by a
tendency to see one element as
being dominant or superior and
associated with maleness and
activity, while the other element or
weaker aspect is associated with
femaleness and passivity.

Time for change
Cixous believes that the authority
of this hierarchical pattern of
thinking is now being called into
question by a new blossoming of
feminist thought. She questions
what the implications of this
change might be, not only for our

Woman must write
herself and bring woman
into literature.
HÃ©lÃ¨ne Cixous

philosophical systems, but also for
our social and political institutions.
Cixous herself, however, refuses to
play the game of setting up binary
oppositions, of victors and losers,
as a structural framework for our
thinking. Instead she conjures up
the image of â€œmillions of species
of mole as yet not recognizedâ€,
tunnelling away under the ediï¬ces
of our world view. And what will
happen when these ediï¬ces start to
crumble? Cixous does not say. It is
as if she is telling us that we can
make no assumptions, that the only
thing we can do is wait and see. â– 

See also: Mary Wollstonecraft 175 â–  Simone de Beauvoir 276â€“77 â– 
Jacques Derrida 308â€“13 â–  Julia Kristeva 323 â–  Martha Nussbaum 339

CONTEMPORARY PHILOSOPHY 323

WHO PLAYS GOD
IN PRESENT-DAY
FEMINISM?
JULIA KRISTEVA (1941â€“ )
IN CONTEXT
BRANCH
Political philosophy
APPROACH
Feminism
BEFORE
1792 Mary Wollstonecraftâ€™s
A Vindication of the Rights
of Woman initiates serious
debate about the nature of the
roles women are conditioned
to play in society.
1807 Georg Hegel explores
the dialectic between
â€œmasterâ€ and â€œslaveâ€ in
Phenomenology of Spirit.
1949 Simone de Beauvoirâ€™s
The Second Sex is published,
rapidly becoming a key text in
the French feminist movement.
AFTER
1999 In their book Fashionable
Nonsense, physics professors
Alan Sokal and Jean Bricmont
criticize Kristevaâ€™s misuse
of scientiï¬c language.

B

ulgarian-born philosopher
and psychoanalyst Julia
Kristeva is often regarded
as one of the leading voices in
French feminism. Nevertheless,
the question of whether, or in what
way, Kristeva is a feminist thinker
has been subject to considerable
debate. Part of the reason for this
is that for Kristeva herself, the very
notion of feminism is problematic.
Feminism has arisen out of the
conï¬‚ict women have had with
the structures that are associated
with male dominance or power.
Because of these roots, Kristeva
warns, feminism carries with it
some of the same male-centered
presuppositions that it is seeking
to question.
If the feminist movement is
to realize its goals fully, Kristeva
believes that it is essential for it to
be more self-critical. She warns
that by seeking to ï¬ght what she
calls the â€œpower principleâ€ of a
male-dominated world, feminism
is at risk of adopting yet another
form of this principle. Kristeva is
convinced that for any movement

to be successful in achieving true
emancipation, it must constantly
question its relationship to power
and established social systemsâ€”
and, if necessary â€œrenounce belief
in its own identity.â€ If the feminist
movement fails to take these steps,
Kristeva fears that it is in serious
danger of developing into little
more than an additional strand
in the ongoing game of power. â– 

Margaret Thatcher, like many
women who have achieved positions
of great power, modiï¬ed her public
image to incorporate classic male
concepts of strength and authority.

See also: Mary Wollstonecraft 175 â–  Georg Hegel 178â€“85 â– 
Simone de Beauvoir 276â€“77 â–  HÃ©lÃ¨ne Cixous 322 â–  Martha Nussbaum 339

324

PHILOSOPHY IS NOT
ONLY A WRITTEN
ENTERPRISE
HENRY ODERA ORUKA (1944â€“1995)
IN CONTEXT
BRANCH
Metaphilosophy
APPROACH
Ethnography
BEFORE
600â€“400 BCE Greek thinkers
such as Thales, Pythagoras,
and Plato all study in Egypt,
Africa, which was a center of
philosophical study in the
ancient world.
AFTER
20th century After the retreat
of European colonial power,
African philosophy begins to
ï¬‚ourish across the continent.
The growth of anthropology
and ethnography also leads
to a deeper understanding
of indigenous traditions of
thought in Africa.
Late 20th century Ghanian
philosopher Kwasi Wiredu
argues that philosophic
sagacity and folk wisdom
must be distinguished from
philosophy proper.

H

enry Odera Oruka was
born in Kenya in 1944
and he was interested in
metaphilosophy, or philosophizing
about philosophy. In his book Sage
Philosophy (1994), he looks at why
philosophy in sub-Saharan Africa
has often been overlooked, and
concludes that it is because it is
primarily an oral tradition, while

Oruka claims that philosophy has
decreed the thoughts of certain races
to be more important than others, but it
must encompass the sayings of African
sages just as it does Greek sages.
See also: Socrates 46â€“49

â– 

philosophers in general tend to work
with written texts. Some people
have claimed that philosophy is
necessarily connected with written
recording, but Oruka disagrees.
In order to explore philosophy
within the oral traditions of Africa,
Oruka proposed an approach that
he called â€œphilosophic sagacityâ€. He
borrowed the ethnographic approach
of anthropology, where people are
observed in their everyday settings,
and their thoughts and actions
recorded in context. Oruka himself
traveled into villages and recorded
conversations with people who
were considered wise by their local
community. His aim was to ï¬nd out
whether they had systematic views
underpinning their perspectives.
Those sages who had critically
examined their ideas about
traditional philosophical topics,
such as God or freedom, and found
a rational foundation for them could,
Oruka believes, be considered
philosophic sages. These systematic
views deserve to be explored in
the light of wider philosophical
concerns and questions. â– 

Friedrich Schlegel 177

â– 

Jacques Derrida 308â€“13

CONTEMPORARY PHILOSOPHY 325

IN SUFFERING,
THE ANIMALS
ARE
OUR
EQUALS
PETER SINGER (1946â€“ )
IN CONTEXT
BRANCH
Ethics
APPROACH
Utilitarianism
BEFORE
c.560 BCE Indian sage and
Jainist leader Mahavira calls
for strict vegetarianism.
1789 Jeremy Bentham sets
out the theory of utilitarianism
in his book, Introduction to
the Principles of Morals and
Legislation, arguing: â€œeach to
count for one, and none for
more than one.â€
1863 In his book Utilitarianism,
John Stuart Mill develops
Benthamâ€™s utilitarianism from
an approach that considers
individual acts to one that
considers moral rules.
AFTER
1983 American philosopher
Tom Regan publishes The
Case for Animal Rights.

T

he Australian philosopher
Peter Singer became known
as one of the most active
advocates of animal rights following
the publication of his book Animal
Liberation in 1975. Singer takes
a utilitarian approach to ethics,
following the tradition developed
by Englishman Jeremy Bentham in
the late 18th century.
Utilitarianism asks us to judge
the moral value of an act by the
consequences of that act. For
Bentham, the way to do this is by
calculating the sum of pleasure or
pain that results from our actions,
like a mathematical equation.

cause such pain. However, like
all utilitarians, Singer applies the
â€œgreatest happiness principleâ€,
which says that we should make
decisions in such a way that they
result in the greatest happiness
for the greatest number. Singer
points out that he has never said
that no experiment on an animal
could ever be justiï¬ed; rather that
we should judge all actions by their
consequences, and â€œthe interests
of animals count among those
consequencesâ€; they form part
of the equation. â– 

Animals are sentient beings
Singerâ€™s utilitarianism is based
on what he refers to as an â€œequal
consideration of interests.â€ Pain, he
says, is pain, whether it is yours or
mine or anybody elseâ€™s. The extent
to which non-human animals can
feel pain is the extent to which we
should take their interests into
account when making decisions
that affect their lives, and we
should refrain from activities that
See also: Jeremy Bentham 174

â– 

The value of life is a
notoriously difï¬cult
ethical question.
Peter Singer

John Stuart Mill 190â€“93

326

ALL THE BEST MARXIST
ANALYSES ARE ALWAYS
ANALYSES OF A FAILURE

SLAVOJ ZIZEK (1949â€“ )

IN CONTEXT
BRANCH
Political philosophy
APPROACH
Marxism
BEFORE
1807 Georg Hegel publishes
The Phenomenology of the
Spirit, laying the groundwork
for Marxist thought.
1867 Karl Marx and Friedrich
Engels publish their
Communist Manifesto.
1867 Marx publishes the ï¬rst
volume of Capital (Das Kapital),
a treatise on political economy.
1899 In The Interpretation
of Dreams, psychoanalyst
Sigmund Freud claims that
much of human behavior is
driven by unconscious forces.
1966 Psychoanalytical theorist
Jacques Lacan, one of Å½iÅ¾ekâ€™s
major inï¬‚uences, revisits
Freud's ideas in Ã‰crits.

T

he idea that all the best
Marxist analyses have
traditionally been analyses
of failure appears in an interview
with Slovenian philosopher Slavoj
Å½iÅ¾ek given in 2008. In this
interview, Å½iÅ¾ek was asked about
the events in Czechoslovakia in
1968, when a period of reform,
aimed at decentralizing and
democratizing the country, was
brutally brought to an end by the
Soviet Union and its allies.
Å½iÅ¾ekâ€™s claim is that the
crushing of the reforms became
the very thing that later sustained
a myth held by the political leftâ€”
namely that, had the reforms gone
ahead, some kind of social and
political paradise would have
followed. According to Å½iÅ¾ek, those
on the political left are prone to
dwelling on their failures, because
doing so allows myths to be
generated about what would have
happened if they had succeeded.
Å½iÅ¾ek says that these failures allow
those on the left to maintain a â€œsafe
moralistic positionâ€, because their
failures mean that they are never in
See also: Immanuel Kant 164â€“71
Martin Heidegger 252â€“55

â– 

The Soviet invasion of Czechoslovakia
in 1968 led to the end of the short-lived
â€œPrague Springâ€ period of liberalization.
All moves toward democracy were
suppressed until 1989.

power, or truly tested by action.
He describes this stance as the
â€œcomfortable position of resistanceâ€,
which allows an avoidance of the
real issuesâ€”such as re-evaluating
the nature of political revolution.
For Å½iÅ¾ek, a dedicated Marxist,
serious questions about the nature
of political power are obscured
by endlessly trying to justify
utopia's elusiveness. â– 

Georg Hegel 178â€“85

â– 

Karl Marx 196â€“203

â– 

DIRECTO

RY

330

DIRECTORY
T

hough the ideas already presented in this book show the broad
range of philosophical thought expressed by some of historyâ€™s
best minds, there are many more people who have helped to shape
the story of philosophy. Some of these thinkersâ€”such as Empedocles,
Plotinus, or William of Ockhamâ€”have had ideas that form the starting
point for other, more well-known theories, and their inï¬‚uence on later
philosophers is clear. Some, such as Friedrich Schelling or Gilles Deleuze,
have taken the works of previous philosophers and added an interesting
twist that sheds new light on the subject. Whatever their relationship is
to the history of philosophy, the people discussed below have all helped
to broaden the boundaries of philosophical thought.

ANAXIMANDER
c.610â€“546 BCE

Born in Miletus, in what is now
southwest Turkey, Anaximander
was a pupil of Thales, the â€œfatherâ€
of Western philosophy. Like Thales,
he thought there was a single basic
substance from which everything
had evolved. He decided it must be
inï¬nite and eternal and called it
apeiron (â€œindeï¬niteâ€). Anaximander
also challenged Thalesâ€™ suggestion
that Earth was supported by a sea
of water, reasoning that this sea
would have to be supported by
something else. Lacking evidence
for this supporting structure, he
declared that Earth was an object
hanging in space. He went on to
publish what is believed to be
the ï¬rst map of the world.
See also: Thales of Miletus 22â€“23

ANAXIMENES OF MILETUS
c.585â€“528 BCE

Like other Milesian philosophers,
Anaximenes searched for the
fundamental material from which

the universe was made. He opted
for air, pointing out that just as air
gives life to the human body, so
a universal kind of air gives life
to the cosmos. He was the ï¬rst
thinker on record to use observed
evidence to support his ideas.
Blowing with pursed lips produced
cold air; with relaxed lips, warm
air. He argued, therefore, that
when something condenses, it
cools; when it expands it heats up.
Likewise, when air condenses, it
becomes visible; ï¬rst as mist, then
as rain, and ultimately, he believed,
as rock, thus giving birth to Earth.
See also: Thales of Miletus 22â€“23

ANAXAGORAS
c.500â€“428 BCE

Born in Ionia, off the southern coast
of present-day Turkey, Anaxagoras
played a key role in making Athens
the world center of philosophy and
scientiï¬c enquiry. Central to his
thinking were his views on the
material world and cosmology. He
reasoned that everything in the
material world was made up of a
small part of everything else,

otherwise it could not have come
into being. Sentenced to death for
impiety after insisting that the sun
was a ï¬ery rock, he ï¬‚ed Athens and
spent his ï¬nal years in exile.
See also: Thales of Miletus 22â€“23

EMPEDOCLES
c.490â€“430 BCE

Empedocles was a member of a
high-ranking political family in
the then-Greek colony of Sicily.
His knowledge of the natural world
led to him being credited with
miraculous powers, such as the
ability to cure diseases and control
the weather. He reasserted the
notion of Heraclitus that we live
in an ever-changing world, as
opposed to Parmenidesâ€™ theory
that everything is ultimately one
ï¬xed entity. He believed that four
elementsâ€”ï¬re, water, earth, and
airâ€”continually combine, move
apart, and recombine in a ï¬nite
number of ways. This idea remained
part of Western thinking up until
the Renaissance period.
See also: Thales of Miletus 22â€“23
Heraclitus 40 Parmenides 41
â– 

â– 

DIRECTORY 331
ZENO OF ELEA

PLOTINUS

IAMBLICHUS

Little is known about Zeno of Elea,
other than his paradoxes of motion,
which are mentioned by Aristotle.
Zeno is thought to have produced
more than 40 of these, although only
a few survive. In them, he defended
the claim of his teacher Parmenides
that the changing and varied
world we perceive around us is not
realityâ€”which is in fact motionless,
uniform, and simple. Movement,
Zeno believed, is an illusion of the
senses. Each of his paradoxes
began from the position that he
wished to refuteâ€”that movement,
and hence change, is realâ€”then
continued by revealing the
contradictory consequences that
lead to the rejection of this notion.
See also: Heraclitus 40
Parmenides 41 Aristotle 56â€“63

Born in Egypt, Plotinus studied
in Alexandria, then considered
the intellectual hub of the world.
He later moved to Rome, where he
taught his own brand of Platonism,
known as Neo-Platonism. Plotinus
divided the cosmos into layers, with
the indeï¬nable source of all being
â€”the â€œOneâ€â€”at the top, followed
by Mind, Soul, Nature, and ï¬nally
the Material World. He believed in
reincarnation and the immortality
of the soul; by striving for
enlightenment individuals could
achieve mystical union with the
â€œOneâ€, and so escape the cycle of
rebirth. His ideas, presented in the
Enneads, were widely inï¬‚uential,
particularly those that supported
Christianity, which was taking root
in the Roman Empire at the time.
See also: Siddhartha Gautama
30â€“33 Plato 50â€“55

A Syrian Neo-Platonist philosopher,
Iamblichus was reputedly born into
an inï¬‚uential aristocratic family.
He founded a school near modernday Antioch, where he taught a
curriculum based mainly on
the ideas of Plato and Aristotle,
although he is best known for
his expansion of the theories of
Pythagoras, which he recorded
in his Collection of Pythagorean
Doctrines. Iamblichus introduced
the concept of the soul being
embodied in matter, both of which
he believed to be divine. Salvation,
or the return of the soul to its pure
immortal form, he stated, was
achieved through the performance
of speciï¬c religious rituals, and not
just the contemplation of abstract
ideas alone.
See also: Pythagoras 26â€“29
Plato 50â€“55 Plotinus 331

WANG BI

HYPATIA OF ALEXANDRIA

In 220 CE, the ruling Chinese Han
Dynasty collapsed, heralding an
era of moral confusion. Philosopher
Wang Bi helped to bring order to
this chaos by reconciling two
dominant schools of thought. He
argued that Daoist texts should
not be read literally, but more
like works of poetry, thus making
them compatible with the highly
practical Confucian ideals of
political and moral wisdom. His
fresh appraisals of Daoism and
Confucianism ensured the survival
of both, and paved the way for the
spread of Buddhism across China.
See also: Laozi 24â€“25 Siddhartha
Gautama 30â€“33 Confucius 34â€“39

Hypatia taught mathematics,
astronomy, and philosophy at the
Museum of Alexandria, eventually
succeeding her father as its head.
Although she was an esteemed
Neo-Platonist intellectual and the
ï¬rst notable female mathematician,
it was her martyrdom that ensured
her fame. She was murdered by a
Christian mob, who blamed her for
the religious turmoil resulting from
conï¬‚ict between her friend, the
Roman prefect Orestos, and Cyril,
Bishop of Alexandria. No works of
hers survive, but she is credited
with inventing a graduated brass
hydrometer and the plane astrolabe.
See also: Plato 50â€“55 Plotinus 331

c.490â€“430 BCE

c.205â€“270 CE

â– 

â– 

c.245â€“325 CE

â– 

PYRRHO

c.360â€“272 BCE
Pyrrho was born on the Ionian
island of Elis. He was exposed to
Asian culture while serving on
Alexander the Greatâ€™s military
campaigns, and was also the ï¬rst
noted philosopher to place doubt
at the center of to his thinking.
Pyrrho treated the suspension of
judgment about beliefs as the only
reasonable reaction to the fallibility
of the senses, and to the fact that
both sides of any argument can
seem to be equally valid. Pyrrho left
no writings, but he did inspire the
Skeptical school in ancient Greek
philosophy, which developed the
idea that the suspension of belief
leads to a tranquil mind.
See also: Socrates 46â€“49
Al-GhazÃ¢lÃ® 332
â– 

226â€“249 CE

â– 

â– 

c.370â€“415 CE

â– 

â– 

â– 

332 DIRECTORY
PROCLUS

AL-KINDI

AL-FARABI

Born in Constantinople, Proclus
succeeded his Platonist teacher
Syrianus as head of the Academy
at Athens. His Commentary on
Euclid is the main account of
the early development of Greek
geometry, and his Commentary on
Platoâ€™s Timaeus has been described
as the most important ancient
Neo-Platonist text. A scientist,
mathematician, lawyer, and
poet, with a deep interest in
religion, he was to become an
inï¬‚uence on many thinkers in
both the medieval Islamic and the
Christian schools of philosophy.
See also: Plato 50â€“55 Boethius
74â€“75 Thomas Aquinas 88â€“95

The Iraqi polymath Al-KindÃ® was
one of the ï¬rst Islamic scholars to
introduce ancient Greek ideas to
the Islamic world. He worked at
Baghdadâ€™s House of Wisdom, where
he supervised the translation of the
great Classical texts into Arabic.
He wrote extensively on a variety of
subjects, most notably psychology
and cosmology, mixing his own
Neo-Platonist approach with the
authority of Aristotelian argument.
He had a special interest in the
compatibility of philosophy and
Islamic theology, and many of his
works are concerned with the
nature of God and the human soul,
as well as prophetic knowledge.
See also: Al-FÃ¢rÃ¢bÃ® 332
Avicenna 76â€“79 Averroes 82â€“83

It is disputed whether Al-FÃ¢rÃ¢bÃ®
was born in what is now Iran or in
Kazakhstan, but it is certain that
he arrived in Baghdad in 901,
where he spent much of his life.
Although a Neo-Platonist, he was
also highly inï¬‚uenced by Aristotle
and wrote commentaries on his
work, as well as on other subjects,
including medicine, science, and
music. He regarded philosophy as
a calling conferred by Allah and as
the only route to true knowledge.
In this life, he said, philosophers
have a duty to guide people in all
matters of daily life; his book The
Ideas of the Citizens of the Virtuous
City describes a Platonic utopia
ruled by philosopher prophets.
See also: Aristotle 56â€“63
Avicenna 76â€“79 Averroes 82â€“83

c.412â€“485 CE

801-873 CE

â– 

â– 

c.872â€“950 CE

â– 

â– 

JOHN PHILOPONUS

â– 

â– 

490â€“570 CE

Almost nothing is known about
Philoponusâ€™s early life other than
he studied in Alexandria with the
Aristotelian Ammonius Hermiae.
A philosopher and natural scientist,
Philoponusâ€™s methods of enquiry
were shaped by Christian beliefs.
By arguing that the universe had an
absolute beginning, and that this
beginning was caused by God, he
became the ï¬rst serious critic of
Aristotle, opening up paths of
enquiry which became major
inï¬‚uences on future scientists,
notably the Italian astronomer
Galileo Galilei. Unpopular with
his colleagues, he later gave up
philosophy and turned to theology,
again causing controversy by
suggesting that the Trinity was
not one but three separate Gods.
See also: Aristotle 56â€“63
Thomas Aquinas 88â€“95
â– 

JOHANNES SCOTUS
ERIUGENA

AL-GHAZALI

His Latin name is often translated
as John the Scot, but the theologian
and philosopher Johannes Scotus
Eriugena was Irishâ€”the medieval
Latin for Ireland being â€œScotiaâ€. He
argued that there was no conï¬‚ict
between knowledge that was
derived from reason and knowledge
from divine revelation. He even set
out to demonstrate that all Christian
doctrine had in fact a rational basis.
This brought him into conï¬‚ict with
the Church, on the grounds that his
theories made both revelation and
faith redundant. Eruigenaâ€™s defense
was that reason is the judge of all
authority, and that it is needed for
us to interpret revelation.
See also: Plato 50â€“55
St. Augustine of Hippo 72â€“73

Born what is now Iran, Al-GhazÃ¢lÃ®
was head of the prestigious
Nizamiyyah school in Baghdad
from 1092 to 1096, when he wrote
The Opinions of the Philosophers,
which explains the Neo-Platonist
and Aristotelian views of Islamic
scholars. His lectures brought him
great respect and wealth, but after
concluding that truth comes from
faith and mystical practices, and not
from philosophy, he abandoned his
teaching post and possessions to
become a wandering Suï¬ preacher.
He came to believe that all causal
links between events were only
made possible by the will of God.
See also: Aristotle 56â€“63
Avicenna 76â€“79 Averroes 82â€“83
Moses Maimonides 84â€“85

c.815â€“877CE

â– 

c.1058â€“1111

â– 

â– 

â– 

DIRECTORY 333
PIERRE ABELARD

IBN BAJJA

MEISTER ECKHART

Remembered less for his philosophy
than for his tragic love affair with
his pupil HÃ©loÃ¯se, Pierre AbÃ©lard
was nevertheless a remarkable
thinker. A brilliant student, he
attended the Cathedral School of
NÃ´tre Dame, Paris, and became a
charismatic teacher. By the age of
22, he had set up his own school,
and went on to become head at
NÃ´tre Dame in 1115. Renowned for
his skills in argument, AbÃ©lard
stood against the popular belief in
universal forms, inherited from
Plato, stating that terms such as
â€œoak treeâ€, are just words that do
not denote anything real about the
many particular oaks that exist.
See also: Plato 50â€“55 Aristotle
56â€“63 Boethius 74â€“75 William
of Ockham 334

A political advisor, poet, scientist,
and philosopher, Ibn BÃ¢jja was one
of the great thinkers of Moorish
Spain. Born in Saragossa, he used
the ideas of Plato and Aristotle
in his treatises, and inï¬‚uenced
Averroes. He set out to show the
compatibility between reason
and faith, stating that the path
to true knowledge, and therefore
enlightenment and a link with the
divine, came only from thinking
and acting rationally. But, Ibn BÃ¢jja
warned, each individual must make
their own journey to enlightenment.
If the enlightened attempt to pass
their wisdom directly to others,
they place themselves at risk of
contamination by the ignorant.
See also: Plato 50â€“55 Aristotle
56â€“63 Averroes 82â€“83

Little is known about the early life
of the German theologian Meister
Eckhart, other than he studied in
Paris, joined the Dominican order,
and held various administrative
and teaching posts around Europe.
A follower of Thomas Aquinas, he
is best known for his vivid sermons,
which dwelt on the presence of God
within the human soul, and for the
mystical imagery of his prose. He
was accused of heresy, and during
his trial he acknowledged that the
ï¬‚orid and emotive language he
used to inspire his listeners might
have led him to stray from the path
of orthodoxy. It is thought that he
died before a verdict was delivered.
See also: St. Anselm 80â€“81
Thomas Aquinas 88â€“95 Ramon
Llull 333 Nikolaus von Kues 96

ROBERT GROSSETESTE

RAMON LLULL

JOHN DUNS SCOTUS

The child of a poor English peasant
family, Grossetesteâ€™s formidable
intelligence was spotted by the
Mayor of Lincoln, who arranged
for him to be educated. Evidence
indicates that he studied at Oxford
University and in Paris, before
joining the clergy and going on
to become Bishop of Lincoln. An
outspoken critic of the Church
in his time, Grosseteste is noted
for his scientiï¬c thinking. He
was one of the ï¬rst medieval
philosophers to grasp Aristotleâ€™s
dual path of scientiï¬c reasoning:
generalizing from particular
observations into a universal law,
and then back again from universal
laws to the prediction of particulars.
See also: Aristotle 56â€“63

Educated at the Majorcan royal
court in Mallorca, Llull developed a
mystical version of Neo-Platonism.
After a vision of Christ, he joined
the Franciscan order and worked
as a missionary in North Africa.
Convinced that rational argument
could persuade Muslims and Jews
to convert to Christianity, Llull
wrote Ars Magna. In this work, he
used complex reasoning to generate
different combinations of the basic
tenets of all monotheistic religions,
hoping to demonstrate the truths of
Christianity. He was convinced
that if everybody was of one faith,
all human knowledge would
combine into a single system.
See also: Plato 50â€“55 St. Anselm
80â€“81 Meister Eckhart 333

Duns Scotus, a Franciscan friar,
was among the most inï¬‚uential of
the medieval philosophers. Born
in Scotland, he taught at Oxford
University and later in Paris. Duns
Scotusâ€™s arguments were noted for
their rigor and intricacy. He argued
against Thomas Aquinas that
attributes, when applied to God,
retain the same meaning as when
used of ordinary objects. On the
issue of universals, he stated that
we can perceive particulars
directly, without the assistance of
general concepts. He also claimed
that knowledge can be acquired by
the proper use of the senses, without
the need for divine â€œillumination.â€
See also: Plato 50â€“55 Aristotle
56â€“63 Thomas Aquinas 88â€“95

1079â€“1142

c.1095â€“1138

â– 

â– 

â– 

1175â€“1253

c.1260â€“1327

â– 

â– 

1232â€“1316

â– 

c.1266â€“1308

â– 

â– 

â– 

â– 

â– 

â– 

334 DIRECTORY
WILLIAM OF OCKHAM

MOSES OF NARBONNE

The English theologian and
philosopher William of Ockham
studied and taught at Oxford. He
was a Franciscan friar, and was
excommunicated for claiming
that the pope had no authority to
exercise temporal power. He is best
known to students of philosophy for
the principle that bears his name:
Ockhamâ€™s Razor, which states that
the best possible explanation of
anything is always the simplest.
In his support for the idea that
universals are abstractions from
experience of particulars, he is
regarded as a forerunner of British
empiricism, a movement begun in
the 17th century by John Locke.
See also: Plato 50â€“55 Aristotle
56â€“63 Francis Bacon 110â€“11
John Locke 130â€“33

Moses of Narbonne, also known
as Moses ben Joshua, was a Jewish
philosopher and physician. Born in
Perpignan, in the Catalan region of
France, he later moved to Spain. He
believed that Judaism was a guide
to the highest degree of truth. He
also stated that the Torah (the ï¬rst
part of the Hebrew Bible and the
basis of Jewish law) has two levels
of meaning: the literal and the
metaphysical. The latter is not
accessible to the layman.
See also: Averroes 82â€“83 Moses
Maimonides 84â€“85

c.1285â€“1347

DIED

â– 

â– 

â– 

NICOLAUS OF AUTRECOURT
c.1298â€“1369

Born near Verdun, France, Nicolaus
of Autrecourt studied theology at
the Sorbonne in Paris. Unusually for
a philosopher of the medieval
period, he explored the logic of
skepticism, concluding that truth
and the truth of its contradiction
are not logically compatible, so that
absolute truth or knowledge, and
the causal links between events or
reactions, cannot be uncovered by
logic alone. In 1346, Pope Clement
VI condemned his ideas as heretical.
He was ordered to recant his
statements and his books were
burnt in public. With the exception
of his Universal Treatise and a few
letters, little of his work survives.
See also: Pyrrho 331 Al-GhazÃ¢lÃ®
332 David Hume 148â€“53
â– 

â– 

c.1362

â– 

GIOVANNI PICO
DELLA MIRANDOLA
1463â€“1494

Pico della Mirandola was a member
of the Platonic Academy in Florence
and is best known for his Oration on
the Dignity of Man, which argued
that the potential of the individual
was limitless, the only restrictions
being self-imposed. It was written
as an introduction to 900 Theses,
his compendium of intellectual
achievement, in which he aimed to
reconcile Platonic and Aristotelian
thinking. Papal objections to the
inclusion of the merits of paganism
saw Mirandola brieï¬‚y jailed, after
which he was forced to ï¬‚ee France.
See also: Plato 50â€“55 Aristotle
56â€“63 Desiderius Erasmus 97
â– 

â– 

FRANCISCO DE VITORIA
1480â€“1546

A Dominican friar, Francisco de
Vitoria was a follower of Thomas
Aquinas and founder of the School

of Salamanca. Called the â€œfather of
international lawâ€, he is primarily
known for developing a code for
international relations. He grew up
at the time of Spainâ€™s uniï¬cation
and its colonization of the Americas.
Although he did not argue against
Spainâ€™s right to build an empire, he
thought that Christianity should
not be imposed on the indigenous
peoples of South America and that
they should be afforded rights to
property and self-government.
See also: Thomas Aquinas 88â€“95

GIORDANO BRUNO
1548â€“1600

The Italian astronomer and thinker
Giordano Bruno was inï¬‚uenced by
Nikolaus von Kues and the Corpus
Hermeticumâ€”a set of occult
treatises believed, at the time, to
predate ancient Greek philosophy.
From von Kues, he took the idea of
an inï¬nite universe, in which our
solar system is just one of many
supporting intelligent life. God,
argued Bruno, is a part of, not
separate from, a universe made
up of â€œmonadsâ€, or animate atoms.
These views, and his interest in
astrology and magic, led to him
being found guilty of heresy and
burned at the stake.
See also: Nikolaus von Kues 96
Gottfried Leibniz 134â€“35
â– 

FRANCISCO SUAREZ
1548â€“1617

Born in Granada, Spain, the Jesuit
philosopher Francisco SuÃ¡rez wrote
on many topics, but is best known
for his writings on metaphysics. In
the controversy over universal
forms that dominated so much
philosophy of the time, he argued

DIRECTORY 335
that only particulars exist. SuÃ¡rez
also maintained that between
Thomas Aquinasâ€™s two types of
divine knowledgeâ€”the knowledge
of what is actual and the knowledge
of what is possibleâ€”there exists
â€œmiddle knowledgeâ€ of what would
have been the case had things
been different. He believed that
God has â€œmiddle knowledgeâ€ of all
our actions, without this meaning
that God caused them to happen
or that they are unavoidable.
See also: Plato 50â€“55 Aristotle
56â€“63 Thomas Aquinas 88â€“95

in a thesis he published in 1745,
stating that emotions are the result
of physical changes in the body,
caused outrage, forcing him to ï¬‚ee
from France to Holland. In 1747 he
published Man a Machine, in which
he expanded his materialist ideas
and rejected Descartesâ€™ theory that
the mind and body are separate.
The bookâ€™s reception caused him
to ï¬‚ee again, this time to Berlin.
See also: Thomas Hobbes 112â€“15
RenÃ© Descartes 116â€“23
â– 

â– 

â– 

BERNARD MANDEVILLE
c.1670â€“1733

Bernard Mandeville was a Dutch
philosopher, satirist, and physician,
who made his home in London. His
best-known work, The Fable of
Bees (1729) concerns a hive of
industrious bees which, when
suddenly made virtuous, stop
working and go and live quietly in
a nearby tree. Its central argument
is that the only way any society can
progress is through vice, and that
virtues are lies employed by the
ruling elite to subdue the lower
classes. Economic growth, stated
Mandeville, stems only from the
individualâ€™s ability to satisfy his
greed. His ideas are often seen as
the forerunners to the theories of
Adam Smith in the 18th century.
See also: Adam Smith 160â€“63

JULIEN OFFRAY DE LA
METTRIE
1709-1751

Julien Offray de la Mettrie was born
in Brittany. He studied medicine
and served as an army physician.
The atheist sentiments expressed

ï¬‚ee. He became a passionate
counter-revolutionary. Mankind
was inherently weak and sinful,
he declared, and the dual powers of
monarch and God were essential to
social order. In On the Pope (1819),
De Maistre argues that government
should be in the hands of a single
authority ï¬gure, ideally linked to
religion, such as the pope.
See also: Edmund Burke 172â€“73

FRIEDRICH SCHELLING
1775â€“1854

NICOLAS DE CONDORCET
1743â€“1794

Nicolas, Marquis de Condorcet, was
an early exponent of the French
tradition of approaching moral and
political issues from a mathematical
perspective. His famous formula,
known as Condorcetâ€™s Paradox,
drew attention to a paradox in the
voting system by showing that
majority preferences become
intransitive when there are more
than three candidates. A liberal
thinker, he advocated equal rights
and free education for all, including
women. He played a key role in the
French Revolution, but was branded
a traitor for opposing the execution
of Louis XVI, and died in prison.
See also: RenÃ© Descartes 116-23
Voltaire 146â€“47 Jean-Jacques
Rousseau 154â€“59
â– 

â– 

Friedrich Schelling started out as
a theologian but, inspired by the
ideas of Immanuel Kant, he turned
to philosophy. Born in southern
Germany, he studied with Georg
Hegel at TÃ¼bingen and taught at the
universities of Jena, Munich, and
Berlin. Schelling coined the term
â€œabsolute idealismâ€ for his view of
nature as an ongoing, evolutionary
process driven by Geist, or spirit.
He argued that all of nature, both
mind and matter, is involved in one
continuous organic process, and
that purely mechanistic accounts
of reality are inadequate. Human
consciousness is nature become
conscious, so that in the form of
man, nature has arrived at a state
of self-awareness.
See also: Benedictus Spinoza
126â€“29 Immanuel Kant 164â€“71
Johann Gottlieb Fichte 176 Georg
Hegel 178â€“85
â– 

â– 

â– 

JOSEPH DE MAISTRE
1753â€“1821

Born in the French region of Savoy,
which was then part of the Kingdom
of Sardinia, Joseph de Maistre was
a lawyer and political philosopher.
He was a ruling senator when the
French revolutionary army invaded
Savoy in 1792, and was forced to

AUGUSTE COMTE
1798â€“1857

The French thinker Auguste Comte
is noted for his theory of intellectual
and social evolution, which divides
human progress into three key
stages. The earliest stage, the

336 DIRECTORY
theological stage, represented by
the medieval period in Europe,
is characterized by belief in the
supernatural. This gave way to
the metaphysical stage, in which
speculation on the nature of reality
developed. Finally, there came the
â€œpositivistâ€ ageâ€”which Comte
saw as emerging at the time he
was writingâ€”with a genuinely
scientiï¬c attitude, based solely on
observable regularities. Comte
believed this positivism would
help to create a new social order,
to redress the chaos generated by
the French Revolution.
See also: John Stuart Mill 190â€“93
Karl Marx 196â€“203

key work Methods of Ethics (1874),
he explored the problems of free will
by examining intuitive principles of
conduct. The pursuit of pleasure, he
claimed, does not exclude altruism,
or the providing of pleasure for
others, since providing pleasure for
others is itself a pleasure. A liberal
philanthropist and a champion of
womenâ€™s rights to education,
Sidgwick was instrumental in
setting up Newnham, Cambridgeâ€™s
ï¬rst college for female students.
See also: Jeremy Bentham 174
John Stuart Mill 190â€“93
â– 

â– 

FRANZ BRENTANO
1838â€“1917

RALPH WALDO EMERSON
1803â€“1882

Born in Boston, the American poet
Ralph Waldo Emerson was also a
noted philosopher. Inspired by the
Romantic movement, he believed
in the unity of nature, with every
single particle of matter and each
individual mind being a microcosm
of the entire universe. Emerson
was famous for his public lectures,
which urged the rejection of social
conformity and traditional authority.
Emerson advocated personal
integrity and self-reliance as the
only moral imperatives, stressing
that every human being has the
power to shape his own destiny.
See also: Henry David Thoreau
204 William James 206â€“09
Friedrich Nietzsche 214â€“21
â– 

(1879), meaning â€œconceptual
notationâ€, and The Foundations
of Arithmetic (1884) effected a
revolution in philosophical logic,
allowing the discipline to develop
rapidly. In On Sense and Reference
(1892) he showed that sentences
are meaningful for two reasonsâ€”
for having a thing that they refer
to, and a unique way in which
that reference is made.
See also: Bertrand Russell 236â€“39
Ludwig Wittgenstein 246â€“51
Rudolf Carnap 257

ALFRED NORTH WHITEHEAD
1861â€“1947

Born in Prussia, the philosopher
Franz Brentano is best known for
establishing psychology as a
discipline in its own right. Initially
a priest, he was unable to reconcile
himself with the concept of papal
infallibility, and left the Church in
1873. Brentano believed that mental
processes were not passive, but
should be seen as intentional acts.
His most highly regarded work is
Psychology from an Empirical
Standpoint. Its publication in
1874 led to him being offered a
professorship at the University
of Vienna, where he taught and
inspired a host of illustrious
students, including the founder of
psychoanalysis, Sigmund Freud.
See also: Edmund Husserl 224â€“25

An English mathematician, Alfred
North Whitehead had a signiï¬cant
inï¬‚uence on ethics, metaphysics,
and the philosophy of science. With
his ex-pupil Bertrand Russell, he
wrote the landmark study on
mathematical logic, Principia
Mathematica (1910â€“13). In 1924, at
the age of 63, he accepted a chair
in philosophy at Harvard. There he
developed what became known as
process philosophy. This was based
on his conviction that traditional
philosophical categories were
inadequate in dealing with the
interactions between matter, space,
and time, and that â€œthe living organ
or experience is the living body as
a wholeâ€ and not just the brain.
See also: Bertrand Russell 236â€“39
Willard Van Orman Quine 278â€“79

GOTTLOB FREGE

NISHIDA KITARO

A professor of mathematics at Jena
University, the German philosopher
Gottlob Frege was a pioneer of the
the analytic tradition in philosophy.
His ï¬rst major work Begriffsschrift

Japanese philosopher Nishida
Kitaro studied Daoism and
Confucianism at school and
Western philosophy at Tokyo
University. He went on to teach

â– 

â– 

HENRY SIDGWICK
1838â€“1900

The English moral philosopher
Henry Sidgwick was a fellow of
Trinity College, Cambridge. In his

â– 

â– 

1848â€“1925

1870â€“1945

DIRECTORY 337
at Kyoto University, where he
established Western philosophy as
an object of serious study in Japan.
Key to his thinking is the â€œlogic
of placeâ€, designed to overcome
traditional Western oppositions
between subject and object through
the â€œpure experienceâ€ of Zen
Buddhism, in which distinctions
between knower and thing known,
self and world, are lost.
See also: Laozi 24â€“25 Siddharta
Gautama 30â€“33 Confucius
34â€“39 Hajime Tanabe 244â€“45

symbolism of dreams and the
phenomenology of imagination. He
contested Auguste Comteâ€™s view
that scientiï¬c advancement was
continuous, claiming instead that
science often moves through shifts
in historical perspective allowing
fresh interpretations of old concepts.
See also: Auguste Comte 335
Thomas Kuhn 293 Michel
Foucault 302â€“03

â– 

1874-1945

Born in Bresslau, in what is now
Poland, the German philosopher
Ernst Cassirer lectured at Berlin
University and then at Hamburg,
where he had access to the vast
collection of studies on tribal
cultures and myths in the Warburg
Library. These were to inform his
major work The Philosophy of
Symbolic Forms (1923â€“29), in which
he incorporated mythical thinking
into a philosophical system similar
to Immanuel Kantâ€™s. In 1933, Cassirer
ï¬‚ed Europe to escape the rise of
Nazism, continuing his work in
America, and later Sweden.
See also: Immanuel Kant 164â€“71
Martin Heidegger 252â€“55

ERNST BLOCH
A German Marxist philosopher,
Ernst Blochâ€™s work focuses on the
possibility of a humanistic utopian
world, free of exploitation and
oppression. During World War I
he took refuge from the conï¬‚ict in
Switzerland, and in 1933 ï¬‚ed the
Nazis, ending up in the United
States. Here he began his key
work, The Principle of Hope (1947).
After World War II, Bloch taught in
Leipzigâ€”but with the building of
the Berlin Wall in 1961, he sought
asylum in West Germany. Although
he was an atheist, Bloch believed
that religionâ€™s mystical vision of
heaven on earth is attainable.
See also: Georg Hegel 178â€“85
Karl Marx 196â€“203
â– 

â– 

GASTON BACHELARD
1884â€“1962

The French philosopher Gaston
Bachelard studied physics before
switching to philosophy. He taught
at Dijon University, going on to
become the ï¬rst professor of history
and philosophy of the sciences at
the Sorbonne in Paris. His study of
thought processes encompasses the

MICHAEL OAKESHOTT
1901â€“1990

c.1885â€“1977

ERNST CASSIRER

â– 

â– 

â– 

â– 

â– 

â– 

Ryle stated, are the cause of much
philosophical confusion, so careful
attention to the underlying function
of ordinary language is the way to
overcome philosophical problems.
See also: Thomas Hobbes 112â€“15
Ludwig Wittgenstein 246â€“51
Daniel Dennett 339

GILBERT RYLE
1900â€“1976

Born in Brighton on the south coast
of England, Gilbert Ryle studied
and taught at Oxford University. He
believed that many problems in
philosophy arise from the abuse of
language. He showed that we often
assume expressions that function
in a similar way grammatically are
members of the same logical
category. Such â€œcategory mistakesâ€,

Michael Oakeshott was a British
political theorist and philosopher.
He taught at Cambridge and Oxford
universities, before becoming
Professor of Political Science at the
London School of Economics. Works
such as On Being Conservative
(1956) and Rationalism in Politics and
Other Essays (1962) cemented his
fame as a political theorist. He
had an important inï¬‚uence on
Conservative party politics in the
late 20th century. However, since
he frequently revised his opinions,
his work deï¬es categorization.
See also: Edmund Burke 172â€“73
Georg Hegel 178â€“85
â– 

AYN RAND
1905â€“1982

The writer and philosopher Ayn
Rand was born in Russia, but
moved to the United States in 1926.
She was working as a screenwriter
when her novel The Fountainhead
(1943), the story of an ideal man,
made her famous. She is the
founder of Objectivism, which
challenges the idea that manâ€™s
moral duty is to live for others.
Reality exists as an objective
absolute and manâ€™s reasoning is
his manner of perceiving it.
See also: Aristotle 56â€“63
Adam Smith 160â€“63
â– 

338 DIRECTORY
JOHN LANGSHAW AUSTIN

LOUIS ALTHUSSER

RENE GIRARD

Educated at Oxford University,
where he also taught, the British
philosopher John Langshaw Austin
was a leading ï¬gure in â€œordinary
languageâ€ or â€œOxfordâ€ philosophy,
which became fashionable in the
1950s. Austin argued that rigorous
analysis of how language operates
in ordinary everyday usage can
lead to the discovery of the subtle
linguistic distinctions needed to
resolve profound philosophical
problems. He is best known from
his papers and lectures that were
published after his death as How
to do Things with Words (1962) and
Sense and Sensibilia (1964).
See also: Bertrand Russell
236â€“39 Gilbert Ryle 337

Born in Algeria, the French Marxist
scholar Louis Althusser argued
that there is a radical difference
between Marxâ€™s early writings and
the â€œscientiï¬câ€ period of Capital
(Das Kapital). The early works of
Marx reï¬‚ect the times with their
focus on Hegelian concepts such as
alienation, whereas in the mature
work, history is seen as having its
own momentum, independent of
the intentions and actions of human
agents. Therefore Althusserâ€™s claim
that we are determined by the
structural conditions of society
involves the controversial rejection
of human autonomy, denying
individual agency a role in history.
See also: Georg Hegel 178â€“85
Karl Marx 196â€“203 Michel
Foucault 302â€“03 Slavoj Å½iÅ¾ek 326

The French philosopher and
historian RenÃ© Girard writes and
teaches across a wide range of
subjects, from economics to literary
criticism. He is best known for his
theory of mimetic desire. In Deceit,
Desire and the Novel (1961), Girard
uses ancient mythology and modern
ï¬ction to show that human desire,
as distinct from animal appetite, is
always aroused by the desire of
another. His study of the origins of
violence, Violence and the Sacred
(1972), goes further by arguing
that this imitated desire leads to
conï¬‚ict and violence. Religion,
Girard states, originated with the
process of victimization or sacriï¬ce
that was used to quell the violence.
See also: Michel Foucault 302â€“03

1911â€“1960

â– 

1918â€“1990

1923â€“

â– 

â– 

DONALD DAVIDSON
1917â€“2003

The American philosopher Donald
Davidson studied at Harvard and
went on to a distinguished career
teaching at various American
universities. He was involved in
several areas of philosophy, notably
the philosophy of mind. He held a
materialist position, stating that
each token mental event was also
a physical event, although he did
not believe that the mental could be
entirely reduced to, or explained in
terms of, the physical. Davidson
also made notable contributions to
the philosophy of language, arguing
that a language must have a ï¬nite
number of elements and that its
meaning is a product of these
elements and rules of combination.
See also: Ludwig Wittgenstein
246â€“51 Willard Van Orman
Quine 278â€“79
â– 

â– 

GILLES DELEUZE

EDGAR MORIN

1925â€“1995

1921â€“

The French philosopher Edgar
Morin was born in Paris, the son of
Jewish immigrants from Greece.
His positive view of the progress of
Western civilization is tempered by
what he perceives as the negative
effects of technical and scientiï¬c
advances. Progress may create
wealth but also seems to bring with
it a breakdown of responsibility and
global awareness. Morin developed
what became known as â€œcomplex
thoughtâ€ and coined the term
â€œpolitics of civilization.â€ His sixvolume Method (1977â€“2004) is a
compendium of his thoughts and
ideas, offering a broad insight into
the nature of human enquiry.
See also: Theodor Adorno 266â€“67
JÃ¼rgen Habermas 306â€“07
â– 

Gilles Deleuze was born in Paris
and spent most of his life there.
He saw philosophy as a creative
process for constructing concepts,
rather than an attempt to discover
and reï¬‚ect reality. Much of his work
was in the history of philosophy,
yet his readings did not attempt to
disclose the â€œtrueâ€ Nietzsche, for
example. Instead they rework the
conceptual mechanisms of a
philosopherâ€™s subject to produce
new ideas, opening up new avenues
of thought. Deleuze is also known for
collaborations with psychoanalyst
FÃ©lix Guattariâ€”Anti-Oedipus (1972)
and What is Philosophy (1991)â€”and
for his commentaries on literature,
ï¬lm, and art.
See also: Henri Bergson 226â€“27
Michel Foucault 302â€“03
â– 

DIRECTORY 339
NIKLAS LUHMANN

DANIEL DENNETT

MARTHA NUSSBAUM

Born in LÃ¼neburg, Germany, Niklas
Luhmann was captured by the
Americans during World War II,
when he was just 17. After the war
he worked as a lawyer until, in
1962, he took a sabbatical to study
sociology in America. He went
on to become one of the most
important and proliï¬c social
theorists of the 20th century.
Luhmann developed a grand
theory, to explain every element
of social life, from complex wellestablished societies to the briefest
of exchanges, lasting just seconds.
In his most important work, The
Society of Society (1997), he argues
that communication is the only
genuinely social phenomenon.
See also: JÃ¼rgen Habermas 306-07

Born in Beirut, the American
philosopher Daniel Dennett is an
acclaimed expert on the nature of
cognitive systems. Professor of
Philosophy at Tufts University,
Massachusetts, he is noted for
his wide-ranging expertise in
linguistics, artiï¬cial intelligence,
neuroscience, and psychology.
Using memorable and creative
labels, such as â€œJoycean machineâ€
for stream of consciousness, he
argues that the source of free will
and consciousness is the brainâ€™s
computational circuitry, which
tricks us into thinking we are more
intelligent than we actually are.
See also: Gilbert Ryle 337
Willard Van Orman Quine 278â€“79
Michel Foucault 302â€“03

Born in New York City, American
philosopher Martha Nussbaum is
the Ernst Freund Distinguished
Service Professor of Law and Ethics
at the University of Chicago. She
has published numerous books
and papers, mainly on ethics and
political philosophy, where the rigor
of her academic enquiry is always
informed by a passionate
liberalism. Her exploration of
ancient Greek ethics, The Fragility
of Goodness (1986), ï¬rst brought
her acclaim, but she is now equally
well-known for her liberal views on
feminism, as expressed in Sex and
Social Justice (1999), which argues
for radical change in gender and
family relationships.
See also: Plato 50â€“55 Aristotle
56â€“63 John Rawls 294â€“95

1927â€“1998

1942â€“

1947â€“

â– 

â– 

â– 

â– 

MICHEL SERRES

MARCEL GAUCHET
1946â€“

ISABELLE STENGERS

The French author and philosopher
Michel Serres studied mathematics
before taking up philosophy. He is
a professor at Stanford University
in California and a member of the
prestigious AcadÃ©mie FranÃ§aise.
His lectures and books are
presented in French, with an
elegance and ï¬‚uidity that is hard
to translate. His post-humanist
enquiries take the form of â€œmapsâ€,
where the journeys themselves
play an major role. He has been
described as a â€œthinker for whom
voyaging is inventionâ€, ï¬nding
truths in the chaos, discord, and
disorder revealed in the links
between the sciences, arts, and
contemporary culture.
See also: Roland Barthes 290â€“91
Jacques Derrida 308â€“13

The French philosopher, historian,
and sociologist Marcel Gauchet
has written widely on democracy
and the role of religion in the
modern world. He is the editor of
the intellectual French periodical
Le DÃ©bat and a professor at the
Ã‰cole des Hautes Etudes en
Sciences Sociales (EHESS) in Paris.
His key work, The Disenchantment
of the World: A Political History of
Religion (1985), explores the modern
cult of individualism in the context
of manâ€™s religious past. As religious
belief declines across the Western
world, Gauchet argues that elements
of the sacred has been incorporated
into human relationships and other
social activities.
See also: Maurice Merleau-Ponty
274â€“75 Michel Foucault 302â€“03

Isabelle Stengers was born in
Belgium and studied chemistry
at the Free University of Brussels,
where she is now Professor of
Philosophy. She was awarded the
grand prize for philosophy by the
AcadÃ©mie FranÃ§aise in 1993. A
distinguished thinker on science,
Stengers has written extensively
about modern scientiï¬c processes,
with a focus on the use of science
for social ends and its relationship
to power and authority. Her books
include Power and Invention (1997)
and The Invention of Modern
Science (2000), and Order Out of
Chaos (1984) with the Nobel Prizewinning chemist Ilya Prigogine.
See also: A lfred North Whitehead
336 Edgar Morin 338

1930â€“

â– 

â– 

1949â€“

â– 

340

GLOSSARY
the Absolute Ultimate reality
conceived of as an all-embracing,
single principle. Some thinkers
have identiï¬ed this principle with
God; others have believed in the
Absolute but not in God; others
have not believed in either. The
philosopher most closely associated
with the idea is Georg Hegel.
Aesthetics A branch of philosophy
concerned with the principles of art
and the notion of beauty.
Agent The doing self, as distinct
from the knowing self; the self that
decides or chooses or acts.
Analysis The search for a deeper
understanding of something by
taking it to pieces and looking at
each part. The opposite approach
is synthesis.
Analytic philosophy A view of
philosophy that sees its aim as
clariï¬cationâ€”the clariï¬cation of
concepts, statements, methods,
arguments, and theories by
carefully taking them apart.
Analytic statement A statement
whose truth or falsehood can be
established by analysis of the
statement itself. The opposite is
a synthetic statement.
Anthropomorphism The
attribution of human characteristics
to something that is not human; for
instance to God or to the weather.

A posteriori Something that can
be considered valid only by means
of experience.

A priori Something known to be
valid in advance of (or without
need of) experience.
Argument A process of reasoning
in logic that purports to show its
conclusion to be true.
Category The broadest class or
group into which things can be
divided. Aristotle and Immanuel
Kant both tried to provide a
complete list of categories.
Concept A thought or idea; the
meaning of a word or term.
Contingent May or may not be the
case; things could be either way.
The opposite is necessary.
Contradictory Two statements
are contradictory if one must be
true and the other false: they
cannot both be true, nor can they
both be false.
Contrary Two statements are
contrary if they cannot both be
true but may both be false.
Corroboration Evidence that
lends support to a conclusion
without necessarily proving it.
Cosmology The study of the whole
universe, the cosmos.
Deduction Reasoning from the
general to the particularâ€”for
instance, â€œIf all men are mortal then
Socrates, being a man, must be
mortal.â€ It is universally agreed that
deduction is valid. The opposite
process is called induction.

Determinism The view that
nothing can happen other than
what does happen, because every
event is the necessary outcome
of causes preceding itâ€”which
themselves were the necessary
outcome of causes preceding them.
The opposite is indeterminism.
Dialectic i) Skill in questioning or
argument. ii) The idea that any
assertion, whether in word or deed,
evokes opposition, the two of which
are reconciled in a synthesis that
includes elements of both.
Dualism A view of something as
made up of two irreducible parts,
such as the idea of human beings
as consisting of bodies and minds,
the two being radically unlike.
Emotive Expressing emotion. In
philosophy the term is often used
in a derogatory way for utterances
that pretend to be objective or
impartial while in fact expressing
emotional attitudes, as for example
in â€œemotive deï¬nition.â€
Empirical knowledge Knowledge
of the empirical world.
Empirical statement A statement
about the empirical world; what is
or could be experienced.
Empirical world The world as
revealed to us by our actual or
possible experience.
Empiricism The view that all
knowledge of anything that
actually exists must be derived
from experience.

GLOSSARY 341
Epistemology The branch of
philosophy concerned with what
sort of thing, if anything, we can
know; how we know it; and what
knowledge is. In practice it is the
dominant branch of philosophy.

Hypothesis A theory whose truth
is assumed for the time being
because it forms a useful starting
point for further investigation,
despite limited evidence to prove
its validity.

Essence The essence of a thing is
that which is distinctive about it
and makes it what it is. For instance,
the essence of a unicorn is that it is
a horse with a single horn on its
head. Unicorns do not exist of
courseâ€”so essence does not imply
existence. This distinction is
important in philosophy.

Idealism The view that reality
consists ultimately of something
nonmaterial, whether it be mind,
the contents of mind, spirits, or
one spirit. The opposite point of
view is materialism.

Ethics A branch of philosophy
that is concerned with questions
about how we should live, and
therefore about the nature of right
and wrong, good and bad, ought
and ought not, duty, and other
such concepts.
Existentialism A philosophy
that begins with the contingent
existence of the individual human
being and regards that as the
primary enigma. It is from this
starting point that philosophical
understanding is pursued.
Fallacy A seriously wrong
argument, or a false conclusion
based on such an argument.
Falsiï¬ability A statement, or set
of statements, is falsiï¬able if it
can be proved wrong by empirical
testing. According to Karl Popper,
falsiï¬ability is what distinguishes
science from nonscience.
Humanism A philosophical
approach based on the assumption
that mankind is the most important
thing that exists, and that there can
be no knowledge of a supernatural
world, if any such world exists.

Indeterminism The view that not
all events are necessary outcomes
of events that may have preceeded
them. The opposite is point of view
is determinism.
Induction Reasoning from the
particular to the general. An
example would be â€œSocrates died,
Plato died, Aristotle died, and each
other individual man who was born
more than 130 years ago has died.
Therefore all men are mortal.â€
Induction does not necessarily yield
results that are true, so whether it
is genuinely a logical process is
disputed. The opposite process is
called deduction.
Intuition Direct knowing, whether
by sensory perception or by insight;
a form of knowledge that makes no
use of reasoning.
Irreducible An irreducible thing
is one that cannot be brought to a
simpler or reduced form.
Linguistic philosophy Also
known as linguistic analysis. The
view that philosophical problems
arise from a muddled use of
language, and are to be solved, or
dissolved, by a careful analysis
of the language in which they
have been expressed.

Logic The branch of philosophy
that makes a study of rational
argument itselfâ€”its terms,
concepts, rules, and methods.
Logical positivism The view that
the only empirical statements
that are meaningful are those that
are veriï¬able.
Materialism The doctrine that
all real existence is ultimately of
something material. The opposite
point of view is idealism.
Metaphilosophy The branch of
philosophy that looks at the nature
and methods of philosophy itself.
Metaphysics The branch of
philosophy concerned with the
ultimate nature of what exists. It
questions the natural world â€œfrom
outsideâ€, and its questions cannot
be answered by science.
Methodology The study of methods
of enquiry and argument.
Monism A view of something as
formed by a single element; for
example, the view that human
beings do not consist of elements
that are ultimately separable, like
a body and a soul, but are of one
single substance.
Mysticism Intuitive knowledge
that transcends the natural world.
Naturalism The view that reality
is explicable without reference to
anything outside the natural world.
Necessary Must be the case. The
opposite is contingent. Hume
believed that necessary connections
existed only in logic, not in the real
world, a view that has been upheld
by many philosophers since.

342 GLOSSARY
Necessary and sufï¬cient
conditions For X to be a husband
it is a necessary condition for X to
be married. However, this is not a
sufï¬cient conditionâ€”for what if X
is female? A sufï¬cient condition for
X to be a husband is that X is both
a man and married. One of the
commonest forms of error in
thinking is to mistake necessary
conditions for sufï¬cient conditions.

Phenomenon An experience that
is immediately present. If I look at
an object, the object as experienced
by me is a phenomenon. Immanuel
Kant distinguished this from the
object as it is in itself, independently
of being experienced: this he called
the noumenon.

Noumenon The unknowable
reality behind what presents itself
to human consciousness, the latter
being known as phenomenon. A
thing as it is in itself, independently
of being experienced, is said to be
the noumenon. â€œThe noumenalâ€ has
therefore become a term for the
ultimate nature of reality.

Philosophy Literally, â€œthe love of
wisdom.â€ The word is widely used
for any sustained rational reï¬‚ection
about general principles that has
the aim of achieving a deeper
understanding. Philosophy provides
training in the disciplined analysis
and clariï¬cation of arguments,
theories, methods, and utterances
of all kinds, and the concepts of
which they make use. Traditionally,
its ultimate aim has been to attain
a better understanding of the world,
though in the 20th century a good
deal of philosophy became devoted
to attaining a better understanding
of its own procedures.

Numinous Anything regarded as
mysterious and awesome, bearing
intimations from outside the natural
realm. Not to be confused with the
noumenal; see noumenon above.

Philosophy of religion The
branch of philosophy that looks at
human belief systems and the real
or imaginary objects, such as gods,
that form the basis for these beliefs.

Ontology A branch of philosophy
that asks what actually exists, as
distinct from the nature of our
knowledge of it, which is covered
by the branch of epistemology.
Ontology and epistemology taken
together constitute the central
tradition of philosophy.

Philosophy of science A branch
of philosophy concerned with the
nature of scientiï¬c knowledge and
the practice of scientiï¬c endeavor.

Noncontradictory Statements are
considered noncontradictory if their
truth-values are independent of
one another.

Phenomenology An approach
to philosophy which investigates
objects of experience (known as
phenomena) only to the extent
that they manifest themselves in
our consciousness, without making
any assumptions about their
nature as independent things.

Political philosophy The branch
of philosophy that questions the
nature and methods of the state
and deals with such subjects as
justice, law, social hierarchies,
political power, and constitutions.
Postmodernism A viewpoint that
holds a general distrust of theories,
narratives, and ideologies that
attempt to put all knowledge into
a single framework.

Pragmatism A theory of truth.
It holds that a statement is true if
it does all the jobs required of it:
accurately describes a situation;
prompts us to anticipate experience
correctly; ï¬ts in with already wellattested statements; and so on.
Premise The starting point of an
argument. Any argument has to
start from at least one premise, and
therefore does not prove its own
premises. A valid argument proves
that its conclusions follow from its
premisesâ€”but this is not the same
as proving that its conclusions are
true, which is something no
argument can do.
Presupposition Something taken
for granted but not expressed. All
utterances have presuppositions,
and these may be conscious or
unconscious. If a presupposition is
mistaken, an utterance based on it
may also be mistaken, though the
mistake may not evident in the
utterance itself. The study of
philosophy teaches us to become
more aware of presuppositions.
Primary and secondary qualities
John Locke divided the properties
of a physical object into those
that are possessed by the object
independently of being experienced,
such as its location, dimensions,
velocity, mass, and so on (which he
called primary qualities), and those
that involve the interaction of an
experiencing observer, such as the
objectâ€™s color and taste (which he
called secondary qualities).
Property In philosophy this
word is commonly used to mean a
characteristic; for example â€œfur or
hair is a deï¬ning property of a
mammal.â€ See also primary and
secondary qualities.

GLOSSARY 343
Rational Based on, or according
to, the principles of reason or logic.
Proposition The content of a
statement that conï¬rms or denies
whether something is the case, and
is capable of being true or false.
Rationalism The view that we
can gain knowledge of the world
through the use of reason, without
relying on sense-perception, which
is regarded by rationalists as
unreliable. The opposite view
is known as empiricism.
Scepticism The view that it is
impossible for us to know anything
for certain.
Semantics The study of meanings
in linguistic expressions.
Semiotics The study of signs
and symbols, in particular their
relationships with the things they
are meant to signify.
Social contract An implicit
agreement among members of a
society to cooperate in order to
achieve goals that beneï¬t the whole
group, sometimes at the expense
of individuals within it.
Solipsism The view that only the
existence of the self can be known.
Sophist Someone whose aim in
argument is not to seek the truth
but to win the argument. In ancient
Greece, young men aspiring to
public life were taught by sophists
to learn the various methods of
winning arguments.
Synthesis Seeking a deeper
understanding of something by
putting the pieces together. The
opposite is analysis.

Synthetic statement A statement
that has to be set against facts
outside itself for its truth to be
determined. The opposite is an
analytic statement.

Universalism The belief that
we should apply to ourselves the
same standards and values that we
apply to others. Not to be confused
with universal, above.

Teleology A study of ends or
goals. A teleological explanation
is one that explains something in
terms of the ends that it serves.

Utilitarianism A theory of politics
and ethics that judges the morality
of actions by their consequences,
that regards the most desirable
consequence of any action as the
greatest good of the greatest
number, and that deï¬nes â€œgoodâ€
in terms of pleasure and the
absence of pain.

Theology Enquiry into scholarly
and intellectual questions
concerning the nature of God.
Philosophy, by contrast, does not
assume the existence of God,
though some philosophers have
attempted to prove his existence.
Thing-in-itself Another term for
a noumenon, from the German
Ding-an-sich.
Transcendental Outside the
world of sense experience.
Someone who believes that ethics
are transcendental believes that
ethics have their source outside the
empirical world. Thoroughgoing
empiricists do not believe that
anything transcendental exists,
and nor did Friedrich Nietzsche
or humanist existentialists.
Truth-value Either of two values,
namely true or false, that can be
applied to a statement.
Universal A concept of general
application, like â€œredâ€ or â€œwoman.â€
It has been disputed whether
universals have an existence of
their own. Does â€œrednessâ€ exist, or
are there only individual red objects?
In the Middle Ages, philosophers
who believed that â€œrednessâ€ had a
real existence were called â€œrealistsâ€,
while philosophers who maintained
that it was no more than a word
were called â€œnominalists.â€

Validity An argument is valid
if its conclusion follows from its
premises. This does not necessarily
mean that the conclusion is true: it
may be false if one of the premises
is false, though the argument itself
is still valid.
Veriï¬ability A statement or set
of statements can be veriï¬ed if it
can be proved to be true by looking
at empirical evidence. Logical
positivists believed that the only
empirical statements that were
meaningful were those that were
veriï¬able. David Hume and Karl
Popper pointed out that scientiï¬c
laws were unveriï¬able.
World In philosophy the word
â€œworldâ€ has been given a special
sense, meaning â€œthe whole of
empirical realityâ€, and may
therefore also be equated with
the totality of actual and possible
experience. True empiricists
believe that the world is all there is,
but philosophers with different
views believe that the world does
not account for total reality. Such
philosophers believe that there is a
transcendental realm as well as
an empirical realm, and they may
believe that both are equally real.


Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the
full breadth of the field, which encompasses logic, probability, and continuous mathematics;
perception, reasoning, learning, and action; and everything from microelectronic devices to
robotic planetary explorers. The book is also big because we go into some depth.
The subtitle of this book is â€œA Modern Approach.â€ The intended meaning of this rather
empty phrase is that we have tried to synthesize what is now known into a common framework, rather than trying to explain each subfield of AI in its own historical context. We
apologize to those whose subfields are, as a result, less recognizable.

New to this edition
This edition captures the changes in AI that have taken place since the last edition in 2003.
There have been important applications of AI technology, such as the widespread deployment of practical speech recognition, machine translation, autonomous vehicles, and household robotics. There have been algorithmic landmarks, such as the solution of the game of
checkers. And there has been a great deal of theoretical progress, particularly in areas such
as probabilistic reasoning, machine learning, and computer vision. Most important from our
point of view is the continued evolution in how we think about the field, and thus how we
organize the book. The major changes are as follows:
â€¢ We place more emphasis on partially observable and nondeterministic environments,
especially in the nonprobabilistic settings of search and planning. The concepts of
belief state (a set of possible worlds) and state estimation (maintaining the belief state)
are introduced in these settings; later in the book, we add probabilities.
â€¢ In addition to discussing the types of environments and types of agents, we now cover
in more depth the types of representations that an agent can use. We distinguish among
atomic representations (in which each state of the world is treated as a black box),
factored representations (in which a state is a set of attribute/value pairs), and structured
representations (in which the world consists of objects and relations between them).
â€¢ Our coverage of planning goes into more depth on contingent planning in partially
observable environments and includes a new approach to hierarchical planning.
â€¢ We have added new material on first-order probabilistic models, including open-universe
models for cases where there is uncertainty as to what objects exist.
â€¢ We have completely rewritten the introductory machine-learning chapter, stressing a
wider variety of more modern learning algorithms and placing them on a firmer theoretical footing.
â€¢ We have expanded coverage of Web search and information extraction, and of techniques for learning from very large data sets.
â€¢ 20% of the citations in this edition are to works published after 2003.
â€¢ We estimate that about 20% of the material is brand new. The remaining 80% reflects
older work but has been largely rewritten to present a more unified picture of the field.
vii

viii

Preface

Overview of the book

NEW TERM

The main unifying theme is the idea of an intelligent agent. We define AI as the study of
agents that receive percepts from the environment and perform actions. Each such agent implements a function that maps percept sequences to actions, and we cover different ways to
represent these functions, such as reactive agents, real-time planners, and decision-theoretic
systems. We explain the role of learning as extending the reach of the designer into unknown
environments, and we show how that role constrains agent design, favoring explicit knowledge representation and reasoning. We treat robotics and vision not as independently defined
problems, but as occurring in the service of achieving goals. We stress the importance of the
task environment in determining the appropriate agent design.
Our primary aim is to convey the ideas that have emerged over the past fifty years of AI
research and the past two millennia of related work. We have tried to avoid excessive formality in the presentation of these ideas while retaining precision. We have included pseudocode
algorithms to make the key ideas concrete; our pseudocode is described in Appendix B.
This book is primarily intended for use in an undergraduate course or course sequence.
The book has 27 chapters, each requiring about a weekâ€™s worth of lectures, so working
through the whole book requires a two-semester sequence. A one-semester course can use
selected chapters to suit the interests of the instructor and students. The book can also be
used in a graduate-level course (perhaps with the addition of some of the primary sources
suggested in the bibliographical notes). Sample syllabi are available at the bookâ€™s Web site,
aima.cs.berkeley.edu. The only prerequisite is familiarity with basic concepts of
computer science (algorithms, data structures, complexity) at a sophomore level. Freshman
calculus and linear algebra are useful for some of the topics; the required mathematical background is supplied in Appendix A.
Exercises are given at the end of each chapter. Exercises requiring significant programming are marked with a keyboard icon. These exercises can best be solved by taking
advantage of the code repository at aima.cs.berkeley.edu. Some of them are large
enough to be considered term projects. A number of exercises require some investigation of
the literature; these are marked with a book icon.
Throughout the book, important points are marked with a pointing icon. We have included an extensive index of around 6,000 items to make it easy to find things in the book.
Wherever a new term is first defined, it is also marked in the margin.

About the Web site
aima.cs.berkeley.edu, the Web site for the book, contains
â€¢ implementations of the algorithms in the book in several programming languages,
â€¢ a list of over 1000 schools that have used the book, many with links to online course
materials and syllabi,
â€¢ an annotated list of over 800 links to sites around the Web with useful AI content,
â€¢ a chapter-by-chapter list of supplementary material and links,
â€¢ instructions on how to join a discussion group for the book,

Preface

ix
â€¢ instructions on how to contact the authors with questions or comments,
â€¢ instructions on how to report errors in the book, in the likely event that some exist, and
â€¢ slides and other materials for instructors.

About the cover
The cover depicts the final position from the decisive game 6 of the 1997 match between
chess champion Garry Kasparov and program D EEP B LUE . Kasparov, playing Black, was
forced to resign, making this the first time a computer had beaten a world champion in a
chess match. Kasparov is shown at the top. To his left is the Asimo humanoid robot and
to his right is Thomas Bayes (1702â€“1761), whose ideas about probability as a measure of
belief underlie much of modern AI technology. Below that we see a Mars Exploration Rover,
a robot that landed on Mars in 2004 and has been exploring the planet ever since. To the
right is Alan Turing (1912â€“1954), whose fundamental work defined the fields of computer
science in general and artificial intelligence in particular. At the bottom is Shakey (1966â€“
1972), the first robot to combine perception, world-modeling, planning, and learning. With
Shakey is project leader Charles Rosen (1917â€“2002). At the bottom right is Aristotle (384
B . C .â€“322 B . C .), who pioneered the study of logic; his work was state of the art until the 19th
century (copy of a bust by Lysippos). At the bottom left, lightly screened behind the authorsâ€™
names, is a planning algorithm by Aristotle from De Motu Animalium in the original Greek.
Behind the title is a portion of the CPSC Bayesian network for medical diagnosis (Pradhan
et al., 1994). Behind the chess board is part of a Bayesian logic model for detecting nuclear
explosions from seismic signals.
Credits: Stan Honda/Getty (Kasparaov), Library of Congress (Bayes), NASA (Mars
rover), National Museum of Rome (Aristotle), Peter Norvig (book), Ian Parker (Berkeley
skyline), Shutterstock (Asimo, Chess pieces), Time Life/Getty (Shakey, Turing).

Acknowledgments
This book would not have been possible without the many contributors whose names did not
make it to the cover. Jitendra Malik and David Forsyth wrote Chapter 24 (computer vision)
and Sebastian Thrun wrote Chapter 25 (robotics). Vibhu Mittal wrote part of Chapter 22
(natural language). Nick Hay, Mehran Sahami, and Ernest Davis wrote some of the exercises.
Zoran Duric (George Mason), Thomas C. Henderson (Utah), Leon Reznik (RIT), Michael
Gourley (Central Oklahoma) and Ernest Davis (NYU) reviewed the manuscript and made
helpful suggestions. We thank Ernie Davis in particular for his tireless ability to read multiple
drafts and help improve the book. Nick Hay whipped the bibliography into shape and on
deadline stayed up to 5:30 AM writing code to make the book better. Jon Barron formatted
and improved the diagrams in this edition, while Tim Huang, Mark Paskin, and Cynthia
Bruyns helped with diagrams and algorithms in previous editions. Ravi Mohan and Ciaran
Oâ€™Reilly wrote and maintain the Java code examples on the Web site. John Canny wrote
the robotics chapter for the first edition and Douglas Edwards researched the historical notes.
Tracy Dunkelberger, Allison Michael, Scott Disanno, and Jane Bonnell at Pearson tried their
best to keep us on schedule and made many helpful suggestions. Most helpful of all has

x

Preface
been Julie Sussman, P. P. A ., who read every chapter and provided extensive improvements. In
previous editions we had proofreaders who would tell us when we left out a comma and said
which when we meant that; Julie told us when we left out a minus sign and said xi when we
meant xj . For every typo or confusing explanation that remains in the book, rest assured that
Julie has fixed at least five. She persevered even when a power failure forced her to work by
lantern light rather than LCD glow.
Stuart would like to thank his parents for their support and encouragement and his
wife, Loy Sheflott, for her endless patience and boundless wisdom. He hopes that Gordon,
Lucy, George, and Isaac will soon be reading this book after they have forgiven him for
working so long on it. RUGS (Russellâ€™s Unusual Group of Students) have been unusually
helpful, as always.
Peter would like to thank his parents (Torsten and Gerda) for getting him started,
and his wife (Kris), children (Bella and Juliet), colleagues, and friends for encouraging and
tolerating him through the long hours of writing and longer hours of rewriting.
We both thank the librarians at Berkeley, Stanford, and NASA and the developers of
CiteSeer, Wikipedia, and Google, who have revolutionized the way we do research. We canâ€™t
acknowledge all the people who have used the book and made suggestions, but we would like
to note the especially helpful comments of Gagan Aggarwal, Eyal Amir, Ion Androutsopoulos, Krzysztof Apt, Warren Haley Armstrong, Ellery Aziel, Jeff Van Baalen, Darius Bacon,
Brian Baker, Shumeet Baluja, Don Barker, Tony Barrett, James Newton Bass, Don Beal,
Howard Beck, Wolfgang Bibel, John Binder, Larry Bookman, David R. Boxall, Ronen Brafman, John Bresina, Gerhard Brewka, Selmer Bringsjord, Carla Brodley, Chris Brown, Emma
Brunskill, Wilhelm Burger, Lauren Burka, Carlos Bustamante, Joao Cachopo, Murray Campbell, Norman Carver, Emmanuel Castro, Anil Chakravarthy, Dan Chisarick, Berthe Choueiry,
Roberto Cipolla, David Cohen, James Coleman, Julie Ann Comparini, Corinna Cortes, Gary
Cottrell, Ernest Davis, Tom Dean, Rina Dechter, Tom Dietterich, Peter Drake, Chuck Dyer,
Doug Edwards, Robert Egginton, Asmaâ€™a El-Budrawy, Barbara Engelhardt, Kutluhan Erol,
Oren Etzioni, Hana Filip, Douglas Fisher, Jeffrey Forbes, Ken Ford, Eric Fosler-Lussier,
John Fosler, Jeremy Frank, Alex Franz, Bob Futrelle, Marek Galecki, Stefan Gerberding,
Stuart Gill, Sabine Glesner, Seth Golub, Gosta Grahne, Russ Greiner, Eric Grimson, Barbara Grosz, Larry Hall, Steve Hanks, Othar Hansson, Ernst Heinz, Jim Hendler, Christoph
Herrmann, Paul Hilfinger, Robert Holte, Vasant Honavar, Tim Huang, Seth Hutchinson, Joost
Jacob, Mark Jelasity, Magnus Johansson, Istvan Jonyer, Dan Jurafsky, Leslie Kaelbling, Keiji
Kanazawa, Surekha Kasibhatla, Simon Kasif, Henry Kautz, Gernot Kerschbaumer, Max
Khesin, Richard Kirby, Dan Klein, Kevin Knight, Roland Koenig, Sven Koenig, Daphne
Koller, Rich Korf, Benjamin Kuipers, James Kurien, John Lafferty, John Laird, Gus Larsson, John Lazzaro, Jon LeBlanc, Jason Leatherman, Frank Lee, Jon Lehto, Edward Lim,
Phil Long, Pierre Louveaux, Don Loveland, Sridhar Mahadevan, Tony Mancill, Jim Martin,
Andy Mayer, John McCarthy, David McGrane, Jay Mendelsohn, Risto Miikkulanien, Brian
Milch, Steve Minton, Vibhu Mittal, Mehryar Mohri, Leora Morgenstern, Stephen Muggleton,
Kevin Murphy, Ron Musick, Sung Myaeng, Eric Nadeau, Lee Naish, Pandu Nayak, Bernhard
Nebel, Stuart Nelson, XuanLong Nguyen, Nils Nilsson, Illah Nourbakhsh, Ali Nouri, Arthur
Nunes-Harwitt, Steve Omohundro, David Page, David Palmer, David Parkes, Ron Parr, Mark

Preface

xi
Paskin, Tony Passera, Amit Patel, Michael Pazzani, Fernando Pereira, Joseph Perla, Wim Pijls, Ira Pohl, Martha Pollack, David Poole, Bruce Porter, Malcolm Pradhan, Bill Pringle, Lorraine Prior, Greg Provan, William Rapaport, Deepak Ravichandran, Ioannis Refanidis, Philip
Resnik, Francesca Rossi, Sam Roweis, Richard Russell, Jonathan Schaeffer, Richard Scherl,
Hinrich Schuetze, Lars Schuster, Bart Selman, Soheil Shams, Stuart Shapiro, Jude Shavlik, Yoram Singer, Satinder Singh, Daniel Sleator, David Smith, Bryan So, Robert Sproull,
Lynn Stein, Larry Stephens, Andreas Stolcke, Paul Stradling, Devika Subramanian, Marek
Suchenek, Rich Sutton, Jonathan Tash, Austin Tate, Bas Terwijn, Olivier Teytaud, Michael
Thielscher, William Thompson, Sebastian Thrun, Eric Tiedemann, Mark Torrance, Randall
Upham, Paul Utgoff, Peter van Beek, Hal Varian, Paulina Varshavskaya, Sunil Vemuri, Vandi
Verma, Ubbo Visser, Jim Waldo, Toby Walsh, Bonnie Webber, Dan Weld, Michael Wellman,
Kamin Whitehouse, Michael Dean White, Brian Williams, David Wolfe, Jason Wolfe, Bill
Woods, Alden Wright, Jay Yagnik, Mark Yasuda, Richard Yen, Eliezer Yudkowsky, Weixiong
Zhang, Ming Zhao, Shlomo Zilberstein, and our esteemed colleague Anonymous Reviewer.

About the Authors
Stuart Russell was born in 1962 in Portsmouth, England. He received his B.A. with firstclass honours in physics from Oxford University in 1982, and his Ph.D. in computer science
from Stanford in 1986. He then joined the faculty of the University of California at Berkeley,
where he is a professor of computer science, director of the Center for Intelligent Systems,
and holder of the Smithâ€“Zadeh Chair in Engineering. In 1990, he received the Presidential
Young Investigator Award of the National Science Foundation, and in 1995 he was cowinner
of the Computers and Thought Award. He was a 1996 Miller Professor of the University of
California and was appointed to a Chancellorâ€™s Professorship in 2000. In 1998, he gave the
Forsythe Memorial Lectures at Stanford University. He is a Fellow and former Executive
Council member of the American Association for Artificial Intelligence. He has published
over 100 papers on a wide range of topics in artificial intelligence. His other books include
The Use of Knowledge in Analogy and Induction and (with Eric Wefald) Do the Right Thing:
Studies in Limited Rationality.
Peter Norvig is currently Director of Research at Google, Inc., and was the director responsible for the core Web search algorithms from 2002 to 2005. He is a Fellow of the American
Association for Artificial Intelligence and the Association for Computing Machinery. Previously, he was head of the Computational Sciences Division at NASA Ames Research Center,
where he oversaw NASAâ€™s research and development in artificial intelligence and robotics,
and chief scientist at Junglee, where he helped develop one of the first Internet information
extraction services. He received a B.S. in applied mathematics from Brown University and
a Ph.D. in computer science from the University of California at Berkeley. He received the
Distinguished Alumni and Engineering Innovation awards from Berkeley and the Exceptional
Achievement Medal from NASA. He has been a professor at the University of Southern California and a research faculty member at Berkeley. His other books are Paradigms of AI
Programming: Case Studies in Common Lisp and Verbmobil: A Translation System for Faceto-Face Dialog and Intelligent Help Systems for UNIX.

xii

Contents
I Artificial Intelligence
1 Introduction
1.1
What Is AI? . . . . . . . . . . . . . . . . . . . . . . . .
1.2
The Foundations of Artificial Intelligence . . . . . . . . .
1.3
The History of Artificial Intelligence . . . . . . . . . . .
1.4
The State of the Art . . . . . . . . . . . . . . . . . . . .
1.5
Summary, Bibliographical and Historical Notes, Exercises
2 Intelligent Agents
2.1
Agents and Environments . . . . . . . . . . . . . . . . .
2.2
Good Behavior: The Concept of Rationality . . . . . . .
2.3
The Nature of Environments . . . . . . . . . . . . . . . .
2.4
The Structure of Agents . . . . . . . . . . . . . . . . . .
2.5
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

1
1
5
16
28
29

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

34
34
36
40
46
59

3 Solving Problems by Searching
3.1
Problem-Solving Agents . . . . . . . . . . . . . . . . . .
3.2
Example Problems . . . . . . . . . . . . . . . . . . . . .
3.3
Searching for Solutions . . . . . . . . . . . . . . . . . .
3.4
Uninformed Search Strategies . . . . . . . . . . . . . . .
3.5
Informed (Heuristic) Search Strategies . . . . . . . . . .
3.6
Heuristic Functions . . . . . . . . . . . . . . . . . . . .
3.7
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

64
64
69
75
81
92
102
108

4 Beyond Classical Search
4.1
Local Search Algorithms and Optimization Problems . .
4.2
Local Search in Continuous Spaces . . . . . . . . . . . .
4.3
Searching with Nondeterministic Actions . . . . . . . . .
4.4
Searching with Partial Observations . . . . . . . . . . . .
4.5
Online Search Agents and Unknown Environments . . .
4.6
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

120
120
129
133
138
147
153

5 Adversarial Search
5.1
Games . . . . . . . . . . . . .
5.2
Optimal Decisions in Games .
5.3
Alphaâ€“Beta Pruning . . . . . .
5.4
Imperfect Real-Time Decisions
5.5
Stochastic Games . . . . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

161
161
163
167
171
177

II Problem-solving

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

xiii

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

xiv

Contents
5.6
5.7
5.8
5.9

Partially Observable Games . . . . . . . . . . . . . . . .
State-of-the-Art Game Programs . . . . . . . . . . . . .
Alternative Approaches . . . . . . . . . . . . . . . . . .
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

180
185
187
189

6 Constraint Satisfaction Problems
6.1
Defining Constraint Satisfaction Problems . . . . . . . .
6.2
Constraint Propagation: Inference in CSPs . . . . . . . .
6.3
Backtracking Search for CSPs . . . . . . . . . . . . . . .
6.4
Local Search for CSPs . . . . . . . . . . . . . . . . . . .
6.5
The Structure of Problems . . . . . . . . . . . . . . . . .
6.6
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

202
202
208
214
220
222
227

7 Logical Agents
7.1
Knowledge-Based Agents . . . . . . . . . . . . . . . . .
7.2
The Wumpus World . . . . . . . . . . . . . . . . . . . .
7.3
Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.4
Propositional Logic: A Very Simple Logic . . . . . . . .
7.5
Propositional Theorem Proving . . . . . . . . . . . . . .
7.6
Effective Propositional Model Checking . . . . . . . . .
7.7
Agents Based on Propositional Logic . . . . . . . . . . .
7.8
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

234
235
236
240
243
249
259
265
274

8 First-Order Logic
8.1
Representation Revisited . . . . . . . . . . . . . . . . .
8.2
Syntax and Semantics of First-Order Logic . . . . . . . .
8.3
Using First-Order Logic . . . . . . . . . . . . . . . . . .
8.4
Knowledge Engineering in First-Order Logic . . . . . . .
8.5
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

285
285
290
300
307
313

.
.
.
.
.
.

322
322
325
330
337
345
357

10 Classical Planning
10.1 Definition of Classical Planning . . . . . . . . . . . . . . . . . . . . . . .
10.2 Algorithms for Planning as State-Space Search . . . . . . . . . . . . . . .
10.3 Planning Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

366
366
373
379

III Knowledge, reasoning, and planning

9 Inference in First-Order Logic
9.1
Propositional vs. First-Order Inference . . . . . . . . . .
9.2
Unification and Lifting . . . . . . . . . . . . . . . . . .
9.3
Forward Chaining . . . . . . . . . . . . . . . . . . . . .
9.4
Backward Chaining . . . . . . . . . . . . . . . . . . . .
9.5
Resolution . . . . . . . . . . . . . . . . . . . . . . . . .
9.6
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

Contents

xv
10.4
10.5
10.6

Other Classical Planning Approaches . . . . . . . . . . . . . . . . . . . .
Analysis of Planning Approaches . . . . . . . . . . . . . . . . . . . . . .
Summary, Bibliographical and Historical Notes, Exercises . . . . . . . . .

11 Planning and Acting in the Real World
11.1 Time, Schedules, and Resources . . . . . . . . . . . . . .
11.2 Hierarchical Planning . . . . . . . . . . . . . . . . . . .
11.3 Planning and Acting in Nondeterministic Domains . . . .
11.4 Multiagent Planning . . . . . . . . . . . . . . . . . . . .
11.5 Summary, Bibliographical and Historical Notes, Exercises
12 Knowledge Representation
12.1 Ontological Engineering . . . . . . . . . . . . . . . . . .
12.2 Categories and Objects . . . . . . . . . . . . . . . . . .
12.3 Events . . . . . . . . . . . . . . . . . . . . . . . . . . .
12.4 Mental Events and Mental Objects . . . . . . . . . . . .
12.5 Reasoning Systems for Categories . . . . . . . . . . . .
12.6 Reasoning with Default Information . . . . . . . . . . .
12.7 The Internet Shopping World . . . . . . . . . . . . . . .
12.8 Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

387
392
393

.
.
.
.
.

401
401
406
415
425
430

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

437
437
440
446
450
453
458
462
467

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

480
480
483
490
494
495
499
503

.
.
.
.
.
.
.
.

510
510
513
518
522
530
539
546
551

15 Probabilistic Reasoning over Time
15.1 Time and Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . .

566
566

IV Uncertain knowledge and reasoning
13 Quantifying Uncertainty
13.1 Acting under Uncertainty . . . . . . . . . . . . . . . . .
13.2 Basic Probability Notation . . . . . . . . . . . . . . . . .
13.3 Inference Using Full Joint Distributions . . . . . . . . . .
13.4 Independence . . . . . . . . . . . . . . . . . . . . . . .
13.5 Bayesâ€™ Rule and Its Use . . . . . . . . . . . . . . . . . .
13.6 The Wumpus World Revisited . . . . . . . . . . . . . . .
13.7 Summary, Bibliographical and Historical Notes, Exercises
14 Probabilistic Reasoning
14.1 Representing Knowledge in an Uncertain Domain . . . .
14.2 The Semantics of Bayesian Networks . . . . . . . . . . .
14.3 Efficient Representation of Conditional Distributions . . .
14.4 Exact Inference in Bayesian Networks . . . . . . . . . .
14.5 Approximate Inference in Bayesian Networks . . . . . .
14.6 Relational and First-Order Probability Models . . . . . .
14.7 Other Approaches to Uncertain Reasoning . . . . . . . .
14.8 Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

xvi

Contents
15.2
15.3
15.4
15.5
15.6
15.7

Inference in Temporal Models . . . . . . . . . . . . . . .
Hidden Markov Models . . . . . . . . . . . . . . . . . .
Kalman Filters . . . . . . . . . . . . . . . . . . . . . . .
Dynamic Bayesian Networks . . . . . . . . . . . . . . .
Keeping Track of Many Objects . . . . . . . . . . . . . .
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.

570
578
584
590
599
603

.
.
.
.
.
.
.
.

610
610
611
615
622
626
628
633
636

.
.
.
.
.
.
.

645
645
652
656
658
666
679
684

.
.
.
.
.
.
.
.
.
.
.
.

693
693
695
697
708
713
717
727
737
744
748
753
757

19 Knowledge in Learning
19.1 A Logical Formulation of Learning . . . . . . . . . . . . . . . . . . . . .

768
768

16 Making Simple Decisions
16.1 Combining Beliefs and Desires under Uncertainty . . . .
16.2 The Basis of Utility Theory . . . . . . . . . . . . . . . .
16.3 Utility Functions . . . . . . . . . . . . . . . . . . . . . .
16.4 Multiattribute Utility Functions . . . . . . . . . . . . . .
16.5 Decision Networks . . . . . . . . . . . . . . . . . . . . .
16.6 The Value of Information . . . . . . . . . . . . . . . . .
16.7 Decision-Theoretic Expert Systems . . . . . . . . . . . .
16.8 Summary, Bibliographical and Historical Notes, Exercises
17 Making Complex Decisions
17.1 Sequential Decision Problems . . . . . . . . . . . . . . .
17.2 Value Iteration . . . . . . . . . . . . . . . . . . . . . . .
17.3 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . .
17.4 Partially Observable MDPs . . . . . . . . . . . . . . . .
17.5 Decisions with Multiple Agents: Game Theory . . . . . .
17.6 Mechanism Design . . . . . . . . . . . . . . . . . . . .
17.7 Summary, Bibliographical and Historical Notes, Exercises

V

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

Learning

18 Learning from Examples
18.1 Forms of Learning . . . . . . . . . . . . . . . . . . . . .
18.2 Supervised Learning . . . . . . . . . . . . . . . . . . . .
18.3 Learning Decision Trees . . . . . . . . . . . . . . . . . .
18.4 Evaluating and Choosing the Best Hypothesis . . . . . .
18.5 The Theory of Learning . . . . . . . . . . . . . . . . . .
18.6 Regression and Classification with Linear Models . . . .
18.7 Artificial Neural Networks . . . . . . . . . . . . . . . .
18.8 Nonparametric Models . . . . . . . . . . . . . . . . . .
18.9 Support Vector Machines . . . . . . . . . . . . . . . . .
18.10 Ensemble Learning . . . . . . . . . . . . . . . . . . . .
18.11 Practical Machine Learning . . . . . . . . . . . . . . . .
18.12 Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

Contents

xvii
19.2
19.3
19.4
19.5
19.6

Knowledge in Learning . . . . . . . . . . . . . . . . . .
Explanation-Based Learning . . . . . . . . . . . . . . .
Learning Using Relevance Information . . . . . . . . . .
Inductive Logic Programming . . . . . . . . . . . . . . .
Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

777
780
784
788
797

20 Learning Probabilistic Models
20.1 Statistical Learning . . . . . . . . . . . . . . . . . . . .
20.2 Learning with Complete Data . . . . . . . . . . . . . . .
20.3 Learning with Hidden Variables: The EM Algorithm . . .
20.4 Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

802
802
806
816
825

21 Reinforcement Learning
21.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . .
21.2 Passive Reinforcement Learning . . . . . . . . . . . . .
21.3 Active Reinforcement Learning . . . . . . . . . . . . . .
21.4 Generalization in Reinforcement Learning . . . . . . . .
21.5 Policy Search . . . . . . . . . . . . . . . . . . . . . . .
21.6 Applications of Reinforcement Learning . . . . . . . . .
21.7 Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

830
830
832
839
845
848
850
853

22 Natural Language Processing
22.1 Language Models . . . . . . . . . . . . . . . . . . . . .
22.2 Text Classification . . . . . . . . . . . . . . . . . . . . .
22.3 Information Retrieval . . . . . . . . . . . . . . . . . . .
22.4 Information Extraction . . . . . . . . . . . . . . . . . . .
22.5 Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

860
860
865
867
873
882

23 Natural Language for Communication
23.1 Phrase Structure Grammars . . . . . . . . . . . . . . . .
23.2 Syntactic Analysis (Parsing) . . . . . . . . . . . . . . . .
23.3 Augmented Grammars and Semantic Interpretation . . .
23.4 Machine Translation . . . . . . . . . . . . . . . . . . . .
23.5 Speech Recognition . . . . . . . . . . . . . . . . . . . .
23.6 Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

888
888
892
897
907
912
918

24 Perception
24.1 Image Formation . . . . . . . . . . . . . . . . .
24.2 Early Image-Processing Operations . . . . . . .
24.3 Object Recognition by Appearance . . . . . . .
24.4 Reconstructing the 3D World . . . . . . . . . .
24.5 Object Recognition from Structural Information

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

928
929
935
942
947
957

VI Communicating, perceiving, and acting

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

xviii

Contents
24.6
24.7

Using Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Summary, Bibliographical and Historical Notes, Exercises . . . . . . . . .

25 Robotics
25.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . .
25.2 Robot Hardware . . . . . . . . . . . . . . . . . . . . . .
25.3 Robotic Perception . . . . . . . . . . . . . . . . . . . . .
25.4 Planning to Move . . . . . . . . . . . . . . . . . . . . .
25.5 Planning Uncertain Movements . . . . . . . . . . . . . .
25.6 Moving . . . . . . . . . . . . . . . . . . . . . . . . . . .
25.7 Robotic Software Architectures . . . . . . . . . . . . . .
25.8 Application Domains . . . . . . . . . . . . . . . . . . .
25.9 Summary, Bibliographical and Historical Notes, Exercises

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

961
965

.
.
.
.
.
.
.
.
.

971
. 971
. 973
. 978
. 986
. 993
. 997
. 1003
. 1006
. 1010

.
.
.
.

.
.
.
.

1020
1020
1026
1034
1040

.
.
.
.

1044
1044
1047
1049
1051

VII Conclusions
26 Philosophical Foundations
26.1 Weak AI: Can Machines Act Intelligently? . . . . . . . .
26.2 Strong AI: Can Machines Really Think? . . . . . . . . .
26.3 The Ethics and Risks of Developing Artificial Intelligence
26.4 Summary, Bibliographical and Historical Notes, Exercises
27 AI: The Present and Future
27.1 Agent Components . . . . . . . . . .
27.2 Agent Architectures . . . . . . . . . .
27.3 Are We Going in the Right Direction?
27.4 What If AI Does Succeed? . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.

A Mathematical background
1053
A.1 Complexity Analysis and O() Notation . . . . . . . . . . . . . . . . . . . 1053
A.2 Vectors, Matrices, and Linear Algebra . . . . . . . . . . . . . . . . . . . 1055
A.3 Probability Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 1057
B Notes on Languages and Algorithms
B.1 Defining Languages with Backusâ€“Naur Form (BNF) . . . . . . . . . . . .
B.2 Describing Algorithms with Pseudocode . . . . . . . . . . . . . . . . . .
B.3 Online Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1060
1060
1061
1062

Bibliography

1063

Index

1095

1

INTRODUCTION

In which we try to explain why we consider artificial intelligence to be a subject
most worthy of study, and in which we try to decide what exactly it is, this being a
good thing to decide before embarking.

INTELLIGENCE

ARTIFICIAL
INTELLIGENCE

1.1

RATIONALITY

We call ourselves Homo sapiensâ€”man the wiseâ€”because our intelligence is so important
to us. For thousands of years, we have tried to understand how we think; that is, how a mere
handful of matter can perceive, understand, predict, and manipulate a world far larger and
more complicated than itself. The field of artificial intelligence, or AI, goes further still: it
attempts not just to understand but also to build intelligent entities.
AI is one of the newest fields in science and engineering. Work started in earnest soon
after World War II, and the name itself was coined in 1956. Along with molecular biology,
AI is regularly cited as the â€œfield I would most like to be inâ€ by scientists in other disciplines.
A student in physics might reasonably feel that all the good ideas have already been taken by
Galileo, Newton, Einstein, and the rest. AI, on the other hand, still has openings for several
full-time Einsteins and Edisons.
AI currently encompasses a huge variety of subfields, ranging from the general (learning
and perception) to the specific, such as playing chess, proving mathematical theorems, writing
poetry, driving a car on a crowded street, and diagnosing diseases. AI is relevant to any
intellectual task; it is truly a universal field.

W HAT I S AI?
We have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see
eight definitions of AI, laid out along two dimensions. The definitions on top are concerned
with thought processes and reasoning, whereas the ones on the bottom address behavior. The
definitions on the left measure success in terms of fidelity to human performance, whereas
the ones on the right measure against an ideal performance measure, called rationality. A
system is rational if it does the â€œright thing,â€ given what it knows.
Historically, all four approaches to AI have been followed, each by different people
with different methods. A human-centered approach must be in part an empirical science, in1

2

Chapter 1.

Introduction

Thinking Humanly

Thinking Rationally

â€œThe exciting new effort to make computers think . . . machines with minds, in the
full and literal sense.â€ (Haugeland, 1985)

â€œThe study of mental faculties through the
use of computational models.â€
(Charniak and McDermott, 1985)

â€œ[The automation of] activities that we
associate with human thinking, activities
such as decision-making, problem solving, learning . . .â€ (Bellman, 1978)

â€œThe study of the computations that make
it possible to perceive, reason, and act.â€
(Winston, 1992)

Acting Humanly

Acting Rationally

â€œThe art of creating machines that perform functions that require intelligence
when performed by people.â€ (Kurzweil,
1990)

â€œComputational Intelligence is the study
of the design of intelligent agents.â€ (Poole
et al., 1998)

â€œThe study of how to make computers do
things at which, at the moment, people are
better.â€ (Rich and Knight, 1991)

â€œAI . . . is concerned with intelligent behavior in artifacts.â€ (Nilsson, 1998)

Figure 1.1

Some definitions of artificial intelligence, organized into four categories.

volving observations and hypotheses about human behavior. A rationalist1 approach involves
a combination of mathematics and engineering. The various group have both disparaged and
helped each other. Let us look at the four approaches in more detail.

1.1.1 Acting humanly: The Turing Test approach
TURING TEST

NATURAL LANGUAGE
PROCESSING
KNOWLEDGE
REPRESENTATION
AUTOMATED
REASONING

MACHINE LEARNING

The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory
operational definition of intelligence. A computer passes the test if a human interrogator, after
posing some written questions, cannot tell whether the written responses come from a person
or from a computer. Chapter 26 discusses the details of the test and whether a computer would
really be intelligent if it passed. For now, we note that programming a computer to pass a
rigorously applied test provides plenty to work on. The computer would need to possess the
following capabilities:
â€¢ natural language processing to enable it to communicate successfully in English;
â€¢ knowledge representation to store what it knows or hears;
â€¢ automated reasoning to use the stored information to answer questions and to draw
new conclusions;
â€¢ machine learning to adapt to new circumstances and to detect and extrapolate patterns.
1 By distinguishing between human and rational behavior, we are not suggesting that humans are necessarily
â€œirrationalâ€ in the sense of â€œemotionally unstableâ€ or â€œinsane.â€ One merely need note that we are not perfect:
not all chess players are grandmasters; and, unfortunately, not everyone gets an A on the exam. Some systematic
errors in human reasoning are cataloged by Kahneman et al. (1982).

Section 1.1.

TOTAL TURING TEST

What Is AI?

3

Turingâ€™s test deliberately avoided direct physical interaction between the interrogator and the
computer, because physical simulation of a person is unnecessary for intelligence. However,
the so-called total Turing Test includes a video signal so that the interrogator can test the
subjectâ€™s perceptual abilities, as well as the opportunity for the interrogator to pass physical
objects â€œthrough the hatch.â€ To pass the total Turing Test, the computer will need

COMPUTER VISION

â€¢ computer vision to perceive objects, and

ROBOTICS

â€¢ robotics to manipulate objects and move about.
These six disciplines compose most of AI, and Turing deserves credit for designing a test
that remains relevant 60 years later. Yet AI researchers have devoted little effort to passing
the Turing Test, believing that it is more important to study the underlying principles of intelligence than to duplicate an exemplar. The quest for â€œartificial flightâ€ succeeded when the
Wright brothers and others stopped imitating birds and started using wind tunnels and learning about aerodynamics. Aeronautical engineering texts do not define the goal of their field
as making â€œmachines that fly so exactly like pigeons that they can fool even other pigeons.â€

1.1.2 Thinking humanly: The cognitive modeling approach

COGNITIVE SCIENCE

If we are going to say that a given program thinks like a human, we must have some way of
determining how humans think. We need to get inside the actual workings of human minds.
There are three ways to do this: through introspectionâ€”trying to catch our own thoughts as
they go by; through psychological experimentsâ€”observing a person in action; and through
brain imagingâ€”observing the brain in action. Once we have a sufficiently precise theory of
the mind, it becomes possible to express the theory as a computer program. If the programâ€™s
inputâ€“output behavior matches corresponding human behavior, that is evidence that some of
the programâ€™s mechanisms could also be operating in humans. For example, Allen Newell
and Herbert Simon, who developed GPS, the â€œGeneral Problem Solverâ€ (Newell and Simon,
1961), were not content merely to have their program solve problems correctly. They were
more concerned with comparing the trace of its reasoning steps to traces of human subjects
solving the same problems. The interdisciplinary field of cognitive science brings together
computer models from AI and experimental techniques from psychology to construct precise
and testable theories of the human mind.
Cognitive science is a fascinating field in itself, worthy of several textbooks and at least
one encyclopedia (Wilson and Keil, 1999). We will occasionally comment on similarities or
differences between AI techniques and human cognition. Real cognitive science, however, is
necessarily based on experimental investigation of actual humans or animals. We will leave
that for other books, as we assume the reader has only a computer for experimentation.
In the early days of AI there was often confusion between the approaches: an author
would argue that an algorithm performs well on a task and that it is therefore a good model
of human performance, or vice versa. Modern authors separate the two kinds of claims;
this distinction has allowed both AI and cognitive science to develop more rapidly. The two
fields continue to fertilize each other, most notably in computer vision, which incorporates
neurophysiological evidence into computational models.

4

Chapter 1.

Introduction

1.1.3 Thinking rationally: The â€œlaws of thoughtâ€ approach
SYLLOGISM

LOGIC

LOGICIST

The Greek philosopher Aristotle was one of the first to attempt to codify â€œright thinking,â€ that
is, irrefutable reasoning processes. His syllogisms provided patterns for argument structures
that always yielded correct conclusions when given correct premisesâ€”for example, â€œSocrates
is a man; all men are mortal; therefore, Socrates is mortal.â€ These laws of thought were
supposed to govern the operation of the mind; their study initiated the field called logic.
Logicians in the 19th century developed a precise notation for statements about all kinds
of objects in the world and the relations among them. (Contrast this with ordinary arithmetic
notation, which provides only for statements about numbers.) By 1965, programs existed
that could, in principle, solve any solvable problem described in logical notation. (Although
if no solution exists, the program might loop forever.) The so-called logicist tradition within
artificial intelligence hopes to build on such programs to create intelligent systems.
There are two main obstacles to this approach. First, it is not easy to take informal
knowledge and state it in the formal terms required by logical notation, particularly when
the knowledge is less than 100% certain. Second, there is a big difference between solving
a problem â€œin principleâ€ and solving it in practice. Even problems with just a few hundred
facts can exhaust the computational resources of any computer unless it has some guidance
as to which reasoning steps to try first. Although both of these obstacles apply to any attempt
to build computational reasoning systems, they appeared first in the logicist tradition.

1.1.4 Acting rationally: The rational agent approach
AGENT

RATIONAL AGENT

An agent is just something that acts (agent comes from the Latin agere, to do). Of course,
all computer programs do something, but computer agents are expected to do more: operate
autonomously, perceive their environment, persist over a prolonged time period, adapt to
change, and create and pursue goals. A rational agent is one that acts so as to achieve the
best outcome or, when there is uncertainty, the best expected outcome.
In the â€œlaws of thoughtâ€ approach to AI, the emphasis was on correct inferences. Making correct inferences is sometimes part of being a rational agent, because one way to act
rationally is to reason logically to the conclusion that a given action will achieve oneâ€™s goals
and then to act on that conclusion. On the other hand, correct inference is not all of rationality; in some situations, there is no provably correct thing to do, but something must still be
done. There are also ways of acting rationally that cannot be said to involve inference. For
example, recoiling from a hot stove is a reflex action that is usually more successful than a
slower action taken after careful deliberation.
All the skills needed for the Turing Test also allow an agent to act rationally. Knowledge
representation and reasoning enable agents to reach good decisions. We need to be able to
generate comprehensible sentences in natural language to get by in a complex society. We
need learning not only for erudition, but also because it improves our ability to generate
effective behavior.
The rational-agent approach has two advantages over the other approaches. First, it
is more general than the â€œlaws of thoughtâ€ approach because correct inference is just one
of several possible mechanisms for achieving rationality. Second, it is more amenable to

Section 1.2.

LIMITED
RATIONALITY

1.2

The Foundations of Artificial Intelligence

5

scientific development than are approaches based on human behavior or human thought. The
standard of rationality is mathematically well defined and completely general, and can be
â€œunpackedâ€ to generate agent designs that provably achieve it. Human behavior, on the other
hand, is well adapted for one specific environment and is defined by, well, the sum total
of all the things that humans do. This book therefore concentrates on general principles
of rational agents and on components for constructing them. We will see that despite the
apparent simplicity with which the problem can be stated, an enormous variety of issues
come up when we try to solve it. Chapter 2 outlines some of these issues in more detail.
One important point to keep in mind: We will see before too long that achieving perfect
rationalityâ€”always doing the right thingâ€”is not feasible in complicated environments. The
computational demands are just too high. For most of the book, however, we will adopt the
working hypothesis that perfect rationality is a good starting point for analysis. It simplifies
the problem and provides the appropriate setting for most of the foundational material in
the field. Chapters 5 and 17 deal explicitly with the issue of limited rationalityâ€”acting
appropriately when there is not enough time to do all the computations one might like.

T HE F OUNDATIONS OF A RTIFICIAL I NTELLIGENCE
In this section, we provide a brief history of the disciplines that contributed ideas, viewpoints,
and techniques to AI. Like any history, this one is forced to concentrate on a small number
of people, events, and ideas and to ignore others that also were important. We organize the
history around a series of questions. We certainly would not wish to give the impression that
these questions are the only ones the disciplines address or that the disciplines have all been
working toward AI as their ultimate fruition.

1.2.1 Philosophy
â€¢
â€¢
â€¢
â€¢

Can formal rules be used to draw valid conclusions?
How does the mind arise from a physical brain?
Where does knowledge come from?
How does knowledge lead to action?

Aristotle (384â€“322 B . C .), whose bust appears on the front cover of this book, was the first
to formulate a precise set of laws governing the rational part of the mind. He developed an
informal system of syllogisms for proper reasoning, which in principle allowed one to generate conclusions mechanically, given initial premises. Much later, Ramon Lull (d. 1315) had
the idea that useful reasoning could actually be carried out by a mechanical artifact. Thomas
Hobbes (1588â€“1679) proposed that reasoning was like numerical computation, that â€œwe add
and subtract in our silent thoughts.â€ The automation of computation itself was already well
under way. Around 1500, Leonardo da Vinci (1452â€“1519) designed but did not build a mechanical calculator; recent reconstructions have shown the design to be functional. The first
known calculating machine was constructed around 1623 by the German scientist Wilhelm
Schickard (1592â€“1635), although the Pascaline, built in 1642 by Blaise Pascal (1623â€“1662),

6

RATIONALISM
DUALISM

MATERIALISM

EMPIRICISM

INDUCTION

LOGICAL POSITIVISM
OBSERVATION
SENTENCES
CONFIRMATION
THEORY

Chapter 1.

Introduction

is more famous. Pascal wrote that â€œthe arithmetical machine produces effects which appear
nearer to thought than all the actions of animals.â€ Gottfried Wilhelm Leibniz (1646â€“1716)
built a mechanical device intended to carry out operations on concepts rather than numbers,
but its scope was rather limited. Leibniz did surpass Pascal by building a calculator that
could add, subtract, multiply, and take roots, whereas the Pascaline could only add and subtract. Some speculated that machines might not just do calculations but actually be able to
think and act on their own. In his 1651 book Leviathan, Thomas Hobbes suggested the idea
of an â€œartificial animal,â€ arguing â€œFor what is the heart but a spring; and the nerves, but so
many strings; and the joints, but so many wheels.â€
Itâ€™s one thing to say that the mind operates, at least in part, according to logical rules, and
to build physical systems that emulate some of those rules; itâ€™s another to say that the mind
itself is such a physical system. ReneÌ Descartes (1596â€“1650) gave the first clear discussion
of the distinction between mind and matter and of the problems that arise. One problem with
a purely physical conception of the mind is that it seems to leave little room for free will:
if the mind is governed entirely by physical laws, then it has no more free will than a rock
â€œdecidingâ€ to fall toward the center of the earth. Descartes was a strong advocate of the power
of reasoning in understanding the world, a philosophy now called rationalism, and one that
counts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.
He held that there is a part of the human mind (or soul or spirit) that is outside of nature,
exempt from physical laws. Animals, on the other hand, did not possess this dual quality;
they could be treated as machines. An alternative to dualism is materialism, which holds
that the brainâ€™s operation according to the laws of physics constitutes the mind. Free will is
simply the way that the perception of available choices appears to the choosing entity.
Given a physical mind that manipulates knowledge, the next problem is to establish
the source of knowledge. The empiricism movement, starting with Francis Baconâ€™s (1561â€“
1626) Novum Organum,2 is characterized by a dictum of John Locke (1632â€“1704): â€œNothing
is in the understanding, which was not first in the senses.â€ David Humeâ€™s (1711â€“1776) A
Treatise of Human Nature (Hume, 1739) proposed what is now known as the principle of
induction: that general rules are acquired by exposure to repeated associations between their
elements. Building on the work of Ludwig Wittgenstein (1889â€“1951) and Bertrand Russell
(1872â€“1970), the famous Vienna Circle, led by Rudolf Carnap (1891â€“1970), developed the
doctrine of logical positivism. This doctrine holds that all knowledge can be characterized by
logical theories connected, ultimately, to observation sentences that correspond to sensory
inputs; thus logical positivism combines rationalism and empiricism.3 The confirmation theory of Carnap and Carl Hempel (1905â€“1997) attempted to analyze the acquisition of knowledge from experience. Carnapâ€™s book The Logical Structure of the World (1928) defined an
explicit computational procedure for extracting knowledge from elementary experiences. It
was probably the first theory of mind as a computational process.
2

The Novum Organum is an update of Aristotleâ€™s Organon, or instrument of thought. Thus Aristotle can be
seen as both an empiricist and a rationalist.
3 In this picture, all meaningful statements can be verified or falsified either by experimentation or by analysis
of the meaning of the words. Because this rules out most of metaphysics, as was the intention, logical positivism
was unpopular in some circles.

Section 1.2.

The Foundations of Artificial Intelligence

7

The final element in the philosophical picture of the mind is the connection between
knowledge and action. This question is vital to AI because intelligence requires action as well
as reasoning. Moreover, only by understanding how actions are justified can we understand
how to build an agent whose actions are justifiable (or rational). Aristotle argued (in De Motu
Animalium) that actions are justified by a logical connection between goals and knowledge of
the actionâ€™s outcome (the last part of this extract also appears on the front cover of this book,
in the original Greek):
But how does it happen that thinking is sometimes accompanied by action and sometimes
not, sometimes by motion, and sometimes not? It looks as if almost the same thing
happens as in the case of reasoning and making inferences about unchanging objects. But
in that case the end is a speculative proposition . . . whereas here the conclusion which
results from the two premises is an action. . . . I need covering; a cloak is a covering. I
need a cloak. What I need, I have to make; I need a cloak. I have to make a cloak. And
the conclusion, the â€œI have to make a cloak,â€ is an action.

In the Nicomachean Ethics (Book III. 3, 1112b), Aristotle further elaborates on this topic,
suggesting an algorithm:
We deliberate not about ends, but about means. For a doctor does not deliberate whether
he shall heal, nor an orator whether he shall persuade, . . . They assume the end and
consider how and by what means it is attained, and if it seems easily and best produced
thereby; while if it is achieved by one means only they consider how it will be achieved
by this and by what means this will be achieved, till they come to the first cause, . . . and
what is last in the order of analysis seems to be first in the order of becoming. And if we
come on an impossibility, we give up the search, e.g., if we need money and this cannot
be got; but if a thing appears possible we try to do it.

Aristotleâ€™s algorithm was implemented 2300 years later by Newell and Simon in their GPS
program. We would now call it a regression planning system (see Chapter 10).
Goal-based analysis is useful, but does not say what to do when several actions will
achieve the goal or when no action will achieve it completely. Antoine Arnauld (1612â€“1694)
correctly described a quantitative formula for deciding what action to take in cases like this
(see Chapter 16). John Stuart Millâ€™s (1806â€“1873) book Utilitarianism (Mill, 1863) promoted
the idea of rational decision criteria in all spheres of human activity. The more formal theory
of decisions is discussed in the following section.

1.2.2 Mathematics
â€¢ What are the formal rules to draw valid conclusions?
â€¢ What can be computed?
â€¢ How do we reason with uncertain information?
Philosophers staked out some of the fundamental ideas of AI, but the leap to a formal science
required a level of mathematical formalization in three fundamental areas: logic, computation, and probability.
The idea of formal logic can be traced back to the philosophers of ancient Greece, but
its mathematical development really began with the work of George Boole (1815â€“1864), who

8

ALGORITHM

INCOMPLETENESS
THEOREM

COMPUTABLE

TRACTABILITY

NP-COMPLETENESS

Chapter 1.

Introduction

worked out the details of propositional, or Boolean, logic (Boole, 1847). In 1879, Gottlob
Frege (1848â€“1925) extended Booleâ€™s logic to include objects and relations, creating the firstorder logic that is used today.4 Alfred Tarski (1902â€“1983) introduced a theory of reference
that shows how to relate the objects in a logic to objects in the real world.
The next step was to determine the limits of what could be done with logic and computation. The first nontrivial algorithm is thought to be Euclidâ€™s algorithm for computing
greatest common divisors. The word algorithm (and the idea of studying them) comes from
al-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introduced
Arabic numerals and algebra to Europe. Boole and others discussed algorithms for logical
deduction, and, by the late 19th century, efforts were under way to formalize general mathematical reasoning as logical deduction. In 1930, Kurt GoÌˆdel (1906â€“1978) showed that there
exists an effective procedure to prove any true statement in the first-order logic of Frege and
Russell, but that first-order logic could not capture the principle of mathematical induction
needed to characterize the natural numbers. In 1931, GoÌˆdel showed that limits on deduction do exist. His incompleteness theorem showed that in any formal theory as strong as
Peano arithmetic (the elementary theory of natural numbers), there are true statements that
are undecidable in the sense that they have no proof within the theory.
This fundamental result can also be interpreted as showing that some functions on the
integers cannot be represented by an algorithmâ€”that is, they cannot be computed. This
motivated Alan Turing (1912â€“1954) to try to characterize exactly which functions are computableâ€”capable of being computed. This notion is actually slightly problematic because
the notion of a computation or effective procedure really cannot be given a formal definition.
However, the Churchâ€“Turing thesis, which states that the Turing machine (Turing, 1936) is
capable of computing any computable function, is generally accepted as providing a sufficient
definition. Turing also showed that there were some functions that no Turing machine can
compute. For example, no machine can tell in general whether a given program will return
an answer on a given input or run forever.
Although decidability and computability are important to an understanding of computation, the notion of tractability has had an even greater impact. Roughly speaking, a problem
is called intractable if the time required to solve instances of the problem grows exponentially
with the size of the instances. The distinction between polynomial and exponential growth
in complexity was first emphasized in the mid-1960s (Cobham, 1964; Edmonds, 1965). It is
important because exponential growth means that even moderately large instances cannot be
solved in any reasonable time. Therefore, one should strive to divide the overall problem of
generating intelligent behavior into tractable subproblems rather than intractable ones.
How can one recognize an intractable problem? The theory of NP-completeness, pioneered by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp
showed the existence of large classes of canonical combinatorial search and reasoning problems that are NP-complete. Any problem class to which the class of NP-complete problems
can be reduced is likely to be intractable. (Although it has not been proved that NP-complete
4

Fregeâ€™s proposed notation for first-order logicâ€”an arcane combination of textual and geometric featuresâ€”
never became popular.

Section 1.2.

PROBABILITY

The Foundations of Artificial Intelligence

9

problems are necessarily intractable, most theoreticians believe it.) These results contrast
with the optimism with which the popular press greeted the first computersâ€”â€œElectronic
Super-Brainsâ€ that were â€œFaster than Einstein!â€ Despite the increasing speed of computers,
careful use of resources will characterize intelligent systems. Put crudely, the world is an
extremely large problem instance! Work in AI has helped explain why some instances of
NP-complete problems are hard, yet others are easy (Cheeseman et al., 1991).
Besides logic and computation, the third great contribution of mathematics to AI is the
theory of probability. The Italian Gerolamo Cardano (1501â€“1576) first framed the idea of
probability, describing it in terms of the possible outcomes of gambling events. In 1654,
Blaise Pascal (1623â€“1662), in a letter to Pierre Fermat (1601â€“1665), showed how to predict the future of an unfinished gambling game and assign average payoffs to the gamblers.
Probability quickly became an invaluable part of all the quantitative sciences, helping to deal
with uncertain measurements and incomplete theories. James Bernoulli (1654â€“1705), Pierre
Laplace (1749â€“1827), and others advanced the theory and introduced new statistical methods. Thomas Bayes (1702â€“1761), who appears on the front cover of this book, proposed
a rule for updating probabilities in the light of new evidence. Bayesâ€™ rule underlies most
modern approaches to uncertain reasoning in AI systems.

1.2.3 Economics
â€¢ How should we make decisions so as to maximize payoff?
â€¢ How should we do this when others may not go along?
â€¢ How should we do this when the payoff may be far in the future?

UTILITY

DECISION THEORY

GAME THEORY

The science of economics got its start in 1776, when Scottish philosopher Adam Smith
(1723â€“1790) published An Inquiry into the Nature and Causes of the Wealth of Nations.
While the ancient Greeks and others had made contributions to economic thought, Smith was
the first to treat it as a science, using the idea that economies can be thought of as consisting of individual agents maximizing their own economic well-being. Most people think of
economics as being about money, but economists will say that they are really studying how
people make choices that lead to preferred outcomes. When McDonaldâ€™s offers a hamburger
for a dollar, they are asserting that they would prefer the dollar and hoping that customers will
prefer the hamburger. The mathematical treatment of â€œpreferred outcomesâ€ or utility was
first formalized by LeÌon Walras (pronounced â€œValrasseâ€) (1834-1910) and was improved by
Frank Ramsey (1931) and later by John von Neumann and Oskar Morgenstern in their book
The Theory of Games and Economic Behavior (1944).
Decision theory, which combines probability theory with utility theory, provides a formal and complete framework for decisions (economic or otherwise) made under uncertaintyâ€”
that is, in cases where probabilistic descriptions appropriately capture the decision makerâ€™s
environment. This is suitable for â€œlargeâ€ economies where each agent need pay no attention
to the actions of other agents as individuals. For â€œsmallâ€ economies, the situation is much
more like a game: the actions of one player can significantly affect the utility of another
(either positively or negatively). Von Neumann and Morgensternâ€™s development of game
theory (see also Luce and Raiffa, 1957) included the surprising result that, for some games,

10

OPERATIONS
RESEARCH

SATISFICING

Chapter 1.

Introduction

a rational agent should adopt policies that are (or least appear to be) randomized. Unlike decision theory, game theory does not offer an unambiguous prescription for selecting actions.
For the most part, economists did not address the third question listed above, namely,
how to make rational decisions when payoffs from actions are not immediate but instead result from several actions taken in sequence. This topic was pursued in the field of operations
research, which emerged in World War II from efforts in Britain to optimize radar installations, and later found civilian applications in complex management decisions. The work of
Richard Bellman (1957) formalized a class of sequential decision problems called Markov
decision processes, which we study in Chapters 17 and 21.
Work in economics and operations research has contributed much to our notion of rational agents, yet for many years AI research developed along entirely separate paths. One
reason was the apparent complexity of making rational decisions. The pioneering AI researcher Herbert Simon (1916â€“2001) won the Nobel Prize in economics in 1978 for his early
work showing that models based on satisficingâ€”making decisions that are â€œgood enough,â€
rather than laboriously calculating an optimal decisionâ€”gave a better description of actual
human behavior (Simon, 1947). Since the 1990s, there has been a resurgence of interest in
decision-theoretic techniques for agent systems (Wellman, 1995).

1.2.4 Neuroscience
â€¢ How do brains process information?
NEUROSCIENCE

NEURON

Neuroscience is the study of the nervous system, particularly the brain. Although the exact
way in which the brain enables thought is one of the great mysteries of science, the fact that it
does enable thought has been appreciated for thousands of years because of the evidence that
strong blows to the head can lead to mental incapacitation. It has also long been known that
human brains are somehow different; in about 335 B . C . Aristotle wrote, â€œOf all the animals,
man has the largest brain in proportion to his size.â€ 5 Still, it was not until the middle of the
18th century that the brain was widely recognized as the seat of consciousness. Before then,
candidate locations included the heart and the spleen.
Paul Brocaâ€™s (1824â€“1880) study of aphasia (speech deficit) in brain-damaged patients
in 1861 demonstrated the existence of localized areas of the brain responsible for specific
cognitive functions. In particular, he showed that speech production was localized to the
portion of the left hemisphere now called Brocaâ€™s area. 6 By that time, it was known that
the brain consisted of nerve cells, or neurons, but it was not until 1873 that Camillo Golgi
(1843â€“1926) developed a staining technique allowing the observation of individual neurons
in the brain (see Figure 1.2). This technique was used by Santiago Ramon y Cajal (1852â€“
1934) in his pioneering studies of the brainâ€™s neuronal structures.7 Nicolas Rashevsky (1936,
1938) was the first to apply mathematical models to the study of the nervous sytem.
5

Since then, it has been discovered that the tree shrew (Scandentia) has a higher ratio of brain to body mass.
Many cite Alexander Hood (1824) as a possible prior source.
7 Golgi persisted in his belief that the brainâ€™s functions were carried out primarily in a continuous medium in
which neurons were embedded, whereas Cajal propounded the â€œneuronal doctrine.â€ The two shared the Nobel
prize in 1906 but gave mutually antagonistic acceptance speeches.
6

Section 1.2.

The Foundations of Artificial Intelligence

11

Axonal arborization
Axon from another cell
Synapse
Dendrite

Axon

Nucleus
Synapses
Cell body or Soma

Figure 1.2 The parts of a nerve cell or neuron. Each neuron consists of a cell body,
or soma, that contains a cell nucleus. Branching out from the cell body are a number of
fibers called dendrites and a single long fiber called the axon. The axon stretches out for a
long distance, much longer than the scale in this diagram indicates. Typically, an axon is
1 cm long (100 times the diameter of the cell body), but can reach up to 1 meter. A neuron
makes connections with 10 to 100,000 other neurons at junctions called synapses. Signals are
propagated from neuron to neuron by a complicated electrochemical reaction. The signals
control brain activity in the short term and also enable long-term changes in the connectivity
of neurons. These mechanisms are thought to form the basis for learning in the brain. Most
information processing goes on in the cerebral cortex, the outer layer of the brain. The basic
organizational unit appears to be a column of tissue about 0.5 mm in diameter, containing
about 20,000 neurons and extending the full depth of the cortex about 4 mm in humans).

We now have some data on the mapping between areas of the brain and the parts of the
body that they control or from which they receive sensory input. Such mappings are able to
change radically over the course of a few weeks, and some animals seem to have multiple
maps. Moreover, we do not fully understand how other areas can take over functions when
one area is damaged. There is almost no theory on how an individual memory is stored.
The measurement of intact brain activity began in 1929 with the invention by Hans
Berger of the electroencephalograph (EEG). The recent development of functional magnetic
resonance imaging (fMRI) (Ogawa et al., 1990; Cabeza and Nyberg, 2001) is giving neuroscientists unprecedentedly detailed images of brain activity, enabling measurements that
correspond in interesting ways to ongoing cognitive processes. These are augmented by
advances in single-cell recording of neuron activity. Individual neurons can be stimulated
electrically, chemically, or even optically (Han and Boyden, 2007), allowing neuronal inputâ€“
output relationships to be mapped. Despite these advances, we are still a long way from
understanding how cognitive processes actually work.
The truly amazing conclusion is that a collection of simple cells can lead to thought,
action, and consciousness or, in the pithy words of John Searle (1992), brains cause minds.

12

Chapter 1.
Supercomputer

Personal Computer

104 CPUs, 1012 transistors 4 CPUs, 109 transistors
1014 bits RAM
1011 bits RAM
15
10 bits disk
1013 bits disk
Cycle time
10âˆ’9 sec
10âˆ’9 sec
15
Operations/sec
10
1010
Memory updates/sec 1014
1010
Computational units
Storage units

Introduction
Human Brain
1011 neurons
1011 neurons
1014 synapses
10âˆ’3 sec
1017
1014

Figure 1.3 A crude comparison of the raw computational resources available to the IBM
B LUE G ENE supercomputer, a typical personal computer of 2008, and the human brain. The
brainâ€™s numbers are essentially fixed, whereas the supercomputerâ€™s numbers have been increasing by a factor of 10 every 5 years or so, allowing it to achieve rough parity with the
brain. The personal computer lags behind on all metrics except cycle time.

SINGULARITY

The only real alternative theory is mysticism: that minds operate in some mystical realm that
is beyond physical science.
Brains and digital computers have somewhat different properties. Figure 1.3 shows that
computers have a cycle time that is a million times faster than a brain. The brain makes up
for that with far more storage and interconnection than even a high-end personal computer,
although the largest supercomputers have a capacity that is similar to the brainâ€™s. (It should
be noted, however, that the brain does not seem to use all of its neurons simultaneously.)
Futurists make much of these numbers, pointing to an approaching singularity at which
computers reach a superhuman level of performance (Vinge, 1993; Kurzweil, 2005), but the
raw comparisons are not especially informative. Even with a computer of virtually unlimited
capacity, we still would not know how to achieve the brainâ€™s level of intelligence.

1.2.5 Psychology
â€¢ How do humans and animals think and act?

BEHAVIORISM

The origins of scientific psychology are usually traced to the work of the German physicist Hermann von Helmholtz (1821â€“1894) and his student Wilhelm Wundt (1832â€“1920).
Helmholtz applied the scientific method to the study of human vision, and his Handbook
of Physiological Optics is even now described as â€œthe single most important treatise on the
physics and physiology of human visionâ€ (Nalwa, 1993, p.15). In 1879, Wundt opened the
first laboratory of experimental psychology, at the University of Leipzig. Wundt insisted
on carefully controlled experiments in which his workers would perform a perceptual or associative task while introspecting on their thought processes. The careful controls went a
long way toward making psychology a science, but the subjective nature of the data made
it unlikely that an experimenter would ever disconfirm his or her own theories. Biologists
studying animal behavior, on the other hand, lacked introspective data and developed an objective methodology, as described by H. S. Jennings (1906) in his influential work Behavior of
the Lower Organisms. Applying this viewpoint to humans, the behaviorism movement, led
by John Watson (1878â€“1958), rejected any theory involving mental processes on the grounds

Section 1.2.

COGNITIVE
PSYCHOLOGY

The Foundations of Artificial Intelligence

13

that introspection could not provide reliable evidence. Behaviorists insisted on studying only
objective measures of the percepts (or stimulus) given to an animal and its resulting actions
(or response). Behaviorism discovered a lot about rats and pigeons but had less success at
understanding humans.
Cognitive psychology, which views the brain as an information-processing device,
can be traced back at least to the works of William James (1842â€“1910). Helmholtz also
insisted that perception involved a form of unconscious logical inference. The cognitive
viewpoint was largely eclipsed by behaviorism in the United States, but at Cambridgeâ€™s Applied Psychology Unit, directed by Frederic Bartlett (1886â€“1969), cognitive modeling was
able to flourish. The Nature of Explanation, by Bartlettâ€™s student and successor Kenneth
Craik (1943), forcefully reestablished the legitimacy of such â€œmentalâ€ terms as beliefs and
goals, arguing that they are just as scientific as, say, using pressure and temperature to talk
about gases, despite their being made of molecules that have neither. Craik specified the
three key steps of a knowledge-based agent: (1) the stimulus must be translated into an internal representation, (2) the representation is manipulated by cognitive processes to derive new
internal representations, and (3) these are in turn retranslated back into action. He clearly
explained why this was a good design for an agent:
If the organism carries a â€œsmall-scale modelâ€ of external reality and of its own possible
actions within its head, it is able to try out various alternatives, conclude which is the best
of them, react to future situations before they arise, utilize the knowledge of past events
in dealing with the present and future, and in every way to react in a much fuller, safer,
and more competent manner to the emergencies which face it. (Craik, 1943)

After Craikâ€™s death in a bicycle accident in 1945, his work was continued by Donald Broadbent, whose book Perception and Communication (1958) was one of the first works to model
psychological phenomena as information processing. Meanwhile, in the United States, the
development of computer modeling led to the creation of the field of cognitive science. The
field can be said to have started at a workshop in September 1956 at MIT. (We shall see that
this is just two months after the conference at which AI itself was â€œborn.â€) At the workshop,
George Miller presented The Magic Number Seven, Noam Chomsky presented Three Models
of Language, and Allen Newell and Herbert Simon presented The Logic Theory Machine.
These three influential papers showed how computer models could be used to address the
psychology of memory, language, and logical thinking, respectively. It is now a common
(although far from universal) view among psychologists that â€œa cognitive theory should be
like a computer programâ€ (Anderson, 1980); that is, it should describe a detailed informationprocessing mechanism whereby some cognitive function might be implemented.

1.2.6 Computer engineering
â€¢ How can we build an efficient computer?
For artificial intelligence to succeed, we need two things: intelligence and an artifact. The
computer has been the artifact of choice. The modern digital electronic computer was invented independently and almost simultaneously by scientists in three countries embattled in

14

Chapter 1.

Introduction

World War II. The first operational computer was the electromechanical Heath Robinson,8
built in 1940 by Alan Turingâ€™s team for a single purpose: deciphering German messages. In
1943, the same group developed the Colossus, a powerful general-purpose machine based
on vacuum tubes.9 The first operational programmable computer was the Z-3, the invention of Konrad Zuse in Germany in 1941. Zuse also invented floating-point numbers and the
first high-level programming language, PlankalkuÌˆl. The first electronic computer, the ABC,
was assembled by John Atanasoff and his student Clifford Berry between 1940 and 1942
at Iowa State University. Atanasoffâ€™s research received little support or recognition; it was
the ENIAC, developed as part of a secret military project at the University of Pennsylvania
by a team including John Mauchly and John Eckert, that proved to be the most influential
forerunner of modern computers.
Since that time, each generation of computer hardware has brought an increase in speed
and capacity and a decrease in price. Performance doubled every 18 months or so until around
2005, when power dissipation problems led manufacturers to start multiplying the number of
CPU cores rather than the clock speed. Current expectations are that future increases in power
will come from massive parallelismâ€”a curious convergence with the properties of the brain.
Of course, there were calculating devices before the electronic computer. The earliest
automated machines, dating from the 17th century, were discussed on page 6. The first programmable machine was a loom, devised in 1805 by Joseph Marie Jacquard (1752â€“1834),
that used punched cards to store instructions for the pattern to be woven. In the mid-19th
century, Charles Babbage (1792â€“1871) designed two machines, neither of which he completed. The Difference Engine was intended to compute mathematical tables for engineering
and scientific projects. It was finally built and shown to work in 1991 at the Science Museum
in London (Swade, 2000). Babbageâ€™s Analytical Engine was far more ambitious: it included
addressable memory, stored programs, and conditional jumps and was the first artifact capable of universal computation. Babbageâ€™s colleague Ada Lovelace, daughter of the poet Lord
Byron, was perhaps the worldâ€™s first programmer. (The programming language Ada is named
after her.) She wrote programs for the unfinished Analytical Engine and even speculated that
the machine could play chess or compose music.
AI also owes a debt to the software side of computer science, which has supplied the
operating systems, programming languages, and tools needed to write modern programs (and
papers about them). But this is one area where the debt has been repaid: work in AI has pioneered many ideas that have made their way back to mainstream computer science, including
time sharing, interactive interpreters, personal computers with windows and mice, rapid development environments, the linked list data type, automatic storage management, and key
concepts of symbolic, functional, declarative, and object-oriented programming.

8 Heath Robinson was a cartoonist famous for his depictions of whimsical and absurdly complicated contraptions for everyday tasks such as buttering toast.
9 In the postwar period, Turing wanted to use these computers for AI researchâ€”for example, one of the first
chess programs (Turing et al., 1953). His efforts were blocked by the British government.

Section 1.2.

The Foundations of Artificial Intelligence

15

1.2.7 Control theory and cybernetics
â€¢ How can artifacts operate under their own control?

CONTROL THEORY

CYBERNETICS

HOMEOSTATIC

OBJECTIVE
FUNCTION

Ktesibios of Alexandria (c. 250 B . C .) built the first self-controlling machine: a water clock
with a regulator that maintained a constant flow rate. This invention changed the definition
of what an artifact could do. Previously, only living things could modify their behavior in
response to changes in the environment. Other examples of self-regulating feedback control
systems include the steam engine governor, created by James Watt (1736â€“1819), and the
thermostat, invented by Cornelis Drebbel (1572â€“1633), who also invented the submarine.
The mathematical theory of stable feedback systems was developed in the 19th century.
The central figure in the creation of what is now called control theory was Norbert
Wiener (1894â€“1964). Wiener was a brilliant mathematician who worked with Bertrand Russell, among others, before developing an interest in biological and mechanical control systems
and their connection to cognition. Like Craik (who also used control systems as psychological
models), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged the
behaviorist orthodoxy (Rosenblueth et al., 1943). They viewed purposive behavior as arising from a regulatory mechanism trying to minimize â€œerrorâ€â€”the difference between current
state and goal state. In the late 1940s, Wiener, along with Warren McCulloch, Walter Pitts,
and John von Neumann, organized a series of influential conferences that explored the new
mathematical and computational models of cognition. Wienerâ€™s book Cybernetics (1948) became a bestseller and awoke the public to the possibility of artificially intelligent machines.
Meanwhile, in Britain, W. Ross Ashby (Ashby, 1940) pioneered similar ideas. Ashby, Alan
Turing, Grey Walter, and others formed the Ratio Club for â€œthose who had Wienerâ€™s ideas
before Wienerâ€™s book appeared.â€ Ashbyâ€™s Design for a Brain (1948, 1952) elaborated on his
idea that intelligence could be created by the use of homeostatic devices containing appropriate feedback loops to achieve stable adaptive behavior.
Modern control theory, especially the branch known as stochastic optimal control, has
as its goal the design of systems that maximize an objective function over time. This roughly
matches our view of AI: designing systems that behave optimally. Why, then, are AI and
control theory two different fields, despite the close connections among their founders? The
answer lies in the close coupling between the mathematical techniques that were familiar to
the participants and the corresponding sets of problems that were encompassed in each world
view. Calculus and matrix algebra, the tools of control theory, lend themselves to systems that
are describable by fixed sets of continuous variables, whereas AI was founded in part as a way
to escape from the these perceived limitations. The tools of logical inference and computation
allowed AI researchers to consider problems such as language, vision, and planning that fell
completely outside the control theoristâ€™s purview.

1.2.8 Linguistics
â€¢ How does language relate to thought?
In 1957, B. F. Skinner published Verbal Behavior. This was a comprehensive, detailed account of the behaviorist approach to language learning, written by the foremost expert in

16

Chapter 1.

COMPUTATIONAL
LINGUISTICS

1.3

Introduction

the field. But curiously, a review of the book became as well known as the book itself, and
served to almost kill off interest in behaviorism. The author of the review was the linguist
Noam Chomsky, who had just published a book on his own theory, Syntactic Structures.
Chomsky pointed out that the behaviorist theory did not address the notion of creativity in
languageâ€”it did not explain how a child could understand and make up sentences that he or
she had never heard before. Chomskyâ€™s theoryâ€”based on syntactic models going back to the
Indian linguist Panini (c. 350 B . C .)â€”could explain this, and unlike previous theories, it was
formal enough that it could in principle be programmed.
Modern linguistics and AI, then, were â€œbornâ€ at about the same time, and grew up
together, intersecting in a hybrid field called computational linguistics or natural language
processing. The problem of understanding language soon turned out to be considerably more
complex than it seemed in 1957. Understanding language requires an understanding of the
subject matter and context, not just an understanding of the structure of sentences. This might
seem obvious, but it was not widely appreciated until the 1960s. Much of the early work in
knowledge representation (the study of how to put knowledge into a form that a computer
can reason with) was tied to language and informed by research in linguistics, which was
connected in turn to decades of work on the philosophical analysis of language.

T HE H ISTORY OF A RTIFICIAL I NTELLIGENCE
With the background material behind us, we are ready to cover the development of AI itself.

1.3.1 The gestation of artificial intelligence (1943â€“1955)

HEBBIAN LEARNING

The first work that is now generally recognized as AI was done by Warren McCulloch and
Walter Pitts (1943). They drew on three sources: knowledge of the basic physiology and
function of neurons in the brain; a formal analysis of propositional logic due to Russell and
Whitehead; and Turingâ€™s theory of computation. They proposed a model of artificial neurons
in which each neuron is characterized as being â€œonâ€ or â€œoff,â€ with a switch to â€œonâ€ occurring
in response to stimulation by a sufficient number of neighboring neurons. The state of a
neuron was conceived of as â€œfactually equivalent to a proposition which proposed its adequate
stimulus.â€ They showed, for example, that any computable function could be computed by
some network of connected neurons, and that all the logical connectives (and, or, not, etc.)
could be implemented by simple net structures. McCulloch and Pitts also suggested that
suitably defined networks could learn. Donald Hebb (1949) demonstrated a simple updating
rule for modifying the connection strengths between neurons. His rule, now called Hebbian
learning, remains an influential model to this day.
Two undergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the
first neural network computer in 1950. The S NARC, as it was called, used 3000 vacuum
tubes and a surplus automatic pilot mechanism from a B-24 bomber to simulate a network of
40 neurons. Later, at Princeton, Minsky studied universal computation in neural networks.
His Ph.D. committee was skeptical about whether this kind of work should be considered

Section 1.3.

The History of Artificial Intelligence

17

mathematics, but von Neumann reportedly said, â€œIf it isnâ€™t now, it will be someday.â€ Minsky
was later to prove influential theorems showing the limitations of neural network research.
There were a number of early examples of work that can be characterized as AI, but
Alan Turingâ€™s vision was perhaps the most influential. He gave lectures on the topic as early
as 1947 at the London Mathematical Society and articulated a persuasive agenda in his 1950
article â€œComputing Machinery and Intelligence.â€ Therein, he introduced the Turing Test,
machine learning, genetic algorithms, and reinforcement learning. He proposed the Child
Programme idea, explaining â€œInstead of trying to produce a programme to simulate the adult
mind, why not rather try to produce one which simulated the childâ€™s?â€

1.3.2 The birth of artificial intelligence (1956)
Princeton was home to another influential figure in AI, John McCarthy. After receiving his
PhD there in 1951 and working for two years as an instructor, McCarthy moved to Stanford and then to Dartmouth College, which was to become the official birthplace of the field.
McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring
together U.S. researchers interested in automata theory, neural nets, and the study of intelligence. They organized a two-month workshop at Dartmouth in the summer of 1956. The
proposal states:10
We propose that a 2 month, 10 man study of artificial intelligence be carried
out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of
learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find
how to make machines use language, form abstractions and concepts, solve kinds
of problems now reserved for humans, and improve themselves. We think that a
significant advance can be made in one or more of these problems if a carefully
selected group of scientists work on it together for a summer.
There were 10 attendees in all, including Trenchard More from Princeton, Arthur Samuel
from IBM, and Ray Solomonoff and Oliver Selfridge from MIT.
Two researchers from Carnegie Tech, 11 Allen Newell and Herbert Simon, rather stole
the show. Although the others had ideas and in some cases programs for particular applications such as checkers, Newell and Simon already had a reasoning program, the Logic
Theorist (LT), about which Simon claimed, â€œWe have invented a computer program capable
of thinking non-numerically, and thereby solved the venerable mindâ€“body problem.â€12 Soon
after the workshop, the program was able to prove most of the theorems in Chapter 2 of Rus10 This was the first official usage of McCarthyâ€™s term artificial intelligence. Perhaps â€œcomputational rationalityâ€
would have been more precise and less threatening, but â€œAIâ€ has stuck. At the 50th anniversary of the Dartmouth
conference, McCarthy stated that he resisted the terms â€œcomputerâ€ or â€œcomputationalâ€ in deference to Norbert
Weiner, who was promoting analog cybernetic devices rather than digital computers.
11 Now Carnegie Mellon University (CMU).
12 Newell and Simon also invented a list-processing language, IPL, to write LT. They had no compiler and
translated it into machine code by hand. To avoid errors, they worked in parallel, calling out binary numbers to
each other as they wrote each instruction to make sure they agreed.

18

Chapter 1.

Introduction

sell and Whiteheadâ€™s Principia Mathematica. Russell was reportedly delighted when Simon
showed him that the program had come up with a proof for one theorem that was shorter than
the one in Principia. The editors of the Journal of Symbolic Logic were less impressed; they
rejected a paper coauthored by Newell, Simon, and Logic Theorist.
The Dartmouth workshop did not lead to any new breakthroughs, but it did introduce
all the major figures to each other. For the next 20 years, the field would be dominated by
these people and their students and colleagues at MIT, CMU, Stanford, and IBM.
Looking at the proposal for the Dartmouth workshop (McCarthy et al., 1955), we can
see why it was necessary for AI to become a separate field. Why couldnâ€™t all the work done
in AI have taken place under the name of control theory or operations research or decision
theory, which, after all, have objectives similar to those of AI? Or why isnâ€™t AI a branch
of mathematics? The first answer is that AI from the start embraced the idea of duplicating
human faculties such as creativity, self-improvement, and language use. None of the other
fields were addressing these issues. The second answer is methodology. AI is the only one
of these fields that is clearly a branch of computer science (although operations research does
share an emphasis on computer simulations), and AI is the only field to attempt to build
machines that will function autonomously in complex, changing environments.

1.3.3 Early enthusiasm, great expectations (1952â€“1969)

PHYSICAL SYMBOL
SYSTEM

The early years of AI were full of successesâ€”in a limited way. Given the primitive computers and programming tools of the time and the fact that only a few years earlier computers
were seen as things that could do arithmetic and no more, it was astonishing whenever a computer did anything remotely clever. The intellectual establishment, by and large, preferred to
believe that â€œa machine can never do X.â€ (See Chapter 26 for a long list of Xâ€™s gathered
by Turing.) AI researchers naturally responded by demonstrating one X after another. John
McCarthy referred to this period as the â€œLook, Ma, no hands!â€ era.
Newell and Simonâ€™s early success was followed up with the General Problem Solver,
or GPS. Unlike Logic Theorist, this program was designed from the start to imitate human
problem-solving protocols. Within the limited class of puzzles it could handle, it turned out
that the order in which the program considered subgoals and possible actions was similar to
that in which humans approached the same problems. Thus, GPS was probably the first program to embody the â€œthinking humanlyâ€ approach. The success of GPS and subsequent programs as models of cognition led Newell and Simon (1976) to formulate the famous physical
symbol system hypothesis, which states that â€œa physical symbol system has the necessary and
sufficient means for general intelligent action.â€ What they meant is that any system (human
or machine) exhibiting intelligence must operate by manipulating data structures composed
of symbols. We will see later that this hypothesis has been challenged from many directions.
At IBM, Nathaniel Rochester and his colleagues produced some of the first AI programs. Herbert Gelernter (1959) constructed the Geometry Theorem Prover, which was
able to prove theorems that many students of mathematics would find quite tricky. Starting
in 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventually
learned to play at a strong amateur level. Along the way, he disproved the idea that comput-

Section 1.3.

LISP

MICROWORLD

The History of Artificial Intelligence

19

ers can do only what they are told to: his program quickly learned to play a better game than
its creator. The program was demonstrated on television in February 1956, creating a strong
impression. Like Turing, Samuel had trouble finding computer time. Working at night, he
used machines that were still on the testing floor at IBMâ€™s manufacturing plant. Chapter 5
covers game playing, and Chapter 21 explains the learning techniques used by Samuel.
John McCarthy moved from Dartmouth to MIT and there made three crucial contributions in one historic year: 1958. In MIT AI Lab Memo No. 1, McCarthy defined the high-level
language Lisp, which was to become the dominant AI programming language for the next 30
years. With Lisp, McCarthy had the tool he needed, but access to scarce and expensive computing resources was also a serious problem. In response, he and others at MIT invented time
sharing. Also in 1958, McCarthy published a paper entitled Programs with Common Sense,
in which he described the Advice Taker, a hypothetical program that can be seen as the first
complete AI system. Like the Logic Theorist and Geometry Theorem Prover, McCarthyâ€™s
program was designed to use knowledge to search for solutions to problems. But unlike the
others, it was to embody general knowledge of the world. For example, he showed how
some simple axioms would enable the program to generate a plan to drive to the airport. The
program was also designed to accept new axioms in the normal course of operation, thereby
allowing it to achieve competence in new areas without being reprogrammed. The Advice
Taker thus embodied the central principles of knowledge representation and reasoning: that
it is useful to have a formal, explicit representation of the world and its workings and to be
able to manipulate that representation with deductive processes. It is remarkable how much
of the 1958 paper remains relevant today.
1958 also marked the year that Marvin Minsky moved to MIT. His initial collaboration
with McCarthy did not last, however. McCarthy stressed representation and reasoning in formal logic, whereas Minsky was more interested in getting programs to work and eventually
developed an anti-logic outlook. In 1963, McCarthy started the AI lab at Stanford. His plan
to use logic to build the ultimate Advice Taker was advanced by J. A. Robinsonâ€™s discovery in 1965 of the resolution method (a complete theorem-proving algorithm for first-order
logic; see Chapter 9). Work at Stanford emphasized general-purpose methods for logical
reasoning. Applications of logic included Cordell Greenâ€™s question-answering and planning
systems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute
(SRI). The latter project, discussed further in Chapter 25, was the first to demonstrate the
complete integration of logical reasoning and physical activity.
Minsky supervised a series of students who chose limited problems that appeared to
require intelligence to solve. These limited domains became known as microworlds. James
Slagleâ€™s S AINT program (1963) was able to solve closed-form calculus integration problems
typical of first-year college courses. Tom Evansâ€™s A NALOGY program (1968) solved geometric analogy problems that appear in IQ tests. Daniel Bobrowâ€™s S TUDENT program (1967)
solved algebra story problems, such as the following:
If the number of customers Tom gets is twice the square of 20 percent of the number
of advertisements he runs, and the number of advertisements he runs is 45, what is the
number of customers Tom gets?

20

Chapter 1.

Introduction

Blue

Red
Red

Green
Blue

Green
Green

Red

Figure 1.4 A scene from the blocks world. S HRDLU (Winograd, 1972) has just completed
the command â€œFind a block which is taller than the one you are holding and put it in the box.â€

The most famous microworld was the blocks world, which consists of a set of solid blocks
placed on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.4.
A typical task in this world is to rearrange the blocks in a certain way, using a robot hand
that can pick up one block at a time. The blocks world was home to the vision project of
David Huffman (1971), the vision and constraint-propagation work of David Waltz (1975),
the learning theory of Patrick Winston (1970), the natural-language-understanding program
of Terry Winograd (1972), and the planner of Scott Fahlman (1974).
Early work building on the neural networks of McCulloch and Pitts also flourished.
The work of Winograd and Cowan (1963) showed how a large number of elements could
collectively represent an individual concept, with a corresponding increase in robustness and
parallelism. Hebbâ€™s learning methods were enhanced by Bernie Widrow (Widrow and Hoff,
1960; Widrow, 1962), who called his networks adalines, and by Frank Rosenblatt (1962)
with his perceptrons. The perceptron convergence theorem (Block et al., 1962) says that
the learning algorithm can adjust the connection strengths of a perceptron to match any input
data, provided such a match exists. These topics are covered in Chapter 20.

1.3.4 A dose of reality (1966â€“1973)
From the beginning, AI researchers were not shy about making predictions of their coming
successes. The following statement by Herbert Simon in 1957 is often quoted:
It is not my aim to surprise or shock youâ€”but the simplest way I can summarize is to say
that there are now in the world machines that think, that learn and that create. Moreover,

Section 1.3.

The History of Artificial Intelligence

21

their ability to do these things is going to increase rapidly untilâ€”in a visible futureâ€”the
range of problems they can handle will be coextensive with the range to which the human
mind has been applied.

MACHINE EVOLUTION
GENETIC
ALGORITHM

Terms such as â€œvisible futureâ€ can be interpreted in various ways, but Simon also made
more concrete predictions: that within 10 years a computer would be chess champion, and
a significant mathematical theorem would be proved by machine. These predictions came
true (or approximately true) within 40 years rather than 10. Simonâ€™s overconfidence was due
to the promising performance of early AI systems on simple examples. In almost all cases,
however, these early systems turned out to fail miserably when tried out on wider selections
of problems and on more difficult problems.
The first kind of difficulty arose because most early programs knew nothing of their
subject matter; they succeeded by means of simple syntactic manipulations. A typical story
occurred in early machine translation efforts, which were generously funded by the U.S. National Research Council in an attempt to speed up the translation of Russian scientific papers
in the wake of the Sputnik launch in 1957. It was thought initially that simple syntactic transformations based on the grammars of Russian and English, and word replacement from an
electronic dictionary, would suffice to preserve the exact meanings of sentences. The fact is
that accurate translation requires background knowledge in order to resolve ambiguity and
establish the content of the sentence. The famous retranslation of â€œthe spirit is willing but
the flesh is weakâ€ as â€œthe vodka is good but the meat is rottenâ€ illustrates the difficulties encountered. In 1966, a report by an advisory committee found that â€œthere has been no machine
translation of general scientific text, and none is in immediate prospect.â€ All U.S. government
funding for academic translation projects was canceled. Today, machine translation is an imperfect but widely used tool for technical, commercial, government, and Internet documents.
The second kind of difficulty was the intractability of many of the problems that AI was
attempting to solve. Most of the early AI programs solved problems by trying out different
combinations of steps until the solution was found. This strategy worked initially because
microworlds contained very few objects and hence very few possible actions and very short
solution sequences. Before the theory of computational complexity was developed, it was
widely thought that â€œscaling upâ€ to larger problems was simply a matter of faster hardware
and larger memories. The optimism that accompanied the development of resolution theorem
proving, for example, was soon dampened when researchers failed to prove theorems involving more than a few dozen facts. The fact that a program can find a solution in principle does
not mean that the program contains any of the mechanisms needed to find it in practice.
The illusion of unlimited computational power was not confined to problem-solving
programs. Early experiments in machine evolution (now called genetic algorithms) (Friedberg, 1958; Friedberg et al., 1959) were based on the undoubtedly correct belief that by
making an appropriate series of small mutations to a machine-code program, one can generate a program with good performance for any particular task. The idea, then, was to try
random mutations with a selection process to preserve mutations that seemed useful. Despite thousands of hours of CPU time, almost no progress was demonstrated. Modern genetic
algorithms use better representations and have shown more success.

22

Chapter 1.

Introduction

Failure to come to grips with the â€œcombinatorial explosionâ€ was one of the main criticisms of AI contained in the Lighthill report (Lighthill, 1973), which formed the basis for the
decision by the British government to end support for AI research in all but two universities.
(Oral tradition paints a somewhat different and more colorful picture, with political ambitions
and personal animosities whose description is beside the point.)
A third difficulty arose because of some fundamental limitations on the basic structures
being used to generate intelligent behavior. For example, Minsky and Papertâ€™s book Perceptrons (1969) proved that, although perceptrons (a simple form of neural network) could be
shown to learn anything they were capable of representing, they could represent very little. In
particular, a two-input perceptron (restricted to be simpler than the form Rosenblatt originally
studied) could not be trained to recognize when its two inputs were different. Although their
results did not apply to more complex, multilayer networks, research funding for neural-net
research soon dwindled to almost nothing. Ironically, the new back-propagation learning algorithms for multilayer networks that were to cause an enormous resurgence in neural-net
research in the late 1980s were actually discovered first in 1969 (Bryson and Ho, 1969).

1.3.5 Knowledge-based systems: The key to power? (1969â€“1979)

WEAK METHOD

The picture of problem solving that had arisen during the first decade of AI research was of
a general-purpose search mechanism trying to string together elementary reasoning steps to
find complete solutions. Such approaches have been called weak methods because, although
general, they do not scale up to large or difficult problem instances. The alternative to weak
methods is to use more powerful, domain-specific knowledge that allows larger reasoning
steps and can more easily handle typically occurring cases in narrow areas of expertise. One
might say that to solve a hard problem, you have to almost know the answer already.
The D ENDRAL program (Buchanan et al., 1969) was an early example of this approach.
It was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon),
Bruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel
laureate geneticist) teamed up to solve the problem of inferring molecular structure from the
information provided by a mass spectrometer. The input to the program consists of the elementary formula of the molecule (e.g., C6 H13 NO2 ) and the mass spectrum giving the masses
of the various fragments of the molecule generated when it is bombarded by an electron beam.
For example, the mass spectrum might contain a peak at m = 15, corresponding to the mass
of a methyl (CH3 ) fragment.
The naive version of the program generated all possible structures consistent with the
formula, and then predicted what mass spectrum would be observed for each, comparing this
with the actual spectrum. As one might expect, this is intractable for even moderate-sized
molecules. The D ENDRAL researchers consulted analytical chemists and found that they
worked by looking for well-known patterns of peaks in the spectrum that suggested common
substructures in the molecule. For example, the following rule is used to recognize a ketone
(C=O) subgroup (which weighs 28):
if there are two peaks at x1 and x2 such that
(a) x1 + x2 = M + 28 (M is the mass of the whole molecule);

Section 1.3.

The History of Artificial Intelligence

23

(b) x1 âˆ’ 28 is a high peak;
(c) x2 âˆ’ 28 is a high peak;
(d) At least one of x1 and x2 is high.
then there is a ketone subgroup

Recognizing that the molecule contains a particular substructure reduces the number of possible candidates enormously. D ENDRAL was powerful because
All the relevant theoretical knowledge to solve these problems has been mapped over from
its general form in the [spectrum prediction component] (â€œfirst principlesâ€) to efficient
special forms (â€œcookbook recipesâ€). (Feigenbaum et al., 1971)

EXPERT SYSTEMS

CERTAINTY FACTOR

The significance of D ENDRAL was that it was the first successful knowledge-intensive system: its expertise derived from large numbers of special-purpose rules. Later systems also
incorporated the main theme of McCarthyâ€™s Advice Taker approachâ€”the clean separation of
the knowledge (in the form of rules) from the reasoning component.
With this lesson in mind, Feigenbaum and others at Stanford began the Heuristic Programming Project (HPP) to investigate the extent to which the new methodology of expert
systems could be applied to other areas of human expertise. The next major effort was in
the area of medical diagnosis. Feigenbaum, Buchanan, and Dr. Edward Shortliffe developed
M YCIN to diagnose blood infections. With about 450 rules, M YCIN was able to perform
as well as some experts, and considerably better than junior doctors. It also contained two
major differences from D ENDRAL. First, unlike the D ENDRAL rules, no general theoretical
model existed from which the M YCIN rules could be deduced. They had to be acquired from
extensive interviewing of experts, who in turn acquired them from textbooks, other experts,
and direct experience of cases. Second, the rules had to reflect the uncertainty associated with
medical knowledge. M YCIN incorporated a calculus of uncertainty called certainty factors
(see Chapter 14), which seemed (at the time) to fit well with how doctors assessed the impact
of evidence on the diagnosis.
The importance of domain knowledge was also apparent in the area of understanding
natural language. Although Winogradâ€™s S HRDLU system for understanding natural language
had engendered a good deal of excitement, its dependence on syntactic analysis caused some
of the same problems as occurred in the early machine translation work. It was able to
overcome ambiguity and understand pronoun references, but this was mainly because it was
designed specifically for one areaâ€”the blocks world. Several researchers, including Eugene
Charniak, a fellow graduate student of Winogradâ€™s at MIT, suggested that robust language
understanding would require general knowledge about the world and a general method for
using that knowledge.
At Yale, linguist-turned-AI-researcher Roger Schank emphasized this point, claiming,
â€œThere is no such thing as syntax,â€ which upset a lot of linguists but did serve to start a useful
discussion. Schank and his students built a series of programs (Schank and Abelson, 1977;
Wilensky, 1978; Schank and Riesbeck, 1981; Dyer, 1983) that all had the task of understanding natural language. The emphasis, however, was less on language per se and more on
the problems of representing and reasoning with the knowledge required for language understanding. The problems included representing stereotypical situations (Cullingford, 1981),

24

FRAMES

Chapter 1.

Introduction

describing human memory organization (Rieger, 1976; Kolodner, 1983), and understanding
plans and goals (Wilensky, 1983).
The widespread growth of applications to real-world problems caused a concurrent increase in the demands for workable knowledge representation schemes. A large number
of different representation and reasoning languages were developed. Some were based on
logicâ€”for example, the Prolog language became popular in Europe, and the P LANNER family in the United States. Others, following Minskyâ€™s idea of frames (1975), adopted a more
structured approach, assembling facts about particular object and event types and arranging
the types into a large taxonomic hierarchy analogous to a biological taxonomy.

1.3.6 AI becomes an industry (1980â€“present)
The first successful commercial expert system, R1, began operation at the Digital Equipment
Corporation (McDermott, 1982). The program helped configure orders for new computer
systems; by 1986, it was saving the company an estimated $40 million a year. By 1988,
DECâ€™s AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in
use and 500 in development, saving an estimated $10 million a year. Nearly every major U.S.
corporation had its own AI group and was either using or investigating expert systems.
In 1981, the Japanese announced the â€œFifth Generationâ€ project, a 10-year plan to build
intelligent computers running Prolog. In response, the United States formed the Microelectronics and Computer Technology Corporation (MCC) as a research consortium designed to
assure national competitiveness. In both cases, AI was part of a broad effort, including chip
design and human-interface research. In Britain, the Alvey report reinstated the funding that
was cut by the Lighthill report.13 In all three countries, however, the projects never met their
ambitious goals.
Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars
in 1988, including hundreds of companies building expert systems, vision systems, robots,
and software and hardware specialized for these purposes. Soon after that came a period
called the â€œAI Winter,â€ in which many companies fell by the wayside as they failed to deliver
on extravagant promises.

1.3.7 The return of neural networks (1986â€“present)
BACK-PROPAGATION

CONNECTIONIST

In the mid-1980s at least four different groups reinvented the back-propagation learning
algorithm first found in 1969 by Bryson and Ho. The algorithm was applied to many learning problems in computer science and psychology, and the widespread dissemination of the
results in the collection Parallel Distributed Processing (Rumelhart and McClelland, 1986)
caused great excitement.
These so-called connectionist models of intelligent systems were seen by some as direct competitors both to the symbolic models promoted by Newell and Simon and to the
logicist approach of McCarthy and others (Smolensky, 1988). It might seem obvious that
at some level humans manipulate symbolsâ€”in fact, Terrence Deaconâ€™s book The Symbolic
13

To save embarrassment, a new field called IKBS (Intelligent Knowledge-Based Systems) was invented because
Artificial Intelligence had been officially canceled.

Section 1.3.

The History of Artificial Intelligence

25

Species (1997) suggests that this is the defining characteristic of humansâ€”but the most ardent connectionists questioned whether symbol manipulation had any real explanatory role in
detailed models of cognition. This question remains unanswered, but the current view is that
connectionist and symbolic approaches are complementary, not competing. As occurred with
the separation of AI and cognitive science, modern neural network research has bifurcated
into two fields, one concerned with creating effective network architectures and algorithms
and understanding their mathematical properties, the other concerned with careful modeling
of the empirical properties of actual neurons and ensembles of neurons.

1.3.8 AI adopts the scientific method (1987â€“present)
Recent years have seen a revolution in both the content and the methodology of work in
artificial intelligence.14 It is now more common to build on existing theories than to propose
brand-new ones, to base claims on rigorous theorems or hard experimental evidence rather
than on intuition, and to show relevance to real-world applications rather than toy examples.
AI was founded in part as a rebellion against the limitations of existing fields like control
theory and statistics, but now it is embracing those fields. As David McAllester (1998) put it:
In the early period of AI it seemed plausible that new forms of symbolic computation,
e.g., frames and semantic networks, made much of classical theory obsolete. This led to
a form of isolationism in which AI became largely separated from the rest of computer
science. This isolationism is currently being abandoned. There is a recognition that
machine learning should not be isolated from information theory, that uncertain reasoning
should not be isolated from stochastic modeling, that search should not be isolated from
classical optimization and control, and that automated reasoning should not be isolated
from formal methods and static analysis.

HIDDEN MARKOV
MODELS

In terms of methodology, AI has finally come firmly under the scientific method. To be accepted, hypotheses must be subjected to rigorous empirical experiments, and the results must
be analyzed statistically for their importance (Cohen, 1995). It is now possible to replicate
experiments by using shared repositories of test data and code.
The field of speech recognition illustrates the pattern. In the 1970s, a wide variety of
different architectures and approaches were tried. Many of these were rather ad hoc and
fragile, and were demonstrated on only a few specially selected examples. In recent years,
approaches based on hidden Markov models (HMMs) have come to dominate the area. Two
aspects of HMMs are relevant. First, they are based on a rigorous mathematical theory. This
has allowed speech researchers to build on several decades of mathematical results developed
in other fields. Second, they are generated by a process of training on a large corpus of
real speech data. This ensures that the performance is robust, and in rigorous blind tests the
HMMs have been improving their scores steadily. Speech technology and the related field of
handwritten character recognition are already making the transition to widespread industrial
14

Some have characterized this change as a victory of the neatsâ€”those who think that AI theories should be
grounded in mathematical rigorâ€”over the scruffiesâ€”those who would rather try out lots of ideas, write some
programs, and then assess what seems to be working. Both approaches are important. A shift toward neatness
implies that the field has reached a level of stability and maturity. Whether that stability will be disrupted by a
new scruffy idea is another question.

26

DATA MINING

BAYESIAN NETWORK

Chapter 1.

Introduction

and consumer applications. Note that there is no scientific claim that humans use HMMs to
recognize speech; rather, HMMs provide a mathematical framework for understanding the
problem and support the engineering claim that they work well in practice.
Machine translation follows the same course as speech recognition. In the 1950s there
was initial enthusiasm for an approach based on sequences of words, with models learned
according to the principles of information theory. That approach fell out of favor in the
1960s, but returned in the late 1990s and now dominates the field.
Neural networks also fit this trend. Much of the work on neural nets in the 1980s was
done in an attempt to scope out what could be done and to learn how neural nets differ from
â€œtraditionalâ€ techniques. Using improved methodology and theoretical frameworks, the field
arrived at an understanding in which neural nets can now be compared with corresponding
techniques from statistics, pattern recognition, and machine learning, and the most promising
technique can be applied to each application. As a result of these developments, so-called
data mining technology has spawned a vigorous new industry.
Judea Pearlâ€™s (1988) Probabilistic Reasoning in Intelligent Systems led to a new acceptance of probability and decision theory in AI, following a resurgence of interest epitomized
by Peter Cheesemanâ€™s (1985) article â€œIn Defense of Probability.â€ The Bayesian network
formalism was invented to allow efficient representation of, and rigorous reasoning with,
uncertain knowledge. This approach largely overcomes many problems of the probabilistic
reasoning systems of the 1960s and 1970s; it now dominates AI research on uncertain reasoning and expert systems. The approach allows for learning from experience, and it combines
the best of classical AI and neural nets. Work by Judea Pearl (1982a) and by Eric Horvitz and
David Heckerman (Horvitz and Heckerman, 1986; Horvitz et al., 1986) promoted the idea of
normative expert systems: ones that act rationally according to the laws of decision theory
and do not try to imitate the thought steps of human experts. The WindowsTM operating system includes several normative diagnostic expert systems for correcting problems. Chapters
13 to 16 cover this area.
Similar gentle revolutions have occurred in robotics, computer vision, and knowledge
representation. A better understanding of the problems and their complexity properties, combined with increased mathematical sophistication, has led to workable research agendas and
robust methods. Although increased formalization and specialization led fields such as vision
and robotics to become somewhat isolated from â€œmainstreamâ€ AI in the 1990s, this trend has
reversed in recent years as tools from machine learning in particular have proved effective for
many problems. The process of reintegration is already yielding significant benefits

1.3.9 The emergence of intelligent agents (1995â€“present)
Perhaps encouraged by the progress in solving the subproblems of AI, researchers have also
started to look at the â€œwhole agentâ€ problem again. The work of Allen Newell, John Laird,
and Paul Rosenbloom on S OAR (Newell, 1990; Laird et al., 1987) is the best-known example
of a complete agent architecture. One of the most important environments for intelligent
agents is the Internet. AI systems have become so common in Web-based applications that
the â€œ-botâ€ suffix has entered everyday language. Moreover, AI technologies underlie many

Section 1.3.

HUMAN-LEVEL AI

ARTIFICIAL GENERAL
INTELLIGENCE

FRIENDLY AI

The History of Artificial Intelligence

27

Internet tools, such as search engines, recommender systems, and Web site aggregators.
One consequence of trying to build complete agents is the realization that the previously
isolated subfields of AI might need to be reorganized somewhat when their results are to be
tied together. In particular, it is now widely appreciated that sensory systems (vision, sonar,
speech recognition, etc.) cannot deliver perfectly reliable information about the environment.
Hence, reasoning and planning systems must be able to handle uncertainty. A second major
consequence of the agent perspective is that AI has been drawn into much closer contact
with other fields, such as control theory and economics, that also deal with agents. Recent
progress in the control of robotic cars has derived from a mixture of approaches ranging from
better sensors, control-theoretic integration of sensing, localization and mapping, as well as
a degree of high-level planning.
Despite these successes, some influential founders of AI, including John McCarthy
(2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal and
Winston, 2009), have expressed discontent with the progress of AI. They think that AI should
put less emphasis on creating ever-improved versions of applications that are good at a specific task, such as driving a car, playing chess, or recognizing speech. Instead, they believe
AI should return to its roots of striving for, in Simonâ€™s words, â€œmachines that think, that learn
and that create.â€ They call the effort human-level AI or HLAI; their first symposium was in
2004 (Minsky et al., 2004). The effort will require very large knowledge bases; Hendler et al.
(1995) discuss where these knowledge bases might come from.
A related idea is the subfield of Artificial General Intelligence or AGI (Goertzel and
Pennachin, 2007), which held its first conference and organized the Journal of Artificial General Intelligence in 2008. AGI looks for a universal algorithm for learning and acting in
any environment, and has its roots in the work of Ray Solomonoff (1964), one of the attendees of the original 1956 Dartmouth conference. Guaranteeing that what we create is really
Friendly AI is also a concern (Yudkowsky, 2008; Omohundro, 2008), one we will return to
in Chapter 26.

1.3.10 The availability of very large data sets (2001â€“present)
Throughout the 60-year history of computer science, the emphasis has been on the algorithm
as the main subject of study. But some recent work in AI suggests that for many problems, it
makes more sense to worry about the data and be less picky about what algorithm to apply.
This is true because of the increasing availability of very large data sources: for example,
trillions of words of English and billions of images from the Web (Kilgarriff and Grefenstette,
2006); or billions of base pairs of genomic sequences (Collins et al., 2003).
One influential paper in this line was Yarowskyâ€™s (1995) work on word-sense disambiguation: given the use of the word â€œplantâ€ in a sentence, does that refer to flora or factory?
Previous approaches to the problem had relied on human-labeled examples combined with
machine learning algorithms. Yarowsky showed that the task can be done, with accuracy
above 96%, with no labeled examples at all. Instead, given a very large corpus of unannotated text and just the dictionary definitions of the two sensesâ€”â€œworks, industrial plantâ€ and
â€œflora, plant lifeâ€â€”one can label examples in the corpus, and from there bootstrap to learn

28

Chapter 1.

Introduction

new patterns that help label new examples. Banko and Brill (2001) show that techniques
like this perform even better as the amount of available text goes from a million words to a
billion and that the increase in performance from using more data exceeds any difference in
algorithm choice; a mediocre algorithm with 100 million words of unlabeled training data
outperforms the best known algorithm with 1 million words.
As another example, Hays and Efros (2007) discuss the problem of filling in holes in a
photograph. Suppose you use Photoshop to mask out an ex-friend from a group photo, but
now you need to fill in the masked area with something that matches the background. Hays
and Efros defined an algorithm that searches through a collection of photos to find something
that will match. They found the performance of their algorithm was poor when they used
a collection of only ten thousand photos, but crossed a threshold into excellent performance
when they grew the collection to two million photos.
Work like this suggests that the â€œknowledge bottleneckâ€ in AIâ€”the problem of how to
express all the knowledge that a system needsâ€”may be solved in many applications by learning methods rather than hand-coded knowledge engineering, provided the learning algorithms
have enough data to go on (Halevy et al., 2009). Reporters have noticed the surge of new applications and have written that â€œAI Winterâ€ may be yielding to a new Spring (Havenstein,
2005). As Kurzweil (2005) writes, â€œtoday, many thousands of AI applications are deeply
embedded in the infrastructure of every industry.â€

1.4

T HE S TATE OF THE A RT
What can AI do today? A concise answer is difficult because there are so many activities in
so many subfields. Here we sample a few applications; others appear throughout the book.
Robotic vehicles: A driverless robotic car named S TANLEY sped through the rough
terrain of the Mojave dessert at 22 mph, finishing the 132-mile course first to win the 2005
DARPA Grand Challenge. S TANLEY is a Volkswagen Touareg outfitted with cameras, radar,
and laser rangefinders to sense the environment and onboard software to command the steering, braking, and acceleration (Thrun, 2006). The following year CMUâ€™s B OSS won the Urban Challenge, safely driving in traffic through the streets of a closed Air Force base, obeying
traffic rules and avoiding pedestrians and other vehicles.
Speech recognition: A traveler calling United Airlines to book a flight can have the entire conversation guided by an automated speech recognition and dialog management system.
Autonomous planning and scheduling: A hundred million miles from Earth, NASAâ€™s
Remote Agent program became the first on-board autonomous planning program to control
the scheduling of operations for a spacecraft (Jonsson et al., 2000). R EMOTE AGENT generated plans from high-level goals specified from the ground and monitored the execution of
those plansâ€”detecting, diagnosing, and recovering from problems as they occurred. Successor program MAPGEN (Al-Chang et al., 2004) plans the daily operations for NASAâ€™s Mars
Exploration Rovers, and MEXAR2 (Cesta et al., 2007) did mission planningâ€”both logistics
and science planningâ€”for the European Space Agencyâ€™s Mars Express mission in 2008.

Section 1.5.

Summary

29

Game playing: IBMâ€™s D EEP B LUE became the first computer program to defeat the
world champion in a chess match when it bested Garry Kasparov by a score of 3.5 to 2.5 in
an exhibition match (Goodman and Keene, 1997). Kasparov said that he felt a â€œnew kind of
intelligenceâ€ across the board from him. Newsweek magazine described the match as â€œThe
brainâ€™s last stand.â€ The value of IBMâ€™s stock increased by $18 billion. Human champions
studied Kasparovâ€™s loss and were able to draw a few matches in subsequent years, but the
most recent human-computer matches have been won convincingly by the computer.
Spam fighting: Each day, learning algorithms classify over a billion messages as spam,
saving the recipient from having to waste time deleting what, for many users, could comprise
80% or 90% of all messages, if not classified away by algorithms. Because the spammers are
continually updating their tactics, it is difficult for a static programmed approach to keep up,
and learning algorithms work best (Sahami et al., 1998; Goodman and Heckerman, 2004).
Logistics planning: During the Persian Gulf crisis of 1991, U.S. forces deployed a
Dynamic Analysis and Replanning Tool, DART (Cross and Walker, 1994), to do automated
logistics planning and scheduling for transportation. This involved up to 50,000 vehicles,
cargo, and people at a time, and had to account for starting points, destinations, routes, and
conflict resolution among all parameters. The AI planning techniques generated in hours
a plan that would have taken weeks with older methods. The Defense Advanced Research
Project Agency (DARPA) stated that this single application more than paid back DARPAâ€™s
30-year investment in AI.
Robotics: The iRobot Corporation has sold over two million Roomba robotic vacuum
cleaners for home use. The company also deploys the more rugged PackBot to Iraq and
Afghanistan, where it is used to handle hazardous materials, clear explosives, and identify
the location of snipers.
Machine Translation: A computer program automatically translates from Arabic to
English, allowing an English speaker to see the headline â€œArdogan Confirms That Turkey
Would Not Accept Any Pressure, Urging Them to Recognize Cyprus.â€ The program uses a
statistical model built from examples of Arabic-to-English translations and from examples of
English text totaling two trillion words (Brants et al., 2007). None of the computer scientists
on the team speak Arabic, but they do understand statistics and machine learning algorithms.
These are just a few examples of artificial intelligence systems that exist today. Not
magic or science fictionâ€”but rather science, engineering, and mathematics, to which this
book provides an introduction.

1.5

S UMMARY
This chapter defines AI and establishes the cultural background against which it has developed. Some of the important points are as follows:
â€¢ Different people approach AI with different goals in mind. Two important questions to
ask are: Are you concerned with thinking or behavior? Do you want to model humans
or work from an ideal standard?

30

Chapter 1.

Introduction

â€¢ In this book, we adopt the view that intelligence is concerned mainly with rational
action. Ideally, an intelligent agent takes the best possible action in a situation. We
study the problem of building agents that are intelligent in this sense.
â€¢ Philosophers (going back to 400 B . C .) made AI conceivable by considering the ideas
that the mind is in some ways like a machine, that it operates on knowledge encoded in
some internal language, and that thought can be used to choose what actions to take.
â€¢ Mathematicians provided the tools to manipulate statements of logical certainty as well
as uncertain, probabilistic statements. They also set the groundwork for understanding
computation and reasoning about algorithms.
â€¢ Economists formalized the problem of making decisions that maximize the expected
outcome to the decision maker.
â€¢ Neuroscientists discovered some facts about how the brain works and the ways in which
it is similar to and different from computers.
â€¢ Psychologists adopted the idea that humans and animals can be considered informationprocessing machines. Linguists showed that language use fits into this model.
â€¢ Computer engineers provided the ever-more-powerful machines that make AI applications possible.
â€¢ Control theory deals with designing devices that act optimally on the basis of feedback
from the environment. Initially, the mathematical tools of control theory were quite
different from AI, but the fields are coming closer together.
â€¢ The history of AI has had cycles of success, misplaced optimism, and resulting cutbacks
in enthusiasm and funding. There have also been cycles of introducing new creative
approaches and systematically refining the best ones.
â€¢ AI has advanced more rapidly in the past decade because of greater use of the scientific
method in experimenting with and comparing approaches.
â€¢ Recent progress in understanding the theoretical basis for intelligence has gone hand in
hand with improvements in the capabilities of real systems. The subfields of AI have
become more integrated, and AI has found common ground with other disciplines.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

The methodological status of artificial intelligence is investigated in The Sciences of the Artificial, by Herb Simon (1981), which discusses research areas concerned with complex artifacts.
It explains how AI can be viewed as both science and mathematics. Cohen (1995) gives an
overview of experimental methodology within AI.
The Turing Test (Turing, 1950) is discussed by Shieber (1994), who severely criticizes
the usefulness of its instantiation in the Loebner Prize competition, and by Ford and Hayes
(1995), who argue that the test itself is not helpful for AI. Bringsjord (2008) gives advice for
a Turing Test judge. Shieber (2004) and Epstein et al. (2008) collect a number of essays on
the Turing Test. Artificial Intelligence: The Very Idea, by John Haugeland (1985), gives a

Exercises

31
readable account of the philosophical and practical problems of AI. Significant early papers
in AI are anthologized in the collections by Webber and Nilsson (1981) and by Luger (1995).
The Encyclopedia of AI (Shapiro, 1992) contains survey articles on almost every topic in
AI, as does Wikipedia. These articles usually provide a good entry point into the research
literature on each topic. An insightful and comprehensive history of AI is given by Nils
Nillson (2009), one of the early pioneers of the field.
The most recent work appears in the proceedings of the major AI conferences: the biennial International Joint Conference on AI (IJCAI), the annual European Conference on AI
(ECAI), and the National Conference on AI, more often known as AAAI, after its sponsoring
organization. The major journals for general AI are Artificial Intelligence, Computational
Intelligence, the IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Intelligent Systems, and the electronic Journal of Artificial Intelligence Research. There are also
many conferences and journals devoted to specific areas, which we cover in the appropriate
chapters. The main professional societies for AI are the American Association for Artificial
Intelligence (AAAI), the ACM Special Interest Group in Artificial Intelligence (SIGART),
and the Society for Artificial Intelligence and Simulation of Behaviour (AISB). AAAIâ€™s AI
Magazine contains many topical and tutorial articles, and its Web site, aaai.org, contains
news, tutorials, and background information.

E XERCISES
These exercises are intended to stimulate discussion, and some might be set as term projects.
Alternatively, preliminary attempts can be made now, and these attempts can be reviewed
after the completion of the book.
1.1 Define in your own words: (a) intelligence, (b) artificial intelligence, (c) agent, (d)
rationality, (e) logical reasoning.
1.2 Read Turingâ€™s original paper on AI (Turing, 1950). In the paper, he discusses several
objections to his proposed enterprise and his test for intelligence. Which objections still carry
weight? Are his refutations valid? Can you think of new objections arising from developments since he wrote the paper? In the paper, he predicts that, by the year 2000, a computer
will have a 30% chance of passing a five-minute Turing Test with an unskilled interrogator.
What chance do you think a computer would have today? In another 50 years?
1.3

Are reflex actions (such as flinching from a hot stove) rational? Are they intelligent?

1.4 Suppose we extend Evansâ€™s A NALOGY program so that it can score 200 on a standard
IQ test. Would we then have a program more intelligent than a human? Explain.
1.5 The neural structure of the sea slug Aplysia has been widely studied (first by Nobel
Laureate Eric Kandel) because it has only about 20,000 neurons, most of them large and
easily manipulated. Assuming that the cycle time for an Aplysia neuron is roughly the same
as for a human neuron, how does the computational power, in terms of memory updates per
second, compare with the high-end computer described in Figure 1.3?

32

Chapter 1.

Introduction

1.6 How could introspectionâ€”reporting on oneâ€™s inner thoughtsâ€”be inaccurate? Could I
be wrong about what Iâ€™m thinking? Discuss.
1.7

To what extent are the following computer systems instances of artificial intelligence:
â€¢
â€¢
â€¢
â€¢

Supermarket bar code scanners.
Web search engines.
Voice-activated telephone menus.
Internet routing algorithms that respond dynamically to the state of the network.

1.8 Many of the computational models of cognitive activities that have been proposed involve quite complex mathematical operations, such as convolving an image with a Gaussian
or finding a minimum of the entropy function. Most humans (and certainly all animals) never
learn this kind of mathematics at all, almost no one learns it before college, and almost no
one can compute the convolution of a function with a Gaussian in their head. What sense
does it make to say that the â€œvision systemâ€ is doing this kind of mathematics, whereas the
actual person has no idea how to do it?
1.9 Why would evolution tend to result in systems that act rationally? What goals are such
systems designed to achieve?
1.10

Is AI a science, or is it engineering? Or neither or both? Explain.

1.11 â€œSurely computers cannot be intelligentâ€”they can do only what their programmers
tell them.â€ Is the latter statement true, and does it imply the former?
1.12 â€œSurely animals cannot be intelligentâ€”they can do only what their genes tell them.â€
Is the latter statement true, and does it imply the former?
1.13 â€œSurely animals, humans, and computers cannot be intelligentâ€”they can do only what
their constituent atoms are told to do by the laws of physics.â€ Is the latter statement true, and
does it imply the former?
1.14 Examine the AI literature to discover whether the following tasks can currently be
solved by computers:
a.
b.
c.
d.
e.
f.
g.
h.
i.
j.
k.

Playing a decent game of table tennis (Ping-Pong).
Driving in the center of Cairo, Egypt.
Driving in Victorville, California.
Buying a weekâ€™s worth of groceries at the market.
Buying a weekâ€™s worth of groceries on the Web.
Playing a decent game of bridge at a competitive level.
Discovering and proving new mathematical theorems.
Writing an intentionally funny story.
Giving competent legal advice in a specialized area of law.
Translating spoken English into spoken Swedish in real time.
Performing a complex surgical operation.

Exercises

33
For the currently infeasible tasks, try to find out what the difficulties are and predict when, if
ever, they will be overcome.
1.15 Various subfields of AI have held contests by defining a standard task and inviting researchers to do their best. Examples include the DARPA Grand Challenge for robotic cars,
The International Planning Competition, the Robocup robotic soccer league, the TREC information retrieval event, and contests in machine translation, speech recognition. Investigate
five of these contests, and describe the progress made over the years. To what degree have the
contests advanced toe state of the art in AI? Do what degree do they hurt the field by drawing
energy away from new ideas?

2

INTELLIGENT AGENTS

In which we discuss the nature of agents, perfect or otherwise, the diversity of
environments, and the resulting menagerie of agent types.

Chapter 1 identified the concept of rational agents as central to our approach to artificial
intelligence. In this chapter, we make this notion more concrete. We will see that the concept
of rationality can be applied to a wide variety of agents operating in any imaginable environment. Our plan in this book is to use this concept to develop a small set of design principles
for building successful agentsâ€”systems that can reasonably be called intelligent.
We begin by examining agents, environments, and the coupling between them. The
observation that some agents behave better than others leads naturally to the idea of a rational
agentâ€”one that behaves as well as possible. How well an agent can behave depends on
the nature of the environment; some environments are more difficult than others. We give a
crude categorization of environments and show how properties of an environment influence
the design of suitable agents for that environment. We describe a number of basic â€œskeletonâ€
agent designs, which we flesh out in the rest of the book.

2.1

AGENTS AND E NVIRONMENTS

ENVIRONMENT
SENSOR
ACTUATOR

PERCEPT
PERCEPT SEQUENCE

An agent is anything that can be viewed as perceiving its environment through sensors and
acting upon that environment through actuators. This simple idea is illustrated in Figure 2.1.
A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so
on for actuators. A robotic agent might have cameras and infrared range finders for sensors
and various motors for actuators. A software agent receives keystrokes, file contents, and
network packets as sensory inputs and acts on the environment by displaying on the screen,
writing files, and sending network packets.
We use the term percept to refer to the agentâ€™s perceptual inputs at any given instant. An
agentâ€™s percept sequence is the complete history of everything the agent has ever perceived.
In general, an agentâ€™s choice of action at any given instant can depend on the entire percept
sequence observed to date, but not on anything it hasnâ€™t perceived. By specifying the agentâ€™s
choice of action for every possible percept sequence, we have said more or less everything
34

Section 2.1.

Agents and Environments

Agent

35

Sensors

Percepts

Environment

?
Actuators

Figure 2.1

AGENT FUNCTION

AGENT PROGRAM

Actions

Agents interact with environments through sensors and actuators.

there is to say about the agent. Mathematically speaking, we say that an agentâ€™s behavior is
described by the agent function that maps any given percept sequence to an action.
We can imagine tabulating the agent function that describes any given agent; for most
agents, this would be a very large tableâ€”infinite, in fact, unless we place a bound on the
length of percept sequences we want to consider. Given an agent to experiment with, we can,
in principle, construct this table by trying out all possible percept sequences and recording
which actions the agent does in response.1 The table is, of course, an external characterization
of the agent. Internally, the agent function for an artificial agent will be implemented by an
agent program. It is important to keep these two ideas distinct. The agent function is an
abstract mathematical description; the agent program is a concrete implementation, running
within some physical system.
To illustrate these ideas, we use a very simple exampleâ€”the vacuum-cleaner world
shown in Figure 2.2. This world is so simple that we can describe everything that happens;
itâ€™s also a made-up world, so we can invent many variations. This particular world has just two
locations: squares A and B. The vacuum agent perceives which square it is in and whether
there is dirt in the square. It can choose to move left, move right, suck up the dirt, or do
nothing. One very simple agent function is the following: if the current square is dirty, then
suck; otherwise, move to the other square. A partial tabulation of this agent function is shown
in Figure 2.3 and an agent program that implements it appears in Figure 2.8 on page 48.
Looking at Figure 2.3, we see that various vacuum-world agents can be defined simply
by filling in the right-hand column in various ways. The obvious question, then, is this: What
is the right way to fill out the table? In other words, what makes an agent good or bad,
intelligent or stupid? We answer these questions in the next section.
1 If the agent uses some randomization to choose its actions, then we would have to try each sequence many
times to identify the probability of each action. One might imagine that acting randomly is rather silly, but we
show later in this chapter that it can be very intelligent.

36

Chapter

A

Figure 2.2

2.

Intelligent Agents

B

A vacuum-cleaner world with just two locations.

Percept sequence

Action

[A, Clean]
[A, Dirty]
[B, Clean]
[B, Dirty]
[A, Clean], [A, Clean]
[A, Clean], [A, Dirty]
..
.

Right
Suck
Left
Suck
Right
Suck
..
.

[A, Clean], [A, Clean], [A, Clean]
[A, Clean], [A, Clean], [A, Dirty]
..
.

Right
Suck
..
.

Figure 2.3 Partial tabulation of a simple agent function for the vacuum-cleaner world
shown in Figure 2.2.

Before closing this section, we should emphasize that the notion of an agent is meant to
be a tool for analyzing systems, not an absolute characterization that divides the world into
agents and non-agents. One could view a hand-held calculator as an agent that chooses the
action of displaying â€œ4â€ when given the percept sequence â€œ2 + 2 =,â€ but such an analysis
would hardly aid our understanding of the calculator. In a sense, all areas of engineering can
be seen as designing artifacts that interact with the world; AI operates at (what the authors
consider to be) the most interesting end of the spectrum, where the artifacts have significant
computational resources and the task environment requires nontrivial decision making.

2.2

G OOD B EHAVIOR : T HE C ONCEPT OF R ATIONALITY

RATIONAL AGENT

A rational agent is one that does the right thingâ€”conceptually speaking, every entry in the
table for the agent function is filled out correctly. Obviously, doing the right thing is better
than doing the wrong thing, but what does it mean to do the right thing?

Section 2.2.

PERFORMANCE
MEASURE

Good Behavior: The Concept of Rationality

We answer this age-old question in an age-old way: by considering the consequences
of the agentâ€™s behavior. When an agent is plunked down in an environment, it generates a
sequence of actions according to the percepts it receives. This sequence of actions causes the
environment to go through a sequence of states. If the sequence is desirable, then the agent
has performed well. This notion of desirability is captured by a performance measure that
evaluates any given sequence of environment states.
Notice that we said environment states, not agent states. If we define success in terms
of agentâ€™s opinion of its own performance, an agent could achieve perfect rationality simply
by deluding itself that its performance was perfect. Human agents in particular are notorious
for â€œsour grapesâ€â€”believing they did not really want something (e.g., a Nobel Prize) after
not getting it.
Obviously, there is not one fixed performance measure for all tasks and agents; typically,
a designer will devise one appropriate to the circumstances. This is not as easy as it sounds.
Consider, for example, the vacuum-cleaner agent from the preceding section. We might
propose to measure performance by the amount of dirt cleaned up in a single eight-hour shift.
With a rational agent, of course, what you ask for is what you get. A rational agent can
maximize this performance measure by cleaning up the dirt, then dumping it all on the floor,
then cleaning it up again, and so on. A more suitable performance measure would reward the
agent for having a clean floor. For example, one point could be awarded for each clean square
at each time step (perhaps with a penalty for electricity consumed and noise generated). As
a general rule, it is better to design performance measures according to what one actually
wants in the environment, rather than according to how one thinks the agent should behave.
Even when the obvious pitfalls are avoided, there remain some knotty issues to untangle.
For example, the notion of â€œclean floorâ€ in the preceding paragraph is based on average
cleanliness over time. Yet the same average cleanliness can be achieved by two different
agents, one of which does a mediocre job all the time while the other cleans energetically but
takes long breaks. Which is preferable might seem to be a fine point of janitorial science, but
in fact it is a deep philosophical question with far-reaching implications. Which is betterâ€”
a reckless life of highs and lows, or a safe but humdrum existence? Which is betterâ€”an
economy where everyone lives in moderate poverty, or one in which some live in plenty
while others are very poor? We leave these questions as an exercise for the diligent reader.

2.2.1 Rationality
What is rational at any given time depends on four things:
â€¢
â€¢
â€¢
â€¢
DEFINITION OF A
RATIONAL AGENT

37

The performance measure that defines the criterion of success.
The agentâ€™s prior knowledge of the environment.
The actions that the agent can perform.
The agentâ€™s percept sequence to date.

This leads to a definition of a rational agent:
For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept
sequence and whatever built-in knowledge the agent has.

38

Chapter

2.

Intelligent Agents

Consider the simple vacuum-cleaner agent that cleans a square if it is dirty and moves to the
other square if not; this is the agent function tabulated in Figure 2.3. Is this a rational agent?
That depends! First, we need to say what the performance measure is, what is known about
the environment, and what sensors and actuators the agent has. Let us assume the following:
â€¢ The performance measure awards one point for each clean square at each time step,
over a â€œlifetimeâ€ of 1000 time steps.
â€¢ The â€œgeographyâ€ of the environment is known a priori (Figure 2.2) but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking
cleans the current square. The Left and Right actions move the agent left and right
except when this would take the agent outside the environment, in which case the agent
remains where it is.
â€¢ The only available actions are Left, Right, and Suck .
â€¢ The agent correctly perceives its location and whether that location contains dirt.
We claim that under these circumstances the agent is indeed rational; its expected performance is at least as high as any other agentâ€™s. Exercise 2.2 asks you to prove this.
One can see easily that the same agent would be irrational under different circumstances. For example, once all the dirt is cleaned up, the agent will oscillate needlessly back
and forth; if the performance measure includes a penalty of one point for each movement left
or right, the agent will fare poorly. A better agent for this case would do nothing once it is
sure that all the squares are clean. If clean squares can become dirty again, the agent should
occasionally check and re-clean them if needed. If the geography of the environment is unknown, the agent will need to explore it rather than stick to squares A and B. Exercise 2.2
asks you to design agents for these cases.

2.2.2 Omniscience, learning, and autonomy
OMNISCIENCE

We need to be careful to distinguish between rationality and omniscience. An omniscient
agent knows the actual outcome of its actions and can act accordingly; but omniscience is
impossible in reality. Consider the following example: I am walking along the Champs
ElyseÌes one day and I see an old friend across the street. There is no traffic nearby and Iâ€™m
not otherwise engaged, so, being rational, I start to cross the street. Meanwhile, at 33,000
feet, a cargo door falls off a passing airliner, 2 and before I make it to the other side of the
street I am flattened. Was I irrational to cross the street? It is unlikely that my obituary would
read â€œIdiot attempts to cross street.â€
This example shows that rationality is not the same as perfection. Rationality maximizes expected performance, while perfection maximizes actual performance. Retreating
from a requirement of perfection is not just a question of being fair to agents. The point is
that if we expect an agent to do what turns out to be the best action after the fact, it will be
impossible to design an agent to fulfill this specificationâ€”unless we improve the performance
of crystal balls or time machines.
2

See N. Henderson, â€œNew door latches urged for Boeing 747 jumbo jets,â€ Washington Post, August 24, 1989.

Section 2.2.

INFORMATION
GATHERING
EXPLORATION

LEARNING

AUTONOMY

Good Behavior: The Concept of Rationality

39

Our definition of rationality does not require omniscience, then, because the rational
choice depends only on the percept sequence to date. We must also ensure that we havenâ€™t
inadvertently allowed the agent to engage in decidedly underintelligent activities. For example, if an agent does not look both ways before crossing a busy road, then its percept sequence
will not tell it that there is a large truck approaching at high speed. Does our definition of
rationality say that itâ€™s now OK to cross the road? Far from it! First, it would not be rational
to cross the road given this uninformative percept sequence: the risk of accident from crossing without looking is too great. Second, a rational agent should choose the â€œlookingâ€ action
before stepping into the street, because looking helps maximize the expected performance.
Doing actions in order to modify future perceptsâ€”sometimes called information gatheringâ€”is an important part of rationality and is covered in depth in Chapter 16. A second
example of information gathering is provided by the exploration that must be undertaken by
a vacuum-cleaning agent in an initially unknown environment.
Our definition requires a rational agent not only to gather information but also to learn
as much as possible from what it perceives. The agentâ€™s initial configuration could reflect
some prior knowledge of the environment, but as the agent gains experience this may be
modified and augmented. There are extreme cases in which the environment is completely
known a priori. In such cases, the agent need not perceive or learn; it simply acts correctly.
Of course, such agents are fragile. Consider the lowly dung beetle. After digging its nest and
laying its eggs, it fetches a ball of dung from a nearby heap to plug the entrance. If the ball of
dung is removed from its grasp en route, the beetle continues its task and pantomimes plugging the nest with the nonexistent dung ball, never noticing that it is missing. Evolution has
built an assumption into the beetleâ€™s behavior, and when it is violated, unsuccessful behavior
results. Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go
out and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is
well, drag the caterpillar inside, and lay its eggs. The caterpillar serves as a food source when
the eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches
away while the sphex is doing the check, it will revert to the â€œdragâ€ step of its plan and will
continue the plan without modification, even after dozens of caterpillar-moving interventions.
The sphex is unable to learn that its innate plan is failing, and thus will not change it.
To the extent that an agent relies on the prior knowledge of its designer rather than
on its own percepts, we say that the agent lacks autonomy. A rational agent should be
autonomousâ€”it should learn what it can to compensate for partial or incorrect prior knowledge. For example, a vacuum-cleaning agent that learns to foresee where and when additional
dirt will appear will do better than one that does not. As a practical matter, one seldom requires complete autonomy from the start: when the agent has had little or no experience, it
would have to act randomly unless the designer gave some assistance. So, just as evolution
provides animals with enough built-in reflexes to survive long enough to learn for themselves,
it would be reasonable to provide an artificial intelligent agent with some initial knowledge
as well as an ability to learn. After sufficient experience of its environment, the behavior
of a rational agent can become effectively independent of its prior knowledge. Hence, the
incorporation of learning allows one to design a single rational agent that will succeed in a
vast variety of environments.

40

2.3

Chapter

2.

Intelligent Agents

T HE NATURE OF E NVIRONMENTS

TASK ENVIRONMENT

Now that we have a definition of rationality, we are almost ready to think about building
rational agents. First, however, we must think about task environments, which are essentially the â€œproblemsâ€ to which rational agents are the â€œsolutions.â€ We begin by showing how
to specify a task environment, illustrating the process with a number of examples. We then
show that task environments come in a variety of flavors. The flavor of the task environment
directly affects the appropriate design for the agent program.

2.3.1 Specifying the task environment

PEAS

In our discussion of the rationality of the simple vacuum-cleaner agent, we had to specify
the performance measure, the environment, and the agentâ€™s actuators and sensors. We group
all these under the heading of the task environment. For the acronymically minded, we call
this the PEAS (Performance, Environment, Actuators, Sensors) description. In designing an
agent, the first step must always be to specify the task environment as fully as possible.
The vacuum world was a simple example; let us consider a more complex problem: an
automated taxi driver. We should point out, before the reader becomes alarmed, that a fully
automated taxi is currently somewhat beyond the capabilities of existing technology. (page 28
describes an existing driving robot.) The full driving task is extremely open-ended. There is
no limit to the novel combinations of circumstances that can ariseâ€”another reason we chose
it as a focus for discussion. Figure 2.4 summarizes the PEAS description for the taxiâ€™s task
environment. We discuss each element in more detail in the following paragraphs.
Agent Type

Performance
Measure

Environment

Actuators

Sensors

Taxi driver

Safe, fast, legal,
comfortable trip,
maximize profits

Roads, other
traffic,
pedestrians,
customers

Steering,
accelerator,
brake, signal,
horn, display

Cameras, sonar,
speedometer,
GPS, odometer,
accelerometer,
engine sensors,
keyboard

Figure 2.4

PEAS description of the task environment for an automated taxi.

First, what is the performance measure to which we would like our automated driver
to aspire? Desirable qualities include getting to the correct destination; minimizing fuel consumption and wear and tear; minimizing the trip time or cost; minimizing violations of traffic
laws and disturbances to other drivers; maximizing safety and passenger comfort; maximizing profits. Obviously, some of these goals conflict, so tradeoffs will be required.
Next, what is the driving environment that the taxi will face? Any taxi driver must
deal with a variety of roads, ranging from rural lanes and urban alleys to 12-lane freeways.
The roads contain other traffic, pedestrians, stray animals, road works, police cars, puddles,

Section 2.3.

SOFTWARE AGENT
SOFTBOT

The Nature of Environments

41

and potholes. The taxi must also interact with potential and actual passengers. There are also
some optional choices. The taxi might need to operate in Southern California, where snow
is seldom a problem, or in Alaska, where it seldom is not. It could always be driving on the
right, or we might want it to be flexible enough to drive on the left when in Britain or Japan.
Obviously, the more restricted the environment, the easier the design problem.
The actuators for an automated taxi include those available to a human driver: control
over the engine through the accelerator and control over steering and braking. In addition, it
will need output to a display screen or voice synthesizer to talk back to the passengers, and
perhaps some way to communicate with other vehicles, politely or otherwise.
The basic sensors for the taxi will include one or more controllable video cameras so
that it can see the road; it might augment these with infrared or sonar sensors to detect distances to other cars and obstacles. To avoid speeding tickets, the taxi should have a speedometer, and to control the vehicle properly, especially on curves, it should have an accelerometer.
To determine the mechanical state of the vehicle, it will need the usual array of engine, fuel,
and electrical system sensors. Like many human drivers, it might want a global positioning
system (GPS) so that it doesnâ€™t get lost. Finally, it will need a keyboard or microphone for
the passenger to request a destination.
In Figure 2.5, we have sketched the basic PEAS elements for a number of additional
agent types. Further examples appear in Exercise 2.4. It may come as a surprise to some readers that our list of agent types includes some programs that operate in the entirely artificial
environment defined by keyboard input and character output on a screen. â€œSurely,â€ one might
say, â€œthis is not a real environment, is it?â€ In fact, what matters is not the distinction between
â€œrealâ€ and â€œartificialâ€ environments, but the complexity of the relationship among the behavior of the agent, the percept sequence generated by the environment, and the performance
measure. Some â€œrealâ€ environments are actually quite simple. For example, a robot designed
to inspect parts as they come by on a conveyor belt can make use of a number of simplifying
assumptions: that the lighting is always just so, that the only thing on the conveyor belt will
be parts of a kind that it knows about, and that only two actions (accept or reject) are possible.
In contrast, some software agents (or software robots or softbots) exist in rich, unlimited domains. Imagine a softbot Web site operator designed to scan Internet news sources and
show the interesting items to its users, while selling advertising space to generate revenue.
To do well, that operator will need some natural language processing abilities, it will need
to learn what each user and advertiser is interested in, and it will need to change its plans
dynamicallyâ€”for example, when the connection for one news source goes down or when a
new one comes online. The Internet is an environment whose complexity rivals that of the
physical world and whose inhabitants include many artificial and human agents.

2.3.2 Properties of task environments
The range of task environments that might arise in AI is obviously vast. We can, however,
identify a fairly small number of dimensions along which task environments can be categorized. These dimensions determine, to a large extent, the appropriate agent design and the
applicability of each of the principal families of techniques for agent implementation. First,

42

Chapter

2.

Intelligent Agents

Agent Type

Performance
Measure

Environment

Actuators

Sensors

Medical
diagnosis system

Healthy patient,
reduced costs

Patient, hospital,
staff

Display of
questions, tests,
diagnoses,
treatments,
referrals

Keyboard entry
of symptoms,
findings, patientâ€™s
answers

Satellite image
analysis system

Correct image
categorization

Downlink from
orbiting satellite

Display of scene
categorization

Color pixel
arrays

Part-picking
robot

Percentage of
parts in correct
bins

Conveyor belt
with parts; bins

Jointed arm and
hand

Camera, joint
angle sensors

Refinery
controller

Purity, yield,
safety

Refinery,
operators

Valves, pumps,
heaters, displays

Temperature,
pressure,
chemical sensors

Interactive
English tutor

Studentâ€™s score
on test

Set of students,
testing agency

Display of
exercises,
suggestions,
corrections

Keyboard entry

Figure 2.5

Examples of agent types and their PEAS descriptions.

we list the dimensions, then we analyze several task environments to illustrate the ideas. The
definitions here are informal; later chapters provide more precise statements and examples of
each kind of environment.
FULLY OBSERVABLE
PARTIALLY
OBSERVABLE

UNOBSERVABLE

SINGLE AGENT
MULTIAGENT

Fully observable vs. partially observable: If an agentâ€™s sensors give it access to the
complete state of the environment at each point in time, then we say that the task environment is fully observable. A task environment is effectively fully observable if the sensors
detect all aspects that are relevant to the choice of action; relevance, in turn, depends on the
performance measure. Fully observable environments are convenient because the agent need
not maintain any internal state to keep track of the world. An environment might be partially
observable because of noisy and inaccurate sensors or because parts of the state are simply
missing from the sensor dataâ€”for example, a vacuum agent with only a local dirt sensor
cannot tell whether there is dirt in other squares, and an automated taxi cannot see what other
drivers are thinking. If the agent has no sensors at all then the environment is unobservable. One might think that in such cases the agentâ€™s plight is hopeless, but, as we discuss in
Chapter 4, the agentâ€™s goals may still be achievable, sometimes with certainty.
Single agent vs. multiagent: The distinction between single-agent and multiagent en-

Section 2.3.

COMPETITIVE

COOPERATIVE

DETERMINISTIC
STOCHASTIC

UNCERTAIN

NONDETERMINISTIC

EPISODIC
SEQUENTIAL

The Nature of Environments

43

vironments may seem simple enough. For example, an agent solving a crossword puzzle by
itself is clearly in a single-agent environment, whereas an agent playing chess is in a twoagent environment. There are, however, some subtle issues. First, we have described how an
entity may be viewed as an agent, but we have not explained which entities must be viewed
as agents. Does an agent A (the taxi driver for example) have to treat an object B (another
vehicle) as an agent, or can it be treated merely as an object behaving according to the laws of
physics, analogous to waves at the beach or leaves blowing in the wind? The key distinction
is whether Bâ€™s behavior is best described as maximizing a performance measure whose value
depends on agent Aâ€™s behavior. For example, in chess, the opponent entity B is trying to
maximize its performance measure, which, by the rules of chess, minimizes agent Aâ€™s performance measure. Thus, chess is a competitive multiagent environment. In the taxi-driving
environment, on the other hand, avoiding collisions maximizes the performance measure of
all agents, so it is a partially cooperative multiagent environment. It is also partially competitive because, for example, only one car can occupy a parking space. The agent-design
problems in multiagent environments are often quite different from those in single-agent environments; for example, communication often emerges as a rational behavior in multiagent
environments; in some competitive environments, randomized behavior is rational because
it avoids the pitfalls of predictability.
Deterministic vs. stochastic. If the next state of the environment is completely determined by the current state and the action executed by the agent, then we say the environment
is deterministic; otherwise, it is stochastic. In principle, an agent need not worry about uncertainty in a fully observable, deterministic environment. (In our definition, we ignore uncertainty that arises purely from the actions of other agents in a multiagent environment; thus,
a game can be deterministic even though each agent may be unable to predict the actions of
the others.) If the environment is partially observable, however, then it could appear to be
stochastic. Most real situations are so complex that it is impossible to keep track of all the
unobserved aspects; for practical purposes, they must be treated as stochastic. Taxi driving is
clearly stochastic in this sense, because one can never predict the behavior of traffic exactly;
moreover, oneâ€™s tires blow out and oneâ€™s engine seizes up without warning. The vacuum
world as we described it is deterministic, but variations can include stochastic elements such
as randomly appearing dirt and an unreliable suction mechanism (Exercise 2.13). We say an
environment is uncertain if it is not fully observable or not deterministic. One final note:
our use of the word â€œstochasticâ€ generally implies that uncertainty about outcomes is quantified in terms of probabilities; a nondeterministic environment is one in which actions are
characterized by their possible outcomes, but no probabilities are attached to them. Nondeterministic environment descriptions are usually associated with performance measures that
require the agent to succeed for all possible outcomes of its actions.
Episodic vs. sequential: In an episodic task environment, the agentâ€™s experience is
divided into atomic episodes. In each episode the agent receives a percept and then performs
a single action. Crucially, the next episode does not depend on the actions taken in previous
episodes. Many classification tasks are episodic. For example, an agent that has to spot
defective parts on an assembly line bases each decision on the current part, regardless of
previous decisions; moreover, the current decision doesnâ€™t affect whether the next part is

44

STATIC
DYNAMIC

SEMIDYNAMIC

DISCRETE
CONTINUOUS

KNOWN
UNKNOWN

Chapter

2.

Intelligent Agents

defective. In sequential environments, on the other hand, the current decision could affect
all future decisions.3 Chess and taxi driving are sequential: in both cases, short-term actions
can have long-term consequences. Episodic environments are much simpler than sequential
environments because the agent does not need to think ahead.
Static vs. dynamic: If the environment can change while an agent is deliberating, then
we say the environment is dynamic for that agent; otherwise, it is static. Static environments
are easy to deal with because the agent need not keep looking at the world while it is deciding
on an action, nor need it worry about the passage of time. Dynamic environments, on the
other hand, are continuously asking the agent what it wants to do; if it hasnâ€™t decided yet,
that counts as deciding to do nothing. If the environment itself does not change with the
passage of time but the agentâ€™s performance score does, then we say the environment is
semidynamic. Taxi driving is clearly dynamic: the other cars and the taxi itself keep moving
while the driving algorithm dithers about what to do next. Chess, when played with a clock,
is semidynamic. Crossword puzzles are static.
Discrete vs. continuous: The discrete/continuous distinction applies to the state of the
environment, to the way time is handled, and to the percepts and actions of the agent. For
example, the chess environment has a finite number of distinct states (excluding the clock).
Chess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and
continuous-time problem: the speed and location of the taxi and of the other vehicles sweep
through a range of continuous values and do so smoothly over time. Taxi-driving actions are
also continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speaking, but is typically treated as representing continuously varying intensities and locations.
Known vs. unknown: Strictly speaking, this distinction refers not to the environment
itself but to the agentâ€™s (or designerâ€™s) state of knowledge about the â€œlaws of physicsâ€ of
the environment. In a known environment, the outcomes (or outcome probabilities if the
environment is stochastic) for all actions are given. Obviously, if the environment is unknown,
the agent will have to learn how it works in order to make good decisions. Note that the
distinction between known and unknown environments is not the same as the one between
fully and partially observable environments. It is quite possible for a known environment
to be partially observableâ€”for example, in solitaire card games, I know the rules but am
still unable to see the cards that have not yet been turned over. Conversely, an unknown
environment can be fully observableâ€”in a new video game, the screen may show the entire
game state but I still donâ€™t know what the buttons do until I try them.
As one might expect, the hardest case is partially observable, multiagent, stochastic,
sequential, dynamic, continuous, and unknown. Taxi driving is hard in all these senses, except
that for the most part the driverâ€™s environment is known. Driving a rented car in a new country
with unfamiliar geography and traffic laws is a lot more exciting.
Figure 2.6 lists the properties of a number of familiar environments. Note that the
answers are not always cut and dried. For example, we describe the part-picking robot as
episodic, because it normally considers each part in isolation. But if one day there is a large
3

The word â€œsequentialâ€ is also used in computer science as the antonym of â€œparallel.â€ The two meanings are
largely unrelated.

Section 2.3.

The Nature of Environments
Task Environment
Crossword puzzle
Chess with a clock

Observable Agents Deterministic Episodic
Fully
Fully

Static

Discrete

Single Deterministic Sequential
Multi Deterministic Sequential

Static
Semi

Discrete
Discrete

Static
Static

Discrete
Discrete

Poker
Backgammon

Partially
Fully

Multi
Multi

Stochastic
Stochastic

Sequential
Sequential

Taxi driving
Medical diagnosis

Partially
Partially

Multi
Single

Stochastic
Stochastic

Sequential Dynamic Continuous
Sequential Dynamic Continuous

Image analysis
Part-picking robot

Fully
Partially

Single Deterministic Episodic
Semi Continuous
Single Stochastic
Episodic Dynamic Continuous

Refinery controller
Interactive English tutor

Partially
Partially

Single
Multi

Figure 2.6

ENVIRONMENT
CLASS

45

Stochastic
Stochastic

Sequential Dynamic Continuous
Sequential Dynamic Discrete

Examples of task environments and their characteristics.

batch of defective parts, the robot should learn from several observations that the distribution
of defects has changed, and should modify its behavior for subsequent parts. We have not
included a â€œknown/unknownâ€ column because, as explained earlier, this is not strictly a property of the environment. For some environments, such as chess and poker, it is quite easy to
supply the agent with full knowledge of the rules, but it is nonetheless interesting to consider
how an agent might learn to play these games without such knowledge.
Several of the answers in the table depend on how the task environment is defined. We
have listed the medical-diagnosis task as single-agent because the disease process in a patient
is not profitably modeled as an agent; but a medical-diagnosis system might also have to
deal with recalcitrant patients and skeptical staff, so the environment could have a multiagent
aspect. Furthermore, medical diagnosis is episodic if one conceives of the task as selecting a
diagnosis given a list of symptoms; the problem is sequential if the task can include proposing
a series of tests, evaluating progress over the course of treatment, and so on. Also, many
environments are episodic at higher levels than the agentâ€™s individual actions. For example,
a chess tournament consists of a sequence of games; each game is an episode because (by
and large) the contribution of the moves in one game to the agentâ€™s overall performance is
not affected by the moves in its previous game. On the other hand, decision making within a
single game is certainly sequential.
The code repository associated with this book (aima.cs.berkeley.edu) includes implementations of a number of environments, together with a general-purpose environment simulator that places one or more agents in a simulated environment, observes their behavior over
time, and evaluates them according to a given performance measure. Such experiments are
often carried out not for a single environment but for many environments drawn from an environment class. For example, to evaluate a taxi driver in simulated traffic, we would want to
run many simulations with different traffic, lighting, and weather conditions. If we designed
the agent for a single scenario, we might be able to take advantage of specific properties
of the particular case but might not identify a good design for driving in general. For this

46

Chapter

ENVIRONMENT
GENERATOR

2.4

2.

Intelligent Agents

reason, the code repository also includes an environment generator for each environment
class that selects particular environments (with certain likelihoods) in which to run the agent.
For example, the vacuum environment generator initializes the dirt pattern and agent location
randomly. We are then interested in the agentâ€™s average performance over the environment
class. A rational agent for a given environment class maximizes this average performance.
Exercises 2.8 to 2.13 take you through the process of developing an environment class and
evaluating various agents therein.

T HE S TRUCTURE OF AGENTS

AGENT PROGRAM

ARCHITECTURE

So far we have talked about agents by describing behaviorâ€”the action that is performed after
any given sequence of percepts. Now we must bite the bullet and talk about how the insides
work. The job of AI is to design an agent program that implements the agent functionâ€”
the mapping from percepts to actions. We assume this program will run on some sort of
computing device with physical sensors and actuatorsâ€”we call this the architecture:
agent = architecture + program .
Obviously, the program we choose has to be one that is appropriate for the architecture. If the
program is going to recommend actions like Walk, the architecture had better have legs. The
architecture might be just an ordinary PC, or it might be a robotic car with several onboard
computers, cameras, and other sensors. In general, the architecture makes the percepts from
the sensors available to the program, runs the program, and feeds the programâ€™s action choices
to the actuators as they are generated. Most of this book is about designing agent programs,
although Chapters 24 and 25 deal directly with the sensors and actuators.

2.4.1 Agent programs
The agent programs that we design in this book all have the same skeleton: they take the
current percept as input from the sensors and return an action to the actuators.4 Notice the
difference between the agent program, which takes the current percept as input, and the agent
function, which takes the entire percept history. The agent program takes just the current
percept as input because nothing more is available from the environment; if the agentâ€™s actions
need to depend on the entire percept sequence, the agent will have to remember the percepts.
We describe the agent programs in the simple pseudocode language that is defined in
Appendix B. (The online code repository contains implementations in real programming
languages.) For example, Figure 2.7 shows a rather trivial agent program that keeps track of
the percept sequence and then uses it to index into a table of actions to decide what to do.
The tableâ€”an example of which is given for the vacuum world in Figure 2.3â€”represents
explicitly the agent function that the agent program embodies. To build a rational agent in
4 There are other choices for the agent program skeleton; for example, we could have the agent programs be
coroutines that run asynchronously with the environment. Each such coroutine has an input and output port and
consists of a loop that reads the input port for percepts and writes actions to the output port.

Section 2.4.

The Structure of Agents

47

function TABLE -D RIVEN -AGENT( percept ) returns an action
persistent: percepts, a sequence, initially empty
table, a table of actions, indexed by percept sequences, initially fully specified
append percept to the end of percepts
action â† L OOKUP( percepts, table)
return action
Figure 2.7 The TABLE -D RIVEN -AGENT program is invoked for each new percept and
returns an action each time. It retains the complete percept sequence in memory.

this way, we as designers must construct a table that contains the appropriate action for every
possible percept sequence.
It is instructive to consider why the table-driven approach to agent construction is
doomed to failure. Let P be the set of possible percepts and let T be the lifetime
 of the
agent (the total number of percepts it will receive). The lookup table will contain Tt= 1 |P|t
entries. Consider the automated taxi: the visual input from a single camera comes in at the
rate of roughly 27 megabytes per second (30 frames per second, 640 Ã— 480 pixels with 24
bits of color information). This gives a lookup table with over 10250,000,000,000 entries for an
hourâ€™s driving. Even the lookup table for chessâ€”a tiny, well-behaved fragment of the real
worldâ€”would have at least 10150 entries. The daunting size of these tables (the number of
atoms in the observable universe is less than 1080 ) means that (a) no physical agent in this
universe will have the space to store the table, (b) the designer would not have time to create
the table, (c) no agent could ever learn all the right table entries from its experience, and (d)
even if the environment is simple enough to yield a feasible table size, the designer still has
no guidance about how to fill in the table entries.
Despite all this, TABLE -D RIVEN -AGENT does do what we want: it implements the
desired agent function. The key challenge for AI is to find out how to write programs that,
to the extent possible, produce rational behavior from a smallish program rather than from
a vast table. We have many examples showing that this can be done successfully in other
areas: for example, the huge tables of square roots used by engineers and schoolchildren prior
to the 1970s have now been replaced by a five-line program for Newtonâ€™s method running
on electronic calculators. The question is, can AI do for general intelligent behavior what
Newton did for square roots? We believe the answer is yes.
In the remainder of this section, we outline four basic kinds of agent programs that
embody the principles underlying almost all intelligent systems:
â€¢
â€¢
â€¢
â€¢

Simple reflex agents;
Model-based reflex agents;
Goal-based agents; and
Utility-based agents.

Each kind of agent program combines particular components in particular ways to generate
actions. Section 2.4.6 explains in general terms how to convert all these agents into learning

48

Chapter

2.

Intelligent Agents

function R EFLEX -VACUUM -AGENT( [location,status]) returns an action
if status = Dirty then return Suck
else if location = A then return Right
else if location = B then return Left
Figure 2.8 The agent program for a simple reflex agent in the two-state vacuum environment. This program implements the agent function tabulated in Figure 2.3.

agents that can improve the performance of their components so as to generate better actions.
Finally, Section 2.4.7 describes the variety of ways in which the components themselves can
be represented within the agent. This variety provides a major organizing principle for the
field and for the book itself.

2.4.2 Simple reflex agents
SIMPLE REFLEX
AGENT

CONDITIONâ€“ACTION
RULE

The simplest kind of agent is the simple reflex agent. These agents select actions on the basis
of the current percept, ignoring the rest of the percept history. For example, the vacuum agent
whose agent function is tabulated in Figure 2.3 is a simple reflex agent, because its decision
is based only on the current location and on whether that location contains dirt. An agent
program for this agent is shown in Figure 2.8.
Notice that the vacuum agent program is very small indeed compared to the corresponding table. The most obvious reduction comes from ignoring the percept history, which cuts
down the number of possibilities from 4T to just 4. A further, small reduction comes from
the fact that when the current square is dirty, the action does not depend on the location.
Simple reflex behaviors occur even in more complex environments. Imagine yourself
as the driver of the automated taxi. If the car in front brakes and its brake lights come on, then
you should notice this and initiate braking. In other words, some processing is done on the
visual input to establish the condition we call â€œThe car in front is braking.â€ Then, this triggers
some established connection in the agent program to the action â€œinitiate braking.â€ We call
such a connection a conditionâ€“action rule,5 written as
if car-in-front-is-braking then initiate-braking.
Humans also have many such connections, some of which are learned responses (as for driving) and some of which are innate reflexes (such as blinking when something approaches the
eye). In the course of the book, we show several different ways in which such connections
can be learned and implemented.
The program in Figure 2.8 is specific to one particular vacuum environment. A more
general and flexible approach is first to build a general-purpose interpreter for conditionâ€“
action rules and then to create rule sets for specific task environments. Figure 2.9 gives the
structure of this general program in schematic form, showing how the conditionâ€“action rules
allow the agent to make the connection from percept to action. (Do not worry if this seems
5

Also called situationâ€“action rules, productions, or ifâ€“then rules.

Section 2.4.

The Structure of Agents

Agent

49

Sensors
What the world
is like now

Environment

Condition-action rules

What action I
should do now

Actuators

Figure 2.9

Schematic diagram of a simple reflex agent.

function S IMPLE -R EFLEX -AGENT( percept ) returns an action
persistent: rules, a set of conditionâ€“action rules
state â† I NTERPRET-I NPUT( percept )
rule â† RULE -M ATCH(state, rules)
action â† rule.ACTION
return action
Figure 2.10 A simple reflex agent. It acts according to a rule whose condition matches
the current state, as defined by the percept.

trivial; it gets more interesting shortly.) We use rectangles to denote the current internal state
of the agentâ€™s decision process, and ovals to represent the background information used in
the process. The agent program, which is also very simple, is shown in Figure 2.10. The
I NTERPRET-I NPUT function generates an abstracted description of the current state from the
percept, and the RULE -M ATCH function returns the first rule in the set of rules that matches
the given state description. Note that the description in terms of â€œrulesâ€ and â€œmatchingâ€ is
purely conceptual; actual implementations can be as simple as a collection of logic gates
implementing a Boolean circuit.
Simple reflex agents have the admirable property of being simple, but they turn out to be
of limited intelligence. The agent in Figure 2.10 will work only if the correct decision can be
made on the basis of only the current perceptâ€”that is, only if the environment is fully observable. Even a little bit of unobservability can cause serious trouble. For example, the braking
rule given earlier assumes that the condition car-in-front-is-braking can be determined from
the current perceptâ€”a single frame of video. This works if the car in front has a centrally
mounted brake light. Unfortunately, older models have different configurations of taillights,

50

RANDOMIZATION

Chapter

2.

Intelligent Agents

brake lights, and turn-signal lights, and it is not always possible to tell from a single image
whether the car is braking. A simple reflex agent driving behind such a car would either brake
continuously and unnecessarily, or, worse, never brake at all.
We can see a similar problem arising in the vacuum world. Suppose that a simple reflex
vacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent
has just two possible percepts: [Dirty] and [Clean]. It can Suck in response to [Dirty]; what
should it do in response to [Clean]? Moving Left fails (forever) if it happens to start in square
A, and moving Right fails (forever) if it happens to start in square B. Infinite loops are often
unavoidable for simple reflex agents operating in partially observable environments.
Escape from infinite loops is possible if the agent can randomize its actions. For example, if the vacuum agent perceives [Clean], it might flip a coin to choose between Left and
Right. It is easy to show that the agent will reach the other square in an average of two steps.
Then, if that square is dirty, the agent will clean it and the task will be complete. Hence, a
randomized simple reflex agent might outperform a deterministic simple reflex agent.
We mentioned in Section 2.3 that randomized behavior of the right kind can be rational
in some multiagent environments. In single-agent environments, randomization is usually not
rational. It is a useful trick that helps a simple reflex agent in some situations, but in most
cases we can do much better with more sophisticated deterministic agents.

2.4.3 Model-based reflex agents

INTERNAL STATE

MODEL-BASED
AGENT

The most effective way to handle partial observability is for the agent to keep track of the
part of the world it canâ€™t see now. That is, the agent should maintain some sort of internal
state that depends on the percept history and thereby reflects at least some of the unobserved
aspects of the current state. For the braking problem, the internal state is not too extensiveâ€”
just the previous frame from the camera, allowing the agent to detect when two red lights at
the edge of the vehicle go on or off simultaneously. For other driving tasks such as changing
lanes, the agent needs to keep track of where the other cars are if it canâ€™t see them all at once.
And for any driving to be possible at all, the agent needs to keep track of where its keys are.
Updating this internal state information as time goes by requires two kinds of knowledge to be encoded in the agent program. First, we need some information about how the
world evolves independently of the agentâ€”for example, that an overtaking car generally will
be closer behind than it was a moment ago. Second, we need some information about how
the agentâ€™s own actions affect the worldâ€”for example, that when the agent turns the steering
wheel clockwise, the car turns to the right, or that after driving for five minutes northbound
on the freeway, one is usually about five miles north of where one was five minutes ago. This
knowledge about â€œhow the world worksâ€â€”whether implemented in simple Boolean circuits
or in complete scientific theoriesâ€”is called a model of the world. An agent that uses such a
model is called a model-based agent.
Figure 2.11 gives the structure of the model-based reflex agent with internal state, showing how the current percept is combined with the old internal state to generate the updated
description of the current state, based on the agentâ€™s model of how the world works. The agent
program is shown in Figure 2.12. The interesting part is the function U PDATE -S TATE , which

Section 2.4.

The Structure of Agents

51

Sensors
State
How the world evolves

What my actions do

Condition-action rules

Agent
Figure 2.11

Environment

What the world
is like now

What action I
should do now
Actuators

A model-based reflex agent.

function M ODEL -BASED -R EFLEX -AGENT( percept ) returns an action
persistent: state, the agentâ€™s current conception of the world state
model , a description of how the next state depends on current state and action
rules, a set of conditionâ€“action rules
action, the most recent action, initially none
state â† U PDATE -S TATE(state, action , percept , model )
rule â† RULE -M ATCH(state, rules)
action â† rule.ACTION
return action
Figure 2.12 A model-based reflex agent. It keeps track of the current state of the world,
using an internal model. It then chooses an action in the same way as the reflex agent.

is responsible for creating the new internal state description. The details of how models and
states are represented vary widely depending on the type of environment and the particular
technology used in the agent design. Detailed examples of models and updating algorithms
appear in Chapters 4, 12, 11, 15, 17, and 25.
Regardless of the kind of representation used, it is seldom possible for the agent to
determine the current state of a partially observable environment exactly. Instead, the box
labeled â€œwhat the world is like nowâ€ (Figure 2.11) represents the agentâ€™s â€œbest guessâ€ (or
sometimes best guesses). For example, an automated taxi may not be able to see around the
large truck that has stopped in front of it and can only guess about what may be causing the
hold-up. Thus, uncertainty about the current state may be unavoidable, but the agent still has
to make a decision.
A perhaps less obvious point about the internal â€œstateâ€ maintained by a model-based
agent is that it does not have to describe â€œwhat the world is like nowâ€ in a literal sense. For

52

Chapter

2.

Intelligent Agents

Sensors
State
What the world
is like now

What my actions do

What it will be like
if I do action A

Goals

What action I
should do now

Agent

Environment

How the world evolves

Actuators

Figure 2.13 A model-based, goal-based agent. It keeps track of the world state as well as
a set of goals it is trying to achieve, and chooses an action that will (eventually) lead to the
achievement of its goals.

example, the taxi may be driving back home, and it may have a rule telling it to fill up with
gas on the way home unless it has at least half a tank. Although â€œdriving back homeâ€ may
seem to an aspect of the world state, the fact of the taxiâ€™s destination is actually an aspect of
the agentâ€™s internal state. If you find this puzzling, consider that the taxi could be in exactly
the same place at the same time, but intending to reach a different destination.

2.4.4 Goal-based agents

GOAL

Knowing something about the current state of the environment is not always enough to decide
what to do. For example, at a road junction, the taxi can turn left, turn right, or go straight
on. The correct decision depends on where the taxi is trying to get to. In other words, as well
as a current state description, the agent needs some sort of goal information that describes
situations that are desirableâ€”for example, being at the passengerâ€™s destination. The agent
program can combine this with the model (the same information as was used in the modelbased reflex agent) to choose actions that achieve the goal. Figure 2.13 shows the goal-based
agentâ€™s structure.
Sometimes goal-based action selection is straightforwardâ€”for example, when goal satisfaction results immediately from a single action. Sometimes it will be more trickyâ€”for
example, when the agent has to consider long sequences of twists and turns in order to find a
way to achieve the goal. Search (Chapters 3 to 5) and planning (Chapters 10 and 11) are the
subfields of AI devoted to finding action sequences that achieve the agentâ€™s goals.
Notice that decision making of this kind is fundamentally different from the conditionâ€“
action rules described earlier, in that it involves consideration of the futureâ€”both â€œWhat will
happen if I do such-and-such?â€ and â€œWill that make me happy?â€ In the reflex agent designs,
this information is not explicitly represented, because the built-in rules map directly from

Section 2.4.

The Structure of Agents

53

percepts to actions. The reflex agent brakes when it sees brake lights. A goal-based agent, in
principle, could reason that if the car in front has its brake lights on, it will slow down. Given
the way the world usually evolves, the only action that will achieve the goal of not hitting
other cars is to brake.
Although the goal-based agent appears less efficient, it is more flexible because the
knowledge that supports its decisions is represented explicitly and can be modified. If it starts
to rain, the agent can update its knowledge of how effectively its brakes will operate; this will
automatically cause all of the relevant behaviors to be altered to suit the new conditions.
For the reflex agent, on the other hand, we would have to rewrite many conditionâ€“action
rules. The goal-based agentâ€™s behavior can easily be changed to go to a different destination,
simply by specifying that destination as the goal. The reflex agentâ€™s rules for when to turn
and when to go straight will work only for a single destination; they must all be replaced to
go somewhere new.

2.4.5 Utility-based agents

UTILITY

UTILITY FUNCTION

EXPECTED UTILITY

Goals alone are not enough to generate high-quality behavior in most environments. For
example, many action sequences will get the taxi to its destination (thereby achieving the
goal) but some are quicker, safer, more reliable, or cheaper than others. Goals just provide a
crude binary distinction between â€œhappyâ€ and â€œunhappyâ€ states. A more general performance
measure should allow a comparison of different world states according to exactly how happy
they would make the agent. Because â€œhappyâ€ does not sound very scientific, economists and
computer scientists use the term utility instead.6
We have already seen that a performance measure assigns a score to any given sequence
of environment states, so it can easily distinguish between more and less desirable ways of
getting to the taxiâ€™s destination. An agentâ€™s utility function is essentially an internalization
of the performance measure. If the internal utility function and the external performance
measure are in agreement, then an agent that chooses actions to maximize its utility will be
rational according to the external performance measure.
Let us emphasize again that this is not the only way to be rationalâ€”we have already
seen a rational agent program for the vacuum world (Figure 2.8) that has no idea what its
utility function isâ€”but, like goal-based agents, a utility-based agent has many advantages in
terms of flexibility and learning. Furthermore, in two kinds of cases, goals are inadequate but
a utility-based agent can still make rational decisions. First, when there are conflicting goals,
only some of which can be achieved (for example, speed and safety), the utility function
specifies the appropriate tradeoff. Second, when there are several goals that the agent can
aim for, none of which can be achieved with certainty, utility provides a way in which the
likelihood of success can be weighed against the importance of the goals.
Partial observability and stochasticity are ubiquitous in the real world, and so, therefore,
is decision making under uncertainty. Technically speaking, a rational utility-based agent
chooses the action that maximizes the expected utility of the action outcomesâ€”that is, the
utility the agent expects to derive, on average, given the probabilities and utilities of each
6

The word â€œutilityâ€ here refers to â€œthe quality of being useful,â€ not to the electric company or waterworks.

54

Chapter

2.

Intelligent Agents

Sensors
State
What the world
is like now

What my actions do

What it will be like
if I do action A

Utility

How happy I will be
in such a state

Environment

How the world evolves

What action I
should do now

Agent

Actuators

Figure 2.14 A model-based, utility-based agent. It uses a model of the world, along with
a utility function that measures its preferences among states of the world. Then it chooses the
action that leads to the best expected utility, where expected utility is computed by averaging
over all possible outcome states, weighted by the probability of the outcome.

outcome. (Appendix A defines expectation more precisely.) In Chapter 16, we show that any
rational agent must behave as if it possesses a utility function whose expected value it tries
to maximize. An agent that possesses an explicit utility function can make rational decisions
with a general-purpose algorithm that does not depend on the specific utility function being
maximized. In this way, the â€œglobalâ€ definition of rationalityâ€”designating as rational those
agent functions that have the highest performanceâ€”is turned into a â€œlocalâ€ constraint on
rational-agent designs that can be expressed in a simple program.
The utility-based agent structure appears in Figure 2.14. Utility-based agent programs
appear in Part IV, where we design decision-making agents that must handle the uncertainty
inherent in stochastic or partially observable environments.
At this point, the reader may be wondering, â€œIs it that simple? We just build agents that
maximize expected utility, and weâ€™re done?â€ Itâ€™s true that such agents would be intelligent,
but itâ€™s not simple. A utility-based agent has to model and keep track of its environment,
tasks that have involved a great deal of research on perception, representation, reasoning,
and learning. The results of this research fill many of the chapters of this book. Choosing
the utility-maximizing course of action is also a difficult task, requiring ingenious algorithms
that fill several more chapters. Even with these algorithms, perfect rationality is usually
unachievable in practice because of computational complexity, as we noted in Chapter 1.

2.4.6 Learning agents
We have described agent programs with various methods for selecting actions. We have
not, so far, explained how the agent programs come into being. In his famous early paper,
Turing (1950) considers the idea of actually programming his intelligent machines by hand.

Section 2.4.

The Structure of Agents

55

Performance standard

Sensors

Critic

changes
Learning
element

knowledge

Performance
element

learning
goals

Environment

feedback

Problem
generator

Agent
Figure 2.15

LEARNING ELEMENT
PERFORMANCE
ELEMENT

CRITIC

Actuators

A general learning agent.

He estimates how much work this might take and concludes â€œSome more expeditious method
seems desirable.â€ The method he proposes is to build learning machines and then to teach
them. In many areas of AI, this is now the preferred method for creating state-of-the-art
systems. Learning has another advantage, as we noted earlier: it allows the agent to operate
in initially unknown environments and to become more competent than its initial knowledge
alone might allow. In this section, we briefly introduce the main ideas of learning agents.
Throughout the book, we comment on opportunities and methods for learning in particular
kinds of agents. Part V goes into much more depth on the learning algorithms themselves.
A learning agent can be divided into four conceptual components, as shown in Figure 2.15. The most important distinction is between the learning element, which is responsible for making improvements, and the performance element, which is responsible for
selecting external actions. The performance element is what we have previously considered
to be the entire agent: it takes in percepts and decides on actions. The learning element uses
feedback from the critic on how the agent is doing and determines how the performance
element should be modified to do better in the future.
The design of the learning element depends very much on the design of the performance
element. When trying to design an agent that learns a certain capability, the first question is
not â€œHow am I going to get it to learn this?â€ but â€œWhat kind of performance element will my
agent need to do this once it has learned how?â€ Given an agent design, learning mechanisms
can be constructed to improve every part of the agent.
The critic tells the learning element how well the agent is doing with respect to a fixed
performance standard. The critic is necessary because the percepts themselves provide no
indication of the agentâ€™s success. For example, a chess program could receive a percept
indicating that it has checkmated its opponent, but it needs a performance standard to know
that this is a good thing; the percept itself does not say so. It is important that the performance

56

PROBLEM
GENERATOR

Chapter

2.

Intelligent Agents

standard be fixed. Conceptually, one should think of it as being outside the agent altogether
because the agent must not modify it to fit its own behavior.
The last component of the learning agent is the problem generator. It is responsible
for suggesting actions that will lead to new and informative experiences. The point is that
if the performance element had its way, it would keep doing the actions that are best, given
what it knows. But if the agent is willing to explore a little and do some perhaps suboptimal
actions in the short run, it might discover much better actions for the long run. The problem
generatorâ€™s job is to suggest these exploratory actions. This is what scientists do when they
carry out experiments. Galileo did not think that dropping rocks from the top of a tower in
Pisa was valuable in itself. He was not trying to break the rocks or to modify the brains of
unfortunate passers-by. His aim was to modify his own brain by identifying a better theory
of the motion of objects.
To make the overall design more concrete, let us return to the automated taxi example.
The performance element consists of whatever collection of knowledge and procedures the
taxi has for selecting its driving actions. The taxi goes out on the road and drives, using
this performance element. The critic observes the world and passes information along to the
learning element. For example, after the taxi makes a quick left turn across three lanes of traffic, the critic observes the shocking language used by other drivers. From this experience, the
learning element is able to formulate a rule saying this was a bad action, and the performance
element is modified by installation of the new rule. The problem generator might identify
certain areas of behavior in need of improvement and suggest experiments, such as trying out
the brakes on different road surfaces under different conditions.
The learning element can make changes to any of the â€œknowledgeâ€ components shown
in the agent diagrams (Figures 2.9, 2.11, 2.13, and 2.14). The simplest cases involve learning
directly from the percept sequence. Observation of pairs of successive states of the environment can allow the agent to learn â€œHow the world evolves,â€ and observation of the results of
its actions can allow the agent to learn â€œWhat my actions do.â€ For example, if the taxi exerts
a certain braking pressure when driving on a wet road, then it will soon find out how much
deceleration is actually achieved. Clearly, these two learning tasks are more difficult if the
environment is only partially observable.
The forms of learning in the preceding paragraph do not need to access the external
performance standardâ€”in a sense, the standard is the universal one of making predictions
that agree with experiment. The situation is slightly more complex for a utility-based agent
that wishes to learn utility information. For example, suppose the taxi-driving agent receives
no tips from passengers who have been thoroughly shaken up during the trip. The external
performance standard must inform the agent that the loss of tips is a negative contribution to
its overall performance; then the agent might be able to learn that violent maneuvers do not
contribute to its own utility. In a sense, the performance standard distinguishes part of the
incoming percept as a reward (or penalty) that provides direct feedback on the quality of the
agentâ€™s behavior. Hard-wired performance standards such as pain and hunger in animals can
be understood in this way. This issue is discussed further in Chapter 21.
In summary, agents have a variety of components, and those components can be represented in many ways within the agent program, so there appears to be great variety among

Section 2.4.

The Structure of Agents

57

learning methods. There is, however, a single unifying theme. Learning in intelligent agents
can be summarized as a process of modification of each component of the agent to bring the
components into closer agreement with the available feedback information, thereby improving the overall performance of the agent.

2.4.7 How the components of agent programs work
We have described agent programs (in very high-level terms) as consisting of various components, whose function it is to answer questions such as: â€œWhat is the world like now?â€ â€œWhat
action should I do now?â€ â€œWhat do my actions do?â€ The next question for a student of AI
is, â€œHow on earth do these components work?â€ It takes about a thousand pages to begin to
answer that question properly, but here we want to draw the readerâ€™s attention to some basic
distinctions among the various ways that the components can represent the environment that
the agent inhabits.
Roughly speaking, we can place the representations along an axis of increasing complexity and expressive powerâ€”atomic, factored, and structured. To illustrate these ideas,
it helps to consider a particular agent component, such as the one that deals with â€œWhat my
actions do.â€ This component describes the changes that might occur in the environment as
the result of taking an action, and Figure 2.16 provides schematic depictions of how those
transitions might be represented.

B

C

B

(a) Atomic

C

(b) Factored

(b) Structured

Figure 2.16 Three ways to represent states and the transitions between them. (a) Atomic
representation: a state (such as B or C) is a black box with no internal structure; (b) Factored
representation: a state consists of a vector of attribute values; values can be Boolean, realvalued, or one of a fixed set of symbols. (c) Structured representation: a state includes
objects, each of which may have attributes of its own as well as relationships to other objects.
ATOMIC
REPRESENTATION

In an atomic representation each state of the world is indivisibleâ€”it has no internal
structure. Consider the problem of finding a driving route from one end of a country to the
other via some sequence of cities (we address this problem in Figure 3.2 on page 68). For the
purposes of solving this problem, it may suffice to reduce the state of world to just the name
of the city we are inâ€”a single atom of knowledge; a â€œblack boxâ€ whose only discernible
property is that of being identical to or different from another black box. The algorithms

58

FACTORED
REPRESENTATION
VARIABLE
ATTRIBUTE
VALUE

STRUCTURED
REPRESENTATION

EXPRESSIVENESS

Chapter

2.

Intelligent Agents

underlying search and game-playing (Chapters 3â€“5), Hidden Markov models (Chapter 15),
and Markov decision processes (Chapter 17) all work with atomic representationsâ€”or, at
least, they treat representations as if they were atomic.
Now consider a higher-fidelity description for the same problem, where we need to be
concerned with more than just atomic location in one city or another; we might need to pay
attention to how much gas is in the tank, our current GPS coordinates, whether or not the oil
warning light is working, how much spare change we have for toll crossings, what station is
on the radio, and so on. A factored representation splits up each state into a fixed set of
variables or attributes, each of which can have a value. While two different atomic states
have nothing in commonâ€”they are just different black boxesâ€”two different factored states
can share some attributes (such as being at some particular GPS location) and not others (such
as having lots of gas or having no gas); this makes it much easier to work out how to turn
one state into another. With factored representations, we can also represent uncertaintyâ€”for
example, ignorance about the amount of gas in the tank can be represented by leaving that
attribute blank. Many important areas of AI are based on factored representations, including
constraint satisfaction algorithms (Chapter 6), propositional logic (Chapter 7), planning
(Chapters 10 and 11), Bayesian networks (Chapters 13â€“16), and the machine learning algorithms in Chapters 18, 20, and 21.
For many purposes, we need to understand the world as having things in it that are
related to each other, not just variables with values. For example, we might notice that a
large truck ahead of us is reversing into the driveway of a dairy farm but a cow has got loose
and is blocking the truckâ€™s path. A factored representation is unlikely to be pre-equipped
with the attribute TruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCow with
value true or false. Instead, we would need a structured representation, in which objects such as cows and trucks and their various and varying relationships can be described
explicitly. (See Figure 2.16(c).) Structured representations underlie relational databases
and first-order logic (Chapters 8, 9, and 12), first-order probability models (Chapter 14),
knowledge-based learning (Chapter 19) and much of natural language understanding
(Chapters 22 and 23). In fact, almost everything that humans express in natural language
concerns objects and their relationships.
As we mentioned earlier, the axis along which atomic, factored, and structured representations lie is the axis of increasing expressiveness. Roughly speaking, a more expressive
representation can capture, at least as concisely, everything a less expressive one can capture,
plus some more. Often, the more expressive language is much more concise; for example, the
rules of chess can be written in a page or two of a structured-representation language such
as first-order logic but require thousands of pages when written in a factored-representation
language such as propositional logic. On the other hand, reasoning and learning become
more complex as the expressive power of the representation increases. To gain the benefits
of expressive representations while avoiding their drawbacks, intelligent systems for the real
world may need to operate at all points along the axis simultaneously.

Section 2.5.

2.5

Summary

59

S UMMARY
This chapter has been something of a whirlwind tour of AI, which we have conceived of as
the science of agent design. The major points to recall are as follows:
â€¢ An agent is something that perceives and acts in an environment. The agent function
for an agent specifies the action taken by the agent in response to any percept sequence.
â€¢ The performance measure evaluates the behavior of the agent in an environment. A
rational agent acts so as to maximize the expected value of the performance measure,
given the percept sequence it has seen so far.
â€¢ A task environment specification includes the performance measure, the external environment, the actuators, and the sensors. In designing an agent, the first step must
always be to specify the task environment as fully as possible.
â€¢ Task environments vary along several significant dimensions. They can be fully or
partially observable, single-agent or multiagent, deterministic or stochastic, episodic or
sequential, static or dynamic, discrete or continuous, and known or unknown.
â€¢ The agent program implements the agent function. There exists a variety of basic
agent-program designs reflecting the kind of information made explicit and used in the
decision process. The designs vary in efficiency, compactness, and flexibility. The
appropriate design of the agent program depends on the nature of the environment.
â€¢ Simple reflex agents respond directly to percepts, whereas model-based reflex agents
maintain internal state to track aspects of the world that are not evident in the current
percept. Goal-based agents act to achieve their goals, and utility-based agents try to
maximize their own expected â€œhappiness.â€
â€¢ All agents can improve their performance through learning.

B IBLIOGRAPHICAL

CONTROLLER

AND

H ISTORICAL N OTES

The central role of action in intelligenceâ€”the notion of practical reasoningâ€”goes back at
least as far as Aristotleâ€™s Nicomachean Ethics. Practical reasoning was also the subject of
McCarthyâ€™s (1958) influential paper â€œPrograms with Common Sense.â€ The fields of robotics
and control theory are, by their very nature, concerned principally with physical agents. The
concept of a controller in control theory is identical to that of an agent in AI. Perhaps surprisingly, AI has concentrated for most of its history on isolated components of agentsâ€”
question-answering systems, theorem-provers, vision systems, and so onâ€”rather than on
whole agents. The discussion of agents in the text by Genesereth and Nilsson (1987) was an
influential exception. The whole-agent view is now widely accepted and is a central theme in
recent texts (Poole et al., 1998; Nilsson, 1998; Padgham and Winikoff, 2004; Jones, 2007).
Chapter 1 traced the roots of the concept of rationality in philosophy and economics. In
AI, the concept was of peripheral interest until the mid-1980s, when it began to suffuse many

60

AUTONOMIC
COMPUTING

MULTIAGENT
SYSTEMS

Chapter

2.

Intelligent Agents

discussions about the proper technical foundations of the field. A paper by Jon Doyle (1983)
predicted that rational agent design would come to be seen as the core mission of AI, while
other popular topics would spin off to form new disciplines.
Careful attention to the properties of the environment and their consequences for rational agent design is most apparent in the control theory traditionâ€”for example, classical
control systems (Dorf and Bishop, 2004; Kirk, 2004) handle fully observable, deterministic
environments; stochastic optimal control (Kumar and Varaiya, 1986; Bertsekas and Shreve,
2007) handles partially observable, stochastic environments; and hybrid control (Henzinger
and Sastry, 1998; Cassandras and Lygeros, 2006) deals with environments containing both
discrete and continuous elements. The distinction between fully and partially observable environments is also central in the dynamic programming literature developed in the field of
operations research (Puterman, 1994), which we discuss in Chapter 17.
Reflex agents were the primary model for psychological behaviorists such as Skinner
(1953), who attempted to reduce the psychology of organisms strictly to input/output or stimulus/response mappings. The advance from behaviorism to functionalism in psychology,
which was at least partly driven by the application of the computer metaphor to agents (Putnam, 1960; Lewis, 1966), introduced the internal state of the agent into the picture. Most
work in AI views the idea of pure reflex agents with state as too simple to provide much
leverage, but work by Rosenschein (1985) and Brooks (1986) questioned this assumption
(see Chapter 25). In recent years, a great deal of work has gone into finding efficient algorithms for keeping track of complex environments (Hamscher et al., 1992; Simon, 2006). The
Remote Agent program (described on page 28) that controlled the Deep Space One spacecraft
is a particularly impressive example (Muscettola et al., 1998; Jonsson et al., 2000).
Goal-based agents are presupposed in everything from Aristotleâ€™s view of practical reasoning to McCarthyâ€™s early papers on logical AI. Shakey the Robot (Fikes and Nilsson,
1971; Nilsson, 1984) was the first robotic embodiment of a logical, goal-based agent. A
full logical analysis of goal-based agents appeared in Genesereth and Nilsson (1987), and a
goal-based programming methodology called agent-oriented programming was developed by
Shoham (1993). The agent-based approach is now extremely popular in software engineering (Ciancarini and Wooldridge, 2001). It has also infiltrated the area of operating systems,
where autonomic computing refers to computer systems and networks that monitor and control themselves with a perceiveâ€“act loop and machine learning methods (Kephart and Chess,
2003). Noting that a collection of agent programs designed to work well together in a true
multiagent environment necessarily exhibits modularityâ€”the programs share no internal state
and communicate with each other only through the environmentâ€”it is common within the
field of multiagent systems to design the agent program of a single agent as a collection of
autonomous sub-agents. In some cases, one can even prove that the resulting system gives
the same optimal solutions as a monolithic design.
The goal-based view of agents also dominates the cognitive psychology tradition in the
area of problem solving, beginning with the enormously influential Human Problem Solving (Newell and Simon, 1972) and running through all of Newellâ€™s later work (Newell, 1990).
Goals, further analyzed as desires (general) and intentions (currently pursued), are central to
the theory of agents developed by Bratman (1987). This theory has been influential both in

Exercises

61
natural language understanding and multiagent systems.
Horvitz et al. (1988) specifically suggest the use of rationality conceived as the maximization of expected utility as a basis for AI. The text by Pearl (1988) was the first in AI to
cover probability and utility theory in depth; its exposition of practical methods for reasoning
and decision making under uncertainty was probably the single biggest factor in the rapid
shift towards utility-based agents in the 1990s (see Part IV).
The general design for learning agents portrayed in Figure 2.15 is classic in the machine
learning literature (Buchanan et al., 1978; Mitchell, 1997). Examples of the design, as embodied in programs, go back at least as far as Arthur Samuelâ€™s (1959, 1967) learning program
for playing checkers. Learning agents are discussed in depth in Part V.
Interest in agents and in agent design has risen rapidly in recent years, partly because of
the growth of the Internet and the perceived need for automated and mobile softbot (Etzioni
and Weld, 1994). Relevant papers are collected in Readings in Agents (Huhns and Singh,
1998) and Foundations of Rational Agency (Wooldridge and Rao, 1999). Texts on multiagent
systems usually provide a good introduction to many aspects of agent design (Weiss, 2000a;
Wooldridge, 2002). Several conference series devoted to agents began in the 1990s, including
the International Workshop on Agent Theories, Architectures, and Languages (ATAL), the
International Conference on Autonomous Agents (AGENTS), and the International Conference on Multi-Agent Systems (ICMAS). In 2002, these three merged to form the International
Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS). The journal
Autonomous Agents and Multi-Agent Systems was founded in 1998. Finally, Dung Beetle
Ecology (Hanski and Cambefort, 1991) provides a wealth of interesting information on the
behavior of dung beetles. YouTube features inspiring video recordings of their activities.

E XERCISES
2.1 Suppose that the performance measure is concerned with just the first T time steps of
the environment and ignores everything thereafter. Show that a rational agentâ€™s action may
depend not just on the state of the environment but also on the time step it has reached.
2.2

Let us examine the rationality of various vacuum-cleaner agent functions.

a. Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed
rational under the assumptions listed on page 38.
b. Describe a rational agent function for the case in which each movement costs one point.
Does the corresponding agent program require internal state?
c. Discuss possible agent designs for the cases in which clean squares can become dirty
and the geography of the environment is unknown. Does it make sense for the agent to
learn from its experience in these cases? If so, what should it learn? If not, why not?
2.3 For each of the following assertions, say whether it is true or false and support your
answer with examples or counterexamples where appropriate.
a. An agent that senses only partial information about the state cannot be perfectly rational.

62

Chapter

2.

Intelligent Agents

b.
c.
d.
e.
f.

There exist task environments in which no pure reflex agent can behave rationally.
There exists a task environment in which every agent is rational.
The input to an agent program is the same as the input to the agent function.
Every agent function is implementable by some program/machine combination.
Suppose an agent selects its action uniformly at random from the set of possible actions.
There exists a deterministic task environment in which this agent is rational.
g. It is possible for a given agent to be perfectly rational in two distinct task environments.
h. Every agent is rational in an unobservable environment.
i. A perfectly rational poker-playing agent never loses.
2.4 For each of the following activities, give a PEAS description of the task environment
and characterize it in terms of the properties listed in Section 2.3.2.
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢

Playing soccer.
Exploring the subsurface oceans of Titan.
Shopping for used AI books on the Internet.
Playing a tennis match.
Practicing tennis against a wall.
Performing a high jump.
Knitting a sweater.
Bidding on an item at an auction.

2.5 Define in your own words the following terms: agent, agent function, agent program,
rationality, autonomy, reflex agent, model-based agent, goal-based agent, utility-based agent,
learning agent.
2.6

This exercise explores the differences between agent functions and agent programs.

a. Can there be more than one agent program that implements a given agent function?
Give an example, or show why one is not possible.
b. Are there agent functions that cannot be implemented by any agent program?
c. Given a fixed machine architecture, does each agent program implement exactly one
agent function?
d. Given an architecture with n bits of storage, how many different possible agent programs are there?
e. Suppose we keep the agent program fixed but speed up the machine by a factor of two.
Does that change the agent function?
2.7

Write pseudocode agent programs for the goal-based and utility-based agents.

The following exercises all concern the implementation of environments and agents for the
vacuum-cleaner world.

Exercises

63
2.8 Implement a performance-measuring environment simulator for the vacuum-cleaner
world depicted in Figure 2.2 and specified on page 38. Your implementation should be modular so that the sensors, actuators, and environment characteristics (size, shape, dirt placement,
etc.) can be changed easily. (Note: for some choices of programming language and operating
system there are already implementations in the online code repository.)
2.9 Implement a simple reflex agent for the vacuum environment in Exercise 2.8. Run the
environment with this agent for all possible initial dirt configurations and agent locations.
Record the performance score for each configuration and the overall average score.
2.10 Consider a modified version of the vacuum environment in Exercise 2.8, in which the
agent is penalized one point for each movement.
a. Can a simple reflex agent be perfectly rational for this environment? Explain.
b. What about a reflex agent with state? Design such an agent.
c. How do your answers to a and b change if the agentâ€™s percepts give it the clean/dirty
status of every square in the environment?
2.11 Consider a modified version of the vacuum environment in Exercise 2.8, in which the
geography of the environmentâ€”its extent, boundaries, and obstaclesâ€”is unknown, as is the
initial dirt configuration. (The agent can go Up and Down as well as Left and Right.)
a. Can a simple reflex agent be perfectly rational for this environment? Explain.
b. Can a simple reflex agent with a randomized agent function outperform a simple reflex
agent? Design such an agent and measure its performance on several environments.
c. Can you design an environment in which your randomized agent will perform poorly?
Show your results.
d. Can a reflex agent with state outperform a simple reflex agent? Design such an agent
and measure its performance on several environments. Can you design a rational agent
of this type?
2.12 Repeat Exercise 2.11 for the case in which the location sensor is replaced with a
â€œbumpâ€ sensor that detects the agentâ€™s attempts to move into an obstacle or to cross the
boundaries of the environment. Suppose the bump sensor stops working; how should the
agent behave?
2.13 The vacuum environments in the preceding exercises have all been deterministic. Discuss possible agent programs for each of the following stochastic versions:
a. Murphyâ€™s law: twenty-five percent of the time, the Suck action fails to clean the floor if
it is dirty and deposits dirt onto the floor if the floor is clean. How is your agent program
affected if the dirt sensor gives the wrong answer 10% of the time?
b. Small children: At each time step, each clean square has a 10% chance of becoming
dirty. Can you come up with a rational agent design for this case?

3

SOLVING PROBLEMS BY
SEARCHING

In which we see how an agent can find a sequence of actions that achieves its
goals when no single action will do.

PROBLEM-SOLVING
AGENT

3.1

The simplest agents discussed in Chapter 2 were the reflex agents, which base their actions on
a direct mapping from states to actions. Such agents cannot operate well in environments for
which this mapping would be too large to store and would take too long to learn. Goal-based
agents, on the other hand, consider future actions and the desirability of their outcomes.
This chapter describes one kind of goal-based agent called a problem-solving agent.
Problem-solving agents use atomic representations, as described in Section 2.4.7â€”that is,
states of the world are considered as wholes, with no internal structure visible to the problemsolving algorithms. Goal-based agents that use more advanced factored or structured representations are usually called planning agents and are discussed in Chapters 7 and 10.
Our discussion of problem solving begins with precise definitions of problems and their
solutions and give several examples to illustrate these definitions. We then describe several
general-purpose search algorithms that can be used to solve these problems. We will see
several uninformed search algorithmsâ€”algorithms that are given no information about the
problem other than its definition. Although some of these algorithms can solve any solvable
problem, none of them can do so efficiently. Informed search algorithms, on the other hand,
can do quite well given some guidance on where to look for solutions.
In this chapter, we limit ourselves to the simplest kind of task environment, for which
the solution to a problem is always a fixed sequence of actions. The more general caseâ€”where
the agentâ€™s future actions may vary depending on future perceptsâ€”is handled in Chapter 4.
This chapter uses the concepts of asymptotic complexity (that is, O() notation) and
NP-completeness. Readers unfamiliar with these concepts should consult Appendix A.

P ROBLEM -S OLVING AGENTS
Intelligent agents are supposed to maximize their performance measure. As we mentioned
in Chapter 2, achieving this is sometimes simplified if the agent can adopt a goal and aim at
satisfying it. Let us first look at why and how an agent might do this.
64

Section 3.1.

GOAL FORMULATION

PROBLEM
FORMULATION

Problem-Solving Agents

65

Imagine an agent in the city of Arad, Romania, enjoying a touring holiday. The agentâ€™s
performance measure contains many factors: it wants to improve its suntan, improve its Romanian, take in the sights, enjoy the nightlife (such as it is), avoid hangovers, and so on. The
decision problem is a complex one involving many tradeoffs and careful reading of guidebooks. Now, suppose the agent has a nonrefundable ticket to fly out of Bucharest the following day. In that case, it makes sense for the agent to adopt the goal of getting to Bucharest.
Courses of action that donâ€™t reach Bucharest on time can be rejected without further consideration and the agentâ€™s decision problem is greatly simplified. Goals help organize behavior
by limiting the objectives that the agent is trying to achieve and hence the actions it needs
to consider. Goal formulation, based on the current situation and the agentâ€™s performance
measure, is the first step in problem solving.
We will consider a goal to be a set of world statesâ€”exactly those states in which the
goal is satisfied. The agentâ€™s task is to find out how to act, now and in the future, so that it
reaches a goal state. Before it can do this, it needs to decide (or we need to decide on its
behalf) what sorts of actions and states it should consider. If it were to consider actions at
the level of â€œmove the left foot forward an inchâ€ or â€œturn the steering wheel one degree left,â€
the agent would probably never find its way out of the parking lot, let alone to Bucharest,
because at that level of detail there is too much uncertainty in the world and there would be
too many steps in a solution. Problem formulation is the process of deciding what actions
and states to consider, given a goal. We discuss this process in more detail later. For now, let
us assume that the agent will consider actions at the level of driving from one major town to
another. Each state therefore corresponds to being in a particular town.
Our agent has now adopted the goal of driving to Bucharest and is considering where
to go from Arad. Three roads lead out of Arad, one toward Sibiu, one to Timisoara, and one
to Zerind. None of these achieves the goal, so unless the agent is familiar with the geography
of Romania, it will not know which road to follow.1 In other words, the agent will not know
which of its possible actions is best, because it does not yet know enough about the state
that results from taking each action. If the agent has no additional informationâ€”i.e., if the
environment is unknown in the sense defined in Section 2.3â€”then it is has no choice but to
try one of the actions at random. This sad situation is discussed in Chapter 4.
But suppose the agent has a map of Romania. The point of a map is to provide the
agent with information about the states it might get itself into and the actions it can take. The
agent can use this information to consider subsequent stages of a hypothetical journey via
each of the three towns, trying to find a journey that eventually gets to Bucharest. Once it has
found a path on the map from Arad to Bucharest, it can achieve its goal by carrying out the
driving actions that correspond to the legs of the journey. In general, an agent with several
immediate options of unknown value can decide what to do by first examining future actions
that eventually lead to states of known value.
To be more specific about what we mean by â€œexamining future actions,â€ we have to
be more specific about properties of the environment, as defined in Section 2.3. For now,
1

We are assuming that most readers are in the same position and can easily imagine themselves to be as clueless
as our agent. We apologize to Romanian readers who are unable to take advantage of this pedagogical device.

66

SEARCH
SOLUTION

EXECUTION

OPEN-LOOP

Chapter

3.

Solving Problems by Searching

we assume that the environment is observable, so the agent always knows the current state.
For the agent driving in Romania, itâ€™s reasonable to suppose that each city on the map has a
sign indicating its presence to arriving drivers. We also assume the environment is discrete,
so at any given state there are only finitely many actions to choose from. This is true for
navigating in Romania because each city is connected to a small number of other cities. We
will assume the environment is known, so the agent knows which states are reached by each
action. (Having an accurate map suffices to meet this condition for navigation problems.)
Finally, we assume that the environment is deterministic, so each action has exactly one
outcome. Under ideal conditions, this is true for the agent in Romaniaâ€”it means that if it
chooses to drive from Arad to Sibiu, it does end up in Sibiu. Of course, conditions are not
always ideal, as we show in Chapter 4.
Under these assumptions, the solution to any problem is a fixed sequence of actions.
â€œOf course!â€ one might say, â€œWhat else could it be?â€ Well, in general it could be a branching
strategy that recommends different actions in the future depending on what percepts arrive.
For example, under less than ideal conditions, the agent might plan to drive from Arad to
Sibiu and then to Rimnicu Vilcea but may also need to have a contingency plan in case it
arrives by accident in Zerind instead of Sibiu. Fortunately, if the agent knows the initial state
and the environment is known and deterministic, it knows exactly where it will be after the
first action and what it will perceive. Since only one percept is possible after the first action,
the solution can specify only one possible second action, and so on.
The process of looking for a sequence of actions that reaches the goal is called search.
A search algorithm takes a problem as input and returns a solution in the form of an action
sequence. Once a solution is found, the actions it recommends can be carried out. This
is called the execution phase. Thus, we have a simple â€œformulate, search, executeâ€ design
for the agent, as shown in Figure 3.1. After formulating a goal and a problem to solve,
the agent calls a search procedure to solve it. It then uses the solution to guide its actions,
doing whatever the solution recommends as the next thing to doâ€”typically, the first action of
the sequenceâ€”and then removing that step from the sequence. Once the solution has been
executed, the agent will formulate a new goal.
Notice that while the agent is executing the solution sequence it ignores its percepts
when choosing an action because it knows in advance what they will be. An agent that
carries out its plans with its eyes closed, so to speak, must be quite certain of what is going
on. Control theorists call this an open-loop system, because ignoring the percepts breaks the
loop between agent and environment.
We first describe the process of problem formulation, and then devote the bulk of the
chapter to various algorithms for the S EARCH function. We do not discuss the workings of
the U PDATE -S TATE and F ORMULATE -G OAL functions further in this chapter.

3.1.1 Well-defined problems and solutions
PROBLEM
INITIAL STATE

A problem can be defined formally by five components:
â€¢ The initial state that the agent starts in. For example, the initial state for our agent in
Romania might be described as In(Arad ).

Section 3.1.

Problem-Solving Agents

67

function S IMPLE -P ROBLEM -S OLVING -AGENT ( percept ) returns an action
persistent: seq, an action sequence, initially empty
state, some description of the current world state
goal , a goal, initially null
problem, a problem formulation
state â† U PDATE -S TATE(state, percept )
if seq is empty then
goal â† F ORMULATE -G OAL(state)
problem â† F ORMULATE -P ROBLEM(state, goal )
seq â† S EARCH ( problem)
if seq = failure then return a null action
action â† F IRST (seq)
seq â† R EST(seq)
return action
Figure 3.1 A simple problem-solving agent. It first formulates a goal and a problem,
searches for a sequence of actions that would solve the problem, and then executes the actions
one at a time. When this is complete, it formulates another goal and starts over.
ACTIONS

APPLICABLE

TRANSITION MODEL
SUCCESSOR

â€¢ A description of the possible actions available to the agent. Given a particular state s,
ACTIONS (s) returns the set of actions that can be executed in s. We say that each of
these actions is applicable in s. For example, from the state In(Arad ), the applicable
actions are {Go(Sibiu), Go(Timisoara ), Go(Zerind )}.
â€¢ A description of what each action does; the formal name for this is the transition
model, specified by a function R ESULT (s, a) that returns the state that results from
doing action a in state s. We also use the term successor to refer to any state reachable
from a given state by a single action.2 For example, we have
R ESULT (In(Arad ), Go(Zerind )) = In(Zerind ) .

STATE SPACE

GRAPH

PATH

GOAL TEST

Together, the initial state, actions, and transition model implicitly define the state space
of the problemâ€”the set of all states reachable from the initial state by any sequence
of actions. The state space forms a directed network or graph in which the nodes
are states and the links between nodes are actions. (The map of Romania shown in
Figure 3.2 can be interpreted as a state-space graph if we view each road as standing
for two driving actions, one in each direction.) A path in the state space is a sequence
of states connected by a sequence of actions.
â€¢ The goal test, which determines whether a given state is a goal state. Sometimes there
is an explicit set of possible goal states, and the test simply checks whether the given
state is one of them. The agentâ€™s goal in Romania is the singleton set {In(Bucharest )}.
2 Many treatments of problem solving, including previous editions of this book, use a successor function, which
returns the set of all successors, instead of separate A CTIONS and R ESULT functions. The successor function
makes it difficult to describe an agent that knows what actions it can try but not what they achieve. Also, note
some author use R ESULT(a, s) instead of R ESULT(s, a), and some use D O instead of R ESULT.

68

Chapter

3.

Solving Problems by Searching

Oradea
71

Neamt
Zerind

87

151

75

Iasi
Arad

140

92

Sibiu

99

Fagaras

118

Vaslui

80
Rimnicu Vilcea

Timisoara
111

Lugoj

142

211

Pitesti

97

70

98
Mehadia

146

75
Drobeta

85

101

Hirsova

Urziceni
86

138

Bucharest

120
90
Craiova

Figure 3.2

Eforie

A simplified road map of part of Romania.

Sometimes the goal is specified by an abstract property rather than an explicitly enumerated set of states. For example, in chess, the goal is to reach a state called â€œcheckmate,â€
where the opponentâ€™s king is under attack and canâ€™t escape.
â€¢ A path cost function that assigns a numeric cost to each path. The problem-solving
agent chooses a cost function that reflects its own performance measure. For the agent
trying to get to Bucharest, time is of the essence, so the cost of a path might be its length
in kilometers. In this chapter, we assume that the cost of a path can be described as the
sum of the costs of the individual actions along the path.3 The step cost of taking action
a in state s to reach state s is denoted by c(s, a, s ). The step costs for Romania are
shown in Figure 3.2 as route distances. We assume that step costs are nonnegative.4

PATH COST

STEP COST

OPTIMAL SOLUTION

Giurgiu

The preceding elements define a problem and can be gathered into a single data structure
that is given as input to a problem-solving algorithm. A solution to a problem is an action
sequence that leads from the initial state to a goal state. Solution quality is measured by the
path cost function, and an optimal solution has the lowest path cost among all solutions.

3.1.2 Formulating problems
In the preceding section we proposed a formulation of the problem of getting to Bucharest in
terms of the initial state, actions, transition model, goal test, and path cost. This formulation
seems reasonable, but it is still a modelâ€”an abstract mathematical descriptionâ€”and not the
3
4

This assumption is algorithmically convenient but also theoretically justifiableâ€”see page 649 in Chapter 17.
The implications of negative costs are explored in Exercise 3.8.

Section 3.2.

ABSTRACTION

3.2

69

real thing. Compare the simple state description we have chosen, In(Arad), to an actual crosscountry trip, where the state of the world includes so many things: the traveling companions,
the current radio program, the scenery out of the window, the proximity of law enforcement
officers, the distance to the next rest stop, the condition of the road, the weather, and so on.
All these considerations are left out of our state descriptions because they are irrelevant to the
problem of finding a route to Bucharest. The process of removing detail from a representation
is called abstraction.
In addition to abstracting the state description, we must abstract the actions themselves.
A driving action has many effects. Besides changing the location of the vehicle and its occupants, it takes up time, consumes fuel, generates pollution, and changes the agent (as they
say, travel is broadening). Our formulation takes into account only the change in location.
Also, there are many actions that we omit altogether: turning on the radio, looking out of
the window, slowing down for law enforcement officers, and so on. And of course, we donâ€™t
specify actions at the level of â€œturn steering wheel to the left by one degree.â€
Can we be more precise about defining the appropriate level of abstraction? Think of the
abstract states and actions we have chosen as corresponding to large sets of detailed world
states and detailed action sequences. Now consider a solution to the abstract problem: for
example, the path from Arad to Sibiu to Rimnicu Vilcea to Pitesti to Bucharest. This abstract
solution corresponds to a large number of more detailed paths. For example, we could drive
with the radio on between Sibiu and Rimnicu Vilcea, and then switch it off for the rest of
the trip. The abstraction is valid if we can expand any abstract solution into a solution in the
more detailed world; a sufficient condition is that for every detailed state that is â€œin Arad,â€
there is a detailed path to some state that is â€œin Sibiu,â€ and so on.5 The abstraction is useful
if carrying out each of the actions in the solution is easier than the original problem; in this
case they are easy enough that they can be carried out without further search or planning by
an average driving agent. The choice of a good abstraction thus involves removing as much
detail as possible while retaining validity and ensuring that the abstract actions are easy to
carry out. Were it not for the ability to construct useful abstractions, intelligent agents would
be completely swamped by the real world.

E XAMPLE P ROBLEMS

TOY PROBLEM

REAL-WORLD
PROBLEM

Example Problems

The problem-solving approach has been applied to a vast array of task environments. We
list some of the best known here, distinguishing between toy and real-world problems. A
toy problem is intended to illustrate or exercise various problem-solving methods. It can be
given a concise, exact description and hence is usable by different researchers to compare the
performance of algorithms. A real-world problem is one whose solutions people actually
care about. Such problems tend not to have a single agreed-upon description, but we can give
the general flavor of their formulations.
5

See Section 11.2 for a more complete set of definitions and algorithms.

70

Chapter

3.

Solving Problems by Searching

R
L

R
L
S

S

R

R

L

R

L

R

L

L
S

S

S

S
R
L

R
L
S

S

Figure 3.3 The state space for the vacuum world. Links denote actions: L = Left, R =
Right, S = Suck.

3.2.1 Toy problems
The first example we examine is the vacuum world first introduced in Chapter 2. (See
Figure 2.2.) This can be formulated as a problem as follows:
â€¢ States: The state is determined by both the agent location and the dirt locations. The
agent is in one of two locations, each of which might or might not contain dirt. Thus,
there are 2 Ã— 22 = 8 possible world states. A larger environment with n locations has
n Â· 2n states.
â€¢ Initial state: Any state can be designated as the initial state.
â€¢ Actions: In this simple environment, each state has just three actions: Left, Right, and
Suck. Larger environments might also include Up and Down.
â€¢ Transition model: The actions have their expected effects, except that moving Left in
the leftmost square, moving Right in the rightmost square, and Sucking in a clean square
have no effect. The complete state space is shown in Figure 3.3.
â€¢ Goal test: This checks whether all the squares are clean.
â€¢ Path cost: Each step costs 1, so the path cost is the number of steps in the path.

8-PUZZLE

Compared with the real world, this toy problem has discrete locations, discrete dirt, reliable
cleaning, and it never gets any dirtier. Chapter 4 relaxes some of these assumptions.
The 8-puzzle, an instance of which is shown in Figure 3.4, consists of a 3Ã—3 board with
eight numbered tiles and a blank space. A tile adjacent to the blank space can slide into the
space. The object is to reach a specified goal state, such as the one shown on the right of the
figure. The standard formulation is as follows:

Section 3.2.

Example Problems

71

7

2

5
8

3

4

2

6

3

4

5

1

6

7

8

Start State
Figure 3.4

1

Goal State

A typical instance of the 8-puzzle.

â€¢ States: A state description specifies the location of each of the eight tiles and the blank
in one of the nine squares.
â€¢ Initial state: Any state can be designated as the initial state. Note that any given goal
can be reached from exactly half of the possible initial states (Exercise 3.4).
â€¢ Actions: The simplest formulation defines the actions as movements of the blank space
Left, Right, Up, or Down. Different subsets of these are possible depending on where
the blank is.
â€¢ Transition model: Given a state and action, this returns the resulting state; for example,
if we apply Left to the start state in Figure 3.4, the resulting state has the 5 and the blank
switched.
â€¢ Goal test: This checks whether the state matches the goal configuration shown in Figure 3.4. (Other goal configurations are possible.)
â€¢ Path cost: Each step costs 1, so the path cost is the number of steps in the path.

SLIDING-BLOCK
PUZZLES

8-QUEENS PROBLEM

What abstractions have we included here? The actions are abstracted to their beginning and
final states, ignoring the intermediate locations where the block is sliding. We have abstracted
away actions such as shaking the board when pieces get stuck and ruled out extracting the
pieces with a knife and putting them back again. We are left with a description of the rules of
the puzzle, avoiding all the details of physical manipulations.
The 8-puzzle belongs to the family of sliding-block puzzles, which are often used as
test problems for new search algorithms in AI. This family is known to be NP-complete,
so one does not expect to find methods significantly better in the worst case than the search
algorithms described in this chapter and the next. The 8-puzzle has 9!/2 = 181, 440 reachable
states and is easily solved. The 15-puzzle (on a 4 Ã— 4 board) has around 1.3 trillion states, and
random instances can be solved optimally in a few milliseconds by the best search algorithms.
The 24-puzzle (on a 5 Ã— 5 board) has around 1025 states, and random instances take several
hours to solve optimally.
The goal of the 8-queens problem is to place eight queens on a chessboard such that
no queen attacks any other. (A queen attacks any piece in the same row, column or diagonal.) Figure 3.5 shows an attempted solution that fails: the queen in the rightmost column is
attacked by the queen at the top left.

72

Chapter

Figure 3.5

INCREMENTAL
FORMULATION
COMPLETE-STATE
FORMULATION

3.

Solving Problems by Searching

Almost a solution to the 8-queens problem. (Solution is left as an exercise.)

Although efficient special-purpose algorithms exist for this problem and for the whole
n-queens family, it remains a useful test problem for search algorithms. There are two main
kinds of formulation. An incremental formulation involves operators that augment the state
description, starting with an empty state; for the 8-queens problem, this means that each
action adds a queen to the state. A complete-state formulation starts with all 8 queens on
the board and moves them around. In either case, the path cost is of no interest because only
the final state counts. The first incremental formulation one might try is the following:
â€¢
â€¢
â€¢
â€¢
â€¢

States: Any arrangement of 0 to 8 queens on the board is a state.
Initial state: No queens on the board.
Actions: Add a queen to any empty square.
Transition model: Returns the board with a queen added to the specified square.
Goal test: 8 queens are on the board, none attacked.

In this formulation, we have 64 Â· 63 Â· Â· Â· 57 â‰ˆ 1.8 Ã— 1014 possible sequences to investigate. A
better formulation would prohibit placing a queen in any square that is already attacked:
â€¢ States: All possible arrangements of n queens (0 â‰¤ n â‰¤ 8), one per column in the
leftmost n columns, with no queen attacking another.
â€¢ Actions: Add a queen to any square in the leftmost empty column such that it is not
attacked by any other queen.
This formulation reduces the 8-queens state space from 1.8 Ã— 1014 to just 2,057, and solutions
are easy to find. On the other hand, for 100 queens the reduction is from roughly 10400 states
to about 1052 states (Exercise 3.5)â€”a big improvement, but not enough to make the problem
tractable. Section 4.1 describes the complete-state formulation, and Chapter 6 gives a simple
algorithm that solves even the million-queens problem with ease.

Section 3.2.

Example Problems

73

Our final toy problem was devised by Donald Knuth (1964) and illustrates how infinite
state spaces can arise. Knuth conjectured that, starting with the number 4, a sequence of factorial, square root, and floor operations will reach any desired positive integer. For example,
we can reach 5 from 4 as follows:

 
	




(4!)! = 5 .
The problem definition is very simple:
â€¢
â€¢
â€¢
â€¢
â€¢

States: Positive numbers.
Initial state: 4.
Actions: Apply factorial, square root, or floor operation (factorial for integers only).
Transition model: As given by the mathematical definitions of the operations.
Goal test: State is the desired positive integer.

To our knowledge there is no bound on how large a number might be constructed in the process of reaching a given targetâ€”for example, the number 620,448,401,733,239,439,360,000
is generated in the expression for 5â€”so the state space for this problem is infinite. Such
state spaces arise frequently in tasks involving the generation of mathematical expressions,
circuits, proofs, programs, and other recursively defined objects.

3.2.2 Real-world problems
ROUTE-FINDING
PROBLEM

We have already seen how the route-finding problem is defined in terms of specified locations and transitions along links between them. Route-finding algorithms are used in a variety
of applications. Some, such as Web sites and in-car systems that provide driving directions,
are relatively straightforward extensions of the Romania example. Others, such as routing
video streams in computer networks, military operations planning, and airline travel-planning
systems, involve much more complex specifications. Consider the airline travel problems that
must be solved by a travel-planning Web site:
â€¢ States: Each state obviously includes a location (e.g., an airport) and the current time.
Furthermore, because the cost of an action (a flight segment) may depend on previous
segments, their fare bases, and their status as domestic or international, the state must
record extra information about these â€œhistoricalâ€ aspects.
â€¢ Initial state: This is specified by the userâ€™s query.
â€¢ Actions: Take any flight from the current location, in any seat class, leaving after the
current time, leaving enough time for within-airport transfer if needed.
â€¢ Transition model: The state resulting from taking a flight will have the flightâ€™s destination as the current location and the flightâ€™s arrival time as the current time.
â€¢ Goal test: Are we at the final destination specified by the user?
â€¢ Path cost: This depends on monetary cost, waiting time, flight time, customs and immigration procedures, seat quality, time of day, type of airplane, frequent-flyer mileage
awards, and so on.

74

TOURING PROBLEM

TRAVELING
SALESPERSON
PROBLEM

VLSI LAYOUT

ROBOT NAVIGATION

AUTOMATIC
ASSEMBLY
SEQUENCING

Chapter

3.

Solving Problems by Searching

Commercial travel advice systems use a problem formulation of this kind, with many additional complications to handle the byzantine fare structures that airlines impose. Any seasoned traveler knows, however, that not all air travel goes according to plan. A really good
system should include contingency plansâ€”such as backup reservations on alternate flightsâ€”
to the extent that these are justified by the cost and likelihood of failure of the original plan.
Touring problems are closely related to route-finding problems, but with an important difference. Consider, for example, the problem â€œVisit every city in Figure 3.2 at least
once, starting and ending in Bucharest.â€ As with route finding, the actions correspond
to trips between adjacent cities. The state space, however, is quite different. Each state
must include not just the current location but also the set of cities the agent has visited.
So the initial state would be In(Bucharest ), Visited ({Bucharest }), a typical intermediate state would be In(Vaslui ), Visited ({Bucharest , Urziceni , Vaslui }), and the goal test
would check whether the agent is in Bucharest and all 20 cities have been visited.
The traveling salesperson problem (TSP) is a touring problem in which each city
must be visited exactly once. The aim is to find the shortest tour. The problem is known to
be NP-hard, but an enormous amount of effort has been expended to improve the capabilities
of TSP algorithms. In addition to planning trips for traveling salespersons, these algorithms
have been used for tasks such as planning movements of automatic circuit-board drills and of
stocking machines on shop floors.
A VLSI layout problem requires positioning millions of components and connections
on a chip to minimize area, minimize circuit delays, minimize stray capacitances, and maximize manufacturing yield. The layout problem comes after the logical design phase and is
usually split into two parts: cell layout and channel routing. In cell layout, the primitive
components of the circuit are grouped into cells, each of which performs some recognized
function. Each cell has a fixed footprint (size and shape) and requires a certain number of
connections to each of the other cells. The aim is to place the cells on the chip so that they do
not overlap and so that there is room for the connecting wires to be placed between the cells.
Channel routing finds a specific route for each wire through the gaps between the cells. These
search problems are extremely complex, but definitely worth solving. Later in this chapter,
we present some algorithms capable of solving them.
Robot navigation is a generalization of the route-finding problem described earlier.
Rather than following a discrete set of routes, a robot can move in a continuous space with
(in principle) an infinite set of possible actions and states. For a circular robot moving on a
flat surface, the space is essentially two-dimensional. When the robot has arms and legs or
wheels that must also be controlled, the search space becomes many-dimensional. Advanced
techniques are required just to make the search space finite. We examine some of these
methods in Chapter 25. In addition to the complexity of the problem, real robots must also
deal with errors in their sensor readings and motor controls.
Automatic assembly sequencing of complex objects by a robot was first demonstrated
by F REDDY (Michie, 1972). Progress since then has been slow but sure, to the point where
the assembly of intricate objects such as electric motors is economically feasible. In assembly
problems, the aim is to find an order in which to assemble the parts of some object. If the
wrong order is chosen, there will be no way to add some part later in the sequence without

Section 3.3.

PROTEIN DESIGN

3.3

75

undoing some of the work already done. Checking a step in the sequence for feasibility is a
difficult geometrical search problem closely related to robot navigation. Thus, the generation
of legal actions is the expensive part of assembly sequencing. Any practical algorithm must
avoid exploring all but a tiny fraction of the state space. Another important assembly problem
is protein design, in which the goal is to find a sequence of amino acids that will fold into a
three-dimensional protein with the right properties to cure some disease.

S EARCHING FOR S OLUTIONS

SEARCH TREE
NODE

EXPANDING
GENERATING
PARENT NODE
CHILD NODE

LEAF NODE

FRONTIER
OPEN LIST

SEARCH STRATEGY

REPEATED STATE
LOOPY PATH

Searching for Solutions

Having formulated some problems, we now need to solve them. A solution is an action
sequence, so search algorithms work by considering various possible action sequences. The
possible action sequences starting at the initial state form a search tree with the initial state
at the root; the branches are actions and the nodes correspond to states in the state space of
the problem. Figure 3.6 shows the first few steps in growing the search tree for finding a route
from Arad to Bucharest. The root node of the tree corresponds to the initial state, In(Arad).
The first step is to test whether this is a goal state. (Clearly it is not, but it is important to
check so that we can solve trick problems like â€œstarting in Arad, get to Arad.â€) Then we
need to consider taking various actions. We do this by expanding the current state; that is,
applying each legal action to the current state, thereby generating a new set of states. In
this case, we add three branches from the parent node In(Arad) leading to three new child
nodes: In(Sibiu), In(Timisoara), and In(Zerind). Now we must choose which of these three
possibilities to consider further.
This is the essence of searchâ€”following up one option now and putting the others aside
for later, in case the first choice does not lead to a solution. Suppose we choose Sibiu first.
We check to see whether it is a goal state (it is not) and then expand it to get In(Arad),
In(Fagaras), In(Oradea), and In(RimnicuVilcea). We can then choose any of these four or go
back and choose Timisoara or Zerind. Each of these six nodes is a leaf node, that is, a node
with no children in the tree. The set of all leaf nodes available for expansion at any given
point is called the frontier. (Many authors call it the open list, which is both geographically
less evocative and less accurate, because other data structures are better suited than a list.) In
Figure 3.6, the frontier of each tree consists of those nodes with bold outlines.
The process of expanding nodes on the frontier continues until either a solution is found
or there are no more states to expand. The general T REE -S EARCH algorithm is shown informally in Figure 3.7. Search algorithms all share this basic structure; they vary primarily
according to how they choose which state to expand nextâ€”the so-called search strategy.
The eagle-eyed reader will notice one peculiar thing about the search tree shown in Figure 3.6: it includes the path from Arad to Sibiu and back to Arad again! We say that In(Arad)
is a repeated state in the search tree, generated in this case by a loopy path. Considering
such loopy paths means that the complete search tree for Romania is infinite because there
is no limit to how often one can traverse a loop. On the other hand, the state spaceâ€”the
map shown in Figure 3.2â€”has only 20 states. As we discuss in Section 3.4, loops can cause

76

REDUNDANT PATH

Chapter

3.

Solving Problems by Searching

certain algorithms to fail, making otherwise solvable problems unsolvable. Fortunately, there
is no need to consider loopy paths. We can rely on more than intuition for this: because path
costs are additive and step costs are nonnegative, a loopy path to any given state is never
better than the same path with the loop removed.
Loopy paths are a special case of the more general concept of redundant paths, which
exist whenever there is more than one way to get from one state to another. Consider the paths
Aradâ€“Sibiu (140 km long) and Aradâ€“Zerindâ€“Oradeaâ€“Sibiu (297 km long). Obviously, the
second path is redundantâ€”itâ€™s just a worse way to get to the same state. If you are concerned
about reaching the goal, thereâ€™s never any reason to keep more than one path to any given
state, because any goal state that is reachable by extending one path is also reachable by
extending the other.
In some cases, it is possible to define the problem itself so as to eliminate redundant
paths. For example, if we formulate the 8-queens problem (page 71) so that a queen can be
placed in any column, then each state with n queens can be reached by n! different paths; but
if we reformulate the problem so that each new queen is placed in the leftmost empty column,
then each state can be reached only through one path.
Arad

(a) The initial state

Timisoara

Sibiu

Arad

Fagaras

Oradea

Rimnicu Vilcea

(b) After expanding Arad

Arad

Fagaras

Oradea

Rimnicu Vilcea

Arad

Zerind

Lugoj

Arad

Timisoara

Oradea

Oradea

Oradea

Arad

Sibiu

Fagaras

Arad

Timisoara

(c) After expanding Sibiu

Arad

Lugoj

Arad

Sibiu

Arad

Zerind

Rimnicu Vilcea

Arad

Zerind

Lugoj

Arad

Oradea

Figure 3.6 Partial search trees for finding a route from Arad to Bucharest. Nodes that
have been expanded are shaded; nodes that have been generated but not yet expanded are
outlined in bold; nodes that have not yet been generated are shown in faint dashed lines.

Section 3.3.

Searching for Solutions

77

function T REE -S EARCH( problem) returns a solution, or failure
initialize the frontier using the initial state of problem
loop do
if the frontier is empty then return failure
choose a leaf node and remove it from the frontier
if the node contains a goal state then return the corresponding solution
expand the chosen node, adding the resulting nodes to the frontier
function G RAPH -S EARCH ( problem) returns a solution, or failure
initialize the frontier using the initial state of problem
initialize the explored set to be empty
loop do
if the frontier is empty then return failure
choose a leaf node and remove it from the frontier
if the node contains a goal state then return the corresponding solution
add the node to the explored set
expand the chosen node, adding the resulting nodes to the frontier
only if not in the frontier or explored set
Figure 3.7 An informal description of the general tree-search and graph-search algorithms. The parts of G RAPH -S EARCH marked in bold italic are the additions needed to
handle repeated states.

RECTANGULAR GRID

EXPLORED SET
CLOSED LIST

SEPARATOR

In other cases, redundant paths are unavoidable. This includes all problems where
the actions are reversible, such as route-finding problems and sliding-block puzzles. Routefinding on a rectangular grid (like the one used later for Figure 3.9) is a particularly important example in computer games. In such a grid, each state has four successors, so a search
tree of depth d that includes repeated states has 4d leaves; but there are only about 2d2 distinct
states within d steps of any given state. For d = 20, this means about a trillion nodes but only
about 800 distinct states. Thus, following redundant paths can cause a tractable problem to
become intractable. This is true even for algorithms that know how to avoid infinite loops.
As the saying goes, algorithms that forget their history are doomed to repeat it. The
way to avoid exploring redundant paths is to remember where one has been. To do this, we
augment the T REE -S EARCH algorithm with a data structure called the explored set (also
known as the closed list), which remembers every expanded node. Newly generated nodes
that match previously generated nodesâ€”ones in the explored set or the frontierâ€”can be discarded instead of being added to the frontier. The new algorithm, called G RAPH -S EARCH , is
shown informally in Figure 3.7. The specific algorithms in this chapter draw on this general
design.
Clearly, the search tree constructed by the G RAPH -S EARCH algorithm contains at most
one copy of each state, so we can think of it as growing a tree directly on the state-space graph,
as shown in Figure 3.8. The algorithm has another nice property: the frontier separates the
state-space graph into the explored region and the unexplored region, so that every path from

78

Chapter

3.

Solving Problems by Searching

Figure 3.8 A sequence of search trees generated by a graph search on the Romania problem of Figure 3.2. At each stage, we have extended each path by one step. Notice that at the
third stage, the northernmost city (Oradea) has become a dead end: both of its successors are
already explored via other paths.

(a)

(b)

(c)

Figure 3.9 The separation property of G RAPH -S EARCH , illustrated on a rectangular-grid
problem. The frontier (white nodes) always separates the explored region of the state space
(black nodes) from the unexplored region (gray nodes). In (a), just the root has been expanded. In (b), one leaf node has been expanded. In (c), the remaining successors of the root
have been expanded in clockwise order.

the initial state to an unexplored state has to pass through a state in the frontier. (If this
seems completely obvious, try Exercise 3.13 now.) This property is illustrated in Figure 3.9.
As every step moves a state from the frontier into the explored region while moving some
states from the unexplored region into the frontier, we see that the algorithm is systematically
examining the states in the state space, one by one, until it finds a solution.

3.3.1 Infrastructure for search algorithms
Search algorithms require a data structure to keep track of the search tree that is being constructed. For each node n of the tree, we have a structure that contains four components:
â€¢
â€¢
â€¢
â€¢

n.S TATE : the state in the state space to which the node corresponds;
n.PARENT: the node in the search tree that generated this node;
n.ACTION: the action that was applied to the parent to generate the node;
n.PATH -C OST : the cost, traditionally denoted by g(n), of the path from the initial state
to the node, as indicated by the parent pointers.

Section 3.3.

Searching for Solutions

79

PARENT

Node

5

4

6

1

88

7

3

22

ACTION = Right
PATH-COST = 6

STATE

Figure 3.10 Nodes are the data structures from which the search tree is constructed. Each
has a parent, a state, and various bookkeeping fields. Arrows point from child to parent.

Given the components for a parent node, it is easy to see how to compute the necessary
components for a child node. The function C HILD -N ODE takes a parent node and an action
and returns the resulting child node:

function C HILD -N ODE( problem, parent , action) returns a node
return a node with
S TATE = problem.R ESULT(parent.S TATE, action ),
PARENT = parent , ACTION = action,
PATH -C OST = parent .PATH -C OST + problem.S TEP -C OST(parent.S TATE, action )

QUEUE

The node data structure is depicted in Figure 3.10. Notice how the PARENT pointers
string the nodes together into a tree structure. These pointers also allow the solution path to be
extracted when a goal node is found; we use the S OLUTION function to return the sequence
of actions obtained by following parent pointers back to the root.
Up to now, we have not been very careful to distinguish between nodes and states, but in
writing detailed algorithms itâ€™s important to make that distinction. A node is a bookkeeping
data structure used to represent the search tree. A state corresponds to a configuration of the
world. Thus, nodes are on particular paths, as defined by PARENT pointers, whereas states
are not. Furthermore, two different nodes can contain the same world state if that state is
generated via two different search paths.
Now that we have nodes, we need somewhere to put them. The frontier needs to be
stored in such a way that the search algorithm can easily choose the next node to expand
according to its preferred strategy. The appropriate data structure for this is a queue. The
operations on a queue are as follows:
â€¢ E MPTY ?(queue) returns true only if there are no more elements in the queue.
â€¢ P OP(queue) removes the first element of the queue and returns it.
â€¢ I NSERT (element, queue) inserts an element and returns the resulting queue.

80

FIFO QUEUE
LIFO QUEUE
PRIORITY QUEUE

CANONICAL FORM

Chapter

3.

Solving Problems by Searching

Queues are characterized by the order in which they store the inserted nodes. Three common
variants are the first-in, first-out or FIFO queue, which pops the oldest element of the queue;
the last-in, first-out or LIFO queue (also known as a stack), which pops the newest element
of the queue; and the priority queue, which pops the element of the queue with the highest
priority according to some ordering function.
The explored set can be implemented with a hash table to allow efficient checking for
repeated states. With a good implementation, insertion and lookup can be done in roughly
constant time no matter how many states are stored. One must take care to implement the
hash table with the right notion of equality between states. For example, in the traveling
salesperson problem (page 74), the hash table needs to know that the set of visited cities
{Bucharest,Urziceni,Vaslui} is the same as {Urziceni,Vaslui,Bucharest}. Sometimes this can
be achieved most easily by insisting that the data structures for states be in some canonical
form; that is, logically equivalent states should map to the same data structure. In the case
of states described by sets, for example, a bit-vector representation or a sorted list without
repetition would be canonical, whereas an unsorted list would not.

3.3.2 Measuring problem-solving performance
Before we get into the design of specific search algorithms, we need to consider the criteria
that might be used to choose among them. We can evaluate an algorithmâ€™s performance in
four ways:
COMPLETENESS
OPTIMALITY
TIME COMPLEXITY
SPACE COMPLEXITY

BRANCHING FACTOR
DEPTH

SEARCH COST

TOTAL COST

â€¢
â€¢
â€¢
â€¢

Completeness: Is the algorithm guaranteed to find a solution when there is one?
Optimality: Does the strategy find the optimal solution, as defined on page 68?
Time complexity: How long does it take to find a solution?
Space complexity: How much memory is needed to perform the search?

Time and space complexity are always considered with respect to some measure of the problem difficulty. In theoretical computer science, the typical measure is the size of the state
space graph, |V | + |E|, where V is the set of vertices (nodes) of the graph and E is the set
of edges (links). This is appropriate when the graph is an explicit data structure that is input
to the search program. (The map of Romania is an example of this.) In AI, the graph is often
represented implicitly by the initial state, actions, and transition model and is frequently infinite. For these reasons, complexity is expressed in terms of three quantities: b, the branching
factor or maximum number of successors of any node; d, the depth of the shallowest goal
node (i.e., the number of steps along the path from the root); and m, the maximum length of
any path in the state space. Time is often measured in terms of the number of nodes generated
during the search, and space in terms of the maximum number of nodes stored in memory.
For the most part, we describe time and space complexity for search on a tree; for a graph,
the answer depends on how â€œredundantâ€ the paths in the state space are.
To assess the effectiveness of a search algorithm, we can consider just the search costâ€”
which typically depends on the time complexity but can also include a term for memory
usageâ€”or we can use the total cost, which combines the search cost and the path cost of the
solution found. For the problem of finding a route from Arad to Bucharest, the search cost
is the amount of time taken by the search and the solution cost is the total length of the path

Section 3.4.

Uninformed Search Strategies

81

in kilometers. Thus, to compute the total cost, we have to add milliseconds and kilometers.
There is no â€œofficial exchange rateâ€ between the two, but it might be reasonable in this case to
convert kilometers into milliseconds by using an estimate of the carâ€™s average speed (because
time is what the agent cares about). This enables the agent to find an optimal tradeoff point
at which further computation to find a shorter path becomes counterproductive. The more
general problem of tradeoffs between different goods is taken up in Chapter 16.

3.4

U NINFORMED S EARCH S TRATEGIES

UNINFORMED
SEARCH
BLIND SEARCH

INFORMED SEARCH
HEURISTIC SEARCH

This section covers several search strategies that come under the heading of uninformed
search (also called blind search). The term means that the strategies have no additional
information about states beyond that provided in the problem definition. All they can do is
generate successors and distinguish a goal state from a non-goal state. All search strategies
are distinguished by the order in which nodes are expanded. Strategies that know whether
one non-goal state is â€œmore promisingâ€ than another are called informed search or heuristic
search strategies; they are covered in Section 3.5.

3.4.1 Breadth-first search
BREADTH-FIRST
SEARCH

Breadth-first search is a simple strategy in which the root node is expanded first, then all the
successors of the root node are expanded next, then their successors, and so on. In general,
all the nodes are expanded at a given depth in the search tree before any nodes at the next
level are expanded.
Breadth-first search is an instance of the general graph-search algorithm (Figure 3.7) in
which the shallowest unexpanded node is chosen for expansion. This is achieved very simply
by using a FIFO queue for the frontier. Thus, new nodes (which are always deeper than their
parents) go to the back of the queue, and old nodes, which are shallower than the new nodes,
get expanded first. There is one slight tweak on the general graph-search algorithm, which is
that the goal test is applied to each node when it is generated rather than when it is selected for
expansion. This decision is explained below, where we discuss time complexity. Note also
that the algorithm, following the general template for graph search, discards any new path to
a state already in the frontier or explored set; it is easy to see that any such path must be at
least as deep as the one already found. Thus, breadth-first search always has the shallowest
path to every node on the frontier.
Pseudocode is given in Figure 3.11. Figure 3.12 shows the progress of the search on a
simple binary tree.
How does breadth-first search rate according to the four criteria from the previous section? We can easily see that it is completeâ€”if the shallowest goal node is at some finite depth
d, breadth-first search will eventually find it after generating all shallower nodes (provided
the branching factor b is finite). Note that as soon as a goal node is generated, we know it
is the shallowest goal node because all shallower nodes must have been generated already
and failed the goal test. Now, the shallowest goal node is not necessarily the optimal one;

82

Chapter

3.

Solving Problems by Searching

function B READTH -F IRST-S EARCH ( problem) returns a solution, or failure
node â† a node with S TATE = problem.I NITIAL -S TATE, PATH -C OST = 0
if problem.G OAL -T EST(node.S TATE) then return S OLUTION(node)
frontier â† a FIFO queue with node as the only element
explored â† an empty set
loop do
if E MPTY ?( frontier ) then return failure
node â† P OP( frontier ) /* chooses the shallowest node in frontier */
add node.S TATE to explored
for each action in problem.ACTIONS(node.S TATE) do
child â† C HILD -N ODE( problem, node, action)
if child .S TATE is not in explored or frontier then
if problem.G OAL -T EST(child .S TATE) then return S OLUTION(child )
frontier â† I NSERT(child , frontier )
Figure 3.11

Breadth-first search on a graph.

technically, breadth-first search is optimal if the path cost is a nondecreasing function of the
depth of the node. The most common such scenario is that all actions have the same cost.
So far, the news about breadth-first search has been good. The news about time and
space is not so good. Imagine searching a uniform tree where every state has b successors.
The root of the search tree generates b nodes at the first level, each of which generates b more
nodes, for a total of b2 at the second level. Each of these generates b more nodes, yielding b3
nodes at the third level, and so on. Now suppose that the solution is at depth d. In the worst
case, it is the last node generated at that level. Then the total number of nodes generated is
b + b2 + b3 + Â· Â· Â· + bd = O(bd ) .
(If the algorithm were to apply the goal test to nodes when selected for expansion, rather than
when generated, the whole layer of nodes at depth d would be expanded before the goal was
detected and the time complexity would be O(bd+1 ).)
As for space complexity: for any kind of graph search, which stores every expanded
node in the explored set, the space complexity is always within a factor of b of the time
complexity. For breadth-first graph search in particular, every node generated remains in
memory. There will be O(bdâˆ’1 ) nodes in the explored set and O(bd ) nodes in the frontier,
A

A

B
D

C
E

F

A

B
G

D

C
E

F

A

B
G

D

C
E

F

B
G

D

C
E

F

Figure 3.12 Breadth-first search on a simple binary tree. At each stage, the node to be
expanded next is indicated by a marker.

G

Section 3.4.

Uninformed Search Strategies

83

so the space complexity is O(bd ), i.e., it is dominated by the size of the frontier. Switching
to a tree search would not save much space, and in a state space with many redundant paths,
switching could cost a great deal of time.
An exponential complexity bound such as O(bd ) is scary. Figure 3.13 shows why. It
lists, for various values of the solution depth d, the time and memory required for a breadthfirst search with branching factor b = 10. The table assumes that 1 million nodes can be
generated per second and that a node requires 1000 bytes of storage. Many search problems
fit roughly within these assumptions (give or take a factor of 100) when run on a modern
personal computer.
Depth

Nodes

2
4
6
8
10
12
14
16

110
11,110
106
108
1010
1012
1014
1016

Time
.11
11
1.1
2
3
13
3.5
350

milliseconds
milliseconds
seconds
minutes
hours
days
years
years

Memory
107
10.6
1
103
10
1
99
10

kilobytes
megabytes
gigabyte
gigabytes
terabytes
petabyte
petabytes
exabytes

Figure 3.13 Time and memory requirements for breadth-first search. The numbers shown
assume branching factor b = 10; 1 million nodes/second; 1000 bytes/node.

Two lessons can be learned from Figure 3.13. First, the memory requirements are a
bigger problem for breadth-first search than is the execution time. One might wait 13 days
for the solution to an important problem with search depth 12, but no personal computer has
the petabyte of memory it would take. Fortunately, other strategies require less memory.
The second lesson is that time is still a major factor. If your problem has a solution at
depth 16, then (given our assumptions) it will take about 350 years for breadth-first search (or
indeed any uninformed search) to find it. In general, exponential-complexity search problems
cannot be solved by uninformed methods for any but the smallest instances.

3.4.2 Uniform-cost search

UNIFORM-COST
SEARCH

When all step costs are equal, breadth-first search is optimal because it always expands the
shallowest unexpanded node. By a simple extension, we can find an algorithm that is optimal
with any step-cost function. Instead of expanding the shallowest node, uniform-cost search
expands the node n with the lowest path cost g(n). This is done by storing the frontier as a
priority queue ordered by g. The algorithm is shown in Figure 3.14.
In addition to the ordering of the queue by path cost, there are two other significant
differences from breadth-first search. The first is that the goal test is applied to a node when
it is selected for expansion (as in the generic graph-search algorithm shown in Figure 3.7)
rather than when it is first generated. The reason is that the first goal node that is generated

84

Chapter

3.

Solving Problems by Searching

function U NIFORM -C OST-S EARCH ( problem) returns a solution, or failure
node â† a node with S TATE = problem.I NITIAL -S TATE, PATH -C OST = 0
frontier â† a priority queue ordered by PATH -C OST, with node as the only element
explored â† an empty set
loop do
if E MPTY ?( frontier ) then return failure
node â† P OP( frontier ) /* chooses the lowest-cost node in frontier */
if problem.G OAL -T EST(node.S TATE) then return S OLUTION (node)
add node.S TATE to explored
for each action in problem.ACTIONS(node.S TATE) do
child â† C HILD -N ODE( problem, node, action)
if child .S TATE is not in explored or frontier then
frontier â† I NSERT(child , frontier )
else if child .S TATE is in frontier with higher PATH -C OST then
replace that frontier node with child
Figure 3.14 Uniform-cost search on a graph. The algorithm is identical to the general
graph search algorithm in Figure 3.7, except for the use of a priority queue and the addition
of an extra check in case a shorter path to a frontier state is discovered. The data structure for
frontier needs to support efficient membership testing, so it should combine the capabilities
of a priority queue and a hash table.

Sibiu

99

Fagaras

80
Rimnicu Vilcea

97

211

Pitesti

101
Bucharest

Figure 3.15

Part of the Romania state space, selected to illustrate uniform-cost search.

may be on a suboptimal path. The second difference is that a test is added in case a better
path is found to a node currently on the frontier.
Both of these modifications come into play in the example shown in Figure 3.15, where
the problem is to get from Sibiu to Bucharest. The successors of Sibiu are Rimnicu Vilcea and
Fagaras, with costs 80 and 99, respectively. The least-cost node, Rimnicu Vilcea, is expanded
next, adding Pitesti with cost 80 + 97 = 177. The least-cost node is now Fagaras, so it is
expanded, adding Bucharest with cost 99 + 211 = 310. Now a goal node has been generated,
but uniform-cost search keeps going, choosing Pitesti for expansion and adding a second path

Section 3.4.

Uninformed Search Strategies

85

to Bucharest with cost 80 + 97 + 101 = 278. Now the algorithm checks to see if this new path
is better than the old one; it is, so the old one is discarded. Bucharest, now with g-cost 278,
is selected for expansion and the solution is returned.
It is easy to see that uniform-cost search is optimal in general. First, we observe that
whenever uniform-cost search selects a node n for expansion, the optimal path to that node
has been found. (Were this not the case, there would have to be another frontier node n on
the optimal path from the start node to n, by the graph separation property of Figure 3.9;
by definition, n would have lower g-cost than n and would have been selected first.) Then,
because step costs are nonnegative, paths never get shorter as nodes are added. These two
facts together imply that uniform-cost search expands nodes in order of their optimal path
cost. Hence, the first goal node selected for expansion must be the optimal solution.
Uniform-cost search does not care about the number of steps a path has, but only about
their total cost. Therefore, it will get stuck in an infinite loop if there is a path with an infinite
sequence of zero-cost actionsâ€”for example, a sequence of NoOp actions.6 Completeness is
guaranteed provided the cost of every step exceeds some small positive constant .
Uniform-cost search is guided by path costs rather than depths, so its complexity is not
easily characterized in terms of b and d. Instead, let C âˆ— be the cost of the optimal solution,7
and assume that every action costs at least . Then the algorithmâ€™s worst-case time and space
âˆ—
complexity is O(b1+C / ), which can be much greater than bd . This is because uniformcost search can explore large trees of small steps before exploring paths involving large and
âˆ—
perhaps useful steps. When all step costs are equal, b1+C / is just bd+1 . When all step
costs are the same, uniform-cost search is similar to breadth-first search, except that the latter
stops as soon as it generates a goal, whereas uniform-cost search examines all the nodes at
the goalâ€™s depth to see if one has a lower cost; thus uniform-cost search does strictly more
work by expanding nodes at depth d unnecessarily.

3.4.3 Depth-first search
DEPTH-FIRST
SEARCH

Depth-first search always expands the deepest node in the current frontier of the search tree.
The progress of the search is illustrated in Figure 3.16. The search proceeds immediately
to the deepest level of the search tree, where the nodes have no successors. As those nodes
are expanded, they are dropped from the frontier, so then the search â€œbacks upâ€ to the next
deepest node that still has unexplored successors.
The depth-first search algorithm is an instance of the graph-search algorithm in Figure 3.7; whereas breadth-first-search uses a FIFO queue, depth-first search uses a LIFO queue.
A LIFO queue means that the most recently generated node is chosen for expansion. This
must be the deepest unexpanded node because it is one deeper than its parentâ€”which, in turn,
was the deepest unexpanded node when it was selected.
As an alternative to the G RAPH -S EARCH -style implementation, it is common to implement depth-first search with a recursive function that calls itself on each of its children in
turn. (A recursive depth-first algorithm incorporating a depth limit is shown in Figure 3.17.)
6
7

NoOp, or â€œno operation,â€ is the name of an assembly language instruction that does nothing.
Here, and throughout the book, the â€œstarâ€ in C âˆ— means an optimal value for C.

86

Chapter
A

H

C
E

I

J

L

N

C

D

G
M

O

H

E
I

J

K

C
E

I

J

L

G
N

M

D
O

O

H

E
I

J

F
K

J

O

C

F
L

K

N

M

A

E
I

L

G

E

G
N

M

O

J

F
K

L

G
N

M

O

A
C

E

B

F
L

K

D

C

A

J

N

M

B

F
K

L

G

C

A

B

H

B

F

A

D

A

B

F
K

Solving Problems by Searching

A

B
D

3.

G
N

M

C
E

O

A

C

F
L

K

G
N

M

F
O

A
C

F

G
M

N

O

C

F
O

N

M

A

C

L

L

G

L

G
M

N

F
O

G
M

N

O

Figure 3.16 Depth-first search on a binary tree. The unexplored region is shown in light
gray. Explored nodes with no descendants in the frontier are removed from memory. Nodes
at depth 3 have no successors and M is the only goal node.

The properties of depth-first search depend strongly on whether the graph-search or
tree-search version is used. The graph-search version, which avoids repeated states and redundant paths, is complete in finite state spaces because it will eventually expand every node.
The tree-search version, on the other hand, is not completeâ€”for example, in Figure 3.6 the
algorithm will follow the Aradâ€“Sibiuâ€“Aradâ€“Sibiu loop forever. Depth-first tree search can be
modified at no extra memory cost so that it checks new states against those on the path from
the root to the current node; this avoids infinite loops in finite state spaces but does not avoid
the proliferation of redundant paths. In infinite state spaces, both versions fail if an infinite
non-goal path is encountered. For example, in Knuthâ€™s 4 problem, depth-first search would
keep applying the factorial operator forever.
For similar reasons, both versions are nonoptimal. For example, in Figure 3.16, depthfirst search will explore the entire left subtree even if node C is a goal node. If node J were
also a goal node, then depth-first search would return it as a solution instead of C, which
would be a better solution; hence, depth-first search is not optimal.

Section 3.4.

BACKTRACKING
SEARCH

Uninformed Search Strategies

87

The time complexity of depth-first graph search is bounded by the size of the state space
(which may be infinite, of course). A depth-first tree search, on the other hand, may generate
all of the O(bm ) nodes in the search tree, where m is the maximum depth of any node; this
can be much greater than the size of the state space. Note that m itself can be much larger
than d (the depth of the shallowest solution) and is infinite if the tree is unbounded.
So far, depth-first search seems to have no clear advantage over breadth-first search,
so why do we include it? The reason is the space complexity. For a graph search, there is
no advantage, but a depth-first tree search needs to store only a single path from the root
to a leaf node, along with the remaining unexpanded sibling nodes for each node on the
path. Once a node has been expanded, it can be removed from memory as soon as all its
descendants have been fully explored. (See Figure 3.16.) For a state space with branching
factor b and maximum depth m, depth-first search requires storage of only O(bm) nodes.
Using the same assumptions as for Figure 3.13 and assuming that nodes at the same depth as
the goal node have no successors, we find that depth-first search would require 156 kilobytes
instead of 10 exabytes at depth d = 16, a factor of 7 trillion times less space. This has
led to the adoption of depth-first tree search as the basic workhorse of many areas of AI,
including constraint satisfaction (Chapter 6), propositional satisfiability (Chapter 7), and logic
programming (Chapter 9). For the remainder of this section, we focus primarily on the treesearch version of depth-first search.
A variant of depth-first search called backtracking search uses still less memory. (See
Chapter 6 for more details.) In backtracking, only one successor is generated at a time rather
than all successors; each partially expanded node remembers which successor to generate
next. In this way, only O(m) memory is needed rather than O(bm). Backtracking search
facilitates yet another memory-saving (and time-saving) trick: the idea of generating a successor by modifying the current state description directly rather than copying it first. This
reduces the memory requirements to just one state description and O(m) actions. For this to
work, we must be able to undo each modification when we go back to generate the next successor. For problems with large state descriptions, such as robotic assembly, these techniques
are critical to success.

3.4.4 Depth-limited search

DEPTH-LIMITED
SEARCH

The embarrassing failure of depth-first search in infinite state spaces can be alleviated by
supplying depth-first search with a predetermined depth limit . That is, nodes at depth  are
treated as if they have no successors. This approach is called depth-limited search. The
depth limit solves the infinite-path problem. Unfortunately, it also introduces an additional
source of incompleteness if we choose  < d, that is, the shallowest goal is beyond the depth
limit. (This is likely when d is unknown.) Depth-limited search will also be nonoptimal if
we choose  > d. Its time complexity is O(b ) and its space complexity is O(b). Depth-first
search can be viewed as a special case of depth-limited search with  = âˆž.
Sometimes, depth limits can be based on knowledge of the problem. For example, on
the map of Romania there are 20 cities. Therefore, we know that if there is a solution, it must
be of length 19 at the longest, so  = 19 is a possible choice. But in fact if we studied the

88

Chapter

3.

Solving Problems by Searching

function D EPTH -L IMITED -S EARCH( problem, limit ) returns a solution, or failure/cutoff
return R ECURSIVE -DLS(M AKE -N ODE(problem.I NITIAL -S TATE), problem, limit )
function R ECURSIVE -DLS(node, problem, limit ) returns a solution, or failure/cutoff
if problem.G OAL -T EST(node.S TATE) then return S OLUTION(node)
else if limit = 0 then return cutoï¬€
else
cutoï¬€ occurred ? â† false
for each action in problem.ACTIONS(node.S TATE) do
child â† C HILD -N ODE( problem, node, action)
result â† R ECURSIVE -DLS(child , problem, limit âˆ’ 1)
if result = cutoï¬€ then cutoï¬€ occurred ? â† true
else if result = failure then return result
if cutoï¬€ occurred ? then return cutoï¬€ else return failure
Figure 3.17

DIAMETER

A recursive implementation of depth-limited tree search.

map carefully, we would discover that any city can be reached from any other city in at most
9 steps. This number, known as the diameter of the state space, gives us a better depth limit,
which leads to a more efficient depth-limited search. For most problems, however, we will
not know a good depth limit until we have solved the problem.
Depth-limited search can be implemented as a simple modification to the general treeor graph-search algorithm. Alternatively, it can be implemented as a simple recursive algorithm as shown in Figure 3.17. Notice that depth-limited search can terminate with two
kinds of failure: the standard failure value indicates no solution; the cutoï¬€ value indicates
no solution within the depth limit.

3.4.5 Iterative deepening depth-first search
ITERATIVE
DEEPENING SEARCH

Iterative deepening search (or iterative deepening depth-first search) is a general strategy,
often used in combination with depth-first tree search, that finds the best depth limit. It does
this by gradually increasing the limitâ€”first 0, then 1, then 2, and so onâ€”until a goal is found.
This will occur when the depth limit reaches d, the depth of the shallowest goal node. The
algorithm is shown in Figure 3.18. Iterative deepening combines the benefits of depth-first
and breadth-first search. Like depth-first search, its memory requirements are modest: O(bd)
to be precise. Like breadth-first search, it is complete when the branching factor is finite and
optimal when the path cost is a nondecreasing function of the depth of the node. Figure 3.19
shows four iterations of I TERATIVE -D EEPENING -S EARCH on a binary search tree, where the
solution is found on the fourth iteration.
Iterative deepening search may seem wasteful because states are generated multiple
times. It turns out this is not too costly. The reason is that in a search tree with the same (or
nearly the same) branching factor at each level, most of the nodes are in the bottom level,
so it does not matter much that the upper levels are generated multiple times. In an iterative
deepening search, the nodes on the bottom level (depth d) are generated once, those on the

Section 3.4.

Uninformed Search Strategies

89

function I TERATIVE -D EEPENING -S EARCH( problem) returns a solution, or failure
for depth = 0 to âˆž do
result â† D EPTH -L IMITED -S EARCH( problem, depth)
if result = cutoff then return result
Figure 3.18 The iterative deepening search algorithm, which repeatedly applies depthlimited search with increasing limits. It terminates when a solution is found or if the depthlimited search returns failure, meaning that no solution exists.

Limit = 0

A

A

Limit = 1

A

A

B

C

B

C

A

Limit = 2

C
E

G

C

D

E

C
E

G

C
E

I

J

G
M

L

O

N

H

I

J

C

H

E
I

J

K

M

L

O

N

H

J

C
E

I

J

Figure 3.19

H

I

L

G
M

N

H

M

L

K

J

O

N

O

N

H

I

J

F
K

H

E
I

J

L

G
M

N

G

H

M

L

O

N

H

E
I

J

F
K

J

M

L

B

F
K

G
O

N

A

E
I

O

N

C

D

C

D
O

M

L

B

F
K

G

A

B

F
K

M

L

C
E

C

D

G

B
D

G

A

E
I

F

A

B
G

C

D
O

E

C

J

G

C

D

A

B

F
K

O

N

F
K

G

F

A

B
D

M

L

E
I

F

B

F

E

C

D

G

A

H

E

D

G

B

F

E

C

D

A

B

D

A

B

F
K

G

C

A

E

A

D

G

C

D

B

F

B

F

B

F
K

E

A

B

H

E

A

D

D

C

D

C

A

B

F

Limit = 3

G

C

A

B

B

A

B

F

A

D

C

A

B

F

A

B

A

B
D

A

L

G
M

N

C

D
O

H

E
I

Four iterations of iterative deepening search on a binary tree.

J

F
K

L

G
M

N

O

90

Chapter

3.

Solving Problems by Searching

next-to-bottom level are generated twice, and so on, up to the children of the root, which are
generated d times. So the total number of nodes generated in the worst case is
N (IDS) = (d)b + (d âˆ’ 1)b2 + Â· Â· Â· + (1)bd ,
which gives a time complexity of O(bd )â€”asymptotically the same as breadth-first search.
There is some extra cost for generating the upper levels multiple times, but it is not large. For
example, if b = 10 and d = 5, the numbers are
N (IDS) = 50 + 400 + 3, 000 + 20, 000 + 100, 000 = 123, 450
N (BFS) = 10 + 100 + 1, 000 + 10, 000 + 100, 000 = 111, 110 .

ITERATIVE
LENGTHENING
SEARCH

If you are really concerned about repeating the repetition, you can use a hybrid approach
that runs breadth-first search until almost all the available memory is consumed, and then
runs iterative deepening from all the nodes in the frontier. In general, iterative deepening is
the preferred uninformed search method when the search space is large and the depth of the
solution is not known.
Iterative deepening search is analogous to breadth-first search in that it explores a complete layer of new nodes at each iteration before going on to the next layer. It would seem
worthwhile to develop an iterative analog to uniform-cost search, inheriting the latter algorithmâ€™s optimality guarantees while avoiding its memory requirements. The idea is to use
increasing path-cost limits instead of increasing depth limits. The resulting algorithm, called
iterative lengthening search, is explored in Exercise 3.17. It turns out, unfortunately, that
iterative lengthening incurs substantial overhead compared to uniform-cost search.

3.4.6 Bidirectional search
The idea behind bidirectional search is to run two simultaneous searchesâ€”one forward from
the initial state and the other backward from the goalâ€”hoping that the two searches meet in
the middle (Figure 3.20). The motivation is that bd/2 + bd/2 is much less than bd , or in the
figure, the area of the two small circles is less than the area of one big circle centered on the
start and reaching to the goal.
Bidirectional search is implemented by replacing the goal test with a check to see
whether the frontiers of the two searches intersect; if they do, a solution has been found.
(It is important to realize that the first such solution found may not be optimal, even if the
two searches are both breadth-first; some additional search is required to make sure there
isnâ€™t another short-cut across the gap.) The check can be done when each node is generated
or selected for expansion and, with a hash table, will take constant time. For example, if a
problem has solution depth d = 6, and each direction runs breadth-first search one node at a
time, then in the worst case the two searches meet when they have generated all of the nodes
at depth 3. For b = 10, this means a total of 2,220 node generations, compared with 1,111,110
for a standard breadth-first search. Thus, the time complexity of bidirectional search using
breadth-first searches in both directions is O(bd/2 ). The space complexity is also O(bd/2 ).
We can reduce this by roughly half if one of the two searches is done by iterative deepening,
but at least one of the frontiers must be kept in memory so that the intersection check can be
done. This space requirement is the most significant weakness of bidirectional search.

Section 3.4.

Uninformed Search Strategies

91

Goal

Start

Figure 3.20 A schematic view of a bidirectional search that is about to succeed when a
branch from the start node meets a branch from the goal node.

PREDECESSOR

The reduction in time complexity makes bidirectional search attractive, but how do we
search backward? This is not as easy as it sounds. Let the predecessors of a state x be all
those states that have x as a successor. Bidirectional search requires a method for computing
predecessors. When all the actions in the state space are reversible, the predecessors of x are
just its successors. Other cases may require substantial ingenuity.
Consider the question of what we mean by â€œthe goalâ€ in searching â€œbackward from the
goal.â€ For the 8-puzzle and for finding a route in Romania, there is just one goal state, so the
backward search is very much like the forward search. If there are several explicitly listed
goal statesâ€”for example, the two dirt-free goal states in Figure 3.3â€”then we can construct a
new dummy goal state whose immediate predecessors are all the actual goal states. But if the
goal is an abstract description, such as the goal that â€œno queen attacks another queenâ€ in the
n-queens problem, then bidirectional search is difficult to use.

3.4.7 Comparing uninformed search strategies
Figure 3.21 compares search strategies in terms of the four evaluation criteria set forth in
Section 3.3.2. This comparison is for tree-search versions. For graph searches, the main
differences are that depth-first search is complete for finite state spaces and that the space and
time complexities are bounded by the size of the state space.
Criterion
Complete?
Time
Space
Optimal?

BreadthFirst

UniformCost

DepthFirst

DepthLimited

Iterative
Deepening

Bidirectional
(if applicable)

Yesa
O(bd )
O(bd )
Yesc

Yesa,b
âˆ—
O(b1+C / )
1+C âˆ— /
O(b
)
Yes

No
O(bm )
O(bm)
No

No
O(b )
O(b)
No

Yesa
O(bd )
O(bd)
Yesc

Yesa,d
O(bd/2 )
O(bd/2 )
Yesc,d

Figure 3.21 Evaluation of tree-search strategies. b is the branching factor; d is the depth
of the shallowest solution; m is the maximum depth of the search tree; l is the depth limit.
Superscript caveats are as follows: a complete if b is finite; b complete if step costs â‰¥  for
positive ; c optimal if step costs are all identical; d if both directions use breadth-first search.

92

3.5

Chapter

3.

Solving Problems by Searching

I NFORMED (H EURISTIC ) S EARCH S TRATEGIES

INFORMED SEARCH

BEST-FIRST SEARCH

EVALUATION
FUNCTION

HEURISTIC
FUNCTION

This section shows how an informed search strategyâ€”one that uses problem-specific knowledge beyond the definition of the problem itselfâ€”can find solutions more efficiently than can
an uninformed strategy.
The general approach we consider is called best-first search. Best-first search is an
instance of the general T REE -S EARCH or G RAPH -S EARCH algorithm in which a node is
selected for expansion based on an evaluation function, f (n). The evaluation function is
construed as a cost estimate, so the node with the lowest evaluation is expanded first. The
implementation of best-first graph search is identical to that for uniform-cost search (Figure 3.14), except for the use of f instead of g to order the priority queue.
The choice of f determines the search strategy. (For example, as Exercise 3.21 shows,
best-first tree search includes depth-first search as a special case.) Most best-first algorithms
include as a component of f a heuristic function, denoted h(n):
h(n) = estimated cost of the cheapest path from the state at node n to a goal state.
(Notice that h(n) takes a node as input, but, unlike g(n), it depends only on the state at that
node.) For example, in Romania, one might estimate the cost of the cheapest path from Arad
to Bucharest via the straight-line distance from Arad to Bucharest.
Heuristic functions are the most common form in which additional knowledge of the
problem is imparted to the search algorithm. We study heuristics in more depth in Section 3.6.
For now, we consider them to be arbitrary, nonnegative, problem-specific functions, with one
constraint: if n is a goal node, then h(n) = 0. The remainder of this section covers two ways
to use heuristic information to guide search.

3.5.1 Greedy best-first search
GREEDY BEST-FIRST
SEARCH

STRAIGHT-LINE
DISTANCE

Greedy best-first search8 tries to expand the node that is closest to the goal, on the grounds
that this is likely to lead to a solution quickly. Thus, it evaluates nodes by using just the
heuristic function; that is, f (n) = h(n).
Let us see how this works for route-finding problems in Romania; we use the straightline distance heuristic, which we will call hSLD . If the goal is Bucharest, we need to
know the straight-line distances to Bucharest, which are shown in Figure 3.22. For example, hSLD (In(Arad )) = 366. Notice that the values of hSLD cannot be computed from the
problem description itself. Moreover, it takes a certain amount of experience to know that
hSLD is correlated with actual road distances and is, therefore, a useful heuristic.
Figure 3.23 shows the progress of a greedy best-first search using hSLD to find a path
from Arad to Bucharest. The first node to be expanded from Arad will be Sibiu because it
is closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will
be Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal. For
this particular problem, greedy best-first search using hSLD finds a solution without ever
8

Our first edition called this greedy search; other authors have called it best-first search. Our more general
usage of the latter term follows Pearl (1984).

Section 3.5.

Informed (Heuristic) Search Strategies
Arad
Bucharest
Craiova
Drobeta
Eforie
Fagaras
Giurgiu
Hirsova
Iasi
Lugoj
Figure 3.22

366
0
160
242
161
176
77
151
226
244

93
Mehadia
Neamt
Oradea
Pitesti
Rimnicu Vilcea
Sibiu
Timisoara
Urziceni
Vaslui
Zerind

241
234
380
100
193
253
329
80
199
374

Values of hSLD â€”straight-line distances to Bucharest.

expanding a node that is not on the solution path; hence, its search cost is minimal. It is
not optimal, however: the path via Sibiu and Fagaras to Bucharest is 32 kilometers longer
than the path through Rimnicu Vilcea and Pitesti. This shows why the algorithm is called
â€œgreedyâ€â€”at each step it tries to get as close to the goal as it can.
Greedy best-first tree search is also incomplete even in a finite state space, much like
depth-first search. Consider the problem of getting from Iasi to Fagaras. The heuristic suggests that Neamt be expanded first because it is closest to Fagaras, but it is a dead end. The
solution is to go first to Vasluiâ€”a step that is actually farther from the goal according to
the heuristicâ€”and then to continue to Urziceni, Bucharest, and Fagaras. The algorithm will
never find this solution, however, because expanding Neamt puts Iasi back into the frontier,
Iasi is closer to Fagaras than Vaslui is, and so Iasi will be expanded again, leading to an infinite loop. (The graph search version is complete in finite spaces, but not in infinite ones.) The
worst-case time and space complexity for the tree version is O(bm ), where m is the maximum
depth of the search space. With a good heuristic function, however, the complexity can be
reduced substantially. The amount of the reduction depends on the particular problem and on
the quality of the heuristic.

3.5.2 A* search: Minimizing the total estimated solution cost
âˆ—

A SEARCH

The most widely known form of best-first search is called Aâˆ— search (pronounced â€œA-star
searchâ€). It evaluates nodes by combining g(n), the cost to reach the node, and h(n), the cost
to get from the node to the goal:
f (n) = g(n) + h(n) .
Since g(n) gives the path cost from the start node to node n, and h(n) is the estimated cost
of the cheapest path from n to the goal, we have
f (n) = estimated cost of the cheapest solution through n .
Thus, if we are trying to find the cheapest solution, a reasonable thing to try first is the
node with the lowest value of g(n) + h(n). It turns out that this strategy is more than just
reasonable: provided that the heuristic function h(n) satisfies certain conditions, Aâˆ— search is
both complete and optimal. The algorithm is identical to U NIFORM -C OST-S EARCH except
that Aâˆ— uses g + h instead of g.

94

Chapter
(a) The initial state

3.

Solving Problems by Searching

Arad
366

(b) After expanding Arad

Arad

Sibiu

Timisoara

Zerind

253

329

374

Timisoara

Zerind

329

374

Timisoara

Zerind

329

374

(c) After expanding Sibiu

Arad

Sibiu

Arad

Fagaras

Oradea

Rimnicu Vilcea

366

176

380

193

(d) After expanding Fagaras

Arad

Sibiu

Arad

Fagaras

Oradea

Rimnicu Vilcea

380

193

366

Sibiu

Bucharest

253

0

Figure 3.23 Stages in a greedy best-first tree search for Bucharest with the straight-line
distance heuristic hSLD . Nodes are labeled with their h-values.

Conditions for optimality: Admissibility and consistency
ADMISSIBLE
HEURISTIC

The first condition we require for optimality is that h(n) be an admissible heuristic. An
admissible heuristic is one that never overestimates the cost to reach the goal. Because g(n)
is the actual cost to reach n along the current path, and f (n) = g(n) + h(n), we have as an
immediate consequence that f (n) never overestimates the true cost of a solution along the
current path through n.
Admissible heuristics are by nature optimistic because they think the cost of solving
the problem is less than it actually is. An obvious example of an admissible heuristic is the
straight-line distance hSLD that we used in getting to Bucharest. Straight-line distance is
admissible because the shortest path between any two points is a straight line, so the straight

Section 3.5.

CONSISTENCY
MONOTONICITY

Informed (Heuristic) Search Strategies

95

line cannot be an overestimate. In Figure 3.24, we show the progress of an Aâˆ— tree search for
Bucharest. The values of g are computed from the step costs in Figure 3.2, and the values of
hSLD are given in Figure 3.22. Notice in particular that Bucharest first appears on the frontier
at step (e), but it is not selected for expansion because its f -cost (450) is higher than that of
Pitesti (417). Another way to say this is that there might be a solution through Pitesti whose
cost is as low as 417, so the algorithm will not settle for a solution that costs 450.
A second, slightly stronger condition called consistency (or sometimes monotonicity)
is required only for applications of Aâˆ— to graph search.9 A heuristic h(n) is consistent if, for
every node n and every successor n of n generated by any action a, the estimated cost of
reaching the goal from n is no greater than the step cost of getting to n plus the estimated
cost of reaching the goal from n :
h(n) â‰¤ c(n, a, n ) + h(n ) .

TRIANGLE
INEQUALITY

This is a form of the general triangle inequality, which stipulates that each side of a triangle
cannot be longer than the sum of the other two sides. Here, the triangle is formed by n, n ,
and the goal Gn closest to n. For an admissible heuristic, the inequality makes perfect sense:
if there were a route from n to Gn via n that was cheaper than h(n), that would violate the
property that h(n) is a lower bound on the cost to reach Gn .
It is fairly easy to show (Exercise 3.29) that every consistent heuristic is also admissible.
Consistency is therefore a stricter requirement than admissibility, but one has to work quite
hard to concoct heuristics that are admissible but not consistent. All the admissible heuristics
we discuss in this chapter are also consistent. Consider, for example, hSLD . We know that
the general triangle inequality is satisfied when each side is measured by the straight-line
distance and that the straight-line distance between n and n is no greater than c(n, a, n ).
Hence, hSLD is a consistent heuristic.
Optimality of A*
As we mentioned earlier, Aâˆ— has the following properties: the tree-search version of Aâˆ— is
optimal if h(n) is admissible, while the graph-search version is optimal if h(n) is consistent.
We show the second of these two claims since it is more useful. The argument essentially mirrors the argument for the optimality of uniform-cost search, with g replaced by
f â€”just as in the Aâˆ— algorithm itself.
The first step is to establish the following: if h(n) is consistent, then the values of
f (n) along any path are nondecreasing. The proof follows directly from the definition of
consistency. Suppose n is a successor of n; then g(n ) = g(n) + c(n, a, n ) for some action
a, and we have
f (n ) = g(n ) + h(n ) = g(n) + c(n, a, n ) + h(n ) â‰¥ g(n) + h(n) = f (n) .
The next step is to prove that whenever Aâˆ— selects a node n for expansion, the optimal path
to that node has been found. Were this not the case, there would have to be another frontier
node n on the optimal path from the start node to n, by the graph separation property of
9

With an admissible but inconsistent heuristic, Aâˆ— requires some extra bookkeeping to ensure optimality.

96

Chapter
(a) The initial state

3.

Solving Problems by Searching

Arad
366=0+366

(b) After expanding Arad

Arad

Sibiu
393=140+253

(c) After expanding Sibiu

Fagaras

Zerind
449=75+374

Arad

Sibiu

Arad

Timisoara
447=118+329

Oradea

Timisoara

Zerind

447=118+329

449=75+374

Rimnicu Vilcea

646=280+366 415=239+176 671=291+380 413=220+193

(d) After expanding Rimnicu Vilcea

Arad

Sibiu

Arad

Fagaras

Oradea

Timisoara

Zerind

447=118+329

449=75+374

Rimnicu Vilcea

646=280+366 415=239+176 671=291+380
Craiova

Pitesti

Sibiu

526=366+160 417=317+100 553=300+253

(e) After expanding Fagaras

Arad

Sibiu

Arad

Fagaras

646=280+366
Sibiu

Oradea

Rimnicu Vilcea

Bucharest

Craiova

Pitesti

Arad

Sibiu

Fagaras

Sibiu

Sibiu

526=366+160 417=317+100 553=300+253

(f) After expanding Pitesti

Arad

Zerind
449=75+374

671=291+380

591=338+253 450=450+0

646=280+366

Timisoara
447=118+329

Timisoara

Zerind

447=118+329

449=75+374

Rimnicu Vilcea

Oradea
671=291+380

Bucharest

591=338+253 450=450+0

Craiova

Pitesti

526=366+160
Bucharest

Sibiu
553=300+253

Craiova

Rimnicu Vilcea

418=418+0 615=455+160 607=414+193

Figure 3.24 Stages in an Aâˆ— search for Bucharest. Nodes are labeled with f = g + h. The
h values are the straight-line distances to Bucharest taken from Figure 3.22.

Section 3.5.

Informed (Heuristic) Search Strategies

97

O
N

Z

I

A
S

380

F

V

400
T

R
P

L
M

B

420

D
C

U

H
E

G

Figure 3.25 Map of Romania showing contours at f = 380, f = 400, and f = 420, with
Arad as the start state. Nodes inside a given contour have f -costs less than or equal to the
contour value.

CONTOUR

Figure 3.9; because f is nondecreasing along any path, n would have lower f -cost than n
and would have been selected first.
From the two preceding observations, it follows that the sequence of nodes expanded
by Aâˆ— using G RAPH -S EARCH is in nondecreasing order of f (n). Hence, the first goal node
selected for expansion must be an optimal solution because f is the true cost for goal nodes
(which have h = 0) and all later goal nodes will be at least as expensive.
The fact that f -costs are nondecreasing along any path also means that we can draw
contours in the state space, just like the contours in a topographic map. Figure 3.25 shows
an example. Inside the contour labeled 400, all nodes have f (n) less than or equal to 400,
and so on. Then, because Aâˆ— expands the frontier node of lowest f -cost, we can see that an
Aâˆ— search fans out from the start node, adding nodes in concentric bands of increasing f -cost.
With uniform-cost search (Aâˆ— search using h(n) = 0), the bands will be â€œcircularâ€
around the start state. With more accurate heuristics, the bands will stretch toward the goal
state and become more narrowly focused around the optimal path. If C âˆ— is the cost of the
optimal solution path, then we can say the following:
â€¢ Aâˆ— expands all nodes with f (n) < C âˆ— .
â€¢ Aâˆ— might then expand some of the nodes right on the â€œgoal contourâ€ (where f (n) = C âˆ— )
before selecting a goal node.
Completeness requires that there be only finitely many nodes with cost less than or equal to
C âˆ— , a condition that is true if all step costs exceed some finite  and if b is finite.
Notice that Aâˆ— expands no nodes with f (n) > C âˆ— â€”for example, Timisoara is not
expanded in Figure 3.24 even though it is a child of the root. We say that the subtree below

98
PRUNING

OPTIMALLY
EFFICIENT

ABSOLUTE ERROR
RELATIVE ERROR

Chapter

3.

Solving Problems by Searching

Timisoara is pruned; because hSLD is admissible, the algorithm can safely ignore this subtree
while still guaranteeing optimality. The concept of pruningâ€”eliminating possibilities from
consideration without having to examine themâ€”is important for many areas of AI.
One final observation is that among optimal algorithms of this typeâ€”algorithms that
extend search paths from the root and use the same heuristic informationâ€”Aâˆ— is optimally
efficient for any given consistent heuristic. That is, no other optimal algorithm is guaranteed to expand fewer nodes than Aâˆ— (except possibly through tie-breaking among nodes with
f (n) = C âˆ— ). This is because any algorithm that does not expand all nodes with f (n) < C âˆ—
runs the risk of missing the optimal solution.
That Aâˆ— search is complete, optimal, and optimally efficient among all such algorithms
is rather satisfying. Unfortunately, it does not mean that Aâˆ— is the answer to all our searching
needs. The catch is that, for most problems, the number of states within the goal contour
search space is still exponential in the length of the solution. The details of the analysis are
beyond the scope of this book, but the basic results are as follows. For problems with constant
step costs, the growth in run time as a function of the optimal solution depth d is analyzed in
terms of the the absolute error or the relative error of the heuristic. The absolute error is
defined as Î” â‰¡ hâˆ— âˆ’ h, where hâˆ— is the actual cost of getting from the root to the goal, and
the relative error is defined as  â‰¡ (hâˆ— âˆ’ h)/hâˆ— .
The complexity results depend very strongly on the assumptions made about the state
space. The simplest model studied is a state space that has a single goal and is essentially a
tree with reversible actions. (The 8-puzzle satisfies the first and third of these assumptions.)
In this case, the time complexity of Aâˆ— is exponential in the maximum absolute error, that is,
O(bÎ” ). For constant step costs, we can write this as O(bd ), where d is the solution depth.
For almost all heuristics in practical use, the absolute error is at least proportional to the path
cost hâˆ— , so  is constant or growing and the time complexity is exponential in d. We can
also see the effect of a more accurate heuristic: O(bd ) = O((b )d ), so the effective branching
factor (defined more formally in the next section) is b .
When the state space has many goal statesâ€”particularly near-optimal goal statesâ€”the
search process can be led astray from the optimal path and there is an extra cost proportional
to the number of goals whose cost is within a factor  of the optimal cost. Finally, in the
general case of a graph, the situation is even worse. There can be exponentially many states
with f (n) < C âˆ— even if the absolute error is bounded by a constant. For example, consider
a version of the vacuum world where the agent can clean up any square for unit cost without
even having to visit it: in that case, squares can be cleaned in any order. With N initially dirty
squares, there are 2N states where some subset has been cleaned and all of them are on an
optimal solution pathâ€”and hence satisfy f (n) < C âˆ— â€”even if the heuristic has an error of 1.
The complexity of Aâˆ— often makes it impractical to insist on finding an optimal solution.
One can use variants of Aâˆ— that find suboptimal solutions quickly, or one can sometimes
design heuristics that are more accurate but not strictly admissible. In any case, the use of a
good heuristic still provides enormous savings compared to the use of an uninformed search.
In Section 3.6, we look at the question of designing good heuristics.
Computation time is not, however, Aâˆ— â€™s main drawback. Because it keeps all generated
nodes in memory (as do all G RAPH -S EARCH algorithms), Aâˆ— usually runs out of space long

Section 3.5.

Informed (Heuristic) Search Strategies

99

function R ECURSIVE -B EST-F IRST-S EARCH ( problem) returns a solution, or failure
return RBFS(problem, M AKE -N ODE(problem.I NITIAL -S TATE), âˆž)
function RBFS(problem, node, f limit ) returns a solution, or failure and a new f -cost limit
if problem.G OAL -T EST(node.S TATE) then return S OLUTION(node)
successors â† [ ]
for each action in problem.ACTIONS (node.S TATE) do
add C HILD -N ODE( problem, node, action) into successors
if successors is empty then return failure, âˆž
for each s in successors do /* update f with value from previous search, if any */
s.f â† max(s.g + s.h, node.f ))
loop do
best â† the lowest f -value node in successors
if best .f > f limit then return failure, best.f
alternative â† the second-lowest f -value among successors
result, best.f â† RBFS(problem, best, min( f limit, alternative))
if result = failure then return result
Figure 3.26

The algorithm for recursive best-first search.

before it runs out of time. For this reason, Aâˆ— is not practical for many large-scale problems. There are, however, algorithms that overcome the space problem without sacrificing
optimality or completeness, at a small cost in execution time. We discuss these next.

3.5.3 Memory-bounded heuristic search
ITERATIVEDEEPENING
âˆ—
A

RECURSIVE
BEST-FIRST SEARCH

BACKED-UP VALUE

The simplest way to reduce memory requirements for Aâˆ— is to adapt the idea of iterative
deepening to the heuristic search context, resulting in the iterative-deepening Aâˆ— (IDAâˆ— ) algorithm. The main difference between IDAâˆ— and standard iterative deepening is that the cutoff
used is the f -cost (g + h) rather than the depth; at each iteration, the cutoff value is the smallest f -cost of any node that exceeded the cutoff on the previous iteration. IDAâˆ— is practical
for many problems with unit step costs and avoids the substantial overhead associated with
keeping a sorted queue of nodes. Unfortunately, it suffers from the same difficulties with realvalued costs as does the iterative version of uniform-cost search described in Exercise 3.17.
This section briefly examines two other memory-bounded algorithms, called RBFS and MAâˆ— .
Recursive best-first search (RBFS) is a simple recursive algorithm that attempts to
mimic the operation of standard best-first search, but using only linear space. The algorithm
is shown in Figure 3.26. Its structure is similar to that of a recursive depth-first search, but
rather than continuing indefinitely down the current path, it uses the f limit variable to keep
track of the f -value of the best alternative path available from any ancestor of the current
node. If the current node exceeds this limit, the recursion unwinds back to the alternative
path. As the recursion unwinds, RBFS replaces the f -value of each node along the path
with a backed-up valueâ€”the best f -value of its children. In this way, RBFS remembers the
f -value of the best leaf in the forgotten subtree and can therefore decide whether itâ€™s worth

100

Chapter
(a) After expanding Arad, Sibiu,
and Rimnicu Vilcea
447
Sibiu

3.

âˆž
Arad

Solving Problems by Searching

366

Timisoara
447

393

Zerind
449

415
Arad
646

Fagaras
415

Oradea

Rimnicu Vilcea

671

413

Craiova
526

Pitesti

(b) After unwinding back to Sibiu
and expanding Fagaras
447
Sibiu

Arad
646

417
Fagaras
415

Sibiu
591

Sibiu
553

417
âˆž
Arad

366

Timisoara
447

393

Oradea

Zerind

449

Rimnicu Vilcea

671

413 417

Bucharest
450

(c) After switching back to Rimnicu Vilcea
and expanding Pitesti
447
Sibiu

âˆž
Arad

366

Timisoara

393

Zerind
447

449

447
Arad
646

Fagaras
415 450

Rimnicu Vilcea

Oradea
671

417
447
Pitesti

Craiova

Bucharest
418

Sibiu
417

526

553
Rimnicu Vilcea

Craiova
615

607

Figure 3.27 Stages in an RBFS search for the shortest route to Bucharest. The f -limit
value for each recursive call is shown on top of each current node, and every node is labeled
with its f -cost. (a) The path via Rimnicu Vilcea is followed until the current best leaf (Pitesti)
has a value that is worse than the best alternative path (Fagaras). (b) The recursion unwinds
and the best leaf value of the forgotten subtree (417) is backed up to Rimnicu Vilcea; then
Fagaras is expanded, revealing a best leaf value of 450. (c) The recursion unwinds and the
best leaf value of the forgotten subtree (450) is backed up to Fagaras; then Rimnicu Vilcea is
expanded. This time, because the best alternative path (through Timisoara) costs at least 447,
the expansion continues to Bucharest.

reexpanding the subtree at some later time. Figure 3.27 shows how RBFS reaches Bucharest.
RBFS is somewhat more efficient than IDAâˆ— , but still suffers from excessive node regeneration. In the example in Figure 3.27, RBFS follows the path via Rimnicu Vilcea, then

Section 3.5.

MA*
SMA*

Informed (Heuristic) Search Strategies

101

â€œchanges its mindâ€ and tries Fagaras, and then changes its mind back again. These mind
changes occur because every time the current best path is extended, its f -value is likely to
increaseâ€”h is usually less optimistic for nodes closer to the goal. When this happens, the
second-best path might become the best path, so the search has to backtrack to follow it.
Each mind change corresponds to an iteration of IDAâˆ— and could require many reexpansions
of forgotten nodes to recreate the best path and extend it one more node.
Like Aâˆ— tree search, RBFS is an optimal algorithm if the heuristic function h(n) is
admissible. Its space complexity is linear in the depth of the deepest optimal solution, but
its time complexity is rather difficult to characterize: it depends both on the accuracy of the
heuristic function and on how often the best path changes as nodes are expanded.
IDAâˆ— and RBFS suffer from using too little memory. Between iterations, IDAâˆ— retains
only a single number: the current f -cost limit. RBFS retains more information in memory,
but it uses only linear space: even if more memory were available, RBFS has no way to make
use of it. Because they forget most of what they have done, both algorithms may end up reexpanding the same states many times over. Furthermore, they suffer the potentially exponential
increase in complexity associated with redundant paths in graphs (see Section 3.3).
It seems sensible, therefore, to use all available memory. Two algorithms that do this
are MAâˆ— (memory-bounded Aâˆ— ) and SMAâˆ— (simplified MAâˆ— ). SMAâˆ— isâ€”wellâ€”simpler, so
we will describe it. SMAâˆ— proceeds just like Aâˆ— , expanding the best leaf until memory is full.
At this point, it cannot add a new node to the search tree without dropping an old one. SMAâˆ—
always drops the worst leaf nodeâ€”the one with the highest f -value. Like RBFS, SMAâˆ—
then backs up the value of the forgotten node to its parent. In this way, the ancestor of a
forgotten subtree knows the quality of the best path in that subtree. With this information,
SMAâˆ— regenerates the subtree only when all other paths have been shown to look worse than
the path it has forgotten. Another way of saying this is that, if all the descendants of a node n
are forgotten, then we will not know which way to go from n, but we will still have an idea
of how worthwhile it is to go anywhere from n.
The complete algorithm is too complicated to reproduce here,10 but there is one subtlety
worth mentioning. We said that SMAâˆ— expands the best leaf and deletes the worst leaf. What
if all the leaf nodes have the same f -value? To avoid selecting the same node for deletion
and expansion, SMAâˆ— expands the newest best leaf and deletes the oldest worst leaf. These
coincide when there is only one leaf, but in that case, the current search tree must be a single
path from root to leaf that fills all of memory. If the leaf is not a goal node, then even if it is on
an optimal solution path, that solution is not reachable with the available memory. Therefore,
the node can be discarded exactly as if it had no successors.
SMAâˆ— is complete if there is any reachable solutionâ€”that is, if d, the depth of the
shallowest goal node, is less than the memory size (expressed in nodes). It is optimal if any
optimal solution is reachable; otherwise, it returns the best reachable solution. In practical
terms, SMAâˆ— is a fairly robust choice for finding optimal solutions, particularly when the state
space is a graph, step costs are not uniform, and node generation is expensive compared to
the overhead of maintaining the frontier and the explored set.
10

A rough sketch appeared in the first edition of this book.

102

Chapter

3.

Solving Problems by Searching

On very hard problems, however, it will often be the case that SMAâˆ— is forced to switch
back and forth continually among many candidate solution paths, only a small subset of which
can fit in memory. (This resembles the problem of thrashing in disk paging systems.) Then
the extra time required for repeated regeneration of the same nodes means that problems
that would be practically solvable by Aâˆ— , given unlimited memory, become intractable for
SMAâˆ— . That is to say, memory limitations can make a problem intractable from the point
of view of computation time. Although no current theory explains the tradeoff between time
and memory, it seems that this is an inescapable problem. The only way out is to drop the
optimality requirement.

THRASHING

3.5.4 Learning to search better

METALEVEL STATE
SPACE
OBJECT-LEVEL STATE
SPACE

METALEVEL
LEARNING

3.6

We have presented several fixed strategiesâ€”breadth-first, greedy best-first, and so onâ€”that
have been designed by computer scientists. Could an agent learn how to search better? The
answer is yes, and the method rests on an important concept called the metalevel state space.
Each state in a metalevel state space captures the internal (computational) state of a program
that is searching in an object-level state space such as Romania. For example, the internal
state of the Aâˆ— algorithm consists of the current search tree. Each action in the metalevel state
space is a computation step that alters the internal state; for example, each computation step
in Aâˆ— expands a leaf node and adds its successors to the tree. Thus, Figure 3.24, which shows
a sequence of larger and larger search trees, can be seen as depicting a path in the metalevel
state space where each state on the path is an object-level search tree.
Now, the path in Figure 3.24 has five steps, including one step, the expansion of Fagaras,
that is not especially helpful. For harder problems, there will be many such missteps, and a
metalevel learning algorithm can learn from these experiences to avoid exploring unpromising subtrees. The techniques used for this kind of learning are described in Chapter 21. The
goal of learning is to minimize the total cost of problem solving, trading off computational
expense and path cost.

H EURISTIC F UNCTIONS
In this section, we look at heuristics for the 8-puzzle, in order to shed light on the nature of
heuristics in general.
The 8-puzzle was one of the earliest heuristic search problems. As mentioned in Section 3.2, the object of the puzzle is to slide the tiles horizontally or vertically into the empty
space until the configuration matches the goal configuration (Figure 3.28).
The average solution cost for a randomly generated 8-puzzle instance is about 22 steps.
The branching factor is about 3. (When the empty tile is in the middle, four moves are
possible; when it is in a corner, two; and when it is along an edge, three.) This means
that an exhaustive tree search to depth 22 would look at about 322 â‰ˆ 3.1 Ã— 1010 states.
A graph search would cut this down by a factor of about 170,000 because only 9!/2 =
181, 440 distinct states are reachable. (See Exercise 3.4.) This is a manageable number, but

Section 3.6.

Heuristic Functions

103

7

2

5
8

3

4

2

6

3

4

5

1

6

7

8

Start State
Figure 3.28

1

Goal State

A typical instance of the 8-puzzle. The solution is 26 steps long.

the corresponding number for the 15-puzzle is roughly 1013 , so the next order of business is
to find a good heuristic function. If we want to find the shortest solutions by using Aâˆ— , we
need a heuristic function that never overestimates the number of steps to the goal. There is a
long history of such heuristics for the 15-puzzle; here are two commonly used candidates:

MANHATTAN
DISTANCE

â€¢ h1 = the number of misplaced tiles. For Figure 3.28, all of the eight tiles are out of
position, so the start state would have h1 = 8. h1 is an admissible heuristic because it
is clear that any tile that is out of place must be moved at least once.
â€¢ h2 = the sum of the distances of the tiles from their goal positions. Because tiles
cannot move along diagonals, the distance we will count is the sum of the horizontal
and vertical distances. This is sometimes called the city block distance or Manhattan
distance. h2 is also admissible because all any move can do is move one tile one step
closer to the goal. Tiles 1 to 8 in the start state give a Manhattan distance of
h2 = 3 + 1 + 2 + 2 + 2 + 3 + 3 + 2 = 18 .
As expected, neither of these overestimates the true solution cost, which is 26.

3.6.1 The effect of heuristic accuracy on performance
EFFECTIVE
BRANCHING FACTOR

One way to characterize the quality of a heuristic is the effective branching factor bâˆ— . If the
total number of nodes generated by Aâˆ— for a particular problem is N and the solution depth is
d, then bâˆ— is the branching factor that a uniform tree of depth d would have to have in order
to contain N + 1 nodes. Thus,
N + 1 = 1 + bâˆ— + (bâˆ— )2 + Â· Â· Â· + (bâˆ— )d .
For example, if Aâˆ— finds a solution at depth 5 using 52 nodes, then the effective branching
factor is 1.92. The effective branching factor can vary across problem instances, but usually
it is fairly constant for sufficiently hard problems. (The existence of an effective branching
factor follows from the result, mentioned earlier, that the number of nodes expanded by Aâˆ—
grows exponentially with solution depth.) Therefore, experimental measurements of bâˆ— on a
small set of problems can provide a good guide to the heuristicâ€™s overall usefulness. A welldesigned heuristic would have a value of bâˆ— close to 1, allowing fairly large problems to be
solved at reasonable computational cost.

104

Chapter

3.

Solving Problems by Searching

To test the heuristic functions h1 and h2 , we generated 1200 random problems with
solution lengths from 2 to 24 (100 for each even number) and solved them with iterative
deepening search and with Aâˆ— tree search using both h1 and h2 . Figure 3.29 gives the average
number of nodes generated by each strategy and the effective branching factor. The results
suggest that h2 is better than h1 , and is far better than using iterative deepening search. Even
for small problems with d = 12, Aâˆ— with h2 is 50,000 times more efficient than uninformed
iterative deepening search.
Search Cost (nodes generated)

Effective Branching Factor

d

IDS

Aâˆ— (h1 )

Aâˆ— (h2 )

IDS

Aâˆ— (h1 )

Aâˆ— (h2 )

2
4
6
8
10
12
14
16
18
20
22
24

10
112
680
6384
47127
3644035
â€“
â€“
â€“
â€“
â€“
â€“

6
13
20
39
93
227
539
1301
3056
7276
18094
39135

6
12
18
25
39
73
113
211
363
676
1219
1641

2.45
2.87
2.73
2.80
2.79
2.78
â€“
â€“
â€“
â€“
â€“
â€“

1.79
1.48
1.34
1.33
1.38
1.42
1.44
1.45
1.46
1.47
1.48
1.48

1.79
1.45
1.30
1.24
1.22
1.24
1.23
1.25
1.26
1.27
1.28
1.26

Figure 3.29 Comparison of the search costs and effective branching factors for the
I TERATIVE -D EEPENING -S EARCH and Aâˆ— algorithms with h1 , h2 . Data are averaged over
100 instances of the 8-puzzle for each of various solution lengths d.

DOMINATION

One might ask whether h2 is always better than h1 . The answer is â€œEssentially, yes.â€ It
is easy to see from the definitions of the two heuristics that, for any node n, h2 (n) â‰¥ h1 (n).
We thus say that h2 dominates h1 . Domination translates directly into efficiency: Aâˆ— using
h2 will never expand more nodes than Aâˆ— using h1 (except possibly for some nodes with
f (n) = C âˆ— ). The argument is simple. Recall the observation on page 97 that every node
with f (n) < C âˆ— will surely be expanded. This is the same as saying that every node with
h(n) < C âˆ— âˆ’ g(n) will surely be expanded. But because h2 is at least as big as h1 for all
nodes, every node that is surely expanded by Aâˆ— search with h2 will also surely be expanded
with h1 , and h1 might cause other nodes to be expanded as well. Hence, it is generally
better to use a heuristic function with higher values, provided it is consistent and that the
computation time for the heuristic is not too long.

3.6.2 Generating admissible heuristics from relaxed problems
We have seen that both h1 (misplaced tiles) and h2 (Manhattan distance) are fairly good
heuristics for the 8-puzzle and that h2 is better. How might one have come up with h2 ? Is it
possible for a computer to invent such a heuristic mechanically?
h1 and h2 are estimates of the remaining path length for the 8-puzzle, but they are also
perfectly accurate path lengths for simplified versions of the puzzle. If the rules of the puzzle

Section 3.6.

RELAXED PROBLEM

Heuristic Functions

105

were changed so that a tile could move anywhere instead of just to the adjacent empty square,
then h1 would give the exact number of steps in the shortest solution. Similarly, if a tile could
move one square in any direction, even onto an occupied square, then h2 would give the exact
number of steps in the shortest solution. A problem with fewer restrictions on the actions is
called a relaxed problem. The state-space graph of the relaxed problem is a supergraph of
the original state space because the removal of restrictions creates added edges in the graph.
Because the relaxed problem adds edges to the state space, any optimal solution in the
original problem is, by definition, also a solution in the relaxed problem; but the relaxed
problem may have better solutions if the added edges provide short cuts. Hence, the cost of
an optimal solution to a relaxed problem is an admissible heuristic for the original problem.
Furthermore, because the derived heuristic is an exact cost for the relaxed problem, it must
obey the triangle inequality and is therefore consistent (see page 95).
If a problem definition is written down in a formal language, it is possible to construct
relaxed problems automatically.11 For example, if the 8-puzzle actions are described as
A tile can move from square A to square B if
A is horizontally or vertically adjacent to B and B is blank,
we can generate three relaxed problems by removing one or both of the conditions:
(a) A tile can move from square A to square B if A is adjacent to B.
(b) A tile can move from square A to square B if B is blank.
(c) A tile can move from square A to square B.
From (a), we can derive h2 (Manhattan distance). The reasoning is that h2 would be the
proper score if we moved each tile in turn to its destination. The heuristic derived from (b) is
discussed in Exercise 3.31. From (c), we can derive h1 (misplaced tiles) because it would be
the proper score if tiles could move to their intended destination in one step. Notice that it is
crucial that the relaxed problems generated by this technique can be solved essentially without
search, because the relaxed rules allow the problem to be decomposed into eight independent
subproblems. If the relaxed problem is hard to solve, then the values of the corresponding
heuristic will be expensive to obtain.12
A program called A BSOLVER can generate heuristics automatically from problem definitions, using the â€œrelaxed problemâ€ method and various other techniques (Prieditis, 1993).
A BSOLVER generated a new heuristic for the 8-puzzle that was better than any preexisting
heuristic and found the first useful heuristic for the famous Rubikâ€™s Cube puzzle.
One problem with generating new heuristic functions is that one often fails to get a
single â€œclearly bestâ€ heuristic. If a collection of admissible heuristics h1 . . . hm is available
for a problem and none of them dominates any of the others, which should we choose? As it
turns out, we need not make a choice. We can have the best of all worlds, by defining
h(n) = max{h1 (n), . . . , hm (n)} .
11 In Chapters 8 and 10, we describe formal languages suitable for this task; with formal descriptions that can be
manipulated, the construction of relaxed problems can be automated. For now, we use English.
12 Note that a perfect heuristic can be obtained simply by allowing h to run a full breadth-first search â€œon the
sly.â€ Thus, there is a tradeoff between accuracy and computation time for heuristic functions.

106

Chapter

2
5
8

3
Start State

3.

4

Solving Problems by Searching

1

2
6

6

3

54

1

7

8
Goal State

Figure 3.30 A subproblem of the 8-puzzle instance given in Figure 3.28. The task is to
get tiles 1, 2, 3, and 4 into their correct positions, without worrying about what happens to
the other tiles.

This composite heuristic uses whichever function is most accurate on the node in question.
Because the component heuristics are admissible, h is admissible; it is also easy to prove that
h is consistent. Furthermore, h dominates all of its component heuristics.

3.6.3 Generating admissible heuristics from subproblems: Pattern databases
SUBPROBLEM

PATTERN DATABASE

Admissible heuristics can also be derived from the solution cost of a subproblem of a given
problem. For example, Figure 3.30 shows a subproblem of the 8-puzzle instance in Figure 3.28. The subproblem involves getting tiles 1, 2, 3, 4 into their correct positions. Clearly,
the cost of the optimal solution of this subproblem is a lower bound on the cost of the complete problem. It turns out to be more accurate than Manhattan distance in some cases.
The idea behind pattern databases is to store these exact solution costs for every possible subproblem instanceâ€”in our example, every possible configuration of the four tiles
and the blank. (The locations of the other four tiles are irrelevant for the purposes of solving the subproblem, but moves of those tiles do count toward the cost.) Then we compute
an admissible heuristic hDB for each complete state encountered during a search simply by
looking up the corresponding subproblem configuration in the database. The database itself is
constructed by searching back13 from the goal and recording the cost of each new pattern encountered; the expense of this search is amortized over many subsequent problem instances.
The choice of 1-2-3-4 is fairly arbitrary; we could also construct databases for 5-6-7-8,
for 2-4-6-8, and so on. Each database yields an admissible heuristic, and these heuristics can
be combined, as explained earlier, by taking the maximum value. A combined heuristic of
this kind is much more accurate than the Manhattan distance; the number of nodes generated
when solving random 15-puzzles can be reduced by a factor of 1000.
One might wonder whether the heuristics obtained from the 1-2-3-4 database and the
5-6-7-8 could be added, since the two subproblems seem not to overlap. Would this still give
an admissible heuristic? The answer is no, because the solutions of the 1-2-3-4 subproblem
and the 5-6-7-8 subproblem for a given state will almost certainly share some movesâ€”it is
13

By working backward from the goal, the exact solution cost of every instance encountered is immediately
available. This is an example of dynamic programming, which we discuss further in Chapter 17.

Section 3.6.

DISJOINT PATTERN
DATABASES

Heuristic Functions

107

unlikely that 1-2-3-4 can be moved into place without touching 5-6-7-8, and vice versa. But
what if we donâ€™t count those moves? That is, we record not the total cost of solving the 1-23-4 subproblem, but just the number of moves involving 1-2-3-4. Then it is easy to see that
the sum of the two costs is still a lower bound on the cost of solving the entire problem. This
is the idea behind disjoint pattern databases. With such databases, it is possible to solve
random 15-puzzles in a few millisecondsâ€”the number of nodes generated is reduced by a
factor of 10,000 compared with the use of Manhattan distance. For 24-puzzles, a speedup of
roughly a factor of a million can be obtained.
Disjoint pattern databases work for sliding-tile puzzles because the problem can be
divided up in such a way that each move affects only one subproblemâ€”because only one tile
is moved at a time. For a problem such as Rubikâ€™s Cube, this kind of subdivision is difficult
because each move affects 8 or 9 of the 26 cubies. More general ways of defining additive,
admissible heuristics have been proposed that do apply to Rubikâ€™s cube (Yang et al., 2008),
but they have not yielded a heuristic better than the best nonadditive heuristic for the problem.

3.6.4 Learning heuristics from experience

FEATURE

A heuristic function h(n) is supposed to estimate the cost of a solution beginning from the
state at node n. How could an agent construct such a function? One solution was given in
the preceding sectionsâ€”namely, to devise relaxed problems for which an optimal solution
can be found easily. Another solution is to learn from experience. â€œExperienceâ€ here means
solving lots of 8-puzzles, for instance. Each optimal solution to an 8-puzzle problem provides
examples from which h(n) can be learned. Each example consists of a state from the solution path and the actual cost of the solution from that point. From these examples, a learning
algorithm can be used to construct a function h(n) that can (with luck) predict solution costs
for other states that arise during search. Techniques for doing just this using neural nets, decision trees, and other methods are demonstrated in Chapter 18. (The reinforcement learning
methods described in Chapter 21 are also applicable.)
Inductive learning methods work best when supplied with features of a state that are
relevant to predicting the stateâ€™s value, rather than with just the raw state description. For
example, the feature â€œnumber of misplaced tilesâ€ might be helpful in predicting the actual
distance of a state from the goal. Letâ€™s call this feature x1 (n). We could take 100 randomly
generated 8-puzzle configurations and gather statistics on their actual solution costs. We
might find that when x1 (n) is 5, the average solution cost is around 14, and so on. Given
these data, the value of x1 can be used to predict h(n). Of course, we can use several features.
A second feature x2 (n) might be â€œnumber of pairs of adjacent tiles that are not adjacent in the
goal state.â€ How should x1 (n) and x2 (n) be combined to predict h(n)? A common approach
is to use a linear combination:
h(n) = c1 x1 (n) + c2 x2 (n) .
The constants c1 and c2 are adjusted to give the best fit to the actual data on solution costs.
One expects both c1 and c2 to be positive because misplaced tiles and incorrect adjacent pairs
make the problem harder to solve. Notice that this heuristic does satisfy the condition that
h(n) = 0 for goal states, but it is not necessarily admissible or consistent.

108

3.7

Chapter

3.

Solving Problems by Searching

S UMMARY
This chapter has introduced methods that an agent can use to select actions in environments
that are deterministic, observable, static, and completely known. In such cases, the agent can
construct sequences of actions that achieve its goals; this process is called search.
â€¢ Before an agent can start searching for solutions, a goal must be identified and a welldefined problem must be formulated.
â€¢ A problem consists of five parts: the initial state, a set of actions, a transition model
describing the results of those actions, a goal test function, and a path cost function.
The environment of the problem is represented by a state space. A path through the
state space from the initial state to a goal state is a solution.
â€¢ Search algorithms treat states and actions as atomic: they do not consider any internal
structure they might possess.
â€¢ A general T REE -S EARCH algorithm considers all possible paths to find a solution,
whereas a G RAPH -S EARCH algorithm avoids consideration of redundant paths.
â€¢ Search algorithms are judged on the basis of completeness, optimality, time complexity, and space complexity. Complexity depends on b, the branching factor in the state
space, and d, the depth of the shallowest solution.
â€¢ Uninformed search methods have access only to the problem definition. The basic
algorithms are as follows:
â€“ Breadth-first search expands the shallowest nodes first; it is complete, optimal
for unit step costs, but has exponential space complexity.
â€“ Uniform-cost search expands the node with lowest path cost, g(n), and is optimal
for general step costs.
â€“ Depth-first search expands the deepest unexpanded node first. It is neither complete nor optimal, but has linear space complexity. Depth-limited search adds a
depth bound.
â€“ Iterative deepening search calls depth-first search with increasing depth limits
until a goal is found. It is complete, optimal for unit step costs, has time complexity
comparable to breadth-first search, and has linear space complexity.
â€“ Bidirectional search can enormously reduce time complexity, but it is not always
applicable and may require too much space.
â€¢ Informed search methods may have access to a heuristic function h(n) that estimates
the cost of a solution from n.
â€“ The generic best-first search algorithm selects a node for expansion according to
an evaluation function.
â€“ Greedy best-first search expands nodes with minimal h(n). It is not optimal but
is often efficient.

Bibliographical and Historical Notes

109

â€“ Aâˆ— search expands nodes with minimal f (n) = g(n) + h(n). Aâˆ— is complete and
optimal, provided that h(n) is admissible (for T REE -S EARCH ) or consistent (for
G RAPH -S EARCH ). The space complexity of Aâˆ— is still prohibitive.
â€“ RBFS (recursive best-first search) and SMAâˆ— (simplified memory-bounded Aâˆ— )
are robust, optimal search algorithms that use limited amounts of memory; given
enough time, they can solve problems that Aâˆ— cannot solve because it runs out of
memory.
â€¢ The performance of heuristic search algorithms depends on the quality of the heuristic
function. One can sometimes construct good heuristics by relaxing the problem definition, by storing precomputed solution costs for subproblems in a pattern database, or
by learning from experience with the problem class.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

The topic of state-space search originated in more or less its current form in the early years of
AI. Newell and Simonâ€™s work on the Logic Theorist (1957) and GPS (1961) led to the establishment of search algorithms as the primary weapons in the armory of 1960s AI researchers
and to the establishment of problem solving as the canonical AI task. Work in operations
research by Richard Bellman (1957) showed the importance of additive path costs in simplifying optimization algorithms. The text on Automated Problem Solving by Nils Nilsson
(1971) established the area on a solid theoretical footing.
Most of the state-space search problems analyzed in this chapter have a long history
in the literature and are less trivial than they might seem. The missionaries and cannibals
problem used in Exercise 3.9 was analyzed in detail by Amarel (1968). It had been considered earlierâ€”in AI by Simon and Newell (1961) and in operations research by Bellman and
Dreyfus (1962).
The 8-puzzle is a smaller cousin of the 15-puzzle, whose history is recounted at length
by Slocum and Sonneveld (2006). It was widely believed to have been invented by the famous American game designer Sam Loyd, based on his claims to that effect from 1891 onward (Loyd, 1959). Actually it was invented by Noyes Chapman, a postmaster in Canastota,
New York, in the mid-1870s. (Chapman was unable to patent his invention, as a generic
patent covering sliding blocks with letters, numbers, or pictures was granted to Ernest Kinsey
in 1878.) It quickly attracted the attention of the public and of mathematicians (Johnson and
Story, 1879; Tait, 1880). The editors of the American Journal of Mathematics stated, â€œThe
â€˜15â€™ puzzle for the last few weeks has been prominently before the American public, and may
safely be said to have engaged the attention of nine out of ten persons of both sexes and all
ages and conditions of the community.â€ Ratner and Warmuth (1986) showed that the general
n Ã— n version of the 15-puzzle belongs to the class of NP-complete problems.
The 8-queens problem was first published anonymously in the German chess magazine Schach in 1848; it was later attributed to one Max Bezzel. It was republished in 1850
and at that time drew the attention of the eminent mathematician Carl Friedrich Gauss, who

110

Chapter

3.

Solving Problems by Searching

attempted to enumerate all possible solutions; initially he found only 72, but eventually he
found the correct answer of 92, although Nauck published all 92 solutions first, in 1850.
Netto (1901) generalized the problem to n queens, and Abramson and Yung (1989) found an
O(n) algorithm.
Each of the real-world search problems listed in the chapter has been the subject of a
good deal of research effort. Methods for selecting optimal airline flights remain proprietary
for the most part, but Carl de Marcken (personal communication) has shown that airline ticket
pricing and restrictions have become so convoluted that the problem of selecting an optimal
flight is formally undecidable. The traveling-salesperson problem is a standard combinatorial problem in theoretical computer science (Lawler et al., 1992). Karp (1972) proved the
TSP to be NP-hard, but effective heuristic approximation methods were developed (Lin and
Kernighan, 1973). Arora (1998) devised a fully polynomial approximation scheme for Euclidean TSPs. VLSI layout methods are surveyed by Shahookar and Mazumder (1991), and
many layout optimization papers appear in VLSI journals. Robotic navigation and assembly
problems are discussed in Chapter 25.
Uninformed search algorithms for problem solving are a central topic of classical computer science (Horowitz and Sahni, 1978) and operations research (Dreyfus, 1969). Breadthfirst search was formulated for solving mazes by Moore (1959). The method of dynamic
programming (Bellman, 1957; Bellman and Dreyfus, 1962), which systematically records
solutions for all subproblems of increasing lengths, can be seen as a form of breadth-first
search on graphs. The two-point shortest-path algorithm of Dijkstra (1959) is the origin
of uniform-cost search. These works also introduced the idea of explored and frontier sets
(closed and open lists).
A version of iterative deepening designed to make efficient use of the chess clock was
first used by Slate and Atkin (1977) in the C HESS 4.5 game-playing program. Martelliâ€™s
algorithm B (1977) includes an iterative deepening aspect and also dominates Aâˆ— â€™s worst-case
performance with admissible but inconsistent heuristics. The iterative deepening technique
came to the fore in work by Korf (1985a). Bidirectional search, which was introduced by
Pohl (1971), can also be effective in some cases.
The use of heuristic information in problem solving appears in an early paper by Simon
and Newell (1958), but the phrase â€œheuristic searchâ€ and the use of heuristic functions that
estimate the distance to the goal came somewhat later (Newell and Ernst, 1965; Lin, 1965).
Doran and Michie (1966) conducted extensive experimental studies of heuristic search. Although they analyzed path length and â€œpenetranceâ€ (the ratio of path length to the total number of nodes examined so far), they appear to have ignored the information provided by the
path cost g(n). The Aâˆ— algorithm, incorporating the current path cost into heuristic search,
was developed by Hart, Nilsson, and Raphael (1968), with some later corrections (Hart et al.,
1972). Dechter and Pearl (1985) demonstrated the optimal efficiency of Aâˆ— .
The original Aâˆ— paper introduced the consistency condition on heuristic functions. The
monotone condition was introduced by Pohl (1977) as a simpler replacement, but Pearl (1984)
showed that the two were equivalent.
Pohl (1977) pioneered the study of the relationship between the error in heuristic functions and the time complexity of Aâˆ— . Basic results were obtained for tree search with unit step

Bibliographical and Historical Notes

ITERATIVE
EXPANSION

111

costs and a single goal node (Pohl, 1977; Gaschnig, 1979; Huyn et al., 1980; Pearl, 1984) and
with multiple goal nodes (Dinh et al., 2007). The â€œeffective branching factorâ€ was proposed
by Nilsson (1971) as an empirical measure of the efficiency; it is equivalent to assuming a
time cost of O((bâˆ— )d ). For tree search applied to a graph, Korf et al. (2001) argue that the time
cost is better modeled as O(bdâˆ’k ), where k depends on the heuristic accuracy; this analysis
has elicited some controversy, however. For graph search, Helmert and RoÌˆger (2008) noted
that several well-known problems contained exponentially many nodes on optimal solution
paths, implying exponential time complexity for Aâˆ— even with constant absolute error in h.
There are many variations on the Aâˆ— algorithm. Pohl (1973) proposed the use of dynamic
weighting, which uses a weighted sum fw (n) = wg g(n) + wh h(n) of the current path length
and the heuristic function as an evaluation function, rather than the simple sum f (n) = g(n)+
h(n) used in Aâˆ— . The weights wg and wh are adjusted dynamically as the search progresses.
Pohlâ€™s algorithm can be shown to be -admissibleâ€”that is, guaranteed to find solutions within
a factor 1 +  of the optimal solution, where  is a parameter supplied to the algorithm. The
same property is exhibited by the Aâˆ— algorithm (Pearl, 1984), which can select any node from
the frontier provided its f -cost is within a factor 1 +  of the lowest-f -cost frontier node. The
selection can be done so as to minimize search cost.
Bidirectional versions of Aâˆ— have been investigated; a combination of bidirectional Aâˆ—
and known landmarks was used to efficiently find driving routes for Microsoftâ€™s online map
service (Goldberg et al., 2006). After caching a set of paths between landmarks, the algorithm
can find an optimal path between any pair of points in a 24 million point graph of the United
States, searching less than 0.1% of the graph. Others approaches to bidirectional search
include a breadth-first search backward from the goal up to a fixed depth, followed by a
forward IDAâˆ— search (Dillenburg and Nelson, 1994; Manzini, 1995).
Aâˆ— and other state-space search algorithms are closely related to the branch-and-bound
techniques that are widely used in operations research (Lawler and Wood, 1966). The
relationships between state-space search and branch-and-bound have been investigated in
depth (Kumar and Kanal, 1983; Nau et al., 1984; Kumar et al., 1988). Martelli and Montanari (1978) demonstrate a connection between dynamic programming (see Chapter 17) and
certain types of state-space search. Kumar and Kanal (1988) attempt a â€œgrand unificationâ€ of
heuristic search, dynamic programming, and branch-and-bound techniques under the name
of CDPâ€”the â€œcomposite decision process.â€
Because computers in the late 1950s and early 1960s had at most a few thousand words
of main memory, memory-bounded heuristic search was an early research topic. The Graph
Traverser (Doran and Michie, 1966), one of the earliest search programs, commits to an
operator after searching best-first up to the memory limit. IDAâˆ— (Korf, 1985a, 1985b) was the
first widely used optimal, memory-bounded heuristic search algorithm, and a large number
of variants have been developed. An analysis of the efficiency of IDAâˆ— and of its difficulties
with real-valued heuristics appears in Patrick et al. (1992).
RBFS (Korf, 1993) is actually somewhat more complicated than the algorithm shown
in Figure 3.26, which is closer to an independently developed algorithm called iterative expansion (Russell, 1992). RBFS uses a lower bound as well as the upper bound; the two algorithms behave identically with admissible heuristics, but RBFS expands nodes in best-first

112

PARALLEL SEARCH

Chapter

3.

Solving Problems by Searching

order even with an inadmissible heuristic. The idea of keeping track of the best alternative
path appeared earlier in Bratkoâ€™s (1986) elegant Prolog implementation of Aâˆ— and in the DTAâˆ—
algorithm (Russell and Wefald, 1991). The latter work also discusses metalevel state spaces
and metalevel learning.
The MAâˆ— algorithm appeared in Chakrabarti et al. (1989). SMAâˆ— , or Simplified MAâˆ— ,
emerged from an attempt to implement MAâˆ— as a comparison algorithm for IE (Russell, 1992).
Kaindl and Khorsand (1994) have applied SMAâˆ— to produce a bidirectional search algorithm
that is substantially faster than previous algorithms. Korf and Zhang (2000) describe a divideand-conquer approach, and Zhou and Hansen (2002) introduce memory-bounded Aâˆ— graph
search and a strategy for switching to breadth-first search to increase memory-efficiency
(Zhou and Hansen, 2006). Korf (1995) surveys memory-bounded search techniques.
The idea that admissible heuristics can be derived by problem relaxation appears in the
seminal paper by Held and Karp (1970), who used the minimum-spanning-tree heuristic to
solve the TSP. (See Exercise 3.30.)
The automation of the relaxation process was implemented successfully by Prieditis (1993), building on earlier work with Mostow (Mostow and Prieditis, 1989). Holte and
Hernadvolgyi (2001) describe more recent steps towards automating the process. The use of
pattern databases to derive admissible heuristics is due to Gasser (1995) and Culberson and
Schaeffer (1996, 1998); disjoint pattern databases are described by Korf and Felner (2002);
a similar method using symbolic patterns is due to Edelkamp (2009). Felner et al. (2007)
show how to compress pattern databases to save space. The probabilistic interpretation of
heuristics was investigated in depth by Pearl (1984) and Hansson and Mayer (1989).
By far the most comprehensive source on heuristics and heuristic search algorithms
is Pearlâ€™s (1984) Heuristics text. This book provides especially good coverage of the wide
variety of offshoots and variations of Aâˆ— , including rigorous proofs of their formal properties.
Kanal and Kumar (1988) present an anthology of important articles on heuristic search, and
Rayward-Smith et al. (1996) cover approaches from Operations Research. Papers about new
search algorithmsâ€”which, remarkably, continue to be discoveredâ€”appear in journals such
as Artificial Intelligence and Journal of the ACM.
The topic of parallel search algorithms was not covered in the chapter, partly because
it requires a lengthy discussion of parallel computer architectures. Parallel search became a
popular topic in the 1990s in both AI and theoretical computer science (Mahanti and Daniels,
1993; Grama and Kumar, 1995; Crauser et al., 1998) and is making a comeback in the era
of new multicore and cluster architectures (Ralphs et al., 2004; Korf and Schultze, 2005).
Also of increasing importance are search algorithms for very large graphs that require disk
storage (Korf, 2008).

E XERCISES
3.1

Explain why problem formulation must follow goal formulation.

3.2

Your goal is to navigate a robot out of a maze. The robot starts in the center of the maze

Exercises

113
facing north. You can turn the robot to face north, east, south, or west. You can direct the
robot to move forward a certain distance, although it will stop before hitting a wall.
a. Formulate this problem. How large is the state space?
b. In navigating a maze, the only place we need to turn is at the intersection of two or
more corridors. Reformulate this problem using this observation. How large is the state
space now?
c. From each point in the maze, we can move in any of the four directions until we reach a
turning point, and this is the only action we need to do. Reformulate the problem using
these actions. Do we need to keep track of the robotâ€™s orientation now?
d. In our initial description of the problem we already abstracted from the real world,
restricting actions and removing details. List three such simplifications we made.
3.3 Suppose two friends live in different cities on a map, such as the Romania map shown
in Figure 3.2. On every turn, we can simultaneously move each friend to a neighboring city
on the map. The amount of time needed to move from city i to neighbor j is equal to the road
distance d(i, j) between the cities, but on each turn the friend that arrives first must wait until
the other one arrives (and calls the first on his/her cell phone) before the next turn can begin.
We want the two friends to meet as quickly as possible.
a. Write a detailed formulation for this search problem. (You will find it helpful to define
some formal notation here.)
b. Let D(i, j) be the straight-line distance between cities i and j. Which of the following
heuristic functions are admissible? (i) D(i, j); (ii) 2 Â· D(i, j); (iii) D(i, j)/2.
c. Are there completely connected maps for which no solution exists?
d. Are there maps in which all solutions require one friend to visit the same city twice?
3.4 Show that the 8-puzzle states are divided into two disjoint sets, such that any state is
reachable from any other state in the same set, while no state is reachable from any state in
the other set. (Hint: See Berlekamp et al. (1982).) Devise a procedure to decide which set a
given state is in, and explain why this is useful for generating random states.
3.5 Consider the n-queens problem using the â€œefficientâ€
incremental formulation given on
âˆš
page 72. Explain why the state space has at least 3 n! states and estimate the largest n for
which exhaustive exploration is feasible. (Hint: Derive a lower bound on the branching factor
by considering the maximum number of squares that a queen can attack in any column.)
3.6 Give a complete problem formulation for each of the following. Choose a formulation
that is precise enough to be implemented.
a. Using only four colors, you have to color a planar map in such a way that no two
adjacent regions have the same color.
b. A 3-foot-tall monkey is in a room where some bananas are suspended from the 8-foot
ceiling. He would like to get the bananas. The room contains two stackable, movable,
climbable 3-foot-high crates.

114

Chapter

3.

Solving Problems by Searching

G

S
Figure 3.31

A scene with polygonal obstacles. S and G are the start and goal states.

c. You have a program that outputs the message â€œillegal input recordâ€ when fed a certain
file of input records. You know that processing of each record is independent of the
other records. You want to discover what record is illegal.
d. You have three jugs, measuring 12 gallons, 8 gallons, and 3 gallons, and a water faucet.
You can fill the jugs up or empty them out from one to another or onto the ground. You
need to measure out exactly one gallon.
3.7 Consider the problem of finding the shortest path between two points on a plane that has
convex polygonal obstacles as shown in Figure 3.31. This is an idealization of the problem
that a robot has to solve to navigate in a crowded environment.
a. Suppose the state space consists of all positions (x, y) in the plane. How many states
are there? How many paths are there to the goal?
b. Explain briefly why the shortest path from one polygon vertex to any other in the scene
must consist of straight-line segments joining some of the vertices of the polygons.
Define a good state space now. How large is this state space?
c. Define the necessary functions to implement the search problem, including an ACTIONS
function that takes a vertex as input and returns a set of vectors, each of which maps the
current vertex to one of the vertices that can be reached in a straight line. (Do not forget
the neighbors on the same polygon.) Use the straight-line distance for the heuristic
function.
d. Apply one or more of the algorithms in this chapter to solve a range of problems in the
domain, and comment on their performance.
3.8 On page 68, we said that we would not consider problems with negative path costs. In
this exercise, we explore this decision in more depth.
a. Suppose that actions can have arbitrarily large negative costs; explain why this possibility would force any optimal algorithm to explore the entire state space.

Exercises

115
b. Does it help if we insist that step costs must be greater than or equal to some negative
constant c? Consider both trees and graphs.
c. Suppose that a set of actions forms a loop in the state space such that executing the set in
some order results in no net change to the state. If all of these actions have negative cost,
what does this imply about the optimal behavior for an agent in such an environment?
d. One can easily imagine actions with high negative cost, even in domains such as route
finding. For example, some stretches of road might have such beautiful scenery as to
far outweigh the normal costs in terms of time and fuel. Explain, in precise terms,
within the context of state-space search, why humans do not drive around scenic loops
indefinitely, and explain how to define the state space and actions for route finding so
that artificial agents can also avoid looping.
e. Can you think of a real domain in which step costs are such as to cause looping?
3.9 The missionaries and cannibals problem is usually stated as follows. Three missionaries and three cannibals are on one side of a river, along with a boat that can hold one or
two people. Find a way to get everyone to the other side without ever leaving a group of missionaries in one place outnumbered by the cannibals in that place. This problem is famous in
AI because it was the subject of the first paper that approached problem formulation from an
analytical viewpoint (Amarel, 1968).
a. Formulate the problem precisely, making only those distinctions necessary to ensure a
valid solution. Draw a diagram of the complete state space.
b. Implement and solve the problem optimally using an appropriate search algorithm. Is it
a good idea to check for repeated states?
c. Why do you think people have a hard time solving this puzzle, given that the state space
is so simple?
3.10 Define in your own words the following terms: state, state space, search tree, search
node, goal, action, transition model, and branching factor.
3.11 Whatâ€™s the difference between a world state, a state description, and a search node?
Why is this distinction useful?
3.12 An action such as Go(Sibiu) really consists of a long sequence of finer-grained actions:
turn on the car, release the brake, accelerate forward, etc. Having composite actions of this
kind reduces the number of steps in a solution sequence, thereby reducing the search time.
Suppose we take this to the logical extreme, by making super-composite actions out of every
possible sequence of Go actions. Then every problem instance is solved by a single supercomposite action, such as Go(Sibiu)Go(Rimnicu Vilcea)Go(Pitesti)Go(Bucharest). Explain
how search would work in this formulation. Is this a practical approach for speeding up
problem solving?
3.13 Prove that G RAPH -S EARCH satisfies the graph separation property illustrated in Figure 3.9. (Hint: Begin by showing that the property holds at the start, then show that if it holds
before an iteration of the algorithm, it holds afterwards.) Describe a search algorithm that
violates the property.

116

Chapter

x 12

x2

3.

Solving Problems by Searching

x2

x 16
Figure 3.32 The track pieces in a wooden railway set; each is labeled with the number of
copies in the set. Note that curved pieces and â€œforkâ€ pieces (â€œswitchesâ€ or â€œpointsâ€) can be
flipped over so they can curve in either direction. Each curve subtends 45 degrees.

3.14

Which of the following are true and which are false? Explain your answers.

a. Depth-first search always expands at least as many nodes as Aâˆ— search with an admissible heuristic.
b. h(n) = 0 is an admissible heuristic for the 8-puzzle.
c. Aâˆ— is of no use in robotics because percepts, states, and actions are continuous.
d. Breadth-first search is complete even if zero step costs are allowed.
e. Assume that a rook can move on a chessboard any number of squares in a straight line,
vertically or horizontally, but cannot jump over other pieces. Manhattan distance is an
admissible heuristic for the problem of moving the rook from square A to square B in
the smallest number of moves.
3.15 Consider a state space where the start state is number 1 and each state k has two
successors: numbers 2k and 2k + 1.
a. Draw the portion of the state space for states 1 to 15.
b. Suppose the goal state is 11. List the order in which nodes will be visited for breadthfirst search, depth-limited search with limit 3, and iterative deepening search.
c. How well would bidirectional search work on this problem? What is the branching
factor in each direction of the bidirectional search?
d. Does the answer to (c) suggest a reformulation of the problem that would allow you to
solve the problem of getting from state 1 to a given goal state with almost no search?
e. Call the action going from k to 2k Left, and the action going to 2k + 1 Right. Can you
find an algorithm that outputs the solution to this problem without any search at all?
3.16 A basic wooden railway set contains the pieces shown in Figure 3.32. The task is to
connect these pieces into a railway that has no overlapping tracks and no loose ends where a
train could run off onto the floor.
a. Suppose that the pieces fit together exactly with no slack. Give a precise formulation of
the task as a search problem.
b. Identify a suitable uninformed search algorithm for this task and explain your choice.
c. Explain why removing any one of the â€œforkâ€ pieces makes the problem unsolvable.

Exercises

117
d. Give an upper bound on the total size of the state space defined by your formulation.
(Hint: think about the maximum branching factor for the construction process and the
maximum depth, ignoring the problem of overlapping pieces and loose ends. Begin by
pretending that every piece is unique.)
3.17 On page 90, we mentioned iterative lengthening search, an iterative analog of uniform cost search. The idea is to use increasing limits on path cost. If a node is generated
whose path cost exceeds the current limit, it is immediately discarded. For each new iteration, the limit is set to the lowest path cost of any node discarded in the previous iteration.
a. Show that this algorithm is optimal for general path costs.
b. Consider a uniform tree with branching factor b, solution depth d, and unit step costs.
How many iterations will iterative lengthening require?
c. Now consider step costs drawn from the continuous range [, 1], where 0 <  < 1. How
many iterations are required in the worst case?
d. Implement the algorithm and apply it to instances of the 8-puzzle and traveling salesperson problems. Compare the algorithmâ€™s performance to that of uniform-cost search,
and comment on your results.
3.18 Describe a state space in which iterative deepening search performs much worse than
depth-first search (for example, O(n2 ) vs. O(n)).
3.19 Write a program that will take as input two Web page URLs and find a path of links
from one to the other. What is an appropriate search strategy? Is bidirectional search a good
idea? Could a search engine be used to implement a predecessor function?
3.20

Consider the vacuum-world problem defined in Figure 2.2.

a. Which of the algorithms defined in this chapter would be appropriate for this problem?
Should the algorithm use tree search or graph search?
b. Apply your chosen algorithm to compute an optimal sequence of actions for a 3 Ã— 3
world whose initial state has dirt in the three top squares and the agent in the center.
c. Construct a search agent for the vacuum world, and evaluate its performance in a set of
3 Ã— 3 worlds with probability 0.2 of dirt in each square. Include the search cost as well
as path cost in the performance measure, using a reasonable exchange rate.
d. Compare your best search agent with a simple randomized reflex agent that sucks if
there is dirt and otherwise moves randomly.
e. Consider what would happen if the world were enlarged to n Ã— n. How does the performance of the search agent and of the reflex agent vary with n?
3.21

Prove each of the following statements, or give a counterexample:

a. Breadth-first search is a special case of uniform-cost search.
b. Depth-first search is a special case of best-first tree search.
c. Uniform-cost search is a special case of Aâˆ— search.

118

Chapter

3.

Solving Problems by Searching

3.22 Compare the performance of Aâˆ— and RBFS on a set of randomly generated problems
in the 8-puzzle (with Manhattan distance) and TSP (with MSTâ€”see Exercise 3.30) domains.
Discuss your results. What happens to the performance of RBFS when a small random number is added to the heuristic values in the 8-puzzle domain?
3.23 Trace the operation of Aâˆ— search applied to the problem of getting to Bucharest from
Lugoj using the straight-line distance heuristic. That is, show the sequence of nodes that the
algorithm will consider and the f , g, and h score for each node.
3.24 Devise a state space in which Aâˆ— using G RAPH -S EARCH returns a suboptimal solution
with an h(n) function that is admissible but inconsistent.
HEURISTIC PATH
ALGORITHM

3.25 The heuristic path algorithm (Pohl, 1977) is a best-first search in which the evaluation function is f (n) = (2 âˆ’ w)g(n) + wh(n). For what values of w is this complete?
For what values is it optimal, assuming that h is admissible? What kind of search does this
perform for w = 0, w = 1, and w = 2?
3.26 Consider the unbounded version of the regular 2D grid shown in Figure 3.9. The start
state is at the origin, (0,0), and the goal state is at (x, y).
a.
b.
c.
d.
e.
f.
g.
h.

What is the branching factor b in this state space?
How many distinct states are there at depth k (for k > 0)?
What is the maximum number of nodes expanded by breadth-first tree search?
What is the maximum number of nodes expanded by breadth-first graph search?
Is h = |u âˆ’ x| + |v âˆ’ y| an admissible heuristic for a state at (u, v)? Explain.
How many nodes are expanded by Aâˆ— graph search using h?
Does h remain admissible if some links are removed?
Does h remain admissible if some links are added between nonadjacent states?

3.27 n vehicles occupy squares (1, 1) through (n, 1) (i.e., the bottom row) of an n Ã— n grid.
The vehicles must be moved to the top row but in reverse order; so the vehicle i that starts in
(i, 1) must end up in (n âˆ’ i + 1, n). On each time step, every one of the n vehicles can move
one square up, down, left, or right, or stay put; but if a vehicle stays put, one other adjacent
vehicle (but not more than one) can hop over it. Two vehicles cannot occupy the same square.
a. Calculate the size of the state space as a function of n.
b. Calculate the branching factor as a function of n.
c. Suppose that vehicle i is at (xi , yi ); write a nontrivial admissible heuristic hi for the
number of moves it will require to get to its goal location (n âˆ’ i + 1, n), assuming no
other vehicles are on the grid.
d. Which of the following heuristics are admissible for the problem of moving all n vehicles to their destinations? Explain.
n
(i)
i = 1 hi .
(ii) max{h1 , . . . , hn }.
(iii) min{h1 , . . . , hn }.

Exercises

119
3.28 Invent a heuristic function for the 8-puzzle that sometimes overestimates, and show
how it can lead to a suboptimal solution on a particular problem. (You can use a computer to
help if you want.) Prove that if h never overestimates by more than c, Aâˆ— using h returns a
solution whose cost exceeds that of the optimal solution by no more than c.
3.29 Prove that if a heuristic is consistent, it must be admissible. Construct an admissible
heuristic that is not consistent.
3.30 The traveling salesperson problem (TSP) can be solved with the minimum-spanningtree (MST) heuristic, which estimates the cost of completing a tour, given that a partial tour
has already been constructed. The MST cost of a set of cities is the smallest sum of the link
costs of any tree that connects all the cities.
a. Show how this heuristic can be derived from a relaxed version of the TSP.
b. Show that the MST heuristic dominates straight-line distance.
c. Write a problem generator for instances of the TSP where cities are represented by
random points in the unit square.
d. Find an efficient algorithm in the literature for constructing the MST, and use it with Aâˆ—
graph search to solve instances of the TSP.
3.31 On page 105, we defined the relaxation of the 8-puzzle in which a tile can move from
square A to square B if B is blank. The exact solution of this problem defines Gaschnigâ€™s
heuristic (Gaschnig, 1979). Explain why Gaschnigâ€™s heuristic is at least as accurate as h1
(misplaced tiles), and show cases where it is more accurate than both h1 and h2 (Manhattan
distance). Explain how to calculate Gaschnigâ€™s heuristic efficiently.
3.32 We gave two simple heuristics for the 8-puzzle: Manhattan distance and misplaced
tiles. Several heuristics in the literature purport to improve on thisâ€”see, for example, Nilsson (1971), Mostow and Prieditis (1989), and Hansson et al. (1992). Test these claims by
implementing the heuristics and comparing the performance of the resulting algorithms.

4

BEYOND CLASSICAL
SEARCH

In which we relax the simplifying assumptions of the previous chapter, thereby
getting closer to the real world.

Chapter 3 addressed a single category of problems: observable, deterministic, known environments where the solution is a sequence of actions. In this chapter, we look at what happens
when these assumptions are relaxed. We begin with a fairly simple case: Sections 4.1 and 4.2
cover algorithms that perform purely local search in the state space, evaluating and modifying one or more current states rather than systematically exploring paths from an initial state.
These algorithms are suitable for problems in which all that matters is the solution state, not
the path cost to reach it. The family of local search algorithms includes methods inspired by
statistical physics (simulated annealing) and evolutionary biology (genetic algorithms).
Then, in Sections 4.3â€“4.4, we examine what happens when we relax the assumptions
of determinism and observability. The key idea is that if an agent cannot predict exactly what
percept it will receive, then it will need to consider what to do under each contingency that
its percepts may reveal. With partial observability, the agent will also need to keep track of
the states it might be in.
Finally, Section 4.5 investigates online search, in which the agent is faced with a state
space that is initially unknown and must be explored.

4.1

L OCAL S EARCH A LGORITHMS AND O PTIMIZATION P ROBLEMS
The search algorithms that we have seen so far are designed to explore search spaces systematically. This systematicity is achieved by keeping one or more paths in memory and by
recording which alternatives have been explored at each point along the path. When a goal is
found, the path to that goal also constitutes a solution to the problem. In many problems, however, the path to the goal is irrelevant. For example, in the 8-queens problem (see page 71),
what matters is the final configuration of queens, not the order in which they are added. The
same general property holds for many important applications such as integrated-circuit design, factory-floor layout, job-shop scheduling, automatic programming, telecommunications
network optimization, vehicle routing, and portfolio management.
120

Section 4.1.

LOCAL SEARCH
CURRENT NODE

OPTIMIZATION
PROBLEM
OBJECTIVE
FUNCTION

STATE-SPACE
LANDSCAPE

GLOBAL MINIMUM
GLOBAL MAXIMUM

Local Search Algorithms and Optimization Problems

121

If the path to the goal does not matter, we might consider a different class of algorithms, ones that do not worry about paths at all. Local search algorithms operate using
a single current node (rather than multiple paths) and generally move only to neighbors
of that node. Typically, the paths followed by the search are not retained. Although local
search algorithms are not systematic, they have two key advantages: (1) they use very little
memoryâ€”usually a constant amount; and (2) they can often find reasonable solutions in large
or infinite (continuous) state spaces for which systematic algorithms are unsuitable.
In addition to finding goals, local search algorithms are useful for solving pure optimization problems, in which the aim is to find the best state according to an objective
function. Many optimization problems do not fit the â€œstandardâ€ search model introduced in
Chapter 3. For example, nature provides an objective functionâ€”reproductive fitnessâ€”that
Darwinian evolution could be seen as attempting to optimize, but there is no â€œgoal testâ€ and
no â€œpath costâ€ for this problem.
To understand local search, we find it useful to consider the state-space landscape (as
in Figure 4.1). A landscape has both â€œlocationâ€ (defined by the state) and â€œelevationâ€ (defined
by the value of the heuristic cost function or objective function). If elevation corresponds to
cost, then the aim is to find the lowest valleyâ€”a global minimum; if elevation corresponds
to an objective function, then the aim is to find the highest peakâ€”a global maximum. (You
can convert from one to the other just by inserting a minus sign.) Local search algorithms
explore this landscape. A complete local search algorithm always finds a goal if one exists;
an optimal algorithm always finds a global minimum/maximum.

objective function

global maximum

shoulder
local maximum
â€œflatâ€ local maximum

state space
current
state

Figure 4.1 A one-dimensional state-space landscape in which elevation corresponds to the
objective function. The aim is to find the global maximum. Hill-climbing search modifies
the current state to try to improve it, as shown by the arrow. The various topographic features
are defined in the text.

122

Chapter 4.

Beyond Classical Search

function H ILL -C LIMBING( problem) returns a state that is a local maximum
current â† M AKE -N ODE(problem.I NITIAL -S TATE)
loop do
neighbor â† a highest-valued successor of current
if neighbor.VALUE â‰¤ current.VALUE then return current.S TATE
current â† neighbor
Figure 4.2 The hill-climbing search algorithm, which is the most basic local search technique. At each step the current node is replaced by the best neighbor; in this version, that
means the neighbor with the highest VALUE, but if a heuristic cost estimate h is used, we
would find the neighbor with the lowest h.

4.1.1 Hill-climbing search
HILL CLIMBING
STEEPEST ASCENT

GREEDY LOCAL
SEARCH

LOCAL MAXIMUM

The hill-climbing search algorithm (steepest-ascent version) is shown in Figure 4.2. It is
simply a loop that continually moves in the direction of increasing valueâ€”that is, uphill. It
terminates when it reaches a â€œpeakâ€ where no neighbor has a higher value. The algorithm
does not maintain a search tree, so the data structure for the current node need only record
the state and the value of the objective function. Hill climbing does not look ahead beyond
the immediate neighbors of the current state. This resembles trying to find the top of Mount
Everest in a thick fog while suffering from amnesia.
To illustrate hill climbing, we will use the 8-queens problem introduced on page 71.
Local search algorithms typically use a complete-state formulation, where each state has
8 queens on the board, one per column. The successors of a state are all possible states
generated by moving a single queen to another square in the same column (so each state has
8 Ã— 7 = 56 successors). The heuristic cost function h is the number of pairs of queens that
are attacking each other, either directly or indirectly. The global minimum of this function
is zero, which occurs only at perfect solutions. Figure 4.3(a) shows a state with h = 17. The
figure also shows the values of all its successors, with the best successors having h = 12.
Hill-climbing algorithms typically choose randomly among the set of best successors if there
is more than one.
Hill climbing is sometimes called greedy local search because it grabs a good neighbor
state without thinking ahead about where to go next. Although greed is considered one of the
seven deadly sins, it turns out that greedy algorithms often perform quite well. Hill climbing
often makes rapid progress toward a solution because it is usually quite easy to improve a bad
state. For example, from the state in Figure 4.3(a), it takes just five steps to reach the state
in Figure 4.3(b), which has h = 1 and is very nearly a solution. Unfortunately, hill climbing
often gets stuck for the following reasons:
â€¢ Local maxima: a local maximum is a peak that is higher than each of its neighboring
states but lower than the global maximum. Hill-climbing algorithms that reach the
vicinity of a local maximum will be drawn upward toward the peak but will then be
stuck with nowhere else to go. Figure 4.1 illustrates the problem schematically. More

Section 4.1.

Local Search Algorithms and Optimization Problems

123

18 12 14 13 13 12 14 14
14 16 13 15 12 14 12 16
14 12 18 13 15 12 14 14
15 14 14

13 16 13 16

14 17 15
17
18 14

14 16 16

16 18 15
15 15 14

15
16

14 14 13 17 12 14 12 18

(a)

(b)

Figure 4.3 (a) An 8-queens state with heuristic cost estimate h = 17, showing the value of
h for each possible successor obtained by moving a queen within its column. The best moves
are marked. (b) A local minimum in the 8-queens state space; the state has h = 1 but every
successor has a higher cost.

RIDGE

PLATEAU
SHOULDER

SIDEWAYS MOVE

concretely, the state in Figure 4.3(b) is a local maximum (i.e., a local minimum for the
cost h); every move of a single queen makes the situation worse.
â€¢ Ridges: a ridge is shown in Figure 4.4. Ridges result in a sequence of local maxima
that is very difficult for greedy algorithms to navigate.
â€¢ Plateaux: a plateau is a flat area of the state-space landscape. It can be a flat local
maximum, from which no uphill exit exists, or a shoulder, from which progress is
possible. (See Figure 4.1.) A hill-climbing search might get lost on the plateau.
In each case, the algorithm reaches a point at which no progress is being made. Starting from
a randomly generated 8-queens state, steepest-ascent hill climbing gets stuck 86% of the time,
solving only 14% of problem instances. It works quickly, taking just 4 steps on average when
it succeeds and 3 when it gets stuckâ€”not bad for a state space with 88 â‰ˆ 17 million states.
The algorithm in Figure 4.2 halts if it reaches a plateau where the best successor has
the same value as the current state. Might it not be a good idea to keep goingâ€”to allow a
sideways move in the hope that the plateau is really a shoulder, as shown in Figure 4.1? The
answer is usually yes, but we must take care. If we always allow sideways moves when there
are no uphill moves, an infinite loop will occur whenever the algorithm reaches a flat local
maximum that is not a shoulder. One common solution is to put a limit on the number of consecutive sideways moves allowed. For example, we could allow up to, say, 100 consecutive
sideways moves in the 8-queens problem. This raises the percentage of problem instances
solved by hill climbing from 14% to 94%. Success comes at a cost: the algorithm averages
roughly 21 steps for each successful instance and 64 for each failure.

124

Chapter 4.

Beyond Classical Search

Figure 4.4 Illustration of why ridges cause difficulties for hill climbing. The grid of states
(dark circles) is superimposed on a ridge rising from left to right, creating a sequence of local
maxima that are not directly connected to each other. From each local maximum, all the
available actions point downhill.
STOCHASTIC HILL
CLIMBING

FIRST-CHOICE HILL
CLIMBING

RANDOM-RESTART
HILL CLIMBING

Many variants of hill climbing have been invented. Stochastic hill climbing chooses at
random from among the uphill moves; the probability of selection can vary with the steepness
of the uphill move. This usually converges more slowly than steepest ascent, but in some
state landscapes, it finds better solutions. First-choice hill climbing implements stochastic
hill climbing by generating successors randomly until one is generated that is better than the
current state. This is a good strategy when a state has many (e.g., thousands) of successors.
The hill-climbing algorithms described so far are incompleteâ€”they often fail to find
a goal when one exists because they can get stuck on local maxima. Random-restart hill
climbing adopts the well-known adage, â€œIf at first you donâ€™t succeed, try, try again.â€ It conducts a series of hill-climbing searches from randomly generated initial states,1 until a goal
is found. It is trivially complete with probability approaching 1, because it will eventually
generate a goal state as the initial state. If each hill-climbing search has a probability p of
success, then the expected number of restarts required is 1/p. For 8-queens instances with
no sideways moves allowed, p â‰ˆ 0.14, so we need roughly 7 iterations to find a goal (6 failures and 1 success). The expected number of steps is the cost of one successful iteration plus
(1âˆ’p)/p times the cost of failure, or roughly 22 steps in all. When we allow sideways moves,
1/0.94 â‰ˆ 1.06 iterations are needed on average and (1 Ã— 21) + (0.06/0.94) Ã— 64 â‰ˆ 25 steps.
For 8-queens, then, random-restart hill climbing is very effective indeed. Even for three million queens, the approach can find solutions in under a minute.2
1

Generating a random state from an implicitly specified state space can be a hard problem in itself.
Luby et al. (1993) prove that it is best, in some cases, to restart a randomized search algorithm after a particular,
fixed amount of time and that this can be much more efficient than letting each search continue indefinitely.
Disallowing or limiting the number of sideways moves is an example of this idea.
2

Section 4.1.

Local Search Algorithms and Optimization Problems

125

The success of hill climbing depends very much on the shape of the state-space landscape: if there are few local maxima and plateaux, random-restart hill climbing will find a
good solution very quickly. On the other hand, many real problems have a landscape that
looks more like a widely scattered family of balding porcupines on a flat floor, with miniature
porcupines living on the tip of each porcupine needle, ad infinitum. NP-hard problems typically have an exponential number of local maxima to get stuck on. Despite this, a reasonably
good local maximum can often be found after a small number of restarts.

4.1.2 Simulated annealing

SIMULATED
ANNEALING

GRADIENT DESCENT

A hill-climbing algorithm that never makes â€œdownhillâ€ moves toward states with lower value
(or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maximum. In contrast, a purely random walkâ€”that is, moving to a successor chosen uniformly
at random from the set of successorsâ€”is complete but extremely inefficient. Therefore, it
seems reasonable to try to combine hill climbing with a random walk in some way that yields
both efficiency and completeness. Simulated annealing is such an algorithm. In metallurgy,
annealing is the process used to temper or harden metals and glass by heating them to a
high temperature and then gradually cooling them, thus allowing the material to reach a lowenergy crystalline state. To explain simulated annealing, we switch our point of view from
hill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a
ping-pong ball into the deepest crevice in a bumpy surface. If we just let the ball roll, it will
come to rest at a local minimum. If we shake the surface, we can bounce the ball out of the
local minimum. The trick is to shake just hard enough to bounce the ball out of local minima but not hard enough to dislodge it from the global minimum. The simulated-annealing
solution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the
intensity of the shaking (i.e., lower the temperature).
The innermost loop of the simulated-annealing algorithm (Figure 4.5) is quite similar to
hill climbing. Instead of picking the best move, however, it picks a random move. If the move
improves the situation, it is always accepted. Otherwise, the algorithm accepts the move with
some probability less than 1. The probability decreases exponentially with the â€œbadnessâ€ of
the moveâ€”the amount Î”E by which the evaluation is worsened. The probability also decreases as the â€œtemperatureâ€ T goes down: â€œbadâ€ moves are more likely to be allowed at the
start when T is high, and they become more unlikely as T decreases. If the schedule lowers
T slowly enough, the algorithm will find a global optimum with probability approaching 1.
Simulated annealing was first used extensively to solve VLSI layout problems in the
early 1980s. It has been applied widely to factory scheduling and other large-scale optimization tasks. In Exercise 4.4, you are asked to compare its performance to that of random-restart
hill climbing on the 8-queens puzzle.

4.1.3 Local beam search
LOCAL BEAM
SEARCH

Keeping just one node in memory might seem to be an extreme reaction to the problem of
memory limitations. The local beam search algorithm3 keeps track of k states rather than
3

Local beam search is an adaptation of beam search, which is a path-based algorithm.

126

Chapter 4.

Beyond Classical Search

function S IMULATED -A NNEALING( problem, schedule) returns a solution state
inputs: problem, a problem
schedule, a mapping from time to â€œtemperatureâ€
current â† M AKE -N ODE(problem.I NITIAL -S TATE)
for t = 1 to âˆž do
T â† schedule(t )
if T = 0 then return current
next â† a randomly selected successor of current
Î”E â† next.VALUE â€“ current.VALUE
if Î”E > 0 then current â† next
else current â† next only with probability eÎ”E/T
Figure 4.5 The simulated annealing algorithm, a version of stochastic hill climbing where
some downhill moves are allowed. Downhill moves are accepted readily early in the annealing schedule and then less often as time goes on. The schedule input determines the value of
the temperature T as a function of time.

STOCHASTIC BEAM
SEARCH

just one. It begins with k randomly generated states. At each step, all the successors of all k
states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best
successors from the complete list and repeats.
At first sight, a local beam search with k states might seem to be nothing more than
running k random restarts in parallel instead of in sequence. In fact, the two algorithms
are quite different. In a random-restart search, each search process runs independently of
the others. In a local beam search, useful information is passed among the parallel search
threads. In effect, the states that generate the best successors say to the others, â€œCome over
here, the grass is greener!â€ The algorithm quickly abandons unfruitful searches and moves
its resources to where the most progress is being made.
In its simplest form, local beam search can suffer from a lack of diversity among the
k statesâ€”they can quickly become concentrated in a small region of the state space, making
the search little more than an expensive version of hill climbing. A variant called stochastic
beam search, analogous to stochastic hill climbing, helps alleviate this problem. Instead
of choosing the best k from the the pool of candidate successors, stochastic beam search
chooses k successors at random, with the probability of choosing a given successor being
an increasing function of its value. Stochastic beam search bears some resemblance to the
process of natural selection, whereby the â€œsuccessorsâ€ (offspring) of a â€œstateâ€ (organism)
populate the next generation according to its â€œvalueâ€ (fitness).

4.1.4 Genetic algorithms
GENETIC
ALGORITHM

A genetic algorithm (or GA) is a variant of stochastic beam search in which successor states
are generated by combining two parent states rather than by modifying a single state. The
analogy to natural selection is the same as in stochastic beam search, except that now we are
dealing with sexual rather than asexual reproduction.

Section 4.1.

Local Search Algorithms and Optimization Problems

127

24748552

24 31%

32752411

32748552

32748152

32752411

23 29%

24748552

24752411

24752411

24415124

20 26%

32752411

32752124

32252124

32543213

11 14%

24415124

24415411

24415417

(a)
Initial Population

(b)
Fitness Function

(c)
Selection

(d)
Crossover

(e)
Mutation

Figure 4.6 The genetic algorithm, illustrated for digit strings representing 8-queens states.
The initial population in (a) is ranked by the fitness function in (b), resulting in pairs for
mating in (c). They produce offspring in (d), which are subject to mutation in (e).

+

=

Figure 4.7 The 8-queens states corresponding to the first two parents in Figure 4.6(c) and
the first offspring in Figure 4.6(d). The shaded columns are lost in the crossover step and the
unshaded columns are retained.

POPULATION
INDIVIDUAL

FITNESS FUNCTION

Like beam searches, GAs begin with a set of k randomly generated states, called the
population. Each state, or individual, is represented as a string over a finite alphabetâ€”most
commonly, a string of 0s and 1s. For example, an 8-queens state must specify the positions of
8 queens, each in a column of 8 squares, and so requires 8 Ã— log2 8 = 24 bits. Alternatively,
the state could be represented as 8 digits, each in the range from 1 to 8. (We demonstrate later
that the two encodings behave differently.) Figure 4.6(a) shows a population of four 8-digit
strings representing 8-queens states.
The production of the next generation of states is shown in Figure 4.6(b)â€“(e). In (b),
each state is rated by the objective function, or (in GA terminology) the fitness function. A
fitness function should return higher values for better states, so, for the 8-queens problem
we use the number of nonattacking pairs of queens, which has a value of 28 for a solution.
The values of the four states are 24, 23, 20, and 11. In this particular variant of the genetic
algorithm, the probability of being chosen for reproducing is directly proportional to the
fitness score, and the percentages are shown next to the raw scores.
In (c), two pairs are selected at random for reproduction, in accordance with the prob-

128

CROSSOVER

MUTATION

SCHEMA

INSTANCE

Chapter 4.

Beyond Classical Search

abilities in (b). Notice that one individual is selected twice and one not at all.4 For each
pair to be mated, a crossover point is chosen randomly from the positions in the string. In
Figure 4.6, the crossover points are after the third digit in the first pair and after the fifth digit
in the second pair.5
In (d), the offspring themselves are created by crossing over the parent strings at the
crossover point. For example, the first child of the first pair gets the first three digits from the
first parent and the remaining digits from the second parent, whereas the second child gets
the first three digits from the second parent and the rest from the first parent. The 8-queens
states involved in this reproduction step are shown in Figure 4.7. The example shows that
when two parent states are quite different, the crossover operation can produce a state that is
a long way from either parent state. It is often the case that the population is quite diverse
early on in the process, so crossover (like simulated annealing) frequently takes large steps in
the state space early in the search process and smaller steps later on when most individuals
are quite similar.
Finally, in (e), each location is subject to random mutation with a small independent
probability. One digit was mutated in the first, third, and fourth offspring. In the 8-queens
problem, this corresponds to choosing a queen at random and moving it to a random square
in its column. Figure 4.8 describes an algorithm that implements all these steps.
Like stochastic beam search, genetic algorithms combine an uphill tendency with random exploration and exchange of information among parallel search threads. The primary
advantage, if any, of genetic algorithms comes from the crossover operation. Yet it can be
shown mathematically that, if the positions of the genetic code are permuted initially in a
random order, crossover conveys no advantage. Intuitively, the advantage comes from the
ability of crossover to combine large blocks of letters that have evolved independently to perform useful functions, thus raising the level of granularity at which the search operates. For
example, it could be that putting the first three queens in positions 2, 4, and 6 (where they do
not attack each other) constitutes a useful block that can be combined with other blocks to
construct a solution.
The theory of genetic algorithms explains how this works using the idea of a schema,
which is a substring in which some of the positions can be left unspecified. For example,
the schema 246***** describes all 8-queens states in which the first three queens are in
positions 2, 4, and 6, respectively. Strings that match the schema (such as 24613578) are
called instances of the schema. It can be shown that if the average fitness of the instances of
a schema is above the mean, then the number of instances of the schema within the population
will grow over time. Clearly, this effect is unlikely to be significant if adjacent bits are totally
unrelated to each other, because then there will be few contiguous blocks that provide a
consistent benefit. Genetic algorithms work best when schemata correspond to meaningful
components of a solution. For example, if the string is a representation of an antenna, then the
schemata may represent components of the antenna, such as reflectors and deflectors. A good
4 There are many variants of this selection rule. The method of culling, in which all individuals below a given
threshold are discarded, can be shown to converge faster than the random version (Baum et al., 1995).
5 It is here that the encoding matters. If a 24-bit encoding is used instead of 8 digits, then the crossover point
has a 2/3 chance of being in the middle of a digit, which results in an essentially arbitrary mutation of that digit.

Section 4.2.

Local Search in Continuous Spaces

129

function G ENETIC -A LGORITHM( population , F ITNESS -F N) returns an individual
inputs: population , a set of individuals
F ITNESS -F N, a function that measures the fitness of an individual
repeat
new population â† empty set
for i = 1 to S IZE( population) do
x â† R ANDOM -S ELECTION ( population, F ITNESS -F N )
y â† R ANDOM -S ELECTION ( population , F ITNESS -F N)
child â† R EPRODUCE (x , y)
if (small random probability) then child â† M UTATE(child )
add child to new population
population â† new population
until some individual is fit enough, or enough time has elapsed
return the best individual in population , according to F ITNESS -F N
function R EPRODUCE (x , y) returns an individual
inputs: x , y, parent individuals
n â† L ENGTH(x ); c â† random number from 1 to n
return A PPEND(S UBSTRING(x , 1, c), S UBSTRING(y, c + 1, n))
Figure 4.8 A genetic algorithm. The algorithm is the same as the one diagrammed in
Figure 4.6, with one variation: in this more popular version, each mating of two parents
produces only one offspring, not two.

component is likely to be good in a variety of different designs. This suggests that successful
use of genetic algorithms requires careful engineering of the representation.
In practice, genetic algorithms have had a widespread impact on optimization problems,
such as circuit layout and job-shop scheduling. At present, it is not clear whether the appeal
of genetic algorithms arises from their performance or from their Ã¦sthetically pleasing origins
in the theory of evolution. Much work remains to be done to identify the conditions under
which genetic algorithms perform well.

4.2

L OCAL S EARCH IN C ONTINUOUS S PACES
In Chapter 2, we explained the distinction between discrete and continuous environments,
pointing out that most real-world environments are continuous. Yet none of the algorithms
we have described (except for first-choice hill climbing and simulated annealing) can handle
continuous state and action spaces, because they have infinite branching factors. This section
provides a very brief introduction to some local search techniques for finding optimal solutions in continuous spaces. The literature on this topic is vast; many of the basic techniques

130

Chapter 4.

E VOLUTION

AND

Beyond Classical Search

S EARCH

The theory of evolution was developed in Charles Darwinâ€™s On the Origin of
Species by Means of Natural Selection (1859) and independently by Alfred Russel
Wallace (1858). The central idea is simple: variations occur in reproduction and
will be preserved in successive generations approximately in proportion to their
effect on reproductive fitness.
Darwinâ€™s theory was developed with no knowledge of how the traits of organisms can be inherited and modified. The probabilistic laws governing these processes were first identified by Gregor Mendel (1866), a monk who experimented
with sweet peas. Much later, Watson and Crick (1953) identified the structure of the
DNA molecule and its alphabet, AGTC (adenine, guanine, thymine, cytosine). In
the standard model, variation occurs both by point mutations in the letter sequence
and by â€œcrossoverâ€ (in which the DNA of an offspring is generated by combining
long sections of DNA from each parent).
The analogy to local search algorithms has already been described; the principal difference between stochastic beam search and evolution is the use of sexual reproduction, wherein successors are generated from multiple organisms rather than
just one. The actual mechanisms of evolution are, however, far richer than most
genetic algorithms allow. For example, mutations can involve reversals, duplications, and movement of large chunks of DNA; some viruses borrow DNA from one
organism and insert it in another; and there are transposable genes that do nothing
but copy themselves many thousands of times within the genome. There are even
genes that poison cells from potential mates that do not carry the gene, thereby increasing their own chances of replication. Most important is the fact that the genes
themselves encode the mechanisms whereby the genome is reproduced and translated into an organism. In genetic algorithms, those mechanisms are a separate
program that is not represented within the strings being manipulated.
Darwinian evolution may appear inefficient, having generated blindly some
45
10 or so organisms without improving its search heuristics one iota. Fifty
years before Darwin, however, the otherwise great French naturalist Jean Lamarck
(1809) proposed a theory of evolution whereby traits acquired by adaptation during an organismâ€™s lifetime would be passed on to its offspring. Such a process
would be effective but does not seem to occur in nature. Much later, James Baldwin (1896) proposed a superficially similar theory: that behavior learned during an
organismâ€™s lifetime could accelerate the rate of evolution. Unlike Lamarckâ€™s, Baldwinâ€™s theory is entirely consistent with Darwinian evolution because it relies on selection pressures operating on individuals that have found local optima among the
set of possible behaviors allowed by their genetic makeup. Computer simulations
confirm that the â€œBaldwin effectâ€ is real, once â€œordinaryâ€ evolution has created
organisms whose internal performance measure correlates with actual fitness.

Section 4.2.

VARIABLE

Local Search in Continuous Spaces

131

originated in the 17th century, after the development of calculus by Newton and Leibniz.6 We
find uses for these techniques at several places in the book, including the chapters on learning,
vision, and robotics.
We begin with an example. Suppose we want to place three new airports anywhere
in Romania, such that the sum of squared distances from each city on the map (Figure 3.2)
to its nearest airport is minimized. The state space is then defined by the coordinates of
the airports: (x1 , y1 ), (x2 , y2 ), and (x3 , y3 ). This is a six-dimensional space; we also say
that states are defined by six variables. (In general, states are defined by an n-dimensional
vector of variables, x.) Moving around in this space corresponds to moving one or more of
the airports on the map. The objective function f (x1 , y1 , x2 , y2 , x3 , y3 ) is relatively easy to
compute for any particular state once we compute the closest cities. Let Ci be the set of
cities whose closest airport (in the current state) is airport i. Then, in the neighborhood of the
current state, where the Ci s remain constant, we have
f (x1 , y1 , x2 , y2 , x3 , y3 ) =

3 


(xi âˆ’ xc )2 + (yi âˆ’ yc )2 .

(4.1)

i = 1 câˆˆCi

DISCRETIZATION

GRADIENT

This expression is correct locally, but not globally because the sets Ci are (discontinuous)
functions of the state.
One way to avoid continuous problems is simply to discretize the neighborhood of each
state. For example, we can move only one airport at a time in either the x or y direction by
a fixed amount Â±Î´. With 6 variables, this gives 12 possible successors for each state. We
can then apply any of the local search algorithms described previously. We could also apply stochastic hill climbing and simulated annealing directly, without discretizing the space.
These algorithms choose successors randomly, which can be done by generating random vectors of length Î´.
Many methods attempt to use the gradient of the landscape to find a maximum. The
gradient of the objective function is a vector âˆ‡f that gives the magnitude and direction of the
steepest slope. For our problem, we have


âˆ‚f âˆ‚f âˆ‚f âˆ‚f âˆ‚f âˆ‚f
,
,
,
,
,
.
âˆ‡f =
âˆ‚x1 âˆ‚y1 âˆ‚x2 âˆ‚y2 âˆ‚x3 âˆ‚y3
In some cases, we can find a maximum by solving the equation âˆ‡f = 0. (This could be done,
for example, if we were placing just one airport; the solution is the arithmetic mean of all the
citiesâ€™ coordinates.) In many cases, however, this equation cannot be solved in closed form.
For example, with three airports, the expression for the gradient depends on what cities are
closest to each airport in the current state. This means we can compute the gradient locally
(but not globally); for example,

âˆ‚f
=2
(xi âˆ’ xc ) .
(4.2)
âˆ‚x1
câˆˆC1

Given a locally correct expression for the gradient, we can perform steepest-ascent hill climb6

A basic knowledge of multivariate calculus and vector arithmetic is useful for reading this section.

132

Chapter 4.

Beyond Classical Search

ing by updating the current state according to the formula
x â† x + Î±âˆ‡f (x) ,
STEP SIZE

EMPIRICAL
GRADIENT

LINE SEARCH

NEWTONâ€“RAPHSON

where Î± is a small constant often called the step size. In other cases, the objective function
might not be available in a differentiable form at allâ€”for example, the value of a particular set
of airport locations might be determined by running some large-scale economic simulation
package. In those cases, we can calculate a so-called empirical gradient by evaluating the
response to small increments and decrements in each coordinate. Empirical gradient search
is the same as steepest-ascent hill climbing in a discretized version of the state space.
Hidden beneath the phrase â€œÎ± is a small constantâ€ lies a huge variety of methods for
adjusting Î±. The basic problem is that, if Î± is too small, too many steps are needed; if Î±
is too large, the search could overshoot the maximum. The technique of line search tries to
overcome this dilemma by extending the current gradient directionâ€”usually by repeatedly
doubling Î±â€”until f starts to decrease again. The point at which this occurs becomes the new
current state. There are several schools of thought about how the new direction should be
chosen at this point.
For many problems, the most effective algorithm is the venerable Newtonâ€“Raphson
method. This is a general technique for finding roots of functionsâ€”that is, solving equations
of the form g(x) = 0. It works by computing a new estimate for the root x according to
Newtonâ€™s formula
x â† x âˆ’ g(x)/g (x) .
To find a maximum or minimum of f , we need to find x such that the gradient is zero (i.e.,
âˆ‡f (x) = 0). Thus, g(x) in Newtonâ€™s formula becomes âˆ‡f (x), and the update equation can
be written in matrixâ€“vector form as
x â† x âˆ’ Hâˆ’1
f (x)âˆ‡f (x) ,

HESSIAN

CONSTRAINED
OPTIMIZATION

where Hf (x) is the Hessian matrix of second derivatives, whose elements Hij are given
by âˆ‚ 2 f /âˆ‚xi âˆ‚xj . For our airport example, we can see from Equation (4.2) that Hf (x) is
particularly simple: the off-diagonal elements are zero and the diagonal elements for airport
i are just twice the number of cities in Ci . A momentâ€™s calculation shows that one step of
the update moves airport i directly to the centroid of Ci , which is the minimum of the local
expression for f from Equation (4.1).7 For high-dimensional problems, however, computing
the n2 entries of the Hessian and inverting it may be expensive, so many approximate versions
of the Newtonâ€“Raphson method have been developed.
Local search methods suffer from local maxima, ridges, and plateaux in continuous
state spaces just as much as in discrete spaces. Random restarts and simulated annealing can
be used and are often helpful. High-dimensional continuous spaces are, however, big places
in which it is easy to get lost.
A final topic with which a passing acquaintance is useful is constrained optimization.
An optimization problem is constrained if solutions must satisfy some hard constraints on the
values of the variables. For example, in our airport-siting problem, we might constrain sites
7

In general, the Newtonâ€“Raphson update can be seen as fitting a quadratic surface to f at x and then moving
directly to the minimum of that surfaceâ€”which is also the minimum of f if f is quadratic.

Section 4.3.

LINEAR
PROGRAMMING
CONVEX SET

CONVEX
OPTIMIZATION

4.3

133

to be inside Romania and on dry land (rather than in the middle of lakes). The difficulty of
constrained optimization problems depends on the nature of the constraints and the objective
function. The best-known category is that of linear programming problems, in which constraints must be linear inequalities forming a convex set 8 and the objective function is also
linear. The time complexity of linear programming is polynomial in the number of variables.
Linear programming is probably the most widely studied and broadly useful class of
optimization problems. It is a special case of the more general problem of convex optimization, which allows the constraint region to be any convex region and the objective to
be any function that is convex within the constraint region. Under certain conditions, convex
optimization problems are also polynomially solvable and may be feasible in practice with
thousands of variables. Several important problems in machine learning and control theory
can be formulated as convex optimization problems (see Chapter 20).

S EARCHING WITH N ONDETERMINISTIC ACTIONS

CONTINGENCY PLAN
STRATEGY

Searching with Nondeterministic Actions

In Chapter 3, we assumed that the environment is fully observable and deterministic and that
the agent knows what the effects of each action are. Therefore, the agent can calculate exactly
which state results from any sequence of actions and always knows which state it is in. Its
percepts provide no new information after each action, although of course they tell the agent
the initial state.
When the environment is either partially observable or nondeterministic (or both), percepts become useful. In a partially observable environment, every percept helps narrow down
the set of possible states the agent might be in, thus making it easier for the agent to achieve
its goals. When the environment is nondeterministic, percepts tell the agent which of the possible outcomes of its actions has actually occurred. In both cases, the future percepts cannot
be determined in advance and the agentâ€™s future actions will depend on those future percepts.
So the solution to a problem is not a sequence but a contingency plan (also known as a strategy) that specifies what to do depending on what percepts are received. In this section, we
examine the case of nondeterminism, deferring partial observability to Section 4.4.

4.3.1 The erratic vacuum world
As an example, we use the vacuum world, first introduced in Chapter 2 and defined as a
search problem in Section 3.2.1. Recall that the state space has eight states, as shown in
Figure 4.9. There are three actionsâ€”Left, Right, and Suckâ€”and the goal is to clean up all
the dirt (states 7 and 8). If the environment is observable, deterministic, and completely
known, then the problem is trivially solvable by any of the algorithms in Chapter 3 and the
solution is an action sequence. For example, if the initial state is 1, then the action sequence
[Suck,Right,Suck] will reach a goal state, 8.
8 A set of points S is convex if the line joining any two points in S is also contained in S. A convex function is
one for which the space â€œaboveâ€ it forms a convex set; by definition, convex functions have no local (as opposed
to global) minima.

134

Chapter 4.

Figure 4.9

ERRATIC VACUUM
WORLD

1

2

3

4

5

6

7

8

Beyond Classical Search

The eight possible states of the vacuum world; states 7 and 8 are goal states.

Now suppose that we introduce nondeterminism in the form of a powerful but erratic
vacuum cleaner. In the erratic vacuum world, the Suck action works as follows:
â€¢ When applied to a dirty square the action cleans the square and sometimes cleans up
dirt in an adjacent square, too.
â€¢ When applied to a clean square the action sometimes deposits dirt on the carpet.9
To provide a precise formulation of this problem, we need to generalize the notion of a transition model from Chapter 3. Instead of defining the transition model by a R ESULT function
that returns a single state, we use a R ESULTS function that returns a set of possible outcome
states. For example, in the erratic vacuum world, the Suck action in state 1 leads to a state in
the set {5, 7}â€”the dirt in the right-hand square may or may not be vacuumed up.
We also need to generalize the notion of a solution to the problem. For example, if we
start in state 1, there is no single sequence of actions that solves the problem. Instead, we
need a contingency plan such as the following:
[Suck, if State = 5 then [Right, Suck] else [ ]] .

(4.3)

Thus, solutions for nondeterministic problems can contain nested ifâ€“thenâ€“else statements;
this means that they are trees rather than sequences. This allows the selection of actions
based on contingencies arising during execution. Many problems in the real, physical world
are contingency problems because exact prediction is impossible. For this reason, many
people keep their eyes open while walking around or driving.
9

We assume that most readers face similar problems and can sympathize with our agent. We apologize to
owners of modern, efficient home appliances who cannot take advantage of this pedagogical device.

Section 4.3.

Searching with Nondeterministic Actions

4.3.2

OR NODE

AND NODE

ANDâ€“OR TREE

AND â€“ OR

135

search trees

The next question is how to find contingent solutions to nondeterministic problems. As in
Chapter 3, we begin by constructing search trees, but here the trees have a different character.
In a deterministic environment, the only branching is introduced by the agentâ€™s own choices
in each state. We call these nodes OR nodes. In the vacuum world, for example, at an OR
node the agent chooses Left or Right or Suck. In a nondeterministic environment, branching
is also introduced by the environmentâ€™s choice of outcome for each action. We call these
nodes AND nodes. For example, the Suck action in state 1 leads to a state in the set {5, 7},
so the agent would need to find a plan for state 5 and for state 7. These two kinds of nodes
alternate, leading to an AND â€“ OR tree as illustrated in Figure 4.10.
A solution for an AND â€“ OR search problem is a subtree that (1) has a goal node at every
leaf, (2) specifies one action at each of its OR nodes, and (3) includes every outcome branch
at each of its AND nodes. The solution is shown in bold lines in the figure; it corresponds
to the plan given in Equation (4.3). (The plan uses ifâ€“thenâ€“else notation to handle the AND
branches, but when there are more than two branches at a node, it might be better to use a case

1

Suck

Right

2

5

7

GOAL

5

Suck

6

1

LOOP

Left

Right

LOOP

Suck

1

Left

Suck

4

8

LOOP

GOAL

5

8

GOAL

LOOP

Figure 4.10 The first two levels of the search tree for the erratic vacuum world. State
nodes are OR nodes where some action must be chosen. At the AND nodes, shown as circles,
every outcome must be handled, as indicated by the arc linking the outgoing branches. The
solution found is shown in bold lines.

136

Chapter 4.

Beyond Classical Search

function A ND -O R -G RAPH -S EARCH (problem) returns a conditional plan, or failure
O R -S EARCH (problem.I NITIAL -S TATE, problem, [ ])
function O R -S EARCH (state, problem, path) returns a conditional plan, or failure
if problem.G OAL -T EST(state) then return the empty plan
if state is on path then return failure
for each action in problem.ACTIONS (state) do
plan â† A ND -S EARCH (R ESULTS(state, action), problem, [state | path])
if plan = failure then return [action | plan]
return failure
function A ND -S EARCH (states, problem, path) returns a conditional plan, or failure
for each si in states do
plan i â† O R -S EARCH (si , problem, path)
if plan i = failure then return failure
return [if s1 then plan 1 else if s2 then plan 2 else . . . if snâˆ’1 then plan nâˆ’1 else plan n ]
Figure 4.11 An algorithm for searching AND â€“ OR graphs generated by nondeterministic
environments. It returns a conditional plan that reaches a goal state in all circumstances. (The
notation [x | l] refers to the list formed by adding object x to the front of list l.)

INTERLEAVING

construct.) Modifying the basic problem-solving agent shown in Figure 3.1 to execute contingent solutions of this kind is straightforward. One may also consider a somewhat different
agent design, in which the agent can act before it has found a guaranteed plan and deals with
some contingencies only as they arise during execution. This type of interleaving of search
and execution is also useful for exploration problems (see Section 4.5) and for game playing
(see Chapter 5).
Figure 4.11 gives a recursive, depth-first algorithm for AND â€“ OR graph search. One
key aspect of the algorithm is the way in which it deals with cycles, which often arise in
nondeterministic problems (e.g., if an action sometimes has no effect or if an unintended
effect can be corrected). If the current state is identical to a state on the path from the root,
then it returns with failure. This doesnâ€™t mean that there is no solution from the current state;
it simply means that if there is a noncyclic solution, it must be reachable from the earlier
incarnation of the current state, so the new incarnation can be discarded. With this check, we
ensure that the algorithm terminates in every finite state space, because every path must reach
a goal, a dead end, or a repeated state. Notice that the algorithm does not check whether the
current state is a repetition of a state on some other path from the root, which is important for
efficiency. Exercise 4.5 investigates this issue.
AND â€“ OR graphs can also be explored by breadth-first or best-first methods. The concept
of a heuristic function must be modified to estimate the cost of a contingent solution rather
than a sequence, but the notion of admissibility carries over and there is an analog of the Aâˆ—
algorithm for finding optimal solutions. Pointers are given in the bibliographical notes at the
end of the chapter.

Section 4.3.

Searching with Nondeterministic Actions

137

1

Suck

Right

5

2

Right

6

Figure 4.12 Part of the search graph for the slippery vacuum world, where we have shown
(some) cycles explicitly. All solutions for this problem are cyclic plans because there is no
way to move reliably.

4.3.3 Try, try again

CYCLIC SOLUTION
LABEL

Consider the slippery vacuum world, which is identical to the ordinary (non-erratic) vacuum world except that movement actions sometimes fail, leaving the agent in the same location. For example, moving Right in state 1 leads to the state set {1, 2}. Figure 4.12 shows
part of the search graph; clearly, there are no longer any acyclic solutions from state 1, and
A ND -O R -G RAPH -S EARCH would return with failure. There is, however, a cyclic solution,
which is to keep trying Right until it works. We can express this solution by adding a label to
denote some portion of the plan and using that label later instead of repeating the plan itself.
Thus, our cyclic solution is
[Suck, L1 : Right, if State = 5 then L1 else Suck] .
(A better syntax for the looping part of this plan would be â€œwhile State = 5 do Right.â€)
In general a cyclic plan may be considered a solution provided that every leaf is a goal
state and that a leaf is reachable from every point in the plan. The modifications needed
to A ND -O R -G RAPH -S EARCH are covered in Exercise 4.6. The key realization is that a loop
in the state space back to a state L translates to a loop in the plan back to the point where the
subplan for state L is executed.
Given the definition of a cyclic solution, an agent executing such a solution will eventually reach the goal provided that each outcome of a nondeterministic action eventually occurs.
Is this condition reasonable? It depends on the reason for the nondeterminism. If the action
rolls a die, then itâ€™s reasonable to suppose that eventually a six will be rolled. If the action is
to insert a hotel card key into the door lock, but it doesnâ€™t work the first time, then perhaps it
will eventually work, or perhaps one has the wrong key (or the wrong room!). After seven or

138

Chapter 4.

Beyond Classical Search

eight tries, most people will assume the problem is with the key and will go back to the front
desk to get a new one. One way to understand this decision is to say that the initial problem
formulation (observable, nondeterministic) is abandoned in favor of a different formulation
(partially observable, deterministic) where the failure is attributed to an unobservable property of the key. We have more to say on this issue in Chapter 13.

4.4

S EARCHING WITH PARTIAL O BSERVATIONS

BELIEF STATE

We now turn to the problem of partial observability, where the agentâ€™s percepts do not suffice to pin down the exact state. As noted at the beginning of the previous section, if the
agent is in one of several possible states, then an action may lead to one of several possible
outcomesâ€”even if the environment is deterministic. The key concept required for solving
partially observable problems is the belief state, representing the agentâ€™s current belief about
the possible physical states it might be in, given the sequence of actions and percepts up to
that point. We begin with the simplest scenario for studying belief states, which is when the
agent has no sensors at all; then we add in partial sensing as well as nondeterministic actions.

4.4.1 Searching with no observation
SENSORLESS
CONFORMANT

COERCION

When the agentâ€™s percepts provide no information at all, we have what is called a sensorless problem or sometimes a conformant problem. At first, one might think the sensorless
agent has no hope of solving a problem if it has no idea what state itâ€™s in; in fact, sensorless
problems are quite often solvable. Moreover, sensorless agents can be surprisingly useful,
primarily because they donâ€™t rely on sensors working properly. In manufacturing systems,
for example, many ingenious methods have been developed for orienting parts correctly from
an unknown initial position by using a sequence of actions with no sensing at all. The high
cost of sensing is another reason to avoid it: for example, doctors often prescribe a broadspectrum antibiotic rather than using the contingent plan of doing an expensive blood test,
then waiting for the results to come back, and then prescribing a more specific antibiotic and
perhaps hospitalization because the infection has progressed too far.
We can make a sensorless version of the vacuum world. Assume that the agent knows
the geography of its world, but doesnâ€™t know its location or the distribution of dirt. In that
case, its initial state could be any element of the set {1, 2, 3, 4, 5, 6, 7, 8}. Now, consider what
happens if it tries the action Right. This will cause it to be in one of the states {2, 4, 6, 8}â€”the
agent now has more information! Furthermore, the action sequence [Right,Suck] will always
end up in one of the states {4, 8}. Finally, the sequence [Right,Suck,Left,Suck] is guaranteed
to reach the goal state 7 no matter what the start state. We say that the agent can coerce the
world into state 7.
To solve sensorless problems, we search in the space of belief states rather than physical
states.10 Notice that in belief-state space, the problem is fully observable because the agent
10

In a fully observable environment, each belief state contains one physical state. Thus, we can view the algorithms in Chapter 3 as searching in a belief-state space of singleton belief states.

Section 4.4.

Searching with Partial Observations

139

always knows its own belief state. Furthermore, the solution (if any) is always a sequence of
actions. This is because, as in the ordinary problems of Chapter 3, the percepts received after
each action are completely predictableâ€”theyâ€™re always empty! So there are no contingencies
to plan for. This is true even if the environment is nondeterminstic.
It is instructive to see how the belief-state search problem is constructed. Suppose
the underlying physical problem P is defined by ACTIONS P , R ESULT P , G OAL -T EST P , and
S TEP -C OST P . Then we can define the corresponding sensorless problem as follows:
â€¢ Belief states: The entire belief-state space contains every possible set of physical states.
If P has N states, then the sensorless problem has up to 2N states, although many may
be unreachable from the initial state.
â€¢ Initial state: Typically the set of all states in P , although in some cases the agent will
have more knowledge than this.
â€¢ Actions: This is slightly tricky. Suppose the agent is in belief state b = {s1 , s2 }, but
ACTIONS P (s1 ) = ACTIONS P (s2 ); then the agent is unsure of which actions are legal.
If we assume that illegal actions have no effect on the environment, then it is safe to
take the union of all the actions in any of the physical states in the current belief state b:

ACTIONS (b) =
ACTIONS P (s) .
sâˆˆb

On the other hand, if an illegal action might be the end of the world, it is safer to allow
only the intersection, that is, the set of actions legal in all the states. For the vacuum
world, every state has the same legal actions, so both methods give the same result.
â€¢ Transition model: The agent doesnâ€™t know which state in the belief state is the right
one; so as far as it knows, it might get to any of the states resulting from applying the
action to one of the physical states in the belief state. For deterministic actions, the set
of states that might be reached is
b = R ESULT (b, a) = {s : s = R ESULT P (s, a) and s âˆˆ b} .
With deterministic actions,

b

(4.4)

is never larger than b. With nondeterminism, we have



b = R ESULT (b, a) = {s : s âˆˆ R ESULTS P (s, a) and s âˆˆ b}

R ESULTS P (s, a) ,
=
sâˆˆb

PREDICTION

which may be larger than b, as shown in Figure 4.13. The process of generating
the new belief state after the action is called the prediction step; the notation b =
P REDICTP (b, a) will come in handy.
â€¢ Goal test: The agent wants a plan that is sure to work, which means that a belief state
satisfies the goal only if all the physical states in it satisfy G OAL -T EST P . The agent
may accidentally achieve the goal earlier, but it wonâ€™t know that it has done so.
â€¢ Path cost: This is also tricky. If the same action can have different costs in different
states, then the cost of taking an action in a given belief state could be one of several
values. (This gives rise to a new class of problems, which we explore in Exercise 4.9.)
For now we assume that the cost of an action is the same in all states and so can be
transferred directly from the underlying physical problem.

140

Chapter 4.

Beyond Classical Search

1

1

2

1

2

3

4

3

4

3

(a)

(b)

Figure 4.13 (a) Predicting the next belief state for the sensorless vacuum world with a
deterministic action, Right . (b) Prediction for the same belief state and action in the slippery
version of the sensorless vacuum world.

Figure 4.14 shows the reachable belief-state space for the deterministic, sensorless vacuum
world. There are only 12 reachable belief states out of 28 = 256 possible belief states.
The preceding definitions enable the automatic construction of the belief-state problem
formulation from the definition of the underlying physical problem. Once this is done, we
can apply any of the search algorithms of Chapter 3. In fact, we can do a little bit more
than that. In â€œordinaryâ€ graph search, newly generated states are tested to see if they are
identical to existing states. This works for belief states, too; for example, in Figure 4.14, the
action sequence [Suck,Left,Suck] starting at the initial state reaches the same belief state as
[Right,Left,Suck], namely, {5, 7}. Now, consider the belief state reached by [Left], namely,
{1, 3, 5, 7}. Obviously, this is not identical to {5, 7}, but it is a superset. It is easy to prove
(Exercise 4.8) that if an action sequence is a solution for a belief state b, it is also a solution for
any subset of b. Hence, we can discard a path reaching {1, 3, 5, 7} if {5, 7} has already been
generated. Conversely, if {1, 3, 5, 7} has already been generated and found to be solvable,
then any subset, such as {5, 7}, is guaranteed to be solvable. This extra level of pruning may
dramatically improve the efficiency of sensorless problem solving.
Even with this improvement, however, sensorless problem-solving as we have described
it is seldom feasible in practice. The difficulty is not so much the vastness of the belief-state
spaceâ€”even though it is exponentially larger than the underlying physical state space; in
most cases the branching factor and solution length in the belief-state space and physical
state space are not so different. The real difficulty lies with the size of each belief state. For
example, the initial belief state for the 10 Ã— 10 vacuum world contains 100 Ã— 2100 or around
1032 physical statesâ€”far too many if we use the atomic representation, which is an explicit
list of states.
One solution is to represent the belief state by some more compact description. In
English, we could say the agent knows â€œNothingâ€ in the initial state; after moving Left, we
could say, â€œNot in the rightmost column,â€ and so on. Chapter 7 explains how to do this in a
formal representation scheme. Another approach is to avoid the standard search algorithms,
which treat belief states as black boxes just like any other problem state. Instead, we can look

Section 4.4.

Searching with Partial Observations

141
L
R

1

3

5

1

2

3

L

7

R
4

5

7

8

6

2

4

6

8

S
4

5

7

8

S

S

L
5

5

S

7

3

R
L

6

4

S

8

7

4
8

R
R

L
6
8

L
S

L
8

7

R

S

R

3
7

Figure 4.14 The reachable portion of the belief-state space for the deterministic, sensorless vacuum world. Each shaded box corresponds to a single belief state. At any given point,
the agent is in a particular belief state but does not know which physical state it is in. The
initial belief state (complete ignorance) is the top center box. Actions are represented by
labeled links. Self-loops are omitted for clarity.
INCREMENTAL
BELIEF-STATE
SEARCH

inside the belief states and develop incremental belief-state search algorithms that build up
the solution one physical state at a time. For example, in the sensorless vacuum world, the
initial belief state is {1, 2, 3, 4, 5, 6, 7, 8}, and we have to find an action sequence that works
in all 8 states. We can do this by first finding a solution that works for state 1; then we check
if it works for state 2; if not, go back and find a different solution for state 1, and so on. Just
as an AND â€“ OR search has to find a solution for every branch at an AND node, this algorithm
has to find a solution for every state in the belief state; the difference is that AND â€“ OR search
can find a different solution for each branch, whereas an incremental belief-state search has
to find one solution that works for all the states.
The main advantage of the incremental approach is that it is typically able to detect
failure quicklyâ€”when a belief state is unsolvable, it is usually the case that a small subset of
the belief state, consisting of the first few states examined, is also unsolvable. In some cases,

142

Chapter 4.

Beyond Classical Search

this leads to a speedup proportional to the size of the belief states, which may themselves be
as large as the physical state space itself.
Even the most efficient solution algorithm is not of much use when no solutions exist.
Many things just cannot be done without sensing. For example, the sensorless 8-puzzle is
impossible. On the other hand, a little bit of sensing can go a long way. For example, every
8-puzzle instance is solvable if just one square is visibleâ€”the solution involves moving each
tile in turn into the visible square and then keeping track of its location.

4.4.2 Searching with observations
For a general partially observable problem, we have to specify how the environment generates
percepts for the agent. For example, we might define the local-sensing vacuum world to be
one in which the agent has a position sensor and a local dirt sensor but has no sensor capable
of detecting dirt in other squares. The formal problem specification includes a P ERCEPT (s)
function that returns the percept received in a given state. (If sensing is nondeterministic,
then we use a P ERCEPTS function that returns a set of possible percepts.) For example, in the
local-sensing vacuum world, the P ERCEPT in state 1 is [A, Dirty]. Fully observable problems
are a special case in which P ERCEPT (s) = s for every state s, while sensorless problems are
a special case in which P ERCEPT (s) = null.
When observations are partial, it will usually be the case that several states could have
produced any given percept. For example, the percept [A, Dirty] is produced by state 3 as
well as by state 1. Hence, given this as the initial percept, the initial belief state for the
local-sensing vacuum world will be {1, 3}. The ACTIONS , S TEP -C OST , and G OAL -T EST
are constructed from the underlying physical problem just as for sensorless problems, but the
transition model is a bit more complicated. We can think of transitions from one belief state
to the next for a particular action as occurring in three stages, as shown in Figure 4.15:
â€¢ The prediction stage is the same as for sensorless problems: given the action a in belief
state b, the predicted belief state is bÌ‚ = P REDICT(b, a).11
â€¢ The observation prediction stage determines the set of percepts o that could be observed in the predicted belief state:
P OSSIBLE -P ERCEPTS (bÌ‚) = {o : o = P ERCEPT (s) and s âˆˆ bÌ‚} .
â€¢ The update stage determines, for each possible percept, the belief state that would
result from the percept. The new belief state bo is just the set of states in bÌ‚ that could
have produced the percept:
bo = U PDATE (bÌ‚, o) = {s : o = P ERCEPT (s) and s âˆˆ bÌ‚} .
Notice that each updated belief state bo can be no larger than the predicted belief state bÌ‚;
observations can only help reduce uncertainty compared to the sensorless case. Moreover, for deterministic sensing, the belief states for the different possible percepts will
be disjoint, forming a partition of the original predicted belief state.
11

Here, and throughout the book, the â€œhatâ€ in bÌ‚ means an estimated or predicted value for b.

Section 4.4.

Searching with Partial Observations

143

Right
1

2

3

4

[B,Dirty]

2

[B,Clean]

4

(a)

[B,Dirty]
Right

2

2

1

1

3

3

[A,Dirty]

1

(b)
3

4

[B,Clean]
4

Figure 4.15 Two example of transitions in local-sensing vacuum worlds. (a) In the deterministic world, Right is applied in the initial belief state, resulting in a new belief state
with two possible physical states; for those states, the possible percepts are [B, Dirty] and
[B, Clean], leading to two belief states, each of which is a singleton. (b) In the slippery
world, Right is applied in the initial belief state, giving a new belief state with four physical states; for those states, the possible percepts are [A, Dirty], [B, Dirty], and [B, Clean],
leading to three belief states as shown.

Putting these three stages together, we obtain the possible belief states resulting from a given
action and the subsequent possible percepts:
R ESULTS (b, a) = {bo : bo = U PDATE (P REDICT (b, a), o) and
o âˆˆ P OSSIBLE -P ERCEPTS (P REDICT (b, a))} .

(4.5)

Again, the nondeterminism in the partially observable problem comes from the inability
to predict exactly which percept will be received after acting; underlying nondeterminism in
the physical environment may contribute to this inability by enlarging the belief state at the
prediction stage, leading to more percepts at the observation stage.

4.4.3 Solving partially observable problems
The preceding section showed how to derive the R ESULTS function for a nondeterministic
belief-state problem from an underlying physical problem and the P ERCEPT function. Given

144

Chapter 4.

Beyond Classical Search

1

3

Right

Suck

[B,Dirty]

[A,Clean]

[B,Clean]

5
2

4

7

Figure 4.16 The first level of the AND â€“ OR search tree for a problem in the local-sensing
vacuum world; Suck is the first step of the solution.

such a formulation, the AND â€“ OR search algorithm of Figure 4.11 can be applied directly to
derive a solution. Figure 4.16 shows part of the search tree for the local-sensing vacuum
world, assuming an initial percept [A, Dirty]. The solution is the conditional plan
[Suck, Right, if Bstate = {6} then Suck else [ ]] .
Notice that, because we supplied a belief-state problem to the AND â€“ OR search algorithm, it
returned a conditional plan that tests the belief state rather than the actual state. This is as it
should be: in a partially observable environment the agent wonâ€™t be able to execute a solution
that requires testing the actual state.
As in the case of standard search algorithms applied to sensorless problems, the AND â€“
OR search algorithm treats belief states as black boxes, just like any other states. One can
improve on this by checking for previously generated belief states that are subsets or supersets
of the current state, just as for sensorless problems. One can also derive incremental search
algorithms, analogous to those described for sensorless problems, that provide substantial
speedups over the black-box approach.

4.4.4 An agent for partially observable environments
The design of a problem-solving agent for partially observable environments is quite similar
to the simple problem-solving agent in Figure 3.1: the agent formulates a problem, calls a
search algorithm (such as A ND -O R -G RAPH -S EARCH ) to solve it, and executes the solution.
There are two main differences. First, the solution to a problem will be a conditional plan
rather than a sequence; if the first step is an ifâ€“thenâ€“else expression, the agent will need to
test the condition in the if-part and execute the then-part or the else-part accordingly. Second,
the agent will need to maintain its belief state as it performs actions and receives percepts.
This process resembles the predictionâ€“observationâ€“update process in Equation (4.5) but is
actually simpler because the percept is given by the environment rather than calculated by the

Section 4.4.

Searching with Partial Observations

Suck

145

[A,Clean]

Right

2

[B,Dirty]

1

5

5

6

2

3

7

7

4

6

8

Figure 4.17 Two predictionâ€“update cycles of belief-state maintenance in the kindergarten
vacuum world with local sensing.

agent. Given an initial belief state b, an action a, and a percept o, the new belief state is:
b = U PDATE (P REDICT (b, a), o) .

MONITORING
FILTERING
STATE ESTIMATION
RECURSIVE

LOCALIZATION

(4.6)

Figure 4.17 shows the belief state being maintained in the kindergarten vacuum world with
local sensing, wherein any square may become dirty at any time unless the agent is actively
cleaning it at that moment.12
In partially observable environmentsâ€”which include the vast majority of real-world
environmentsâ€”maintaining oneâ€™s belief state is a core function of any intelligent system.
This function goes under various names, including monitoring, filtering and state estimation. Equation (4.6) is called a recursive state estimator because it computes the new belief
state from the previous one rather than by examining the entire percept sequence. If the agent
is not to â€œfall behind,â€ the computation has to happen as fast as percepts are coming in. As
the environment becomes more complex, the exact update computation becomes infeasible
and the agent will have to compute an approximate belief state, perhaps focusing on the implications of the percept for the aspects of the environment that are of current interest. Most
work on this problem has been done for stochastic, continuous-state environments with the
tools of probability theory, as explained in Chapter 15. Here we will show an example in a
discrete environment with detrministic sensors and nondeterministic actions.
The example concerns a robot with the task of localization: working out where it is,
given a map of the world and a sequence of percepts and actions. Our robot is placed in the
maze-like environment of Figure 4.18. The robot is equipped with four sonar sensors that
tell whether there is an obstacleâ€”the outer wall or a black square in the figureâ€”in each of
the four compass directions. We assume that the sensors give perfectly correct data, and that
the robot has a correct map of the enviornment. But unfortunately the robotâ€™s navigational
system is broken, so when it executes a Move action, it moves randomly to one of the adjacent
squares. The robotâ€™s task is to determine its current location.
Suppose the robot has just been switched on, so it does not know where it is. Thus its
initial belief state b consists of the set of all locations. The the robot receives the percept
12

The usual apologies to those who are unfamiliar with the effect of small children on the environment.

146

Chapter 4.

Beyond Classical Search

(a) Possible locations of robot after E 1 = N SW

(b) Possible locations of robot After E 1 = N SW, E 2 = N S
Figure 4.18 Possible positions of the robot, 
, (a) after one observation E1 = N SW and
(b) after a second observation E2 = N S. When sensors are noiseless and the transition model
is accurate, there are no other possible locations for the robot consistent with this sequence
of two observations.

NSW, meaning there are obstacles to the north, west, and south, and does an update using the
equation bo = U PDATE (b), yielding the 4 locations shown in Figure 4.18(a). You can inspect
the maze to see that those are the only four locations that yield the percept NWS .
Next the robot executes a Move action, but the result is nondeterministic. The new belief state, ba = P REDICT (bo , Move), contains all the locations that are one step away from the
locations in bo . When the second percept, NS , arrives, the robot does U PDATE (ba , NS ) and
finds that the belief state has collapsed down to the single location shown in Figure 4.18(b).
Thatâ€™s the only location that could be the result of
U PDATE (P REDICT (U PDATE (b, NSW ), Move), NS ) .
With nondetermnistic actions the P REDICT step grows the belief state, but the U PDATE step
shrinks it back downâ€”as long as the percepts provide some useful identifying information.
Sometimes the percepts donâ€™t help much for localization: If there were one or more long
east-west corridors, then a robot could receive a long sequence of N S percepts, but never
know where in the corridor(s) it was.

Section 4.5.

4.5

Online Search Agents and Unknown Environments

147

O NLINE S EARCH AGENTS AND U NKNOWN E NVIRONMENTS

OFFLINE SEARCH

ONLINE SEARCH

EXPLORATION
PROBLEM

So far we have concentrated on agents that use offline search algorithms. They compute
a complete solution before setting foot in the real world and then execute the solution. In
contrast, an online search13 agent interleaves computation and action: first it takes an action,
then it observes the environment and computes the next action. Online search is a good idea
in dynamic or semidynamic domainsâ€”domains where there is a penalty for sitting around
and computing too long. Online search is also helpful in nondeterministic domains because
it allows the agent to focus its computational efforts on the contingencies that actually arise
rather than those that might happen but probably wonâ€™t. Of course, there is a tradeoff: the
more an agent plans ahead, the less often it will find itself up the creek without a paddle.
Online search is a necessary idea for unknown environments, where the agent does not
know what states exist or what its actions do. In this state of ignorance, the agent faces an
exploration problem and must use its actions as experiments in order to learn enough to
make deliberation worthwhile.
The canonical example of online search is a robot that is placed in a new building and
must explore it to build a map that it can use for getting from A to B. Methods for escaping
from labyrinthsâ€”required knowledge for aspiring heroes of antiquityâ€”are also examples of
online search algorithms. Spatial exploration is not the only form of exploration, however.
Consider a newborn baby: it has many possible actions but knows the outcomes of none of
them, and it has experienced only a few of the possible states that it can reach. The babyâ€™s
gradual discovery of how the world works is, in part, an online search process.

4.5.1 Online search problems
An online search problem must be solved by an agent executing actions, rather than by pure
computation. We assume a deterministic and fully observable environment (Chapter 17 relaxes these assumptions), but we stipulate that the agent knows only the following:
â€¢ ACTIONS (s), which returns a list of actions allowed in state s;
â€¢ The step-cost function c(s, a, s )â€”note that this cannot be used until the agent knows
that s is the outcome; and
â€¢ G OAL -T EST (s).
Note in particular that the agent cannot determine R ESULT (s, a) except by actually being
in s and doing a. For example, in the maze problem shown in Figure 4.19, the agent does
not know that going Up from (1,1) leads to (1,2); nor, having done that, does it know that
going Down will take it back to (1,1). This degree of ignorance can be reduced in some
applicationsâ€”for example, a robot explorer might know how its movement actions work and
be ignorant only of the locations of obstacles.
13

The term â€œonlineâ€ is commonly used in computer science to refer to algorithms that must process input data
as they are received rather than waiting for the entire input data set to become available.

148

Chapter 4.

Beyond Classical Search

G

3

2

1

S
1

2

3

Figure 4.19 A simple maze problem. The agent starts at S and must reach G but knows
nothing of the environment.

G
S

A

G

S

S

A
G
(a)

(b)

Figure 4.20 (a) Two state spaces that might lead an online search agent into a dead end.
Any given agent will fail in at least one of these spaces. (b) A two-dimensional environment
that can cause an online search agent to follow an arbitrarily inefficient route to the goal.
Whichever choice the agent makes, the adversary blocks that route with another long, thin
wall, so that the path followed is much longer than the best possible path.

COMPETITIVE RATIO

Finally, the agent might have access to an admissible heuristic function h(s) that estimates the distance from the current state to a goal state. For example, in Figure 4.19, the
agent might know the location of the goal and be able to use the Manhattan-distance heuristic.
Typically, the agentâ€™s objective is to reach a goal state while minimizing cost. (Another
possible objective is simply to explore the entire environment.) The cost is the total path cost
of the path that the agent actually travels. It is common to compare this cost with the path
cost of the path the agent would follow if it knew the search space in advanceâ€”that is, the
actual shortest path (or shortest complete exploration). In the language of online algorithms,
this is called the competitive ratio; we would like it to be as small as possible.

Section 4.5.

IRREVERSIBLE

DEAD END

ADVERSARY
ARGUMENT

SAFELY EXPLORABLE

Online Search Agents and Unknown Environments

149

Although this sounds like a reasonable request, it is easy to see that the best achievable
competitive ratio is infinite in some cases. For example, if some actions are irreversibleâ€”
i.e., they lead to a state from which no action leads back to the previous stateâ€”the online
search might accidentally reach a dead-end state from which no goal state is reachable. Perhaps the term â€œaccidentallyâ€ is unconvincingâ€”after all, there might be an algorithm that
happens not to take the dead-end path as it explores. Our claim, to be more precise, is that no
algorithm can avoid dead ends in all state spaces. Consider the two dead-end state spaces in
Figure 4.20(a). To an online search algorithm that has visited states S and A, the two state
spaces look identical, so it must make the same decision in both. Therefore, it will fail in
one of them. This is an example of an adversary argumentâ€”we can imagine an adversary
constructing the state space while the agent explores it and putting the goals and dead ends
wherever it chooses.
Dead ends are a real difficulty for robot explorationâ€”staircases, ramps, cliffs, one-way
streets, and all kinds of natural terrain present opportunities for irreversible actions. To make
progress, we simply assume that the state space is safely explorableâ€”that is, some goal state
is reachable from every reachable state. State spaces with reversible actions, such as mazes
and 8-puzzles, can be viewed as undirected graphs and are clearly safely explorable.
Even in safely explorable environments, no bounded competitive ratio can be guaranteed if there are paths of unbounded cost. This is easy to show in environments with irreversible actions, but in fact it remains true for the reversible case as well, as Figure 4.20(b)
shows. For this reason, it is common to describe the performance of online search algorithms
in terms of the size of the entire state space rather than just the depth of the shallowest goal.

4.5.2 Online search agents
After each action, an online agent receives a percept telling it what state it has reached; from
this information, it can augment its map of the environment. The current map is used to
decide where to go next. This interleaving of planning and action means that online search
algorithms are quite different from the offline search algorithms we have seen previously. For
example, offline algorithms such as Aâˆ— can expand a node in one part of the space and then
immediately expand a node in another part of the space, because node expansion involves
simulated rather than real actions. An online algorithm, on the other hand, can discover
successors only for a node that it physically occupies. To avoid traveling all the way across
the tree to expand the next node, it seems better to expand nodes in a local order. Depth-first
search has exactly this property because (except when backtracking) the next node expanded
is a child of the previous node expanded.
An online depth-first search agent is shown in Figure 4.21. This agent stores its map
in a table, R ESULT [s, a], that records the state resulting from executing action a in state s.
Whenever an action from the current state has not been explored, the agent tries that action.
The difficulty comes when the agent has tried all the actions in a state. In offline depth-first
search, the state is simply dropped from the queue; in an online search, the agent has to
backtrack physically. In depth-first search, this means going back to the state from which the
agent most recently entered the current state. To achieve that, the algorithm keeps a table that

150

Chapter 4.

Beyond Classical Search

function O NLINE -DFS-AGENT(s  ) returns an action
inputs: s  , a percept that identifies the current state
persistent: result, a table indexed by state and action, initially empty
untried, a table that lists, for each state, the actions not yet tried
unbacktracked , a table that lists, for each state, the backtracks not yet tried
s, a, the previous state and action, initially null
if G OAL -T EST(s  ) then return stop
if s  is a new state (not in untried) then untried[s  ] â† ACTIONS (s  )
if s is not null then
result[s, a] â† s 
add s to the front of unbacktracked [s  ]
if untried[s  ] is empty then
if unbacktracked [s  ] is empty then return stop
else a â† an action b such that result[s  , b] = P OP(unbacktracked [s  ])
else a â† P OP (untried[s  ])
s â† s
return a
Figure 4.21 An online search agent that uses depth-first exploration. The agent is applicable only in state spaces in which every action can be â€œundoneâ€ by some other action.

lists, for each state, the predecessor states to which the agent has not yet backtracked. If the
agent has run out of states to which it can backtrack, then its search is complete.
We recommend that the reader trace through the progress of O NLINE -DFS-AGENT
when applied to the maze given in Figure 4.19. It is fairly easy to see that the agent will, in
the worst case, end up traversing every link in the state space exactly twice. For exploration,
this is optimal; for finding a goal, on the other hand, the agentâ€™s competitive ratio could be
arbitrarily bad if it goes off on a long excursion when there is a goal right next to the initial
state. An online variant of iterative deepening solves this problem; for an environment that is
a uniform tree, the competitive ratio of such an agent is a small constant.
Because of its method of backtracking, O NLINE -DFS-AGENT works only in state
spaces where the actions are reversible. There are slightly more complex algorithms that
work in general state spaces, but no such algorithm has a bounded competitive ratio.

4.5.3 Online local search

RANDOM WALK

Like depth-first search, hill-climbing search has the property of locality in its node expansions. In fact, because it keeps just one current state in memory, hill-climbing search is
already an online search algorithm! Unfortunately, it is not very useful in its simplest form
because it leaves the agent sitting at local maxima with nowhere to go. Moreover, random
restarts cannot be used, because the agent cannot transport itself to a new state.
Instead of random restarts, one might consider using a random walk to explore the
environment. A random walk simply selects at random one of the available actions from the

Section 4.5.

Online Search Agents and Unknown Environments

S

151

G

Figure 4.22 An environment in which a random walk will take exponentially many steps
to find the goal.

LRTA*

OPTIMISM UNDER
UNCERTAINTY

current state; preference can be given to actions that have not yet been tried. It is easy to
prove that a random walk will eventually find a goal or complete its exploration, provided
that the space is finite.14 On the other hand, the process can be very slow. Figure 4.22 shows
an environment in which a random walk will take exponentially many steps to find the goal
because, at each step, backward progress is twice as likely as forward progress. The example
is contrived, of course, but there are many real-world state spaces whose topology causes
these kinds of â€œtrapsâ€ for random walks.
Augmenting hill climbing with memory rather than randomness turns out to be a more
effective approach. The basic idea is to store a â€œcurrent best estimateâ€ H(s) of the cost to
reach the goal from each state that has been visited. H(s) starts out being just the heuristic
estimate h(s) and is updated as the agent gains experience in the state space. Figure 4.23
shows a simple example in a one-dimensional state space. In (a), the agent seems to be
stuck in a flat local minimum at the shaded state. Rather than staying where it is, the agent
should follow what seems to be the best path to the goal given the current cost estimates for
its neighbors. The estimated cost to reach the goal through a neighbor s is the cost to get
to s plus the estimated cost to get to a goal from thereâ€”that is, c(s, a, s ) + H(s ). In the
example, there are two actions, with estimated costs 1 + 9 and 1 + 2, so it seems best to move
right. Now, it is clear that the cost estimate of 2 for the shaded state was overly optimistic.
Since the best move cost 1 and led to a state that is at least 2 steps from a goal, the shaded
state must be at least 3 steps from a goal, so its H should be updated accordingly, as shown
in Figure 4.23(b). Continuing this process, the agent will move back and forth twice more,
updating H each time and â€œflattening outâ€ the local minimum until it escapes to the right.
An agent implementing this scheme, which is called learning real-time Aâˆ— (LRTAâˆ— ), is
shown in Figure 4.24. Like O NLINE -DFS-AGENT , it builds a map of the environment in
the result table. It updates the cost estimate for the state it has just left and then chooses the
â€œapparently bestâ€ move according to its current cost estimates. One important detail is that
actions that have not yet been tried in a state s are always assumed to lead immediately to the
goal with the least possible cost, namely h(s). This optimism under uncertainty encourages
the agent to explore new, possibly promising paths.
An LRTAâˆ— agent is guaranteed to find a goal in any finite, safely explorable environment.
Unlike Aâˆ— , however, it is not complete for infinite state spacesâ€”there are cases where it can be
led infinitely astray. It can explore an environment of n states in O(n2 ) steps in the worst case,
14

Random walks are complete on infinite one-dimensional and two-dimensional grids. On a three-dimensional
grid, the probability that the walk ever returns to the starting point is only about 0.3405 (Hughes, 1995).

152

Chapter 4.

(a)

(b)

(c)

(d)

(e)

1

1

1

1

1

8
8

8

8

8

1

1

1

1

1

9
9

9

9

9

1

1

1

1

1

2
3

3

5

5

1

1

1

1

1

2
2

4

4

5

1

1

1

1

1

Beyond Classical Search

4
4

4

4

4

1

1

1

1

1

3
3

3

3

3

1

1

1

1

1

Figure 4.23 Five iterations of LRTAâˆ— on a one-dimensional state space. Each state is
labeled with H(s), the current cost estimate to reach a goal, and each link is labeled with its
step cost. The shaded state marks the location of the agent, and the updated cost estimates at
each iteration are circled.

function LRTA*-AGENT(s  ) returns an action
inputs: s  , a percept that identifies the current state
persistent: result, a table, indexed by state and action, initially empty
H , a table of cost estimates indexed by state, initially empty
s, a, the previous state and action, initially null
if G OAL -T EST(s  ) then return stop
if s  is a new state (not in H ) then H [s  ] â† h(s  )
if s is not null
result[s, a] â† s 
H [s] â†
min LRTA*-C OST(s, b, result[s, b], H )
b âˆˆ A CTIONS(s)

a â† an action b in ACTIONS(s  ) that minimizes LRTA*-C OST (s  , b, result[s  , b], H )
s â† s
return a
function LRTA*-C OST(s, a, s  , H ) returns a cost estimate
if s  is undefined then return h(s)
else return c(s, a, s ) + H[s ]
Figure 4.24
LRTA*-AGENT selects an action according to the values of neighboring
states, which are updated as the agent moves about the state space.

Section 4.6.

Summary

153

but often does much better. The LRTAâˆ— agent is just one of a large family of online agents that
one can define by specifying the action selection rule and the update rule in different ways.
We discuss this family, developed originally for stochastic environments, in Chapter 21.

4.5.4 Learning in online search
The initial ignorance of online search agents provides several opportunities for learning. First,
the agents learn a â€œmapâ€ of the environmentâ€”more precisely, the outcome of each action in
each stateâ€”simply by recording each of their experiences. (Notice that the assumption of
deterministic environments means that one experience is enough for each action.) Second,
the local search agents acquire more accurate estimates of the cost of each state by using local
updating rules, as in LRTAâˆ—. In Chapter 21, we show that these updates eventually converge
to exact values for every state, provided that the agent explores the state space in the right
way. Once exact values are known, optimal decisions can be taken simply by moving to the
lowest-cost successorâ€”that is, pure hill climbing is then an optimal strategy.
If you followed our suggestion to trace the behavior of O NLINE -DFS-AGENT in the
environment of Figure 4.19, you will have noticed that the agent is not very bright. For
example, after it has seen that the Up action goes from (1,1) to (1,2), the agent still has no
idea that the Down action goes back to (1,1) or that the Up action also goes from (2,1) to
(2,2), from (2,2) to (2,3), and so on. In general, we would like the agent to learn that Up
increases the y-coordinate unless there is a wall in the way, that Down reduces it, and so on.
For this to happen, we need two things. First, we need a formal and explicitly manipulable
representation for these kinds of general rules; so far, we have hidden the information inside
the black box called the R ESULT function. Part III is devoted to this issue. Second, we need
algorithms that can construct suitable general rules from the specific observations made by
the agent. These are covered in Chapter 18.

4.6

S UMMARY
This chapter has examined search algorithms for problems beyond the â€œclassicalâ€ case of
finding the shortest path to a goal in an observable, deterministic, discrete environment.
â€¢ Local search methods such as hill climbing operate on complete-state formulations,
keeping only a small number of nodes in memory. Several stochastic algorithms have
been developed, including simulated annealing, which returns optimal solutions when
given an appropriate cooling schedule.
â€¢ Many local search methods apply also to problems in continuous spaces. Linear programming and convex optimization problems obey certain restrictions on the shape
of the state space and the nature of the objective function, and admit polynomial-time
algorithms that are often extremely efficient in practice.
â€¢ A genetic algorithm is a stochastic hill-climbing search in which a large population of
states is maintained. New states are generated by mutation and by crossover, which
combines pairs of states from the population.

154

Chapter 4.

Beyond Classical Search

â€¢ In nondeterministic environments, agents can apply AND â€“ OR search to generate contingent plans that reach the goal regardless of which outcomes occur during execution.
â€¢ When the environment is partially observable, the belief state represents the set of
possible states that the agent might be in.
â€¢ Standard search algorithms can be applied directly to belief-state space to solve sensorless problems, and belief-state AND â€“ OR search can solve general partially observable
problems. Incremental algorithms that construct solutions state-by-state within a belief
state are often more efficient.
â€¢ Exploration problems arise when the agent has no idea about the states and actions of
its environment. For safely explorable environments, online search agents can build a
map and find a goal if one exists. Updating heuristic estimates from experience provides
an effective method to escape from local minima.

B IBLIOGRAPHICAL

TABU SEARCH

HEAVY-TAILED
DISTRIBUTION

AND

H ISTORICAL N OTES

Local search techniques have a long history in mathematics and computer science. Indeed,
the Newtonâ€“Raphson method (Newton, 1671; Raphson, 1690) can be seen as a very efficient local search method for continuous spaces in which gradient information is available.
Brent (1973) is a classic reference for optimization algorithms that do not require such information. Beam search, which we have presented as a local search algorithm, originated
as a bounded-width variant of dynamic programming for speech recognition in the H ARPY
system (Lowerre, 1976). A related algorithm is analyzed in depth by Pearl (1984, Ch. 5).
The topic of local search was reinvigorated in the early 1990s by surprisingly good results for large constraint-satisfaction problems such as n-queens (Minton et al., 1992) and
logical reasoning (Selman et al., 1992) and by the incorporation of randomness, multiple
simultaneous searches, and other improvements. This renaissance of what Christos Papadimitriou has called â€œNew Ageâ€ algorithms also sparked increased interest among theoretical
computer scientists (Koutsoupias and Papadimitriou, 1992; Aldous and Vazirani, 1994). In
the field of operations research, a variant of hill climbing called tabu search has gained popularity (Glover and Laguna, 1997). This algorithm maintains a tabu list of k previously visited
states that cannot be revisited; as well as improving efficiency when searching graphs, this list
can allow the algorithm to escape from some local minima. Another useful improvement on
hill climbing is the S TAGE algorithm (Boyan and Moore, 1998). The idea is to use the local
maxima found by random-restart hill climbing to get an idea of the overall shape of the landscape. The algorithm fits a smooth surface to the set of local maxima and then calculates the
global maximum of that surface analytically. This becomes the new restart point. The algorithm has been shown to work in practice on hard problems. Gomes et al. (1998) showed that
the run times of systematic backtracking algorithms often have a heavy-tailed distribution,
which means that the probability of a very long run time is more than would be predicted if
the run times were exponentially distributed. When the run time distribution is heavy-tailed,
random restarts find a solution faster, on average, than a single run to completion.

Bibliographical and Historical Notes

EVOLUTION
STRATEGY

ARTIFICIAL LIFE

GENETIC
PROGRAMMING

155

Simulated annealing was first described by Kirkpatrick et al. (1983), who borrowed
directly from the Metropolis algorithm (which is used to simulate complex systems in
physics (Metropolis et al., 1953) and was supposedly invented at a Los Alamos dinner party).
Simulated annealing is now a field in itself, with hundreds of papers published every year.
Finding optimal solutions in continuous spaces is the subject matter of several fields,
including optimization theory, optimal control theory, and the calculus of variations. The
basic techniques are explained well by Bishop (1995); Press et al. (2007) cover a wide range
of algorithms and provide working software.
As Andrew Moore points out, researchers have taken inspiration for search and optimization algorithms from a wide variety of fields of study: metallurgy (simulated annealing),
biology (genetic algorithms), economics (market-based algorithms), entomology (ant colony
optimization), neurology (neural networks), animal behavior (reinforcement learning), mountaineering (hill climbing), and others.
Linear programming (LP) was first studied systematically by the Russian mathematician Leonid Kantorovich (1939). It was one of the first applications of computers; the simplex algorithm (Dantzig, 1949) is still used despite worst-case exponential complexity. Karmarkar (1984) developed the far more efficient family of interior-point methods, which was
shown to have polynomial complexity for the more general class of convex optimization problems by Nesterov and Nemirovski (1994). Excellent introductions to convex optimization are
provided by Ben-Tal and Nemirovski (2001) and Boyd and Vandenberghe (2004).
Work by Sewall Wright (1931) on the concept of a fitness landscape was an important precursor to the development of genetic algorithms. In the 1950s, several statisticians,
including Box (1957) and Friedman (1959), used evolutionary techniques for optimization
problems, but it wasnâ€™t until Rechenberg (1965) introduced evolution strategies to solve optimization problems for airfoils that the approach gained popularity. In the 1960s and 1970s,
John Holland (1975) championed genetic algorithms, both as a useful tool and as a method
to expand our understanding of adaptation, biological or otherwise (Holland, 1995). The artificial life movement (Langton, 1995) takes this idea one step further, viewing the products
of genetic algorithms as organisms rather than solutions to problems. Work in this field by
Hinton and Nowlan (1987) and Ackley and Littman (1991) has done much to clarify the implications of the Baldwin effect. For general background on evolution, we recommend Smith
and SzathmaÌry (1999), Ridley (2004), and Carroll (2007).
Most comparisons of genetic algorithms to other approaches (especially stochastic hill
climbing) have found that the genetic algorithms are slower to converge (Oâ€™Reilly and Oppacher, 1994; Mitchell et al., 1996; Juels and Wattenberg, 1996; Baluja, 1997). Such findings
are not universally popular within the GA community, but recent attempts within that community to understand population-based search as an approximate form of Bayesian learning
(see Chapter 20) might help close the gap between the field and its critics (Pelikan et al.,
1999). The theory of quadratic dynamical systems may also explain the performance of
GAs (Rabani et al., 1998). See Lohn et al. (2001) for an example of GAs applied to antenna
design, and Renner and Ekart (2003) for an application to computer-aided design.
The field of genetic programming is closely related to genetic algorithms. The principal difference is that the representations that are mutated and combined are programs rather

156

Chapter 4.

Beyond Classical Search

than bit strings. The programs are represented in the form of expression trees; the expressions
can be in a standard language such as Lisp or can be specially designed to represent circuits,
robot controllers, and so on. Crossover involves splicing together subtrees rather than substrings. This form of mutation guarantees that the offspring are well-formed expressions,
which would not be the case if programs were manipulated as strings.
Interest in genetic programming was spurred by John Kozaâ€™s work (Koza, 1992, 1994),
but it goes back at least to early experiments with machine code by Friedberg (1958) and
with finite-state automata by Fogel et al. (1966). As with genetic algorithms, there is debate
about the effectiveness of the technique. Koza et al. (1999) describe experiments in the use
of genetic programming to design circuit devices.
The journals Evolutionary Computation and IEEE Transactions on Evolutionary Computation cover genetic algorithms and genetic programming; articles are also found in Complex Systems, Adaptive Behavior, and Artificial Life. The main conference is the Genetic
and Evolutionary Computation Conference (GECCO). Good overview texts on genetic algorithms are given by Mitchell (1996), Fogel (2000), and Langdon and Poli (2002), and by the
free online book by Poli et al. (2008).
The unpredictability and partial observability of real environments were recognized
early on in robotics projects that used planning techniques, including Shakey (Fikes et al.,
1972) and F REDDY (Michie, 1974). The problems received more attention after the publication of McDermottâ€™s (1978a) influential article, Planning and Acting.
The first work to make explicit use of AND â€“ OR trees seems to have been Slagleâ€™s S AINT
program for symbolic integration, mentioned in Chapter 1. Amarel (1967) applied the idea
to propositional theorem proving, a topic discussed in Chapter 7, and introduced a search
algorithm similar to A ND -O R -G RAPH -S EARCH . The algorithm was further developed and
formalized by Nilsson (1971), who also described AOâˆ— â€”which, as its name suggests, finds
optimal solutions given an admissible heuristic. AOâˆ— was analyzed and improved by Martelli
and Montanari (1973). AOâˆ— is a top-down algorithm; a bottom-up generalization of Aâˆ— is
Aâˆ— LD, for Aâˆ— Lightest Derivation (Felzenszwalb and McAllester, 2007). Interest in AND â€“ OR
search has undergone a revival in recent years, with new algorithms for finding cyclic solutions (Jimenez and Torras, 2000; Hansen and Zilberstein, 2001) and new techniques inspired
by dynamic programming (Bonet and Geffner, 2005).
The idea of transforming partially observable problems into belief-state problems originated with Astrom (1965) for the much more complex case of probabilistic uncertainty (see
Chapter 17). Erdmann and Mason (1988) studied the problem of robotic manipulation without sensors, using a continuous form of belief-state search. They showed that it was possible
to orient a part on a table from an arbitrary initial position by a well-designed sequence of tilting actions. More practical methods, based on a series of precisely oriented diagonal barriers
across a conveyor belt, use the same algorithmic insights (Wiegley et al., 1996).
The belief-state approach was reinvented in the context of sensorless and partially observable search problems by Genesereth and Nourbakhsh (1993). Additional work was done
on sensorless problems in the logic-based planning community (Goldman and Boddy, 1996;
Smith and Weld, 1998). This work has emphasized concise representations for belief states,
as explained in Chapter 11. Bonet and Geffner (2000) introduced the first effective heuristics

Exercises

EULERIAN GRAPH

REAL-TIME SEARCH

157
for belief-state search; these were refined by Bryce et al. (2006). The incremental approach
to belief-state search, in which solutions are constructed incrementally for subsets of states
within each belief state, was studied in the planning literature by Kurien et al. (2002); several
new incremental algorithms were introduced for nondeterministic, partially observable problems by Russell and Wolfe (2005). Additional references for planning in stochastic, partially
observable environments appear in Chapter 17.
Algorithms for exploring unknown state spaces have been of interest for many centuries.
Depth-first search in a maze can be implemented by keeping oneâ€™s left hand on the wall; loops
can be avoided by marking each junction. Depth-first search fails with irreversible actions;
the more general problem of exploring Eulerian graphs (i.e., graphs in which each node has
equal numbers of incoming and outgoing edges) was solved by an algorithm due to Hierholzer
(1873). The first thorough algorithmic study of the exploration problem for arbitrary graphs
was carried out by Deng and Papadimitriou (1990), who developed a completely general
algorithm but showed that no bounded competitive ratio is possible for exploring a general
graph. Papadimitriou and Yannakakis (1991) examined the question of finding paths to a goal
in geometric path-planning environments (where all actions are reversible). They showed that
a small competitive ratio is achievable with square obstacles, but with general rectangular
obstacles no bounded ratio can be achieved. (See Figure 4.20.)
The LRTAâˆ— algorithm was developed by Korf (1990) as part of an investigation into
real-time search for environments in which the agent must act after searching for only a
fixed amount of time (a common situation in two-player games). LRTAâˆ— is in fact a special
case of reinforcement learning algorithms for stochastic environments (Barto et al., 1995). Its
policy of optimism under uncertaintyâ€”always head for the closest unvisited stateâ€”can result
in an exploration pattern that is less efficient in the uninformed case than simple depth-first
search (Koenig, 2000). Dasgupta et al. (1994) show that online iterative deepening search is
optimally efficient for finding a goal in a uniform tree with no heuristic information. Several informed variants on the LRTAâˆ— theme have been developed with different methods for
searching and updating within the known portion of the graph (Pemberton and Korf, 1992).
As yet, there is no good understanding of how to find goals with optimal efficiency when
using heuristic information.

E XERCISES
4.1

Give the name of the algorithm that results from each of the following special cases:

a. Local beam search with k = 1.
b. Local beam search with one initial state and no limit on the number of states retained.
c. Simulated annealing with T = 0 at all times (and omitting the termination test).
d. Simulated annealing with T = âˆž at all times.
e. Genetic algorithm with population size N = 1.

158

Chapter 4.

Beyond Classical Search

4.2 Exercise 3.16 considers the problem of building railway tracks under the assumption
that pieces fit exactly with no slack. Now consider the real problem, in which pieces donâ€™t
fit exactly but allow for up to 10 degrees of rotation to either side of the â€œproperâ€ alignment.
Explain how to formulate the problem so it could be solved by simulated annealing.
4.3 In this exercise, we explore the use of local search methods to solve TSPs of the type
defined in Exercise 3.30.
a. Implement and test a hill-climbing method to solve TSPs. Compare the results with optimal solutions obtained from the Aâˆ— algorithm with the MST heuristic (Exercise 3.30).
b. Repeat part (a) using a genetic algorithm instead of hill climbing. You may want to
consult LarranÌƒaga et al. (1999) for some suggestions for representations.
4.4 Generate a large number of 8-puzzle and 8-queens instances and solve them (where possible) by hill climbing (steepest-ascent and first-choice variants), hill climbing with random
restart, and simulated annealing. Measure the search cost and percentage of solved problems
and graph these against the optimal solution cost. Comment on your results.
4.5 The A ND -O R -G RAPH -S EARCH algorithm in Figure 4.11 checks for repeated states
only on the path from the root to the current state. Suppose that, in addition, the algorithm
were to store every visited state and check against that list. (See B READTH -F IRST-S EARCH
in Figure 3.11 for an example.) Determine the information that should be stored and how the
algorithm should use that information when a repeated state is found. (Hint: You will need to
distinguish at least between states for which a successful subplan was constructed previously
and states for which no subplan could be found.) Explain how to use labels, as defined in
Section 4.3.3, to avoid having multiple copies of subplans.
4.6 Explain precisely how to modify the A ND -O R -G RAPH -S EARCH algorithm to generate
a cyclic plan if no acyclic plan exists. You will need to deal with three issues: labeling the plan
steps so that a cyclic plan can point back to an earlier part of the plan, modifying O R -S EARCH
so that it continues to look for acyclic plans after finding a cyclic plan, and augmenting the
plan representation to indicate whether a plan is cyclic. Show how your algorithm works on
(a) the slippery vacuum world, and (b) the slippery, erratic vacuum world. You might wish to
use a computer implementation to check your results.
4.7 In Section 4.4.1 we introduced belief states to solve sensorless search problems. A
sequence of actions solves a sensorless problem if it maps every physical state in the initial
belief state b to a goal state. Suppose the agent knows hâˆ— (s), the true optimal cost of solving
the physical state s in the fully observable problem, for every state s in b. Find an admissible
heuristic h(b) for the sensorless problem in terms of these costs, and prove its admissibilty.
Comment on the accuracy of this heuristic on the sensorless vacuum problem of Figure 4.14.
How well does Aâˆ— perform?
4.8 This exercise explores subsetâ€“superset relations between belief states in sensorless or
partially observable environments.
a. Prove that if an action sequence is a solution for a belief state b, it is also a solution for
any subset of b. Can anything be said about supersets of b?

Exercises

159
b. Explain in detail how to modify graph search for sensorless problems to take advantage
of your answers in (a).
c. Explain in detail how to modify AND â€“ OR search for partially observable problems,
beyond the modifications you describe in (b).
4.9 On page 139 it was assumed that a given action would have the same cost when executed in any physical state within a given belief state. (This leads to a belief-state search
problem with well-defined step costs.) Now consider what happens when the assumption
does not hold. Does the notion of optimality still make sense in this context, or does it require
modification? Consider also various possible definitions of the â€œcostâ€ of executing an action
in a belief state; for example, we could use the minimum of the physical costs; or the maximum; or a cost interval with the lower bound being the minimum cost and the upper bound
being the maximum; or just keep the set of all possible costs for that action. For each of these,
explore whether Aâˆ— (with modifications if necessary) can return optimal solutions.
4.10 Consider the sensorless version of the erratic vacuum world. Draw the belief-state
space reachable from the initial belief state {1, 2, 3, 4, 5, 6, 7, 8}, and explain why the problem
is unsolvable.
4.11

We can turn the navigation problem in Exercise 3.7 into an environment as follows:

â€¢ The percept will be a list of the positions, relative to the agent, of the visible vertices.
The percept does not include the position of the robot! The robot must learn its own position from the map; for now, you can assume that each location has a different â€œview.â€
â€¢ Each action will be a vector describing a straight-line path to follow. If the path is
unobstructed, the action succeeds; otherwise, the robot stops at the point where its
path first intersects an obstacle. If the agent returns a zero motion vector and is at the
goal (which is fixed and known), then the environment teleports the agent to a random
location (not inside an obstacle).
â€¢ The performance measure charges the agent 1 point for each unit of distance traversed
and awards 1000 points each time the goal is reached.
a. Implement this environment and a problem-solving agent for it. After each teleportation, the agent will need to formulate a new problem, which will involve discovering its
current location.
b. Document your agentâ€™s performance (by having the agent generate suitable commentary
as it moves around) and report its performance over 100 episodes.
c. Modify the environment so that 30% of the time the agent ends up at an unintended
destination (chosen randomly from the other visible vertices if any; otherwise, no move
at all). This is a crude model of the motion errors of a real robot. Modify the agent
so that when such an error is detected, it finds out where it is and then constructs a
plan to get back to where it was and resume the old plan. Remember that sometimes
getting back to where it was might also fail! Show an example of the agent successfully
overcoming two successive motion errors and still reaching the goal.

160

Chapter 4.

Beyond Classical Search

d. Now try two different recovery schemes after an error: (1) head for the closest vertex on
the original route; and (2) replan a route to the goal from the new location. Compare the
performance of the three recovery schemes. Would the inclusion of search costs affect
the comparison?
e. Now suppose that there are locations from which the view is identical. (For example,
suppose the world is a grid with square obstacles.) What kind of problem does the agent
now face? What do solutions look like?
4.12 Suppose that an agent is in a 3 Ã— 3 maze environment like the one shown in Figure 4.19. The agent knows that its initial location is (1,1), that the goal is at (3,3), and that the
actions Up, Down, Left, Right have their usual effects unless blocked by a wall. The agent
does not know where the internal walls are. In any given state, the agent perceives the set of
legal actions; it can also tell whether the state is one it has visited before.
a. Explain how this online search problem can be viewed as an offline search in belief-state
space, where the initial belief state includes all possible environment configurations.
How large is the initial belief state? How large is the space of belief states?
b. How many distinct percepts are possible in the initial state?
c. Describe the first few branches of a contingency plan for this problem. How large
(roughly) is the complete plan?
Notice that this contingency plan is a solution for every possible environment fitting the given
description. Therefore, interleaving of search and execution is not strictly necessary even in
unknown environments.
4.13 In this exercise, we examine hill climbing in the context of robot navigation, using the
environment in Figure 3.31 as an example.
a. Repeat Exercise 4.11 using hill climbing. Does your agent ever get stuck in a local
minimum? Is it possible for it to get stuck with convex obstacles?
b. Construct a nonconvex polygonal environment in which the agent gets stuck.
c. Modify the hill-climbing algorithm so that, instead of doing a depth-1 search to decide
where to go next, it does a depth-k search. It should find the best k-step path and do
one step along it, and then repeat the process.
d. Is there some k for which the new algorithm is guaranteed to escape from local minima?
e. Explain how LRTAâˆ— enables the agent to escape from local minima in this case.
4.14 Like DFS, online DFS is incomplete for reversible state spaces with infinite paths. For
example, suppose that states are points on the infinite two-dimensional grid and actions are
unit vectors (1, 0), (0, 1), (âˆ’1, 0), (0, âˆ’1), tried in that order. Show that online DFS starting
at (0, 0) will not reach (1, âˆ’1). Suppose the agent can observe, in addition to its current
state, all successor states and the actions that would lead to them. Write an algorithm that
is complete even for bidirected state spaces with infinite paths. What states does it visit in
reaching (1, âˆ’1)?

5

ADVERSARIAL SEARCH

In which we examine the problems that arise when we try to plan ahead in a world
where other agents are planning against us.

5.1

G AMES

GAME

ZERO-SUM GAMES
PERFECT
INFORMATION

Chapter 2 introduced multiagent environments, in which each agent needs to consider the
actions of other agents and how they affect its own welfare. The unpredictability of these
other agents can introduce contingencies into the agentâ€™s problem-solving process, as discussed in Chapter 4. In this chapter we cover competitive environments, in which the agentsâ€™
goals are in conflict, giving rise to adversarial search problemsâ€”often known as games.
Mathematical game theory, a branch of economics, views any multiagent environment
as a game, provided that the impact of each agent on the others is â€œsignificant,â€ regardless
of whether the agents are cooperative or competitive. 1 In AI, the most common games are
of a rather specialized kindâ€”what game theorists call deterministic, turn-taking, two-player,
zero-sum games of perfect information (such as chess). In our terminology, this means
deterministic, fully observable environments in which two agents act alternately and in which
the utility values at the end of the game are always equal and opposite. For example, if one
player wins a game of chess, the other player necessarily loses. It is this opposition between
the agentsâ€™ utility functions that makes the situation adversarial.
Games have engaged the intellectual faculties of humansâ€”sometimes to an alarming
degreeâ€”for as long as civilization has existed. For AI researchers, the abstract nature of
games makes them an appealing subject for study. The state of a game is easy to represent,
and agents are usually restricted to a small number of actions whose outcomes are defined by
precise rules. Physical games, such as croquet and ice hockey, have much more complicated
descriptions, a much larger range of possible actions, and rather imprecise rules defining
the legality of actions. With the exception of robot soccer, these physical games have not
attracted much interest in the AI community.
1

Environments with very many agents are often viewed as economies rather than games.

161

162

PRUNING

IMPERFECT
INFORMATION

TERMINAL TEST
TERMINAL STATES

GAME TREE

Chapter

5.

Adversarial Search

Games, unlike most of the toy problems studied in Chapter 3, are interesting because
they are too hard to solve. For example, chess has an average branching factor of about 35,
and games often go to 50 moves by each player, so the search tree has about 35100 or 10154
nodes (although the search graph has â€œonlyâ€ about 1040 distinct nodes). Games, like the real
world, therefore require the ability to make some decision even when calculating the optimal
decision is infeasible. Games also penalize inefficiency severely. Whereas an implementation
of Aâˆ— search that is half as efficient will simply take twice as long to run to completion, a chess
program that is half as efficient in using its available time probably will be beaten into the
ground, other things being equal. Game-playing research has therefore spawned a number of
interesting ideas on how to make the best possible use of time.
We begin with a definition of the optimal move and an algorithm for finding it. We
then look at techniques for choosing a good move when time is limited. Pruning allows us
to ignore portions of the search tree that make no difference to the final choice, and heuristic
evaluation functions allow us to approximate the true utility of a state without doing a complete search. Section 5.5 discusses games such as backgammon that include an element of
chance; we also discuss bridge, which includes elements of imperfect information because
not all cards are visible to each player. Finally, we look at how state-of-the-art game-playing
programs fare against human opposition and at directions for future developments.
We first consider games with two players, whom we call MAX and MIN for reasons that
will soon become obvious. MAX moves first, and then they take turns moving until the game
is over. At the end of the game, points are awarded to the winning player and penalties are
given to the loser. A game can be formally defined as a kind of search problem with the
following elements:
â€¢
â€¢
â€¢
â€¢
â€¢

S0 : The initial state, which specifies how the game is set up at the start.
P LAYER (s): Defines which player has the move in a state.
ACTIONS (s): Returns the set of legal moves in a state.
R ESULT (s, a): The transition model, which defines the result of a move.
T ERMINAL-T EST (s): A terminal test, which is true when the game is over and false
otherwise. States where the game has ended are called terminal states.
â€¢ U TILITY (s, p): A utility function (also called an objective function or payoff function),
defines the final numeric value for a game that ends in terminal state s for a player p. In
chess, the outcome is a win, loss, or draw, with values +1, 0, or 12 . Some games have a
wider variety of possible outcomes; the payoffs in backgammon range from 0 to +192.
A zero-sum game is (confusingly) defined as one where the total payoff to all players
is the same for every instance of the game. Chess is zero-sum because every game has
payoff of either 0 + 1, 1 + 0 or 12 + 12 . â€œConstant-sumâ€ would have been a better term,
but zero-sum is traditional and makes sense if you imagine each player is charged an
entry fee of 12 .

The initial state, ACTIONS function, and R ESULT function define the game tree for the
gameâ€”a tree where the nodes are game states and the edges are moves. Figure 5.1 shows
part of the game tree for tic-tac-toe (noughts and crosses). From the initial state, MAX has
nine possible moves. Play alternates between MAXâ€™s placing an X and MINâ€™s placing an O

Section 5.2.

SEARCH TREE

Optimal Decisions in Games

163

until we reach leaf nodes corresponding to terminal states such that one player has three in
a row or all the squares are filled. The number on each leaf node indicates the utility value
of the terminal state from the point of view of MAX; high values are assumed to be good for
MAX and bad for MIN (which is how the players get their names).
For tic-tac-toe the game tree is relatively smallâ€”fewer than 9! = 362, 880 terminal
nodes. But for chess there are over 1040 nodes, so the game tree is best thought of as a
theoretical construct that we cannot realize in the physical world. But regardless of the size
of the game tree, it is MAXâ€™s job to search for a good move. We use the term search tree for a
tree that is superimposed on the full game tree, and examines enough nodes to allow a player
to determine what move to make.
MAX (X)

MIN (O)

X

X

X
X

X

X
X

MAX (X)

MIN (O)

TERMINAL
Utility

XO

X

X
O

...

X O X

X O
X

X O
X

...

...

...

...

...

O

X O X
O X
O

X O X
O O X
X X O

X O X
X
X O O

â€“1

0

+1

X

X

...

Figure 5.1 A (partial) game tree for the game of tic-tac-toe. The top node is the initial
state, and MAX moves first, placing an X in an empty square. We show part of the tree, giving
alternating moves by MIN ( O ) and MAX ( X ), until we eventually reach terminal states, which
can be assigned utilities according to the rules of the game.

5.2

STRATEGY

O PTIMAL D ECISIONS IN G AMES
In a normal search problem, the optimal solution would be a sequence of actions leading to
a goal stateâ€”a terminal state that is a win. In adversarial search, MIN has something to say
about it. MAX therefore must find a contingent strategy, which specifies MAXâ€™s move in
the initial state, then MAXâ€™s moves in the states resulting from every possible response by

164

Chapter
MAX

5.

Adversarial Search

3 A
a1
3 B

MIN
b1

2 C
b3

c1

b2

3

12

a3

a2

2 D
c3

d1

c2

8

2

d3
d2

4

6

14

5

2

Figure 5.2 A two-ply game tree. The  nodes are â€œ MAX nodes,â€ in which it is MAXâ€™s
turn to move, and the  nodes are â€œ MIN nodes.â€ The terminal nodes show the utility values
for MAX; the other nodes are labeled with their minimax values. MAXâ€™s best move at the root
is a1 , because it leads to the state with the highest minimax value, and MINâ€™s best reply is b1 ,
because it leads to the state with the lowest minimax value.
MIN,

PLY
MINIMAX VALUE

then MAXâ€™s moves in the states resulting from every possible response by MIN to those
moves, and so on. This is exactly analogous to the AND â€“ OR search algorithm (Figure 4.11)
with MAX playing the role of OR and MIN equivalent to AND. Roughly speaking, an optimal
strategy leads to outcomes at least as good as any other strategy when one is playing an
infallible opponent. We begin by showing how to find this optimal strategy.
Even a simple game like tic-tac-toe is too complex for us to draw the entire game tree
on one page, so we will switch to the trivial game in Figure 5.2. The possible moves for MAX
at the root node are labeled a1 , a2 , and a3 . The possible replies to a1 for MIN are b1 , b2 ,
b3 , and so on. This particular game ends after one move each by MAX and MIN. (In game
parlance, we say that this tree is one move deep, consisting of two half-moves, each of which
is called a ply.) The utilities of the terminal states in this game range from 2 to 14.
Given a game tree, the optimal strategy can be determined from the minimax value
of each node, which we write as M INIMAX (n). The minimax value of a node is the utility
(for MAX) of being in the corresponding state, assuming that both players play optimally
from there to the end of the game. Obviously, the minimax value of a terminal state is just
its utility. Furthermore, given a choice, M AX prefers to move to a state of maximum value,
whereas M IN prefers a state of minimum value. So we have the following:
M INIMAX (s) =
âŽ§
if T ERMINAL-T EST (s)
âŽ¨ U TILITY (s)
maxaâˆˆActions(s) M INIMAX (R ESULT (s, a)) if P LAYER (s) = MAX
âŽ©
minaâˆˆActions(s) M INIMAX (R ESULT (s, a)) if P LAYER (s) = MIN
Let us apply these definitions to the game tree in Figure 5.2. The terminal nodes on the bottom
level get their utility values from the gameâ€™s U TILITY function. The first MIN node, labeled
B, has three successor states with values 3, 12, and 8, so its minimax value is 3. Similarly,
the other two MIN nodes have minimax value 2. The root node is a MAX node; its successor
states have minimax values 3, 2, and 2; so it has a minimax value of 3. We can also identify

Section 5.2.
MINIMAX DECISION

Optimal Decisions in Games

165

the minimax decision at the root: action a1 is the optimal choice for MAX because it leads to
the state with the highest minimax value.
This definition of optimal play for MAX assumes that MIN also plays optimallyâ€”it
maximizes the worst-case outcome for MAX. What if MIN does not play optimally? Then it is
easy to show (Exercise 5.7) that MAX will do even better. Other strategies against suboptimal
opponents may do better than the minimax strategy, but these strategies necessarily do worse
against optimal opponents.

5.2.1 The minimax algorithm
MINIMAX ALGORITHM

The minimax algorithm (Figure 5.3) computes the minimax decision from the current state.
It uses a simple recursive computation of the minimax values of each successor state, directly
implementing the defining equations. The recursion proceeds all the way down to the leaves
of the tree, and then the minimax values are backed up through the tree as the recursion
unwinds. For example, in Figure 5.2, the algorithm first recurses down to the three bottomleft nodes and uses the U TILITY function on them to discover that their values are 3, 12, and
8, respectively. Then it takes the minimum of these values, 3, and returns it as the backedup value of node B. A similar process gives the backed-up values of 2 for C and 2 for D.
Finally, we take the maximum of 3, 2, and 2 to get the backed-up value of 3 for the root node.
The minimax algorithm performs a complete depth-first exploration of the game tree.
If the maximum depth of the tree is m and there are b legal moves at each point, then the
time complexity of the minimax algorithm is O(b m ). The space complexity is O(bm) for an
algorithm that generates all actions at once, or O(m) for an algorithm that generates actions
one at a time (see page 87). For real games, of course, the time cost is totally impractical,
but this algorithm serves as the basis for the mathematical analysis of games and for more
practical algorithms.

5.2.2 Optimal decisions in multiplayer games
Many popular games allow more than two players. Let us examine how to extend the minimax
idea to multiplayer games. This is straightforward from the technical viewpoint, but raises
some interesting new conceptual issues.
First, we need to replace the single value for each node with a vector of values. For
example, in a three-player game with players A, B, and C, a vector vA , vB , vC  is associated
with each node. For terminal states, this vector gives the utility of the state from each playerâ€™s
viewpoint. (In two-player, zero-sum games, the two-element vector can be reduced to a single
value because the values are always opposite.) The simplest way to implement this is to have
the U TILITY function return a vector of utilities.
Now we have to consider nonterminal states. Consider the node marked X in the game
tree shown in Figure 5.4. In that state, player C chooses what to do. The two choices lead
to terminal states with utility vectors vA = 1, vB = 2, vC = 6 and vA = 4, vB = 2, vC = 3.
Since 6 is bigger than 3, C should choose the first move. This means that if state X is reached,
subsequent play will lead to a terminal state with utilities vA = 1, vB = 2, vC = 6. Hence,
the backed-up value of X is this vector. The backed-up value of a node n is always the utility

166

Chapter

5.

Adversarial Search

function M INIMAX -D ECISION(state) returns an action
return arg maxa âˆˆ ACTIONS(s) M IN -VALUE(R ESULT(state, a))
function M AX -VALUE(state) returns a utility value
if T ERMINAL -T EST(state) then return U TILITY(state)
v â† âˆ’âˆž
for each a in ACTIONS (state) do
v â† M AX(v , M IN -VALUE(R ESULT(s, a)))
return v
function M IN -VALUE(state) returns a utility value
if T ERMINAL -T EST(state) then return U TILITY(state)
v â†âˆž
for each a in ACTIONS (state) do
v â† M IN(v , M AX -VALUE(R ESULT(s, a)))
return v
Figure 5.3 An algorithm for calculating minimax decisions. It returns the action corresponding to the best possible move, that is, the move that leads to the outcome with the
best utility, under the assumption that the opponent plays to minimize utility. The functions
M AX -VALUE and M IN -VALUE go through the whole game tree, all the way to the leaves,
to determine the backed-up value of a state. The notation argmaxa âˆˆ S f (a) computes the
element a of set S that has the maximum value of f (a).

to move
A

(1, 2, 6)

B
C

(1, 2, 6)

(1, 2, 6)

(1, 5, 2)

(6, 1, 2)

X

(1, 5, 2)

(5, 4, 5)

A
(1, 2, 6)

(4, 2, 3)

(6, 1, 2)

(7, 4,1)

(5,1,1)

(1, 5, 2)

(7, 7,1)

(5, 4, 5)

Figure 5.4 The first three plies of a game tree with three players (A, B, C). Each node is
labeled with values from the viewpoint of each player. The best move is marked at the root.

ALLIANCE

vector of the successor state with the highest value for the player choosing at n. Anyone
who plays multiplayer games, such as Diplomacy, quickly becomes aware that much more
is going on than in two-player games. Multiplayer games usually involve alliances, whether
formal or informal, among the players. Alliances are made and broken as the game proceeds.
How are we to understand such behavior? Are alliances a natural consequence of optimal
strategies for each player in a multiplayer game? It turns out that they can be. For example,

Section 5.3.

Alphaâ€“Beta Pruning

167

suppose A and B are in weak positions and C is in a stronger position. Then it is often
optimal for both A and B to attack C rather than each other, lest C destroy each of them
individually. In this way, collaboration emerges from purely selfish behavior. Of course,
as soon as C weakens under the joint onslaught, the alliance loses its value, and either A
or B could violate the agreement. In some cases, explicit alliances merely make concrete
what would have happened anyway. In other cases, a social stigma attaches to breaking an
alliance, so players must balance the immediate advantage of breaking an alliance against the
long-term disadvantage of being perceived as untrustworthy. See Section 17.5 for more on
these complications.
If the game is not zero-sum, then collaboration can also occur with just two players.
Suppose, for example, that there is a terminal state with utilities vA = 1000, vB = 1000 and
that 1000 is the highest possible utility for each player. Then the optimal strategy is for both
players to do everything possible to reach this stateâ€”that is, the players will automatically
cooperate to achieve a mutually desirable goal.

5.3

ALPHAâ€“BETA
PRUNING

A LPHA â€“B ETA P RUNING
The problem with minimax search is that the number of game states it has to examine is
exponential in the depth of the tree. Unfortunately, we canâ€™t eliminate the exponent, but it
turns out we can effectively cut it in half. The trick is that it is possible to compute the correct
minimax decision without looking at every node in the game tree. That is, we can borrow the
idea of pruning from Chapter 3 to eliminate large parts of the tree from consideration. The
particular technique we examine is called alphaâ€“beta pruning. When applied to a standard
minimax tree, it returns the same move as minimax would, but prunes away branches that
cannot possibly influence the final decision.
Consider again the two-ply game tree from Figure 5.2. Letâ€™s go through the calculation
of the optimal decision once more, this time paying careful attention to what we know at
each point in the process. The steps are explained in Figure 5.5. The outcome is that we can
identify the minimax decision without ever evaluating two of the leaf nodes.
Another way to look at this is as a simplification of the formula for M INIMAX . Let the
two unevaluated successors of node C in Figure 5.5 have values x and y. Then the value of
the root node is given by
M INIMAX (root ) = max(min(3, 12, 8), min(2, x, y), min(14, 5, 2))
= max(3, min(2, x, y), 2)
= max(3, z, 2)

where z = min(2, x, y) â‰¤ 2

= 3.
In other words, the value of the root and hence the minimax decision are independent of the
values of the pruned leaves x and y.
Alphaâ€“beta pruning can be applied to trees of any depth, and it is often possible to
prune entire subtrees rather than just leaves. The general principle is this: consider a node n

168

Chapter
[âˆ’âˆž, +âˆž]

(a)
[âˆ’âˆž, 3]

(b)

A

[âˆ’âˆž, 3]

B

3

3
[3, +âˆž]

(c)
[3, 3]

B

3

12

8

(e)
[3, 3]

B

3

12

8

[3, 14]

A

[âˆ’âˆž, 2]

C

2

A

[3, +âˆž]

A

[âˆ’âˆž, 2]

C

B

12

[3, 3]

B

3

12

8

(f)
[âˆ’âˆž, 14]

14

D

Adversarial Search

[âˆ’âˆž, +âˆž]

(d)

A

5.

[3, 3]

B

3

12

8

2
[3, 3]

A

[âˆ’âˆž, 2]

C

2

[2, 2]

D

14

5

2

Figure 5.5 Stages in the calculation of the optimal decision for the game tree in Figure 5.2.
At each point, we show the range of possible values for each node. (a) The first leaf below B
has the value 3. Hence, B, which is a MIN node, has a value of at most 3. (b) The second leaf
below B has a value of 12; MIN would avoid this move, so the value of B is still at most 3.
(c) The third leaf below B has a value of 8; we have seen all Bâ€™s successor states, so the
value of B is exactly 3. Now, we can infer that the value of the root is at least 3, because
MAX has a choice worth 3 at the root. (d) The first leaf below C has the value 2. Hence,
C, which is a MIN node, has a value of at most 2. But we know that B is worth 3, so MAX
would never choose C. Therefore, there is no point in looking at the other successor states
of C. This is an example of alphaâ€“beta pruning. (e) The first leaf below D has the value 14,
so D is worth at most 14. This is still higher than MAXâ€™s best alternative (i.e., 3), so we need
to keep exploring Dâ€™s successor states. Notice also that we now have bounds on all of the
successors of the root, so the rootâ€™s value is also at most 14. (f) The second successor of D
is worth 5, so again we need to keep exploring. The third successor is worth 2, so now D is
worth exactly 2. MAXâ€™s decision at the root is to move to B, giving a value of 3.

somewhere in the tree (see Figure 5.6), such that Player has a choice of moving to that node.
If Player has a better choice m either at the parent node of n or at any choice point further up,
then n will never be reached in actual play. So once we have found out enough about n (by
examining some of its descendants) to reach this conclusion, we can prune it.
Remember that minimax search is depth-first, so at any one time we just have to consider the nodes along a single path in the tree. Alphaâ€“beta pruning gets its name from the
following two parameters that describe bounds on the backed-up values that appear anywhere
along the path:

Section 5.3.

Alphaâ€“Beta Pruning

169

Player

Opponent

m

â€¢
â€¢
â€¢
Player
Opponent

n

Figure 5.6 The general case for alphaâ€“beta pruning. If m is better than n for Player, we
will never get to n in play.

Î± = the value of the best (i.e., highest-value) choice we have found so far at any choice point
along the path for MAX.
Î² = the value of the best (i.e., lowest-value) choice we have found so far at any choice point
along the path for MIN.
Alphaâ€“beta search updates the values of Î± and Î² as it goes along and prunes the remaining
branches at a node (i.e., terminates the recursive call) as soon as the value of the current
node is known to be worse than the current Î± or Î² value for MAX or MIN, respectively. The
complete algorithm is given in Figure 5.7. We encourage you to trace its behavior when
applied to the tree in Figure 5.5.

5.3.1 Move ordering
The effectiveness of alphaâ€“beta pruning is highly dependent on the order in which the states
are examined. For example, in Figure 5.5(e) and (f), we could not prune any successors of D
at all because the worst successors (from the point of view of MIN) were generated first. If
the third successor of D had been generated first, we would have been able to prune the other
two. This suggests that it might be worthwhile to try to examine first the successors that are
likely to be best.
If this can be done,2 then it turns out that alphaâ€“beta needs to examine only O(bm/2 )
m
nodes to pick the best move,
âˆš instead of O(b ) for minimax. This means that the effective
branching factor becomes b instead of bâ€”for chess, about 6 instead of 35. Put another
way, alphaâ€“beta can solve a tree roughly twice as deep as minimax in the same amount of
time. If successors are examined in random order rather than best-first, the total number of
nodes examined will be roughly O(b3m/4 ) for moderate b. For chess, a fairly simple ordering
function (such as trying captures first, then threats, then forward moves, and then backward
moves) gets you to within about a factor of 2 of the best-case O(bm/2 ) result.
2

Obviously, it cannot be done perfectly; otherwise, the ordering function could be used to play a perfect game!

170

Chapter

5.

Adversarial Search

function A LPHA -B ETA -S EARCH(state) returns an action
v â† M AX -VALUE(state, âˆ’âˆž, +âˆž)
return the action in ACTIONS (state) with value v
function M AX -VALUE(state, Î±, Î²) returns a utility value
if T ERMINAL -T EST(state) then return U TILITY(state)
v â† âˆ’âˆž
for each a in ACTIONS (state) do
v â† M AX(v , M IN -VALUE(R ESULT(s,a), Î±, Î²))
if v â‰¥ Î² then return v
Î± â† M AX(Î±, v )
return v
function M IN -VALUE(state, Î±, Î²) returns a utility value
if T ERMINAL -T EST(state) then return U TILITY(state)
v â† +âˆž
for each a in ACTIONS (state) do
v â† M IN(v , M AX -VALUE(R ESULT(s,a) , Î±, Î²))
if v â‰¤ Î± then return v
Î² â† M IN(Î², v )
return v
Figure 5.7 The alphaâ€“beta search algorithm. Notice that these routines are the same as
the M INIMAX functions in Figure 5.3, except for the two lines in each of M IN -VALUE and
M AX -VALUE that maintain Î± and Î² (and the bookkeeping to pass these parameters along).

KILLER MOVES

TRANSPOSITION

TRANSPOSITION
TABLE

Adding dynamic move-ordering schemes, such as trying first the moves that were found
to be best in the past, brings us quite close to the theoretical limit. The past could be the
previous moveâ€”often the same threats remainâ€”or it could come from previous exploration
of the current move. One way to gain information from the current move is with iterative
deepening search. First, search 1 ply deep and record the best path of moves. Then search
1 ply deeper, but use the recorded path to inform move ordering. As we saw in Chapter 3,
iterative deepening on an exponential game tree adds only a constant fraction to the total
search time, which can be more than made up from better move ordering. The best moves are
often called killer moves and to try them first is called the killer move heuristic.
In Chapter 3, we noted that repeated states in the search tree can cause an exponential
increase in search cost. In many games, repeated states occur frequently because of transpositionsâ€”different permutations of the move sequence that end up in the same position. For
example, if White has one move, a1 , that can be answered by Black with b1 and an unrelated move a2 on the other side of the board that can be answered by b2 , then the sequences
[a1 , b1 , a2 , b2 ] and [a2 , b2 , a1 , b1 ] both end up in the same position. It is worthwhile to store
the evaluation of the resulting position in a hash table the first time it is encountered so that
we donâ€™t have to recompute it on subsequent occurrences. The hash table of previously seen
positions is traditionally called a transposition table; it is essentially identical to the explored

Section 5.4.

Imperfect Real-Time Decisions

171

list in G RAPH -S EARCH (Section 3.3). Using a transposition table can have a dramatic effect,
sometimes as much as doubling the reachable search depth in chess. On the other hand, if we
are evaluating a million nodes per second, at some point it is not practical to keep all of them
in the transposition table. Various strategies have been used to choose which nodes to keep
and which to discard.

5.4

I MPERFECT R EAL -T IME D ECISIONS

EVALUATION
FUNCTION

CUTOFF TEST

The minimax algorithm generates the entire game search space, whereas the alphaâ€“beta algorithm allows us to prune large parts of it. However, alphaâ€“beta still has to search all the way
to terminal states for at least a portion of the search space. This depth is usually not practical,
because moves must be made in a reasonable amount of timeâ€”typically a few minutes at
most. Claude Shannonâ€™s paper Programming a Computer for Playing Chess (1950) proposed
instead that programs should cut off the search earlier and apply a heuristic evaluation function to states in the search, effectively turning nonterminal nodes into terminal leaves. In
other words, the suggestion is to alter minimax or alphaâ€“beta in two ways: replace the utility
function by a heuristic evaluation function E VAL, which estimates the positionâ€™s utility, and
replace the terminal test by a cutoff test that decides when to apply E VAL. That gives us the
following for heuristic minimax for state s and maximum depth d:
H-M INIMAX (s, d) =
âŽ§
if C UTOFF -T EST (s, d)
âŽ¨ E VAL(s)
maxaâˆˆActions(s) H-M INIMAX (R ESULT (s, a), d + 1) if P LAYER (s) = MAX
âŽ©
minaâˆˆActions(s) H-M INIMAX (R ESULT (s, a), d + 1) if P LAYER (s) = MIN.

5.4.1 Evaluation functions
An evaluation function returns an estimate of the expected utility of the game from a given
position, just as the heuristic functions of Chapter 3 return an estimate of the distance to
the goal. The idea of an estimator was not new when Shannon proposed it. For centuries,
chess players (and aficionados of other games) have developed ways of judging the value of
a position because humans are even more limited in the amount of search they can do than
are computer programs. It should be clear that the performance of a game-playing program
depends strongly on the quality of its evaluation function. An inaccurate evaluation function
will guide an agent toward positions that turn out to be lost. How exactly do we design good
evaluation functions?
First, the evaluation function should order the terminal states in the same way as the
true utility function: states that are wins must evaluate better than draws, which in turn must
be better than losses. Otherwise, an agent using the evaluation function might err even if it
can see ahead all the way to the end of the game. Second, the computation must not take
too long! (The whole point is to search faster.) Third, for nonterminal states, the evaluation
function should be strongly correlated with the actual chances of winning.

172

EXPECTED VALUE

MATERIAL VALUE

WEIGHTED LINEAR
FUNCTION

Chapter

5.

Adversarial Search

One might well wonder about the phrase â€œchances of winning.â€ After all, chess is not a
game of chance: we know the current state with certainty, and no dice are involved. But if the
search must be cut off at nonterminal states, then the algorithm will necessarily be uncertain
about the final outcomes of those states. This type of uncertainty is induced by computational,
rather than informational, limitations. Given the limited amount of computation that the
evaluation function is allowed to do for a given state, the best it can do is make a guess about
the final outcome.
Let us make this idea more concrete. Most evaluation functions work by calculating
various features of the stateâ€”for example, in chess, we would have features for the number
of white pawns, black pawns, white queens, black queens, and so on. The features, taken
together, define various categories or equivalence classes of states: the states in each category
have the same values for all the features. For example, one category contains all two-pawn
vs. one-pawn endgames. Any given category, generally speaking, will contain some states
that lead to wins, some that lead to draws, and some that lead to losses. The evaluation
function cannot know which states are which, but it can return a single value that reflects the
proportion of states with each outcome. For example, suppose our experience suggests that
72% of the states encountered in the two-pawns vs. one-pawn category lead to a win (utility
+1); 20% to a loss (0), and 8% to a draw (1/2). Then a reasonable evaluation for states in
the category is the expected value: (0.72 Ã— +1) + (0.20 Ã— 0) + (0.08 Ã— 1/2) = 0.76. In
principle, the expected value can be determined for each category, resulting in an evaluation
function that works for any state. As with terminal states, the evaluation function need not
return actual expected values as long as the ordering of the states is the same.
In practice, this kind of analysis requires too many categories and hence too much
experience to estimate all the probabilities of winning. Instead, most evaluation functions
compute separate numerical contributions from each feature and then combine them to find
the total value. For example, introductory chess books give an approximate material value
for each piece: each pawn is worth 1, a knight or bishop is worth 3, a rook 5, and the queen 9.
Other features such as â€œgood pawn structureâ€ and â€œking safetyâ€ might be worth half a pawn,
say. These feature values are then simply added up to obtain the evaluation of the position.
A secure advantage equivalent to a pawn gives a substantial likelihood of winning, and
a secure advantage equivalent to three pawns should give almost certain victory, as illustrated
in Figure 5.8(a). Mathematically, this kind of evaluation function is called a weighted linear
function because it can be expressed as
n

E VAL(s) = w1 f1 (s) + w2 f2 (s) + Â· Â· Â· + wn fn (s) =
wi fi (s) ,
i=1

where each wi is a weight and each fi is a feature of the position. For chess, the fi could be
the numbers of each kind of piece on the board, and the wi could be the values of the pieces
(1 for pawn, 3 for bishop, etc.).
Adding up the values of features seems like a reasonable thing to do, but in fact it
involves a strong assumption: that the contribution of each feature is independent of the
values of the other features. For example, assigning the value 3 to a bishop ignores the fact
that bishops are more powerful in the endgame, when they have a lot of space to maneuver.

Section 5.4.

Imperfect Real-Time Decisions

(a) White to move

173

(b) White to move

Figure 5.8 Two chess positions that differ only in the position of the rook at lower right.
In (a), Black has an advantage of a knight and two pawns, which should be enough to win
the game. In (b), White will capture the queen, giving it an advantage that should be strong
enough to win.

For this reason, current programs for chess and other games also use nonlinear combinations
of features. For example, a pair of bishops might be worth slightly more than twice the value
of a single bishop, and a bishop is worth more in the endgame (that is, when the move number
feature is high or the number of remaining pieces feature is low).
The astute reader will have noticed that the features and weights are not part of the rules
of chess! They come from centuries of human chess-playing experience. In games where this
kind of experience is not available, the weights of the evaluation function can be estimated
by the machine learning techniques of Chapter 18. Reassuringly, applying these techniques
to chess has confirmed that a bishop is indeed worth about three pawns.

5.4.2 Cutting off search
The next step is to modify A LPHA -B ETA -S EARCH so that it will call the heuristic E VAL
function when it is appropriate to cut off the search. We replace the two lines in Figure 5.7
that mention T ERMINAL-T EST with the following line:
if C UTOFF -T EST (state, depth) then return E VAL(state)
We also must arrange for some bookkeeping so that the current depth is incremented on each
recursive call. The most straightforward approach to controlling the amount of search is to set
a fixed depth limit so that C UTOFF -T EST (state, depth) returns true for all depth greater than
some fixed depth d. (It must also return true for all terminal states, just as T ERMINAL-T EST
did.) The depth d is chosen so that a move is selected within the allocated time. A more
robust approach is to apply iterative deepening. (See Chapter 3.) When time runs out, the
program returns the move selected by the deepest completed search. As a bonus, iterative
deepening also helps with move ordering.

174

QUIESCENCE

QUIESCENCE
SEARCH

HORIZON EFFECT

SINGULAR
EXTENSION

Chapter

5.

Adversarial Search

These simple approaches can lead to errors due to the approximate nature of the evaluation function. Consider again the simple evaluation function for chess based on material
advantage. Suppose the program searches to the depth limit, reaching the position in Figure 5.8(b), where Black is ahead by a knight and two pawns. It would report this as the
heuristic value of the state, thereby declaring that the state is a probable win by Black. But
Whiteâ€™s next move captures Blackâ€™s queen with no compensation. Hence, the position is
really won for White, but this can be seen only by looking ahead one more ply.
Obviously, a more sophisticated cutoff test is needed. The evaluation function should be
applied only to positions that are quiescentâ€”that is, unlikely to exhibit wild swings in value
in the near future. In chess, for example, positions in which favorable captures can be made
are not quiescent for an evaluation function that just counts material. Nonquiescent positions
can be expanded further until quiescent positions are reached. This extra search is called a
quiescence search; sometimes it is restricted to consider only certain types of moves, such
as capture moves, that will quickly resolve the uncertainties in the position.
The horizon effect is more difficult to eliminate. It arises when the program is facing
an opponentâ€™s move that causes serious damage and is ultimately unavoidable, but can be
temporarily avoided by delaying tactics. Consider the chess game in Figure 5.9. It is clear
that there is no way for the black bishop to escape. For example, the white rook can capture
it by moving to h1, then a1, then a2; a capture at depth 6 ply. But Black does have a sequence
of moves that pushes the capture of the bishop â€œover the horizon.â€ Suppose Black searches
to depth 8 ply. Most moves by Black will lead to the eventual capture of the bishop, and thus
will be marked as â€œbadâ€ moves. But Black will consider checking the white king with the
pawn at e4. This will lead to the king capturing the pawn. Now Black will consider checking
again, with the pawn at f5, leading to another pawn capture. That takes up 4 ply, and from
there the remaining 4 ply is not enough to capture the bishop. Black thinks that the line of
play has saved the bishop at the price of two pawns, when actually all it has done is push the
inevitable capture of the bishop beyond the horizon that Black can see.
One strategy to mitigate the horizon effect is the singular extension, a move that is
â€œclearly betterâ€ than all other moves in a given position. Once discovered anywhere in the
tree in the course of a search, this singular move is remembered. When the search reaches the
normal depth limit, the algorithm checks to see if the singular extension is a legal move; if it
is, the algorithm allows the move to be considered. This makes the tree deeper, but because
there will be few singular extensions, it does not add many total nodes to the tree.

5.4.3 Forward pruning

FORWARD PRUNING

BEAM SEARCH

So far, we have talked about cutting off search at a certain level and about doing alphaâ€“
beta pruning that provably has no effect on the result (at least with respect to the heuristic
evaluation values). It is also possible to do forward pruning, meaning that some moves at
a given node are pruned immediately without further consideration. Clearly, most humans
playing chess consider only a few moves from each position (at least consciously). One
approach to forward pruning is beam search: on each ply, consider only a â€œbeamâ€ of the n
best moves (according to the evaluation function) rather than considering all possible moves.

Section 5.4.

Imperfect Real-Time Decisions

175

1
2
3
4
5
6
7
8
a

b

c

d

e

f

g

h

Figure 5.9 The horizon effect. With Black to move, the black bishop is surely doomed.
But Black can forestall that event by checking the white king with its pawns, forcing the king
to capture the pawns. This pushes the inevitable loss of the bishop over the horizon, and thus
the pawn sacrifices are seen by the search algorithm as good moves rather than bad ones.

Unfortunately, this approach is rather dangerous because there is no guarantee that the best
move will not be pruned away.
The P ROB C UT, or probabilistic cut, algorithm (Buro, 1995) is a forward-pruning version of alphaâ€“beta search that uses statistics gained from prior experience to lessen the chance
that the best move will be pruned. Alphaâ€“beta search prunes any node that is provably outside the current (Î±, Î²) window. P ROB C UT also prunes nodes that are probably outside the
window. It computes this probability by doing a shallow search to compute the backed-up
value v of a node and then using past experience to estimate how likely it is that a score of v
at depth d in the tree would be outside (Î±, Î²). Buro applied this technique to his Othello program, L OGISTELLO , and found that a version of his program with P ROB C UT beat the regular
version 64% of the time, even when the regular version was given twice as much time.
Combining all the techniques described here results in a program that can play creditable chess (or other games). Let us assume we have implemented an evaluation function for
chess, a reasonable cutoff test with a quiescence search, and a large transposition table. Let
us also assume that, after months of tedious bit-bashing, we can generate and evaluate around
a million nodes per second on the latest PC, allowing us to search roughly 200 million nodes
per move under standard time controls (three minutes per move). The branching factor for
chess is about 35, on average, and 355 is about 50 million, so if we used minimax search,
we could look ahead only about five plies. Though not incompetent, such a program can be
fooled easily by an average human chess player, who can occasionally plan six or eight plies
ahead. With alphaâ€“beta search we get to about 10 plies, which results in an expert level of
play. Section 5.8 describes additional pruning techniques that can extend the effective search
depth to roughly 14 plies. To reach grandmaster status we would need an extensively tuned
evaluation function and a large database of optimal opening and endgame moves.

176

Chapter

5.

Adversarial Search

5.4.4 Search versus lookup

POLICY

RETROGRADE

Somehow it seems like overkill for a chess program to start a game by considering a tree of a
billion game states, only to conclude that it will move its pawn to e4. Books describing good
play in the opening and endgame in chess have been available for about a century (Tattersall,
1911). It is not surprising, therefore, that many game-playing programs use table lookup
rather than search for the opening and ending of games.
For the openings, the computer is mostly relying on the expertise of humans. The best
advice of human experts on how to play each opening is copied from books and entered into
tables for the computerâ€™s use. However, computers can also gather statistics from a database
of previously played games to see which opening sequences most often lead to a win. In
the early moves there are few choices, and thus much expert commentary and past games on
which to draw. Usually after ten moves we end up in a rarely seen position, and the program
must switch from table lookup to search.
Near the end of the game there are again fewer possible positions, and thus more chance
to do lookup. But here it is the computer that has the expertise: computer analysis of
endgames goes far beyond anything achieved by humans. A human can tell you the general strategy for playing a king-and-rook-versus-king (KRK) endgame: reduce the opposing
kingâ€™s mobility by squeezing it toward one edge of the board, using your king to prevent the
opponent from escaping the squeeze. Other endings, such as king, bishop, and knight versus
king (KBNK), are difficult to master and have no succinct strategy description. A computer,
on the other hand, can completely solve the endgame by producing a policy, which is a mapping from every possible state to the best move in that state. Then we can just look up the best
move rather than recompute it anew. How big will the KBNK lookup table be? It turns out
there are 462 ways that two kings can be placed on the board without being adjacent. After
the kings are placed, there are 62 empty squares for the bishop, 61 for the knight, and two
possible players to move next, so there are just 462 Ã— 62 Ã— 61 Ã— 2 = 3, 494, 568 possible
positions. Some of these are checkmates; mark them as such in a table. Then do a retrograde
minimax search: reverse the rules of chess to do unmoves rather than moves. Any move by
White that, no matter what move Black responds with, ends up in a position marked as a win,
must also be a win. Continue this search until all 3,494,568 positions are resolved as win,
loss, or draw, and you have an infallible lookup table for all KBNK endgames.
Using this technique and a tour de force of optimization tricks, Ken Thompson (1986,
1996) and Lewis Stiller (1992, 1996) solved all chess endgames with up to five pieces and
some with six pieces, making them available on the Internet. Stiller discovered one case
where a forced mate existed but required 262 moves; this caused some consternation because
the rules of chess require a capture or pawn move to occur within 50 moves. Later work by
Marc Bourzutschky and Yakov Konoval (Bourzutschky, 2006) solved all pawnless six-piece
and some seven-piece endgames; there is a KQNKRBN endgame that with best play requires
517 moves until a capture, which then leads to a mate.
If we could extend the chess endgame tables from 6 pieces to 32, then White would
know on the opening move whether it would be a win, loss, or draw. This has not happened
so far for chess, but it has happened for checkers, as explained in the historical notes section.

Section 5.5.

5.5

Stochastic Games

177

S TOCHASTIC G AMES

STOCHASTIC GAMES

In real life, many unpredictable external events can put us into unforeseen situations. Many
games mirror this unpredictability by including a random element, such as the throwing of
dice. We call these stochastic games. Backgammon is a typical game that combines luck
and skill. Dice are rolled at the beginning of a playerâ€™s turn to determine the legal moves. In
the backgammon position of Figure 5.10, for example, White has rolled a 6â€“5 and has four
possible moves.
0

1

2

3

4

5

6

7

8

9

10

11

12

25

24

23

22

21

20

19

18

17

16

15

14

13

Figure 5.10 A typical backgammon position. The goal of the game is to move all oneâ€™s
pieces off the board. White moves clockwise toward 25, and Black moves counterclockwise
toward 0. A piece can move to any position unless multiple opponent pieces are there; if there
is one opponent, it is captured and must start over. In the position shown, White has rolled
6â€“5 and must choose among four legal moves: (5â€“10,5â€“11), (5â€“11,19â€“24), (5â€“10,10â€“16),
and (5â€“11,11â€“16), where the notation (5â€“11,11â€“16) means move one piece from position 5
to 11, and then move a piece from 11 to 16.

CHANCE NODES

Although White knows what his or her own legal moves are, White does not know what
Black is going to roll and thus does not know what Blackâ€™s legal moves will be. That means
White cannot construct a standard game tree of the sort we saw in chess and tic-tac-toe. A
game tree in backgammon must include chance nodes in addition to MAX and MIN nodes.
Chance nodes are shown as circles in Figure 5.11. The branches leading from each chance
node denote the possible dice rolls; each branch is labeled with the roll and its probability.
There are 36 ways to roll two dice, each equally likely; but because a 6â€“5 is the same as a 5â€“6,
there are only 21 distinct rolls. The six doubles (1â€“1 through 6â€“6) each have a probability of
1/36, so we say P (1â€“1) = 1/36. The other 15 distinct rolls each have a 1/18 probability.

178

Chapter

5.

Adversarial Search

MAX

CHANCE

...

B

...

...
1/36
1,1

...
1/18
6,5

1/18
1,2

MIN

...
...

...

CHANCE

...

...

C

...
1/36
1,1

TERMINAL

EXPECTIMINIMAX
VALUE

...

...

1/36
6,6

...
...

EXPECTED VALUE

1/18
6,5

1/18
1,2

MAX

Figure 5.11

...
1/36
6,6

...

2

...

â€“1

1

â€“1

1

Schematic game tree for a backgammon position.

The next step is to understand how to make correct decisions. Obviously, we still want
to pick the move that leads to the best position. However, positions do not have definite
minimax values. Instead, we can only calculate the expected value of a position: the average
over all possible outcomes of the chance nodes.
This leads us to generalize the minimax value for deterministic games to an expectiminimax value for games with chance nodes. Terminal nodes and MAX and MIN nodes (for
which the dice roll is known) work exactly the same way as before. For chance nodes we
compute the expected value, which is the sum of the value over all outcomes, weighted by
the probability of each chance action:
E XPECTIMINIMAX (s) =
âŽ§
U TILITY (s)
âŽª
âŽª
âŽ¨
maxa E XPECTIMINIMAX (R ESULT (s, a))
min E XPECTIMINIMAX (R ESULT (s, a))
âŽª
âŽª
âŽ© a
r P (r)E XPECTIMINIMAX (R ESULT (s, r))

if T ERMINAL-T EST (s)
if P LAYER (s) = MAX
if P LAYER (s) = MIN
if P LAYER (s) = CHANCE

where r represents a possible dice roll (or other chance event) and R ESULT (s, r) is the same
state as s, with the additional fact that the result of the dice roll is r.

5.5.1 Evaluation functions for games of chance
As with minimax, the obvious approximation to make with expectiminimax is to cut the
search off at some point and apply an evaluation function to each leaf. One might think that
evaluation functions for games such as backgammon should be just like evaluation functions

Section 5.5.

Stochastic Games

179

for chessâ€”they just need to give higher scores to better positions. But in fact, the presence of
chance nodes means that one has to be more careful about what the evaluation values mean.
Figure 5.12 shows what happens: with an evaluation function that assigns the values [1, 2,
3, 4] to the leaves, move a1 is best; with values [1, 20, 30, 400], move a2 is best. Hence,
the program behaves totally differently if we make a change in the scale of some evaluation
values! It turns out that to avoid this sensitivity, the evaluation function must be a positive
linear transformation of the probability of winning from a position (or, more generally, of the
expected utility of the position). This is an important and general property of situations in
which uncertainty is involved, and we discuss it further in Chapter 16.

MAX

a1

a1

2.1

CHANCE

1.3

.9
MIN

a2

.1

2

2

Figure 5.12

.9

3

2

3

3

1

21

.1

1

4

40.9

.9

4

1

a2

.1

20

4

20

.9

30

20

30

.1

1

30

1

400

1

400

400

An order-preserving transformation on leaf values changes the best move.

If the program knew in advance all the dice rolls that would occur for the rest of the
game, solving a game with dice would be just like solving a game without dice, which minimax does in O(bm ) time, where b is the branching factor and m is the maximum depth of the
game tree. Because expectiminimax is also considering all the possible dice-roll sequences,
it will take O(bm nm ), where n is the number of distinct rolls.
Even if the search depth is limited to some small depth d, the extra cost compared with
that of minimax makes it unrealistic to consider looking ahead very far in most games of
chance. In backgammon n is 21 and b is usually around 20, but in some situations can be as
high as 4000 for dice rolls that are doubles. Three plies is probably all we could manage.
Another way to think about the problem is this: the advantage of alphaâ€“beta is that
it ignores future developments that just are not going to happen, given best play. Thus, it
concentrates on likely occurrences. In games with dice, there are no likely sequences of
moves, because for those moves to take place, the dice would first have to come out the right
way to make them legal. This is a general problem whenever uncertainty enters the picture:
the possibilities are multiplied enormously, and forming detailed plans of action becomes
pointless because the world probably will not play along.
It may have occurred to you that something like alphaâ€“beta pruning could be applied

180

Chapter

MONTE CARLO
SIMULATION

ROLLOUT

5.6

5.

Adversarial Search

to game trees with chance nodes. It turns out that it can. The analysis for MIN and MAX
nodes is unchanged, but we can also prune chance nodes, using a bit of ingenuity. Consider
the chance node C in Figure 5.11 and what happens to its value as we examine and evaluate
its children. Is it possible to find an upper bound on the value of C before we have looked
at all its children? (Recall that this is what alphaâ€“beta needs in order to prune a node and its
subtree.) At first sight, it might seem impossible because the value of C is the average of its
childrenâ€™s values, and in order to compute the average of a set of numbers, we must look at
all the numbers. But if we put bounds on the possible values of the utility function, then we
can arrive at bounds for the average without looking at every number. For example, say that
all utility values are between âˆ’2 and +2; then the value of leaf nodes is bounded, and in turn
we can place an upper bound on the value of a chance node without looking at all its children.
An alternative is to do Monte Carlo simulation to evaluate a position. Start with
an alphaâ€“beta (or other) search algorithm. From a start position, have the algorithm play
thousands of games against itself, using random dice rolls. In the case of backgammon, the
resulting win percentage has been shown to be a good approximation of the value of the
position, even if the algorithm has an imperfect heuristic and is searching only a few plies
(Tesauro, 1995). For games with dice, this type of simulation is called a rollout.

PARTIALLY O BSERVABLE G AMES
Chess has often been described as war in miniature, but it lacks at least one major characteristic of real wars, namely, partial observability. In the â€œfog of war,â€ the existence and
disposition of enemy units is often unknown until revealed by direct contact. As a result,
warfare includes the use of scouts and spies to gather information and the use of concealment
and bluff to confuse the enemy. Partially observable games share these characteristics and
are thus qualitatively different from the games described in the preceding sections.

5.6.1 Kriegspiel: Partially observable chess

KRIEGSPIEL

In deterministic partially observable games, uncertainty about the state of the board arises entirely from lack of access to the choices made by the opponent. This class includes childrenâ€™s
games such as Battleships (where each playerâ€™s ships are placed in locations hidden from the
opponent but do not move) and Stratego (where piece locations are known but piece types are
hidden). We will examine the game of Kriegspiel, a partially observable variant of chess in
which pieces can move but are completely invisible to the opponent.
The rules of Kriegspiel are as follows: White and Black each see a board containing
only their own pieces. A referee, who can see all the pieces, adjudicates the game and periodically makes announcements that are heard by both players. On his turn, White proposes to
the referee any move that would be legal if there were no black pieces. If the move is in fact
not legal (because of the black pieces), the referee announces â€œillegal.â€ In this case, White
may keep proposing moves until a legal one is foundâ€”and learns more about the location of
Blackâ€™s pieces in the process. Once a legal move is proposed, the referee announces one or

Section 5.6.

GUARANTEED
CHECKMATE

PROBABILISTIC
CHECKMATE

Partially Observable Games

181

more of the following: â€œCapture on square Xâ€ if there is a capture, and â€œCheck by Dâ€ if the
black king is in check, where D is the direction of the check, and can be one of â€œKnight,â€
â€œRank,â€ â€œFile,â€ â€œLong diagonal,â€ or â€œShort diagonal.â€ (In case of discovered check, the referee may make two â€œCheckâ€ announcements.) If Black is checkmated or stalemated, the
referee says so; otherwise, it is Blackâ€™s turn to move.
Kriegspiel may seem terrifyingly impossible, but humans manage it quite well and computer programs are beginning to catch up. It helps to recall the notion of a belief state as
defined in Section 4.4 and illustrated in Figure 4.14â€”the set of all logically possible board
states given the complete history of percepts to date. Initially, Whiteâ€™s belief state is a singleton because Blackâ€™s pieces havenâ€™t moved yet. After White makes a move and Black responds, Whiteâ€™s belief state contains 20 positions because Black has 20 replies to any White
move. Keeping track of the belief state as the game progresses is exactly the problem of state
estimation, for which the update step is given in Equation (4.6). We can map Kriegspiel
state estimation directly onto the partially observable, nondeterministic framework of Section 4.4 if we consider the opponent as the source of nondeterminism; that is, the R ESULTS
of Whiteâ€™s move are composed from the (predictable) outcome of Whiteâ€™s own move and the
unpredictable outcome given by Blackâ€™s reply.3
Given a current belief state, White may ask, â€œCan I win the game?â€ For a partially
observable game, the notion of a strategy is altered; instead of specifying a move to make
for each possible move the opponent might make, we need a move for every possible percept
sequence that might be received. For Kriegspiel, a winning strategy, or guaranteed checkmate, is one that, for each possible percept sequence, leads to an actual checkmate for every
possible board state in the current belief state, regardless of how the opponent moves. With
this definition, the opponentâ€™s belief state is irrelevantâ€”the strategy has to work even if the
opponent can see all the pieces. This greatly simplifies the computation. Figure 5.13 shows
part of a guaranteed checkmate for the KRK (king and rook against king) endgame. In this
case, Black has just one piece (the king), so a belief state for White can be shown in a single
board by marking each possible position of the Black king.
The general AND - OR search algorithm can be applied to the belief-state space to find
guaranteed checkmates, just as in Section 4.4. The incremental belief-state algorithm mentioned in that section often finds midgame checkmates up to depth 9â€”probably well beyond
the abilities of human players.
In addition to guaranteed checkmates, Kriegspiel admits an entirely new concept that
makes no sense in fully observable games: probabilistic checkmate. Such checkmates are
still required to work in every board state in the belief state; they are probabilistic with respect
to randomization of the winning playerâ€™s moves. To get the basic idea, consider the problem
of finding a lone black king using just the white king. Simply by moving randomly, the
white king will eventually bump into the black king even if the latter tries to avoid this fate,
since Black cannot keep guessing the right evasive moves indefinitely. In the terminology of
probability theory, detection occurs with probability 1. The KBNK endgameâ€”king, bishop
3

Sometimes, the belief state will become too large to represent just as a list of board states, but we will ignore
this issue for now; Chapters 7 and 8 suggest methods for compactly representing very large belief states.

182

Chapter

5.

Adversarial Search

4
3
2
1
a

b

c

d

Kc3 ?
â€œOKâ€

â€œIllegalâ€

Rc3 ?
â€œOKâ€

â€œCheckâ€

Figure 5.13 Part of a guaranteed checkmate in the KRK endgame, shown on a reduced
board. In the initial belief state, Blackâ€™s king is in one of three possible locations. By a
combination of probing moves, the strategy narrows this down to one. Completion of the
checkmate is left as an exercise.

ACCIDENTAL
CHECKMATE

and knight against kingâ€”is won in this sense; White presents Black with an infinite random
sequence of choices, for one of which Black will guess incorrectly and reveal his position,
leading to checkmate. The KBBK endgame, on the other hand, is won with probability 1 âˆ’ .
White can force a win only by leaving one of his bishops unprotected for one move. If
Black happens to be in the right place and captures the bishop (a move that would lose if the
bishops are protected), the game is drawn. White can choose to make the risky move at some
randomly chosen point in the middle of a very long sequence, thus reducing  to an arbitrarily
small constant, but cannot reduce  to zero.
It is quite rare that a guaranteed or probabilistic checkmate can be found within any
reasonable depth, except in the endgame. Sometimes a checkmate strategy works for some of
the board states in the current belief state but not others. Trying such a strategy may succeed,
leading to an accidental checkmateâ€”accidental in the sense that White could not know that
it would be checkmateâ€”if Blackâ€™s pieces happen to be in the right places. (Most checkmates
in games between humans are of this accidental nature.) This idea leads naturally to the
question of how likely it is that a given strategy will win, which leads in turn to the question
of how likely it is that each board state in the current belief state is the true board state.

Section 5.6.

Partially Observable Games

183

Oneâ€™s first inclination might be to propose that all board states in the current belief state
are equally likelyâ€”but this canâ€™t be right. Consider, for example, Whiteâ€™s belief state after
Blackâ€™s first move of the game. By definition (assuming that Black plays optimally), Black
must have played an optimal move, so all board states resulting from suboptimal moves ought
to be assigned zero probability. This argument is not quite right either, because each playerâ€™s
goal is not just to move pieces to the right squares but also to minimize the information that
the opponent has about their location. Playing any predictable â€œoptimalâ€ strategy provides
the opponent with information. Hence, optimal play in partially observable games requires
a willingness to play somewhat randomly. (This is why restaurant hygiene inspectors do
random inspection visits.) This means occasionally selecting moves that may seem â€œintrinsicallyâ€ weakâ€”but they gain strength from their very unpredictability, because the opponent is
unlikely to have prepared any defense against them.
From these considerations, it seems that the probabilities associated with the board
states in the current belief state can only be calculated given an optimal randomized strategy; in turn, computing that strategy seems to require knowing the probabilities of the various states the board might be in. This conundrum can be resolved by adopting the gametheoretic notion of an equilibrium solution, which we pursue further in Chapter 17. An
equilibrium specifies an optimal randomized strategy for each player. Computing equilibria is prohibitively expensive, however, even for small games, and is out of the question for
Kriegspiel. At present, the design of effective algorithms for general Kriegspiel play is an
open research topic. Most systems perform bounded-depth lookahead in their own beliefstate space, ignoring the opponentâ€™s belief state. Evaluation functions resemble those for the
observable game but include a component for the size of the belief stateâ€”smaller is better!

5.6.2 Card games
Card games provide many examples of stochastic partial observability, where the missing
information is generated randomly. For example, in many games, cards are dealt randomly at
the beginning of the game, with each player receiving a hand that is not visible to the other
players. Such games include bridge, whist, hearts, and some forms of poker.
At first sight, it might seem that these card games are just like dice games: the cards are
dealt randomly and determine the moves available to each player, but all the â€œdiceâ€ are rolled
at the beginning! Even though this analogy turns out to be incorrect, it suggests an effective
algorithm: consider all possible deals of the invisible cards; solve each one as if it were a
fully observable game; and then choose the move that has the best outcome averaged over all
the deals. Suppose that each deal s occurs with probability P (s); then the move we want is

P (s) M INIMAX (R ESULT (s, a)) .
(5.1)
argmax
a

s

Here, we run exact M INIMAX if computationally feasible; otherwise, we run H-M INIMAX .
Now, in most card games, the number of possible deals is rather large. For example,
in bridge play, each player sees just two of the four hands; there are two unseen hands of 13
cards each, so the number of deals is 26
13 = 10, 400, 600. Solving even one deal is quite
difficult, so solving ten million is out of the question. Instead, we resort to a Monte Carlo

184

Chapter

5.

Adversarial Search

approximation: instead of adding up all the deals, we take a random sample of N deals,
where the probability of deal s appearing in the sample is proportional to P (s):
argmax
a

N
1 
M INIMAX (R ESULT (si , a)) .
N

(5.2)

i=1

(Notice that P (s) does not appear explicitly in the summation, because the samples are already drawn according to P (s).) As N grows large, the sum over the random sample tends
to the exact value, but even for fairly small N â€”say, 100 to 1,000â€”the method gives a good
approximation. It can also be applied to deterministic games such as Kriegspiel, given some
reasonable estimate of P (s).
For games like whist and hearts, where there is no bidding or betting phase before play
commences, each deal will be equally likely and so the values of P (s) are all equal. For
bridge, play is preceded by a bidding phase in which each team indicates how many tricks it
expects to win. Since players bid based on the cards they hold, the other players learn more
about the probability of each deal. Taking this into account in deciding how to play the hand
is tricky, for the reasons mentioned in our description of Kriegspiel: players may bid in such
a way as to minimize the information conveyed to their opponents. Even so, the approach is
quite effective for bridge, as we show in Section 5.7.
The strategy described in Equations 5.1 and 5.2 is sometimes called averaging over
clairvoyance because it assumes that the game will become observable to both players immediately after the first move. Despite its intuitive appeal, the strategy can lead one astray.
Consider the following story:
Day 1: Road A leads to a heap of gold; Road B leads to a fork. Take the left fork and
youâ€™ll find a bigger heap of gold, but take the right fork and youâ€™ll be run over by a bus.
Day 2: Road A leads to a heap of gold; Road B leads to a fork. Take the right fork and
youâ€™ll find a bigger heap of gold, but take the left fork and youâ€™ll be run over by a bus.
Day 3: Road A leads to a heap of gold; Road B leads to a fork. One branch of the
fork leads to a bigger heap of gold, but take the wrong fork and youâ€™ll be hit by a bus.
Unfortunately you donâ€™t know which fork is which.

BLUFF

Averaging over clairvoyance leads to the following reasoning: on Day 1, B is the right choice;
on Day 2, B is the right choice; on Day 3, the situation is the same as either Day 1 or Day 2,
so B must still be the right choice.
Now we can see how averaging over clairvoyance fails: it does not consider the belief
state that the agent will be in after acting. A belief state of total ignorance is not desirable, especially when one possibility is certain death. Because it assumes that every future state will
automatically be one of perfect knowledge, the approach never selects actions that gather information (like the first move in Figure 5.13); nor will it choose actions that hide information
from the opponent or provide information to a partner because it assumes that they already
know the information; and it will never bluff in poker,4 because it assumes the opponent can
see its cards. In Chapter 17, we show how to construct algorithms that do all these things by
virtue of solving the true partially observable decision problem.
4

Bluffingâ€”betting as if oneâ€™s hand is good, even when itâ€™s notâ€”is a core part of poker strategy.

Section 5.7.

5.7

State-of-the-Art Game Programs

185

S TATE - OF - THE -A RT G AME P ROGRAMS
In 1965, the Russian mathematician Alexander Kronrod called chess â€œthe Drosophila of artificial intelligence.â€ John McCarthy disagrees: whereas geneticists use fruit flies to make
discoveries that apply to biology more broadly, AI has used chess to do the equivalent of
breeding very fast fruit flies. Perhaps a better analogy is that chess is to AI as Grand Prix
motor racing is to the car industry: state-of-the-art game programs are blindingly fast, highly
optimized machines that incorporate the latest engineering advances, but they arenâ€™t much
use for doing the shopping or driving off-road. Nonetheless, racing and game-playing generate excitement and a steady stream of innovations that have been adopted by the wider
community. In this section we look at what it takes to come out on top in various games.

CHESS

NULL MOVE

FUTILITY PRUNING

Chess: IBMâ€™s D EEP B LUE chess program, now retired, is well known for defeating world
champion Garry Kasparov in a widely publicized exhibition match. Deep Blue ran on a parallel computer with 30 IBM RS/6000 processors doing alphaâ€“beta search. The unique part
was a configuration of 480 custom VLSI chess processors that performed move generation
and move ordering for the last few levels of the tree, and evaluated the leaf nodes. Deep Blue
searched up to 30 billion positions per move, reaching depth 14 routinely. The key to its
success seems to have been its ability to generate singular extensions beyond the depth limit
for sufficiently interesting lines of forcing/forced moves. In some cases the search reached a
depth of 40 plies. The evaluation function had over 8000 features, many of them describing
highly specific patterns of pieces. An â€œopening bookâ€ of about 4000 positions was used, as
well as a database of 700,000 grandmaster games from which consensus recommendations
could be extracted. The system also used a large endgame database of solved positions containing all positions with five pieces and many with six pieces. This database had the effect
of substantially extending the effective search depth, allowing Deep Blue to play perfectly in
some cases even when it was many moves away from checkmate.
The success of D EEP B LUE reinforced the widely held belief that progress in computer
game-playing has come primarily from ever-more-powerful hardwareâ€”a view encouraged
by IBM. But algorithmic improvements have allowed programs running on standard PCs
to win World Computer Chess Championships. A variety of pruning heuristics are used to
reduce the effective branching factor to less than 3 (compared with the actual branching factor
of about 35). The most important of these is the null move heuristic, which generates a good
lower bound on the value of a position, using a shallow search in which the opponent gets
to move twice at the beginning. This lower bound often allows alphaâ€“beta pruning without
the expense of a full-depth search. Also important is futility pruning, which helps decide in
advance which moves will cause a beta cutoff in the successor nodes.
H YDRA can be seen as the successor to D EEP B LUE . H YDRA runs on a 64-processor
cluster with 1 gigabyte per processor and with custom hardware in the form of FPGA (Field
Programmable Gate Array) chips. H YDRA reaches 200 million evaluations per second, about
the same as Deep Blue, but H YDRA reaches 18 plies deep rather than just 14 because of
aggressive use of the null move heuristic and forward pruning.

186

CHECKERS

OTHELLO

BACKGAMMON

GO

COMBINATORIAL
GAME THEORY

BRIDGE

Chapter

5.

Adversarial Search

RYBKA, winner of the 2008 and 2009 World Computer Chess Championships, is considered the strongest current computer player. It uses an off-the-shelf 8-core 3.2 GHz Intel
Xeon processor, but little is known about the design of the program. RYBKAâ€™s main advantage appears to be its evaluation function, which has been tuned by its main developer,
International Master Vasik Rajlich, and at least three other grandmasters.
The most recent matches suggest that the top computer chess programs have pulled
ahead of all human contenders. (See the historical notes for details.)
Checkers: Jonathan Schaeffer and colleagues developed C HINOOK, which runs on regular
PCs and uses alphaâ€“beta search. Chinook defeated the long-running human champion in an
abbreviated match in 1990, and since 2007 C HINOOK has been able to play perfectly by using
alphaâ€“beta search combined with a database of 39 trillion endgame positions.
Othello, also called Reversi, is probably more popular as a computer game than as a board
game. It has a smaller search space than chess, usually 5 to 15 legal moves, but evaluation
expertise had to be developed from scratch. In 1997, the L OGISTELLO program (Buro, 2002)
defeated the human world champion, Takeshi Murakami, by six games to none. It is generally
acknowledged that humans are no match for computers at Othello.
Backgammon: Section 5.5 explained why the inclusion of uncertainty from dice rolls makes
deep search an expensive luxury. Most work on backgammon has gone into improving the
evaluation function. Gerry Tesauro (1992) combined reinforcement learning with neural
networks to develop a remarkably accurate evaluator that is used with a search to depth 2
or 3. After playing more than a million training games against itself, Tesauroâ€™s program,
TD-G AMMON , is competitive with top human players. The programâ€™s opinions on the opening moves of the game have in some cases radically altered the received wisdom.
Go is the most popular board game in Asia. Because the board is 19 Ã— 19 and moves are
allowed into (almost) every empty square, the branching factor starts at 361, which is too
daunting for regular alphaâ€“beta search methods. In addition, it is difficult to write an evaluation function because control of territory is often very unpredictable until the endgame.
Therefore the top programs, such as M O G O , avoid alphaâ€“beta search and instead use Monte
Carlo rollouts. The trick is to decide what moves to make in the course of the rollout. There is
no aggressive pruning; all moves are possible. The UCT (upper confidence bounds on trees)
method works by making random moves in the first few iterations, and over time guiding
the sampling process to prefer moves that have led to wins in previous samples. Some tricks
are added, including knowledge-based rules that suggest particular moves whenever a given
pattern is detected and limited local search to decide tactical questions. Some programs also
include special techniques from combinatorial game theory to analyze endgames. These
techniques decompose a position into sub-positions that can be analyzed separately and then
combined (Berlekamp and Wolfe, 1994; MuÌˆller, 2003). The optimal solutions obtained in
this way have surprised many professional Go players, who thought they had been playing
optimally all along. Current Go programs play at the master level on a reduced 9 Ã— 9 board,
but are still at advanced amateur level on a full board.
Bridge is a card game of imperfect information: a playerâ€™s cards are hidden from the other
players. Bridge is also a multiplayer game with four players instead of two, although the

Section 5.8.

EXPLANATIONBASED
GENERALIZATION

SCRABBLE

5.8

Alternative Approaches

187

players are paired into two teams. As in Section 5.6, optimal play in partially observable
games like bridge can include elements of information gathering, communication, and careful
weighing of probabilities. Many of these techniques are used in the Bridge Baron program
(Smith et al., 1998), which won the 1997 computer bridge championship. While it does
not play optimally, Bridge Baron is one of the few successful game-playing systems to use
complex, hierarchical plans (see Chapter 11) involving high-level ideas, such as finessing and
squeezing, that are familiar to bridge players.
The GIB program (Ginsberg, 1999) won the 2000 computer bridge championship quite
decisively using the Monte Carlo method. Since then, other winning programs have followed
GIBâ€™s lead. GIBâ€™s major innovation is using explanation-based generalization to compute
and cache general rules for optimal play in various standard classes of situations rather than
evaluating each situation individually. For example, in a situation where one player has the
cards A-K-Q-J-4-3-2 of one suit and another player has 10-9-8-7-6-5, there are 7 Ã— 6 = 42
ways that the first player can lead from that suit and the second player can follow. But GIB
treats these situations as just two: the first player can lead either a high card or a low card;
the exact cards played donâ€™t matter. With this optimization (and a few others), GIB can solve
a 52-card, fully observable deal exactly in about a second. GIBâ€™s tactical accuracy makes up
for its inability to reason about information. It finished 12th in a field of 35 in the par contest
(involving just play of the hand, not bidding) at the 1998 human world championship, far
exceeding the expectations of many human experts.
There are several reasons why GIB plays at expert level with Monte Carlo simulation,
whereas Kriegspiel programs do not. First, GIBâ€™s evaluation of the fully observable version
of the game is exact, searching the full game tree, while Kriegspiel programs rely on inexact
heuristics. But far more important is the fact that in bridge, most of the uncertainty in the
partially observable information comes from the randomness of the deal, not from the adversarial play of the opponent. Monte Carlo simulation handles randomness well, but does not
always handle strategy well, especially when the strategy involves the value of information.
Scrabble: Most people think the hard part about Scrabble is coming up with good words, but
given the official dictionary, it turns out to be rather easy to program a move generator to find
the highest-scoring move (Gordon, 1994). That doesnâ€™t mean the game is solved, however:
merely taking the top-scoring move each turn results in a good but not expert player. The
problem is that Scrabble is both partially observable and stochastic: you donâ€™t know what
letters the other player has or what letters you will draw next. So playing Scrabble well
combines the difficulties of backgammon and bridge. Nevertheless, in 2006, the Q UACKLE
program defeated the former world champion, David Boys, 3â€“2.

A LTERNATIVE A PPROACHES
Because calculating optimal decisions in games is intractable in most cases, all algorithms
must make some assumptions and approximations. The standard approach, based on minimax, evaluation functions, and alphaâ€“beta, is just one way to do this. Probably because it has

188

Chapter

5.

Adversarial Search

MAX

MIN

99

Figure 5.14

100

99

1000

1000

1000

100

101

102

100

A two-ply game tree for which heuristic minimax may make an error.

been worked on for so long, the standard approach dominates other methods in tournament
play. Some believe that this has caused game playing to become divorced from the mainstream of AI research: the standard approach no longer provides much room for new insight
into general questions of decision making. In this section, we look at the alternatives.
First, let us consider heuristic minimax. It selects an optimal move in a given search
tree provided that the leaf node evaluations are exactly correct. In reality, evaluations are
usually crude estimates of the value of a position and can be considered to have large errors
associated with them. Figure 5.14 shows a two-ply game tree for which minimax suggests
taking the right-hand branch because 100 > 99. That is the correct move if the evaluations
are all correct. But of course the evaluation function is only approximate. Suppose that
the evaluation of each node has an error that is independent of other nodes and is randomly
distributed with mean zero and standard deviation of Ïƒ. Then when Ïƒ = 5, the left-hand
branch is actually better 71% of the time, and 58% of the time when Ïƒ = 2. The intuition
behind this is that the right-hand branch has four nodes that are close to 99; if an error in
the evaluation of any one of the four makes the right-hand branch slip below 99, then the
left-hand branch is better.
In reality, circumstances are actually worse than this because the error in the evaluation
function is not independent. If we get one node wrong, the chances are high that nearby nodes
in the tree will also be wrong. The fact that the node labeled 99 has siblings labeled 1000
suggests that in fact it might have a higher true value. We can use an evaluation function
that returns a probability distribution over possible values, but it is difficult to combine these
distributions properly, because we wonâ€™t have a good model of the very strong dependencies
that exist between the values of sibling nodes
Next, we consider the search algorithm that generates the tree. The aim of an algorithm
designer is to specify a computation that runs quickly and yields a good move. The alphaâ€“beta
algorithm is designed not just to select a good move but also to calculate bounds on the values
of all the legal moves. To see why this extra information is unnecessary, consider a position
in which there is only one legal move. Alphaâ€“beta search still will generate and evaluate a
large search tree, telling us that the only move is the best move and assigning it a value. But
since we have to make the move anyway, knowing the moveâ€™s value is useless. Similarly, if
there is one obviously good move and several moves that are legal but lead to a quick loss, we

Section 5.9.

METAREASONING

5.9

Summary

189

would not want alphaâ€“beta to waste time determining a precise value for the lone good move.
Better to just make the move quickly and save the time for later. This leads to the idea of the
utility of a node expansion. A good search algorithm should select node expansions of high
utilityâ€”that is, ones that are likely to lead to the discovery of a significantly better move. If
there are no node expansions whose utility is higher than their cost (in terms of time), then
the algorithm should stop searching and make a move. Notice that this works not only for
clear-favorite situations but also for the case of symmetrical moves, for which no amount of
search will show that one move is better than another.
This kind of reasoning about what computations to do is called metareasoning (reasoning about reasoning). It applies not just to game playing but to any kind of reasoning
at all. All computations are done in the service of trying to reach better decisions, all have
costs, and all have some likelihood of resulting in a certain improvement in decision quality.
Alphaâ€“beta incorporates the simplest kind of metareasoning, namely, a theorem to the effect
that certain branches of the tree can be ignored without loss. It is possible to do much better.
In Chapter 16, we see how these ideas can be made precise and implementable.
Finally, let us reexamine the nature of search itself. Algorithms for heuristic search
and for game playing generate sequences of concrete states, starting from the initial state
and then applying an evaluation function. Clearly, this is not how humans play games. In
chess, one often has a particular goal in mindâ€”for example, trapping the opponentâ€™s queenâ€”
and can use this goal to selectively generate plausible plans for achieving it. This kind of
goal-directed reasoning or planning sometimes eliminates combinatorial search altogether.
David Wilkinsâ€™ (1980) PARADISE is the only program to have used goal-directed reasoning
successfully in chess: it was capable of solving some chess problems requiring an 18-move
combination. As yet there is no good understanding of how to combine the two kinds of
algorithms into a robust and efficient system, although Bridge Baron might be a step in the
right direction. A fully integrated system would be a significant achievement not just for
game-playing research but also for AI research in general, because it would be a good basis
for a general intelligent agent.

S UMMARY
We have looked at a variety of games to understand what optimal play means and to understand how to play well in practice. The most important ideas are as follows:
â€¢ A game can be defined by the initial state (how the board is set up), the legal actions
in each state, the result of each action, a terminal test (which says when the game is
over), and a utility function that applies to terminal states.
â€¢ In two-player zero-sum games with perfect information, the minimax algorithm can
select optimal moves by a depth-first enumeration of the game tree.
â€¢ The alphaâ€“beta search algorithm computes the same optimal move as minimax, but
achieves much greater efficiency by eliminating subtrees that are provably irrelevant.
â€¢ Usually, it is not feasible to consider the whole game tree (even with alphaâ€“beta), so we

190

Chapter

5.

Adversarial Search

need to cut the search off at some point and apply a heuristic evaluation function that
estimates the utility of a state.
â€¢ Many game programs precompute tables of best moves in the opening and endgame so
that they can look up a move rather than search.
â€¢ Games of chance can be handled by an extension to the minimax algorithm that evaluates a chance node by taking the average utility of all its children, weighted by the
probability of each child.
â€¢ Optimal play in games of imperfect information, such as Kriegspiel and bridge, requires reasoning about the current and future belief states of each player. A simple
approximation can be obtained by averaging the value of an action over each possible
configuration of missing information.
â€¢ Programs have bested even champion human players at games such as chess, checkers,
and Othello. Humans retain the edge in several games of imperfect information, such
as poker, bridge, and Kriegspiel, and in games with very large branching factors and
little good heuristic knowledge, such as Go.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

The early history of mechanical game playing was marred by numerous frauds. The most
notorious of these was Baron Wolfgang von Kempelenâ€™s (1734â€“1804) â€œThe Turk,â€ a supposed
chess-playing automaton that defeated Napoleon before being exposed as a magicianâ€™s trick
cabinet housing a human chess expert (see Levitt, 2000). It played from 1769 to 1854. In
1846, Charles Babbage (who had been fascinated by the Turk) appears to have contributed
the first serious discussion of the feasibility of computer chess and checkers (Morrison and
Morrison, 1961). He did not understand the exponential complexity of search trees, claiming
â€œthe combinations involved in the Analytical Engine enormously surpassed any required,
even by the game of chess.â€ Babbage also designed, but did not build, a special-purpose
machine for playing tic-tac-toe. The first true game-playing machine was built around 1890
by the Spanish engineer Leonardo Torres y Quevedo. It specialized in the â€œKRKâ€ (king and
rook vs. king) chess endgame, guaranteeing a win with king and rook from any position.
The minimax algorithm is traced to a 1912 paper by Ernst Zermelo, the developer of
modern set theory. The paper unfortunately contained several errors and did not describe minimax correctly. On the other hand, it did lay out the ideas of retrograde analysis and proposed
(but did not prove) what became known as Zermeloâ€™s theorem: that chess is determinedâ€”
White can force a win or Black can or it is a draw; we just donâ€™t know which. Zermelo says
that should we eventually know, â€œChess would of course lose the character of a game at all.â€
A solid foundation for game theory was developed in the seminal work Theory of Games
and Economic Behavior (von Neumann and Morgenstern, 1944), which included an analysis
showing that some games require strategies that are randomized (or otherwise unpredictable).
See Chapter 17 for more information.

Bibliographical and Historical Notes

191

John McCarthy conceived the idea of alphaâ€“beta search in 1956, although he did not
publish it. The NSS chess program (Newell et al., 1958) used a simplified version of alphaâ€“
beta; it was the first chess program to do so. Alphaâ€“beta pruning was described by Hart and
Edwards (1961) and Hart et al. (1972). Alphaâ€“beta was used by the â€œKotokâ€“McCarthyâ€ chess
program written by a student of John McCarthy (Kotok, 1962). Knuth and Moore (1975)
proved the correctness of alphaâ€“beta and analysed its time complexity. Pearl (1982b) shows
alphaâ€“beta to be asymptotically optimal among all fixed-depth game-tree search algorithms.
Several attempts have been made to overcome the problems with the â€œstandard approachâ€ that were outlined in Section 5.8. The first nonexhaustive heuristic search algorithm
with some theoretical grounding was probably B âˆ— (Berliner, 1979), which attempts to maintain interval bounds on the possible value of a node in the game tree rather than giving it
a single point-valued estimate. Leaf nodes are selected for expansion in an attempt to refine the top-level bounds until one move is â€œclearly best.â€ Palay (1985) extends the Bâˆ— idea
using probability distributions on values in place of intervals. David McAllesterâ€™s (1988)
conspiracy number search expands leaf nodes that, by changing their values, could cause
the program to prefer a new move at the root. MGSS âˆ— (Russell and Wefald, 1989) uses the
decision-theoretic techniques of Chapter 16 to estimate the value of expanding each leaf in
terms of the expected improvement in decision quality at the root. It outplayed an alphaâ€“
beta algorithm at Othello despite searching an order of magnitude fewer nodes. The MGSSâˆ—
approach is, in principle, applicable to the control of any form of deliberation.
Alphaâ€“beta search is in many ways the two-player analog of depth-first branch-andbound, which is dominated by Aâˆ— in the single-agent case. The SSSâˆ— algorithm (Stockman,
1979) can be viewed as a two-player Aâˆ— and never expands more nodes than alphaâ€“beta to
reach the same decision. The memory requirements and computational overhead of the queue
make SSSâˆ— in its original form impractical, but a linear-space version has been developed
from the RBFS algorithm (Korf and Chickering, 1996). Plaat et al. (1996) developed a new
view of SSSâˆ— as a combination of alphaâ€“beta and transposition tables, showing how to overcome the drawbacks of the original algorithm and developing a new variant called MTD(f)
that has been adopted by a number of top programs.
D. F. Beal (1980) and Dana Nau (1980, 1983) studied the weaknesses of minimax applied to approximate evaluations. They showed that under certain assumptions about the distribution of leaf values in the tree, minimaxing can yield values at the root that are actually less
reliable than the direct use of the evaluation function itself. Pearlâ€™s book Heuristics (1984)
partially explains this apparent paradox and analyzes many game-playing algorithms. Baum
and Smith (1997) propose a probability-based replacement for minimax, showing that it results in better choices in certain games. The expectiminimax algorithm was proposed by
Donald Michie (1966). Bruce Ballard (1983) extended alphaâ€“beta pruning to cover trees
with chance nodes and Hauk (2004) reexamines this work and provides empirical results.
Koller and Pfeffer (1997) describe a system for completely solving partially observable games. The system is quite general, handling games whose optimal strategy requires
randomized moves and games that are more complex than those handled by any previous
system. Still, it canâ€™t handle games as complex as poker, bridge, and Kriegspiel. Frank
et al. (1998) describe several variants of Monte Carlo search, including one where MIN has

192

Chapter

5.

Adversarial Search

complete information but MAX does not. Among deterministic, partially observable games,
Kriegspiel has received the most attention. Ferguson demonstrated hand-derived randomized strategies for winning Kriegspiel with a bishop and knight (1992) or two bishops (1995)
against a king. The first Kriegspiel programs concentrated on finding endgame checkmates
and performed AND â€“ OR search in belief-state space (Sakuta and Iida, 2002; Bolognesi and
Ciancarini, 2003). Incremental belief-state algorithms enabled much more complex midgame
checkmates to be found (Russell and Wolfe, 2005; Wolfe and Russell, 2007), but efficient
state estimation remains the primary obstacle to effective general play (Parker et al., 2005).
Chess was one of the first tasks undertaken in AI, with early efforts by many of the pioneers of computing, including Konrad Zuse in 1945, Norbert Wiener in his book Cybernetics
(1948), and Alan Turing in 1950 (see Turing et al., 1953). But it was Claude Shannonâ€™s
article Programming a Computer for Playing Chess (1950) that had the most complete set
of ideas, describing a representation for board positions, an evaluation function, quiescence
search, and some ideas for selective (nonexhaustive) game-tree search. Slater (1950) and the
commentators on his article also explored the possibilities for computer chess play.
D. G. Prinz (1952) completed a program that solved chess endgame problems but did
not play a full game. Stan Ulam and a group at the Los Alamos National Lab produced a
program that played chess on a 6 Ã— 6 board with no bishops (Kister et al., 1957). It could
search 4 plies deep in about 12 minutes. Alex Bernstein wrote the first documented program
to play a full game of standard chess (Bernstein and Roberts, 1958).5
The first computer chess match featured the Kotokâ€“McCarthy program from MIT (Kotok, 1962) and the ITEP program written in the mid-1960s at Moscowâ€™s Institute of Theoretical and Experimental Physics (Adelson-Velsky et al., 1970). This intercontinental match
was played by telegraph. It ended with a 3â€“1 victory for the ITEP program in 1967. The first
chess program to compete successfully with humans was MITâ€™s M AC H ACK -6 (Greenblatt
et al., 1967). Its Elo rating of approximately 1400 was well above the novice level of 1000.
The Fredkin Prize, established in 1980, offered awards for progressive milestones in
chess play. The $5,000 prize for the first program to achieve a master rating went to B ELLE
(Condon and Thompson, 1982), which achieved a rating of 2250. The $10,000 prize for the
first program to achieve a USCF (United States Chess Federation) rating of 2500 (near the
grandmaster level) was awarded to D EEP T HOUGHT (Hsu et al., 1990) in 1989. The grand
prize, $100,000, went to D EEP B LUE (Campbell et al., 2002; Hsu, 2004) for its landmark
victory over world champion Garry Kasparov in a 1997 exhibition match. Kasparov wrote:
The decisive game of the match was Game 2, which left a scar in my memory . . . we saw
something that went well beyond our wildest expectations of how well a computer would
be able to foresee the long-term positional consequences of its decisions. The machine
refused to move to a position that had a decisive short-term advantageâ€”showing a very
human sense of danger. (Kasparov, 1997)

Probably the most complete description of a modern chess program is provided by Ernst
Heinz (2000), whose DARK T HOUGHT program was the highest-ranked noncommercial PC
program at the 1999 world championships.
5

A Russian program, BESM may have predated Bernsteinâ€™s program.

Bibliographical and Historical Notes

193

(a)

(b)

Figure 5.15 Pioneers in computer chess: (a) Herbert Simon and Allen Newell, developers
of the NSS program (1958); (b) John McCarthy and the Kotokâ€“McCarthy program on an
IBM 7090 (1967).

In recent years, chess programs are pulling ahead of even the worldâ€™s best humans.
In 2004â€“2005 H YDRA defeated grand master Evgeny Vladimirov 3.5â€“0.5, world champion
Ruslan Ponomariov 2â€“0, and seventh-ranked Michael Adams 5.5â€“0.5. In 2006, D EEP F RITZ
beat world champion Vladimir Kramnik 4â€“2, and in 2007 RYBKA defeated several grand
masters in games in which it gave odds (such as a pawn) to the human players. As of 2009,
the highest Elo rating ever recorded was Kasparovâ€™s 2851. H YDRA (Donninger and Lorenz,
2004) is rated somewhere between 2850 and 3000, based mostly on its trouncing of Michael
Adams. The RYBKA program is rated between 2900 and 3100, but this is based on a small
number of games and is not considered reliable. Ross (2004) shows how human players have
learned to exploit some of the weaknesses of the computer programs.
Checkers was the first of the classic games fully played by a computer. Christopher
Strachey (1952) wrote the first working program for checkers. Beginning in 1952, Arthur
Samuel of IBM, working in his spare time, developed a checkers program that learned its
own evaluation function by playing itself thousands of times (Samuel, 1959, 1967). We
describe this idea in more detail in Chapter 21. Samuelâ€™s program began as a novice but
after only a few daysâ€™ self-play had improved itself beyond Samuelâ€™s own level. In 1962 it
defeated Robert Nealy, a champion at â€œblind checkers,â€ through an error on his part. When
one considers that Samuelâ€™s computing equipment (an IBM 704) had 10,000 words of main
memory, magnetic tape for long-term storage, and a .000001 GHz processor, the win remains
a great accomplishment.
The challenge started by Samuel was taken up by Jonathan Schaeffer of the University
of Alberta. His C HINOOK program came in second in the 1990 U.S. Open and earned the
right to challenge for the world championship. It then ran up against a problem, in the form
of Marion Tinsley. Dr. Tinsley had been world champion for over 40 years, losing only
three games in all that time. In the first match against C HINOOK, Tinsley suffered his fourth

194

Chapter

5.

Adversarial Search

and fifth losses, but won the match 20.5â€“18.5. A rematch at the 1994 world championship
ended prematurely when Tinsley had to withdraw for health reasons. C HINOOK became the
official world champion. Schaeffer kept on building on his database of endgames, and in
2007 â€œsolvedâ€ checkers (Schaeffer et al., 2007; Schaeffer, 2008). This had been predicted by
Richard Bellman (1965). In the paper that introduced the dynamic programming approach
to retrograde analysis, he wrote, â€œIn checkers, the number of possible moves in any given
situation is so small that we can confidently expect a complete digital computer solution to
the problem of optimal play in this game.â€ Bellman did not, however, fully appreciate the
size of the checkers game tree. There are about 500 quadrillion positions. After 18 years
of computation on a cluster of 50 or more machines, Jonathan Schaefferâ€™s team completed
an endgame table for all checkers positions with 10 or fewer pieces: over 39 trillion entries.
From there, they were able to do forward alphaâ€“beta search to derive a policy that proves
that checkers is in fact a draw with best play by both sides. Note that this is an application
of bidirectional search (Section 3.4.6). Building an endgame table for all of checkers would
be impractical: it would require a billion gigabytes of storage. Searching without any table
would also be impractical: the search tree has about 847 positions, and would take thousands
of years to search with todayâ€™s technology. Only a combination of clever search, endgame
data, and a drop in the price of processors and memory could solve checkers. Thus, checkers
joins Qubic (Patashnik, 1980), Connect Four (Allis, 1988), and Nine-Menâ€™s Morris (Gasser,
1998) as games that have been solved by computer analysis.
Backgammon, a game of chance, was analyzed mathematically by Gerolamo Cardano
(1663), but only taken up for computer play in the late 1970s, first with the BKG program (Berliner, 1980b); it used a complex, manually constructed evaluation function and
searched only to depth 1. It was the first program to defeat a human world champion at a major classic game (Berliner, 1980a). Berliner readily acknowledged that BKG was very lucky
with the dice. Gerry Tesauroâ€™s (1995) TD-G AMMON played consistently at world champion
level. The BGB LITZ program was the winner of the 2008 Computer Olympiad.
Go is a deterministic game, but the large branching factor makes it challeging. The key
issues and early literature in computer Go are summarized by Bouzy and Cazenave (2001) and
MuÌˆller (2002). Up to 1997 there were no competent Go programs. Now the best programs
play most of their moves at the master level; the only problem is that over the course of a
game they usually make at least one serious blunder that allows a strong opponent to win.
Whereas alphaâ€“beta search reigns in most games, many recent Go programs have adopted
Monte Carlo methods based on the UCT (upper confidence bounds on trees) scheme (Kocsis
and Szepesvari, 2006). The strongest Go program as of 2009 is Gelly and Silverâ€™s M O G O
(Wang and Gelly, 2007; Gelly and Silver, 2008). In August 2008, M O G O scored a surprising
win against top professional Myungwan Kim, albeit with MO G O receiving a handicap of
nine stones (about the equivalent of a queen handicap in chess). Kim estimated MO G Oâ€™s
strength at 2â€“3 dan, the low end of advanced amateur. For this match, M O G O was run on
an 800-processor 15 teraflop supercomputer (1000 times Deep Blue). A few weeks later,
M O G O , with only a five-stone handicap, won against a 6-dan professional. In the 9 Ã— 9 form
of Go, M O G O is at approximately the 1-dan professional level. Rapid advances are likely
as experimentation continues with new forms of Monte Carlo search. The Computer Go

Exercises

195
Newsletter, published by the Computer Go Association, describes current developments.
Bridge: Smith et al. (1998) report on how their planning-based program won the 1998
computer bridge championship, and (Ginsberg, 2001) describes how his GIB program, based
on Monte Carlo simulation, won the following computer championship and did surprisingly
well against human players and standard book problem sets. From 2001â€“2007, the computer
bridge championship was won five times by JACK and twice by W BRIDGE5. Neither has
had academic articles explaining their structure, but both are rumored to use the Monte Carlo
technique, which was first proposed for bridge by Levy (1989).
Scrabble: A good description of a top program, M AVEN , is given by its creator, Brian
Sheppard (2002). Generating the highest-scoring move is described by Gordon (1994), and
modeling opponents is covered by Richards and Amir (2007).
Soccer (Kitano et al., 1997b; Visser et al., 2008) and billiards (Lam and Greenspan,
2008; Archibald et al., 2009) and other stochastic games with a continuous space of actions
are beginning to attract attention in AI, both in simulation and with physical robot players.
Computer game competitions occur annually, and papers appear in a variety of venues.
The rather misleadingly named conference proceedings Heuristic Programming in Artificial
Intelligence report on the Computer Olympiads, which include a wide variety of games. The
General Game Competition (Love et al., 2006) tests programs that must learn to play an unknown game given only a logical description of the rules of the game. There are also several
edited collections of important papers on game-playing research (Levy, 1988a, 1988b; Marsland and Schaeffer, 1990). The International Computer Chess Association (ICCA), founded
in 1977, publishes the ICGA Journal (formerly the ICCA Journal). Important papers have
been published in the serial anthology Advances in Computer Chess, starting with Clarke
(1977). Volume 134 of the journal Artificial Intelligence (2002) contains descriptions of
state-of-the-art programs for chess, Othello, Hex, shogi, Go, backgammon, poker, Scrabble,
and other games. Since 1998, a biennial Computers and Games conference has been held.

E XERCISES
5.1 Suppose you have an oracle, OM (s), that correctly predicts the opponentâ€™s move in
any state. Using this, formulate the definition of a game as a (single-agent) search problem.
Describe an algorithm for finding the optimal move.
5.2

Consider the problem of solving two 8-puzzles.

a. Give a complete problem formulation in the style of Chapter 3.
b. How large is the reachable state space? Give an exact numerical expression.
c. Suppose we make the problem adversarial as follows: the two players take turns moving; a coin is flipped to determine the puzzle on which to make a move in that turn; and
the winner is the first to solve one puzzle. Which algorithm can be used to choose a
move in this setting?
d. Give an informal proof that someone will eventually win if both play perfectly.

196

Chapter

5.

Adversarial Search

e
a

(a)

b

c

d

P

E

f

bd

cd

(b)

ce

ad

cc

cf
?

?

de

df

dd

dd

ae

af

ac

?

?

?

Figure 5.16 (a) A map where the cost of every edge is 1. Initially the pursuer P is at node
b and the evader E is at node d. (b) A partial game tree for this map. Each node is labeled
with the P, E positions. P moves first. Branches marked â€œ?â€ have yet to be explored.

PURSUITâ€“EVASION

5.3 Imagine that, in Exercise 3.3, one of the friends wants to avoid the other. The problem
then becomes a two-player pursuitâ€“evasion game. We assume now that the players take
turns moving. The game ends only when the players are on the same node; the terminal
payoff to the pursuer is minus the total time taken. (The evader â€œwinsâ€ by never losing.) An
example is shown in Figure 5.16.
a. Copy the game tree and mark the values of the terminal nodes.
b. Next to each internal node, write the strongest fact you can infer about its value (a
number, one or more inequalities such as â€œâ‰¥ 14â€, or a â€œ?â€).
c. Beneath each question mark, write the name of the node reached by that branch.
d. Explain how a bound on the value of the nodes in (c) can be derived from consideration
of shortest-path lengths on the map, and derive such bounds for these nodes. Remember
the cost to get to each leaf as well as the cost to solve it.
e. Now suppose that the tree as given, with the leaf bounds from (d), is evaluated from left
to right. Circle those â€œ?â€ nodes that would not need to be expanded further, given the
bounds from part (d), and cross out those that need not be considered at all.
f. Can you prove anything in general about who wins the game on a map that is a tree?

Exercises

197
5.4 Describe and implement state descriptions, move generators, terminal tests, utility functions, and evaluation functions for one or more of the following stochastic games: Monopoly,
Scrabble, bridge play with a given contract, or Texas holdâ€™em poker.
5.5 Describe and implement a real-time, multiplayer game-playing environment, where
time is part of the environment state and players are given fixed time allocations.
5.6 Discuss how well the standard approach to game playing would apply to games such as
tennis, pool, and croquet, which take place in a continuous physical state space.
5.7 Prove the following assertion: For every game tree, the utility obtained by MAX using
minimax decisions against a suboptimal MIN will be never be lower than the utility obtained
playing against an optimal MIN. Can you come up with a game tree in which MAX can do
still better using a suboptimal strategy against a suboptimal MIN?

A
1

B
2

3

4

Figure 5.17 The starting position of a simple game. Player A moves first. The two players
take turns moving, and each player must move his token to an open adjacent space in either
direction. If the opponent occupies an adjacent space, then a player may jump over the
opponent to the next open space if any. (For example, if A is on 3 and B is on 2, then A may
move back to 1.) The game ends when one player reaches the opposite end of the board. If
player A reaches space 4 first, then the value of the game to A is +1; if player B reaches
space 1 first, then the value of the game to A is âˆ’1.

5.8

Consider the two-player game described in Figure 5.17.

a. Draw the complete game tree, using the following conventions:
â€¢ Write each state as (sA , sB ), where sA and sB denote the token locations.
â€¢ Put each terminal state in a square box and write its game value in a circle.
â€¢ Put loop states (states that already appear on the path to the root) in double square
boxes. Since their value is unclear, annotate each with a â€œ?â€ in a circle.
b. Now mark each node with its backed-up minimax value (also in a circle). Explain how
you handled the â€œ?â€ values and why.
c. Explain why the standard minimax algorithm would fail on this game tree and briefly
sketch how you might fix it, drawing on your answer to (b). Does your modified algorithm give optimal decisions for all games with loops?
d. This 4-square game can be generalized to n squares for any n > 2. Prove that A wins
if n is even and loses if n is odd.
5.9 This problem exercises the basic concepts of game playing, using tic-tac-toe (noughts
and crosses) as an example. We define Xn as the number of rows, columns, or diagonals

198

Chapter

5.

Adversarial Search

with exactly n Xâ€™s and no Oâ€™s. Similarly, On is the number of rows, columns, or diagonals
with just n Oâ€™s. The utility function assigns +1 to any position with X3 = 1 and âˆ’1 to any
position with O3 = 1. All other terminal positions have utility 0. For nonterminal positions,
we use a linear evaluation function defined as Eval (s) = 3X2 (s)+X1 (s)âˆ’(3O2 (s)+O1 (s)).
a. Approximately how many possible games of tic-tac-toe are there?
b. Show the whole game tree starting from an empty board down to depth 2 (i.e., one X
and one O on the board), taking symmetry into account.
c. Mark on your tree the evaluations of all the positions at depth 2.
d. Using the minimax algorithm, mark on your tree the backed-up values for the positions
at depths 1 and 0, and use those values to choose the best starting move.
e. Circle the nodes at depth 2 that would not be evaluated if alphaâ€“beta pruning were
applied, assuming the nodes are generated in the optimal order for alphaâ€“beta pruning.
5.10 Consider the family of generalized tic-tac-toe games, defined as follows. Each particular game is specified by a set S of squares and a collection W of winning positions. Each
winning position is a subset of S. For example, in standard tic-tac-toe, S is a set of 9 squares
and W is a collection of 8 subsets of W: the three rows, the three columns, and the two diagonals. In other respects, the game is identical to standard tic-tac-toe. Starting from an empty
board, players alternate placing their marks on an empty square. A player who marks every
square in a winning position wins the game. It is a tie if all squares are marked and neither
player has won.
a. Let N = |S|, the number of squares. Give an upper bound on the number of nodes in
the complete game tree for generalized tic-tac-toe as a function of N .
b. Give a lower bound on the size of the game tree for the worst case, where W = { }.
c. Propose a plausible evaluation function that can be used for any instance of generalized
tic-tac-toe. The function may depend on S and W.
d. Assume that it is possible to generate a new board and check whether it is a winning
position in 100N machine instructions and assume a 2 gigahertz processor. Ignore
memory limitations. Using your estimate in (a), roughly how large a game tree can be
completely solved by alphaâ€“beta in a second of CPU time? a minute? an hour?
5.11

Develop a general game-playing program, capable of playing a variety of games.

a. Implement move generators and evaluation functions for one or more of the following
games: Kalah, Othello, checkers, and chess.
b. Construct a general alphaâ€“beta game-playing agent.
c. Compare the effect of increasing search depth, improving move ordering, and improving the evaluation function. How close does your effective branching factor come to the
ideal case of perfect move ordering?
d. Implement a selective search algorithm, such as B* (Berliner, 1979), conspiracy number
search (McAllester, 1988), or MGSS* (Russell and Wefald, 1989) and compare its
performance to A*.

Exercises

199

n1

n2

nj
Figure 5.18

Situation when considering whether to prune node nj .

5.12 Describe how the minimax and alphaâ€“beta algorithms change for two-player, nonzero-sum games in which each player has a distinct utility function and both utility functions
are known to both players. If there are no constraints on the two terminal utilities, is it possible
for any node to be pruned by alphaâ€“beta? What if the playerâ€™s utility functions on any state
differ by at most a constant k, making the game almost cooperative?
5.13 Develop a formal proof of correctness for alphaâ€“beta pruning. To do this, consider the
situation shown in Figure 5.18. The question is whether to prune node nj , which is a maxnode and a descendant of node n1 . The basic idea is to prune it if and only if the minimax
value of n1 can be shown to be independent of the value of nj .
a. Mode n1 takes on the minimum value among its children: n1 = min(n2 , n21 , . . . , n2b2 ).
Find a similar expression for n2 and hence an expression for n1 in terms of nj .
b. Let li be the minimum (or maximum) value of the nodes to the left of node ni at depth i,
whose minimax value is already known. Similarly, let ri be the minimum (or maximum)
value of the unexplored nodes to the right of ni at depth i. Rewrite your expression for
n1 in terms of the li and ri values.
c. Now reformulate the expression to show that in order to affect n1 , nj must not exceed
a certain bound derived from the li values.
d. Repeat the process for the case where nj is a min-node.
5.14 Prove that alphaâ€“beta pruning takes time O(2m/2 ) with optimal move ordering, where
m is the maximum depth of the game tree.
5.15 Suppose you have a chess program that can evaluate 10 million nodes per second.
Decide on a compact representation of a game state for storage in a transposition table. About
how many entries can you fit in a 2-gigabyte in-memory table? Will that be enough for the

200

Chapter

2

Figure 5.19

0.5

0.5

2

1

2

0

5.

0.5

0.5

2

-1

Adversarial Search

0

The complete game tree for a trivial game with chance nodes.

three minutes of search allocated for one move? How many table lookups can you do in the
time it would take to do one evaluation? Now suppose the transposition table is stored on
disk. About how many evaluations could you do in the time it takes to do one disk seek with
standard disk hardware?
5.16 This question considers pruning in games with chance nodes. Figure 5.19 shows the
complete game tree for a trivial game. Assume that the leaf nodes are to be evaluated in leftto-right order, and that before a leaf node is evaluated, we know nothing about its valueâ€”the
range of possible values is âˆ’âˆž to âˆž.
a. Copy the figure, mark the value of all the internal nodes, and indicate the best move at
the root with an arrow.
b. Given the values of the first six leaves, do we need to evaluate the seventh and eighth
leaves? Given the values of the first seven leaves, do we need to evaluate the eighth
leaf? Explain your answers.
c. Suppose the leaf node values are known to lie between â€“2 and 2 inclusive. After the
first two leaves are evaluated, what is the value range for the left-hand chance node?
d. Circle all the leaves that need not be evaluated under the assumption in (c).
5.17 Implement the expectiminimax algorithm and the *-alphaâ€“beta algorithm, which is
described by Ballard (1983), for pruning game trees with chance nodes. Try them on a game
such as backgammon and measure the pruning effectiveness of *-alphaâ€“beta.
5.18 Prove that with a positive linear transformation of leaf values (i.e., transforming a
value x to ax + b where a > 0), the choice of move remains unchanged in a game tree, even
when there are chance nodes.
5.19

Consider the following procedure for choosing moves in games with chance nodes:

â€¢ Generate some dice-roll sequences (say, 50) down to a suitable depth (say, 8).
â€¢ With known dice rolls, the game tree becomes deterministic. For each dice-roll sequence, solve the resulting deterministic game tree using alphaâ€“beta.

Exercises

201
â€¢ Use the results to estimate the value of each move and to choose the best.
Will this procedure work well? Why (or why not)?
5.20 In the following, a â€œmaxâ€ tree consists only of max nodes, whereas an â€œexpectimaxâ€
tree consists of a max node at the root with alternating layers of chance and max nodes. At
chance nodes, all outcome probabilities are nonzero. The goal is to find the value of the root
with a bounded-depth search. For each of (a)â€“(f), either give an example or explain why this
is impossible.
a. Assuming that leaf values are finite but unbounded, is pruning (as in alphaâ€“beta) ever
possible in a max tree?
b. Is pruning ever possible in an expectimax tree under the same conditions?
c. If leaf values are all nonnegative, is pruning ever possible in a max tree? Give an
example, or explain why not.
d. If leaf values are all nonnegative, is pruning ever possible in an expectimax tree? Give
an example, or explain why not.
e. If leaf values are all in the range [0, 1], is pruning ever possible in a max tree? Give an
example, or explain why not.
f. If leaf values are all in the range [0, 1], is pruning ever possible in an expectimax tree?
g. Consider the outcomes of a chance node in an expectimax tree. Which of the following
evaluation orders is most likely to yield pruning opportunities?
(i) Lowest probability first
(ii) Highest probability first
(iii) Doesnâ€™t make any difference
5.21

Which of the following are true and which are false? Give brief explanations.

a. In a fully observable, turn-taking, zero-sum game between two perfectly rational players, it does not help the first player to know what strategy the second player is usingâ€”
that is, what move the second player will make, given the first playerâ€™s move.
b. In a partially observable, turn-taking, zero-sum game between two perfectly rational
players, it does not help the first player to know what move the second player will
make, given the first playerâ€™s move.
c. A perfectly rational backgammon agent never loses.
5.22 Consider carefully the interplay of chance events and partial information in each of the
games in Exercise 5.4.
a. For which is the standard expectiminimax model appropriate? Implement the algorithm
and run it in your game-playing agent, with appropriate modifications to the gameplaying environment.
b. For which would the scheme described in Exercise 5.19 be appropriate?
c. Discuss how you might deal with the fact that in some of the games, the players do not
have the same knowledge of the current state.

6

CONSTRAINT
SATISFACTION PROBLEMS

In which we see how treating states as more than just little black boxes leads to the
invention of a range of powerful new search methods and a deeper understanding
of problem structure and complexity.

CONSTRAINT
SATISFACTION
PROBLEM

6.1

Chapters 3 and 4 explored the idea that problems can be solved by searching in a space of
states. These states can be evaluated by domain-specific heuristics and tested to see whether
they are goal states. From the point of view of the search algorithm, however, each state is
atomic, or indivisibleâ€”a black box with no internal structure.
This chapter describes a way to solve a wide variety of problems more efficiently. We
use a factored representation for each state: a set of variables, each of which has a value.
A problem is solved when each variable has a value that satisfies all the constraints on the
variable. A problem described this way is called a constraint satisfaction problem, or CSP.
CSP search algorithms take advantage of the structure of states and use general-purpose
rather than problem-specific heuristics to enable the solution of complex problems. The main
idea is to eliminate large portions of the search space all at once by identifying variable/value
combinations that violate the constraints.

D EFINING C ONSTRAINT S ATISFACTION P ROBLEMS
A constraint satisfaction problem consists of three components, X, D, and C:
X is a set of variables, {X1 , . . . , Xn }.
D is a set of domains, {D1 , . . . , Dn }, one for each variable.
C is a set of constraints that specify allowable combinations of values.
Each domain Di consists of a set of allowable values, {v1 , . . . , vk } for variable Xi . Each
constraint Ci consists of a pair scope, rel , where scope is a tuple of variables that participate
in the constraint and rel is a relation that defines the values that those variables can take on. A
relation can be represented as an explicit list of all tuples of values that satisfy the constraint,
or as an abstract relation that supports two operations: testing if a tuple is a member of the
relation and enumerating the members of the relation. For example, if X1 and X2 both have
202

Section 6.1.

ASSIGNMENT
CONSISTENT
COMPLETE
ASSIGNMENT
SOLUTION
PARTIAL
ASSIGNMENT

Defining Constraint Satisfaction Problems

203

the domain {A,B}, then the constraint saying the two variables must have different values
can be written as (X1 , X2 ), [(A, B), (B, A)] or as (X1 , X2 ), X1 = X2 .
To solve a CSP, we need to define a state space and the notion of a solution. Each
state in a CSP is defined by an assignment of values to some or all of the variables, {Xi =
vi , Xj = vj , . . .}. An assignment that does not violate any constraints is called a consistent
or legal assignment. A complete assignment is one in which every variable is assigned, and
a solution to a CSP is a consistent, complete assignment. A partial assignment is one that
assigns values to only some of the variables.

6.1.1 Example problem: Map coloring
Suppose that, having tired of Romania, we are looking at a map of Australia showing each
of its states and territories (Figure 6.1(a)). We are given the task of coloring each region
either red, green, or blue in such a way that no neighboring regions have the same color. To
formulate this as a CSP, we define the variables to be the regions
X = {WA, NT , Q, NSW , V, SA, T } .
The domain of each variable is the set Di = {red , green, blue}. The constraints require
neighboring regions to have distinct colors. Since there are nine places where regions border,
there are nine constraints:
C = {SA = WA, SA = NT , SA = Q, SA = NSW , SA = V,
WA = NT , NT = Q, Q = NSW , NSW = V } .
Here we are using abbreviations; SA = WA is a shortcut for (SA, WA), SA = WA, where
SA = WA can be fully enumerated in turn as
{(red , green), (red , blue), (green, red ), (green, blue), (blue, red ), (blue, green)} .
There are many possible solutions to this problem, such as
{WA = red , NT = green, Q = red , NSW = green, V = red , SA = blue, T = red }.
CONSTRAINT GRAPH

It can be helpful to visualize a CSP as a constraint graph, as shown in Figure 6.1(b). The
nodes of the graph correspond to variables of the problem, and a link connects any two variables that participate in a constraint.
Why formulate a problem as a CSP? One reason is that the CSPs yield a natural representation for a wide variety of problems; if you already have a CSP-solving system, it is
often easier to solve a problem using it than to design a custom solution using another search
technique. In addition, CSP solvers can be faster than state-space searchers because the CSP
solver can quickly eliminate large swatches of the search space. For example, once we have
chosen {SA = blue} in the Australia problem, we can conclude that none of the five neighboring variables can take on the value blue. Without taking advantage of constraint propagation,
a search procedure would have to consider 35 = 243 assignments for the five neighboring
variables; with constraint propagation we never have to consider blue as a value, so we have
only 25 = 32 assignments to look at, a reduction of 87%.
In regular state-space search we can only ask: is this specific state a goal? No? What
about this one? With CSPs, once we find out that a partial assignment is not a solution, we can

204

Chapter

6.

Constraint Satisfaction Problems

NT
Q
WA

Northern
Territory
Queensland
Western
Australia

SA
South
Australia

New
South
Wales

NSW
V

Victoria

T

Tasmania

(a)

(b)

Figure 6.1 (a) The principal states and territories of Australia. Coloring this map can
be viewed as a constraint satisfaction problem (CSP). The goal is to assign colors to each
region so that no neighboring regions have the same color. (b) The map-coloring problem
represented as a constraint graph.

immediately discard further refinements of the partial assignment. Furthermore, we can see
why the assignment is not a solutionâ€”we see which variables violate a constraintâ€”so we can
focus attention on the variables that matter. As a result, many problems that are intractable
for regular state-space search can be solved quickly when formulated as a CSP.

6.1.2 Example problem: Job-shop scheduling

PRECEDENCE
CONSTRAINTS

Factories have the problem of scheduling a dayâ€™s worth of jobs, subject to various constraints.
In practice, many of these problems are solved with CSP techniques. Consider the problem of
scheduling the assembly of a car. The whole job is composed of tasks, and we can model each
task as a variable, where the value of each variable is the time that the task starts, expressed
as an integer number of minutes. Constraints can assert that one task must occur before
anotherâ€”for example, a wheel must be installed before the hubcap is put onâ€”and that only
so many tasks can go on at once. Constraints can also specify that a task takes a certain
amount of time to complete.
We consider a small part of the car assembly, consisting of 15 tasks: install axles (front
and back), affix all four wheels (right and left, front and back), tighten nuts for each wheel,
affix hubcaps, and inspect the final assembly. We can represent the tasks with 15 variables:
X = {Axle F , Axle B , Wheel RF , Wheel LF , Wheel RB , Wheel LB , Nuts RF ,
Nuts LF , Nuts RB , Nuts LB , Cap RF , Cap LF , Cap RB , Cap LB , Inspect } .
The value of each variable is the time that the task starts. Next we represent precedence
constraints between individual tasks. Whenever a task T1 must occur before task T2 , and
task T1 takes duration d1 to complete, we add an arithmetic constraint of the form
T1 + d1 â‰¤ T2 .

Section 6.1.

Defining Constraint Satisfaction Problems

205

In our example, the axles have to be in place before the wheels are put on, and it takes 10
minutes to install an axle, so we write
Axle F + 10 â‰¤ Wheel RF ; Axle F + 10 â‰¤ Wheel LF ;
Axle B + 10 â‰¤ Wheel RB ; Axle B + 10 â‰¤ Wheel LB .
Next we say that, for each wheel, we must affix the wheel (which takes 1 minute), then tighten
the nuts (2 minutes), and finally attach the hubcap (1 minute, but not represented yet):

DISJUNCTIVE
CONSTRAINT

Wheel RF + 1 â‰¤ Nuts RF ; Nuts RF + 2 â‰¤ Cap RF ;
Wheel LF + 1 â‰¤ Nuts LF ; Nuts LF + 2 â‰¤ Cap LF ;
Wheel RB + 1 â‰¤ Nuts RB ; Nuts RB + 2 â‰¤ Cap RB ;
Wheel LB + 1 â‰¤ Nuts LB ; Nuts LB + 2 â‰¤ Cap LB .
Suppose we have four workers to install wheels, but they have to share one tool that helps put
the axle in place. We need a disjunctive constraint to say that Axle F and Axle B must not
overlap in time; either one comes first or the other does:
(Axle F + 10 â‰¤ Axle B )

or (Axle B + 10 â‰¤ Axle F ) .

This looks like a more complicated constraint, combining arithmetic and logic. But it still
reduces to a set of pairs of values that Axle F and Axle F can take on.
We also need to assert that the inspection comes last and takes 3 minutes. For every
variable except Inspect we add a constraint of the form X + dX â‰¤ Inspect . Finally, suppose
there is a requirement to get the whole assembly done in 30 minutes. We can achieve that by
limiting the domain of all variables:
Di = {1, 2, 3, . . . , 27} .
This particular problem is trivial to solve, but CSPs have been applied to job-shop scheduling problems like this with thousands of variables. In some cases, there are complicated
constraints that are difficult to specify in the CSP formalism, and more advanced planning
techniques are used, as discussed in Chapter 11.

6.1.3 Variations on the CSP formalism
DISCRETE DOMAIN
FINITE DOMAIN

INFINITE

CONSTRAINT
LANGUAGE

LINEAR
CONSTRAINTS
NONLINEAR
CONSTRAINTS

The simplest kind of CSP involves variables that have discrete, finite domains. Mapcoloring problems and scheduling with time limits are both of this kind. The 8-queens problem described in Chapter 3 can also be viewed as a finite-domain CSP, where the variables
Q1 , . . . , Q8 are the positions of each queen in columns 1, . . . , 8 and each variable has the
domain Di = {1, 2, 3, 4, 5, 6, 7, 8}.
A discrete domain can be infinite, such as the set of integers or strings. (If we didnâ€™t put
a deadline on the job-scheduling problem, there would be an infinite number of start times
for each variable.) With infinite domains, it is no longer possible to describe constraints by
enumerating all allowed combinations of values. Instead, a constraint language must be
used that understands constraints such as T1 + d1 â‰¤ T2 directly, without enumerating the
set of pairs of allowable values for (T1 , T2 ). Special solution algorithms (which we do not
discuss here) exist for linear constraints on integer variablesâ€”that is, constraints, such as
the one just given, in which each variable appears only in linear form. It can be shown that
no algorithm exists for solving general nonlinear constraints on integer variables.

206
CONTINUOUS
DOMAINS

UNARY CONSTRAINT

BINARY CONSTRAINT

GLOBAL
CONSTRAINT

CRYPTARITHMETIC

Chapter

6.

Constraint Satisfaction Problems

Constraint satisfaction problems with continuous domains are common in the real
world and are widely studied in the field of operations research. For example, the scheduling
of experiments on the Hubble Space Telescope requires very precise timing of observations;
the start and finish of each observation and maneuver are continuous-valued variables that
must obey a variety of astronomical, precedence, and power constraints. The best-known
category of continuous-domain CSPs is that of linear programming problems, where constraints must be linear equalities or inequalities. Linear programming problems can be solved
in time polynomial in the number of variables. Problems with different types of constraints
and objective functions have also been studiedâ€”quadratic programming, second-order conic
programming, and so on.
In addition to examining the types of variables that can appear in CSPs, it is useful to
look at the types of constraints. The simplest type is the unary constraint, which restricts
the value of a single variable. For example, in the map-coloring problem it could be the case
that South Australians wonâ€™t tolerate the color green; we can express that with the unary
constraint (SA), SA = green
A binary constraint relates two variables. For example, SA = NSW is a binary
constraint. A binary CSP is one with only binary constraints; it can be represented as a
constraint graph, as in Figure 6.1(b).
We can also describe higher-order constraints, such as asserting that the value of Y is
between X and Z, with the ternary constraint Between(X, Y, Z).
A constraint involving an arbitrary number of variables is called a global constraint.
(The name is traditional but confusing because it need not involve all the variables in a problem). One of the most common global constraints is Alldiï¬€ , which says that all of the
variables involved in the constraint must have different values. In Sudoku problems (see
Section 6.2.6), all variables in a row or column must satisfy an Alldiï¬€ constraint. Another example is provided by cryptarithmetic puzzles. (See Figure 6.2(a).) Each letter in a
cryptarithmetic puzzle represents a different digit. For the case in Figure 6.2(a), this would
be represented as the global constraint Alldiï¬€ (F, T, U, W, R, O). The addition constraints
on the four columns of the puzzle can be written as the following n-ary constraints:
O + O = R + 10 Â· C10
C10 + W + W = U + 10 Â· C100
C100 + T + T = O + 10 Â· C1000
C1000 = F ,

CONSTRAINT
HYPERGRAPH

DUAL GRAPH

where C10 , C100 , and C1000 are auxiliary variables representing the digit carried over into the
tens, hundreds, or thousands column. These constraints can be represented in a constraint
hypergraph, such as the one shown in Figure 6.2(b). A hypergraph consists of ordinary nodes
(the circles in the figure) and hypernodes (the squares), which represent n-ary constraints.
Alternatively, as Exercise 6.6 asks you to prove, every finite-domain constraint can be
reduced to a set of binary constraints if enough auxiliary variables are introduced, so we could
transform any CSP into one with only binary constraints; this makes the algorithms simpler.
Another way to convert an n-ary CSP to a binary one is the dual graph transformation: create
a new graph in which there will be one variable for each constraint in the original graph, and

Section 6.1.

Defining Constraint Satisfaction Problems

T W O
+ T W O

207

F

T

U

W

R

O

F O U R
C3
(a)

C1

C2
(b)

Figure 6.2 (a) A cryptarithmetic problem. Each letter stands for a distinct digit; the aim is
to find a substitution of digits for letters such that the resulting sum is arithmetically correct,
with the added restriction that no leading zeroes are allowed. (b) The constraint hypergraph
for the cryptarithmetic problem, showing the Alldiï¬€ constraint (square box at the top) as
well as the column addition constraints (four square boxes in the middle). The variables C1 ,
C2 , and C3 represent the carry digits for the three columns.

PREFERENCE
CONSTRAINTS

CONSTRAINT
OPTIMIZATION
PROBLEM

one binary constraint for each pair of constraints in the original graph that share variables. For
example, if the original graph has variables {X, Y, Z} and constraints (X, Y, Z), C1  and
(X, Y ), C2  then the dual graph would have variables {C1 , C2 } with the binary constraint
(X, Y ), R1 , where (X, Y ) are the shared variables and R1 is a new relation that defines the
constraint between the shared variables, as specified by the original C1 and C2 .
There are however two reasons why we might prefer a global constraint such as Alldiï¬€
rather than a set of binary constraints. First, it is easier and less error-prone to write the
problem description using Alldiï¬€ . Second, it is possible to design special-purpose inference
algorithms for global constraints that are not available for a set of more primitive constraints.
We describe these inference algorithms in Section 6.2.5.
The constraints we have described so far have all been absolute constraints, violation of
which rules out a potential solution. Many real-world CSPs include preference constraints
indicating which solutions are preferred. For example, in a university class-scheduling problem there are absolute constraints that no professor can teach two classes at the same time.
But we also may allow preference constraints: Prof. R might prefer teaching in the morning,
whereas Prof. N prefers teaching in the afternoon. A schedule that has Prof. R teaching at
2 p.m. would still be an allowable solution (unless Prof. R happens to be the department chair)
but would not be an optimal one. Preference constraints can often be encoded as costs on individual variable assignmentsâ€”for example, assigning an afternoon slot for Prof. R costs
2 points against the overall objective function, whereas a morning slot costs 1. With this
formulation, CSPs with preferences can be solved with optimization search methods, either
path-based or local. We call such a problem a constraint optimization problem, or COP.
Linear programming problems do this kind of optimization.

208

6.2

Chapter

6.

Constraint Satisfaction Problems

C ONSTRAINT P ROPAGATION : I NFERENCE IN CSP S

INFERENCE
CONSTRAINT
PROPAGATION

LOCAL
CONSISTENCY

In regular state-space search, an algorithm can do only one thing: search. In CSPs there is a
choice: an algorithm can search (choose a new variable assignment from several possibilities)
or do a specific type of inference called constraint propagation: using the constraints to
reduce the number of legal values for a variable, which in turn can reduce the legal values
for another variable, and so on. Constraint propagation may be intertwined with search, or it
may be done as a preprocessing step, before search starts. Sometimes this preprocessing can
solve the whole problem, so no search is required at all.
The key idea is local consistency. If we treat each variable as a node in a graph (see
Figure 6.1(b)) and each binary constraint as an arc, then the process of enforcing local consistency in each part of the graph causes inconsistent values to be eliminated throughout the
graph. There are different types of local consistency, which we now cover in turn.

6.2.1 Node consistency
NODE CONSISTENCY

A single variable (corresponding to a node in the CSP network) is node-consistent if all
the values in the variableâ€™s domain satisfy the variableâ€™s unary constraints. For example,
in the variant of the Australia map-coloring problem (Figure 6.1) where South Australians
dislike green, the variable SA starts with domain {red , green, blue}, and we can make it
node consistent by eliminating green, leaving SA with the reduced domain {red , blue}. We
say that a network is node-consistent if every variable in the network is node-consistent.
It is always possible to eliminate all the unary constraints in a CSP by running node
consistency. It is also possible to transform all n-ary constraints into binary ones (see Exercise 6.6). Because of this, it is common to define CSP solvers that work with only binary
constraints; we make that assumption for the rest of this chapter, except where noted.

6.2.2 Arc consistency
ARC CONSISTENCY

A variable in a CSP is arc-consistent if every value in its domain satisfies the variableâ€™s
binary constraints. More formally, Xi is arc-consistent with respect to another variable Xj if
for every value in the current domain Di there is some value in the domain Dj that satisfies
the binary constraint on the arc (Xi , Xj ). A network is arc-consistent if every variable is arc
consistent with every other variable. For example, consider the constraint Y = X 2 where the
domain of both X and Y is the set of digits. We can write this constraint explicitly as
(X, Y ), {(0, 0), (1, 1), (2, 4), (3, 9))} .
To make X arc-consistent with respect to Y , we reduce Xâ€™s domain to {0, 1, 2, 3}. If we
also make Y arc-consistent with respect to X, then Y â€™s domain becomes {0, 1, 4, 9} and the
whole CSP is arc-consistent.
On the other hand, arc consistency can do nothing for the Australia map-coloring problem. Consider the following inequality constraint on (SA, WA):
{(red , green), (red , blue), (green, red ), (green, blue), (blue, red ), (blue, green)} .

Section 6.2.

Constraint Propagation: Inference in CSPs

209

function AC-3( csp) returns false if an inconsistency is found and true otherwise
inputs: csp, a binary CSP with components (X, D, C)
local variables: queue, a queue of arcs, initially all the arcs in csp
while queue is not empty do
(Xi , Xj ) â† R EMOVE -F IRST(queue)
if R EVISE(csp, Xi , Xj ) then
if size of Di = 0 then return false
for each Xk in Xi .N EIGHBORS - {Xj } do
add (Xk , Xi ) to queue
return true
function R EVISE( csp, Xi , Xj ) returns true iff we revise the domain of Xi
revised â† false
for each x in Di do
if no value y in Dj allows (x ,y) to satisfy the constraint between Xi and Xj then
delete x from Di
revised â† true
return revised
Figure 6.3 The arc-consistency algorithm AC-3. After applying AC-3, either every arc
is arc-consistent, or some variable has an empty domain, indicating that the CSP cannot be
solved. The name â€œAC-3â€ was used by the algorithmâ€™s inventor (Mackworth, 1977) because
itâ€™s the third version developed in the paper.

No matter what value you choose for SA (or for WA), there is a valid value for the other
variable. So applying arc consistency has no effect on the domains of either variable.
The most popular algorithm for arc consistency is called AC-3 (see Figure 6.3). To
make every variable arc-consistent, the AC-3 algorithm maintains a queue of arcs to consider.
(Actually, the order of consideration is not important, so the data structure is really a set, but
tradition calls it a queue.) Initially, the queue contains all the arcs in the CSP. AC-3 then pops
off an arbitrary arc (Xi , Xj ) from the queue and makes Xi arc-consistent with respect to Xj .
If this leaves Di unchanged, the algorithm just moves on to the next arc. But if this revises
Di (makes the domain smaller), then we add to the queue all arcs (Xk , Xi ) where Xk is a
neighbor of Xi . We need to do that because the change in Di might enable further reductions
in the domains of Dk , even if we have previously considered Xk . If Di is revised down to
nothing, then we know the whole CSP has no consistent solution, and AC-3 can immediately
return failure. Otherwise, we keep checking, trying to remove values from the domains of
variables until no more arcs are in the queue. At that point, we are left with a CSP that is
equivalent to the original CSPâ€”they both have the same solutionsâ€”but the arc-consistent
CSP will in most cases be faster to search because its variables have smaller domains.
The complexity of AC-3 can be analyzed as follows. Assume a CSP with n variables,
each with domain size at most d, and with c binary constraints (arcs). Each arc (Xk , Xi ) can
be inserted in the queue only d times because Xi has at most d values to delete. Checking

210

GENERALIZED ARC
CONSISTENT

Chapter

6.

Constraint Satisfaction Problems

consistency of an arc can be done in O(d2 ) time, so we get O(cd3 ) total worst-case time.1
It is possible to extend the notion of arc consistency to handle n-ary rather than just
binary constraints; this is called generalized arc consistency or sometimes hyperarc consistency, depending on the author. A variable Xi is generalized arc consistent with respect to
an n-ary constraint if for every value v in the domain of Xi there exists a tuple of values that
is a member of the constraint, has all its values taken from the domains of the corresponding
variables, and has its Xi component equal to v. For example, if all variables have the domain {0, 1, 2, 3}, then to make the variable X consistent with the constraint X < Y < Z,
we would have to eliminate 2 and 3 from the domain of X because the constraint cannot be
satisfied when X is 2 or 3.

6.2.3 Path consistency

PATH CONSISTENCY

Arc consistency can go a long way toward reducing the domains of variables, sometimes
finding a solution (by reducing every domain to size 1) and sometimes finding that the CSP
cannot be solved (by reducing some domain to size 0). But for other networks, arc consistency
fails to make enough inferences. Consider the map-coloring problem on Australia, but with
only two colors allowed, red and blue. Arc consistency can do nothing because every variable
is already arc consistent: each can be red with blue at the other end of the arc (or vice versa).
But clearly there is no solution to the problem: because Western Australia, Northern Territory
and South Australia all touch each other, we need at least three colors for them alone.
Arc consistency tightens down the domains (unary constraints) using the arcs (binary
constraints). To make progress on problems like map coloring, we need a stronger notion of
consistency. Path consistency tightens the binary constraints by using implicit constraints
that are inferred by looking at triples of variables.
A two-variable set {Xi , Xj } is path-consistent with respect to a third variable Xm if,
for every assignment {Xi = a, Xj = b} consistent with the constraints on {Xi , Xj }, there is
an assignment to Xm that satisfies the constraints on {Xi , Xm } and {Xm , Xj }. This is called
path consistency because one can think of it as looking at a path from Xi to Xj with Xm in
the middle.
Letâ€™s see how path consistency fares in coloring the Australia map with two colors. We
will make the set {WA, SA} path consistent with respect to NT . We start by enumerating the
consistent assignments to the set. In this case, there are only two: {WA = red , SA = blue}
and {WA = blue, SA = red }. We can see that with both of these assignments NT can be
neither red nor blue (because it would conflict with either WA or SA). Because there is no
valid choice for NT , we eliminate both assignments, and we end up with no valid assignments
for {WA, SA}. Therefore, we know that there can be no solution to this problem. The PC-2
algorithm (Mackworth, 1977) achieves path consistency in much the same way that AC-3
achieves arc consistency. Because it is so similar, we do not show it here.
The AC-4 algorithm (Mohr and Henderson, 1986) runs in O(cd2 ) worst-case time but can be slower than AC-3
on average cases. See Exercise 6.13.
1

Section 6.2.

Constraint Propagation: Inference in CSPs

211

6.2.4 K-consistency
K-CONSISTENCY

STRONGLY
K-CONSISTENT

Stronger forms of propagation can be defined with the notion of k-consistency. A CSP is
k-consistent if, for any set of k âˆ’ 1 variables and for any consistent assignment to those
variables, a consistent value can always be assigned to any kth variable. 1-consistency says
that, given the empty set, we can make any set of one variable consistent: this is what we
called node consistency. 2-consistency is the same as arc consistency. For binary constraint
networks, 3-consistency is the same as path consistency.
A CSP is strongly k-consistent if it is k-consistent and is also (k âˆ’ 1)-consistent,
(k âˆ’ 2)-consistent, . . . all the way down to 1-consistent. Now suppose we have a CSP with
n nodes and make it strongly n-consistent (i.e., strongly k-consistent for k = n). We can
then solve the problem as follows: First, we choose a consistent value for X1 . We are then
guaranteed to be able to choose a value for X2 because the graph is 2-consistent, for X3
because it is 3-consistent, and so on. For each variable Xi , we need only search through the d
values in the domain to find a value consistent with X1 , . . . , Xiâˆ’1 . We are guaranteed to find
a solution in time O(n2 d). Of course, there is no free lunch: any algorithm for establishing
n-consistency must take time exponential in n in the worst case. Worse, n-consistency also
requires space that is exponential in n. The memory issue is even more severe than the time.
In practice, determining the appropriate level of consistency checking is mostly an empirical
science. It can be said practitioners commonly compute 2-consistency and less commonly
3-consistency.

6.2.5 Global constraints
Remember that a global constraint is one involving an arbitrary number of variables (but not
necessarily all variables). Global constraints occur frequently in real problems and can be
handled by special-purpose algorithms that are more efficient than the general-purpose methods described so far. For example, the Alldiï¬€ constraint says that all the variables involved
must have distinct values (as in the cryptarithmetic problem above and Sudoku puzzles below). One simple form of inconsistency detection for Alldiï¬€ constraints works as follows:
if m variables are involved in the constraint, and if they have n possible distinct values altogether, and m > n, then the constraint cannot be satisfied.
This leads to the following simple algorithm: First, remove any variable in the constraint that has a singleton domain, and delete that variableâ€™s value from the domains of the
remaining variables. Repeat as long as there are singleton variables. If at any point an empty
domain is produced or there are more variables than domain values left, then an inconsistency
has been detected.
This method can detect the inconsistency in the assignment {WA = red , NSW = red }
for Figure 6.1. Notice that the variables SA, NT , and Q are effectively connected by an
Alldiï¬€ constraint because each pair must have two different colors. After applying AC-3
with the partial assignment, the domain of each variable is reduced to {green, blue}. That
is, we have three variables and only two colors, so the Alldiï¬€ constraint is violated. Thus,
a simple consistency procedure for a higher-order constraint is sometimes more effective
than applying arc consistency to an equivalent set of binary constraints. There are more

212

RESOURCE
CONSTRAINT

BOUNDS
PROPAGATION

Chapter

6.

Constraint Satisfaction Problems

complex inference algorithms for Alldiï¬€ (see van Hoeve and Katriel, 2006) that propagate
more constraints but are more computationally expensive to run.
Another important higher-order constraint is the resource constraint, sometimes called
the atmost constraint. For example, in a scheduling problem, let P1 , . . . , P4 denote the
numbers of personnel assigned to each of four tasks. The constraint that no more than 10
personnel are assigned in total is written as Atmost (10, P1 , P2 , P3 , P4 ). We can detect an
inconsistency simply by checking the sum of the minimum values of the current domains;
for example, if each variable has the domain {3, 4, 5, 6}, the Atmost constraint cannot be
satisfied. We can also enforce consistency by deleting the maximum value of any domain if it
is not consistent with the minimum values of the other domains. Thus, if each variable in our
example has the domain {2, 3, 4, 5, 6}, the values 5 and 6 can be deleted from each domain.
For large resource-limited problems with integer valuesâ€”such as logistical problems
involving moving thousands of people in hundreds of vehiclesâ€”it is usually not possible to
represent the domain of each variable as a large set of integers and gradually reduce that set by
consistency-checking methods. Instead, domains are represented by upper and lower bounds
and are managed by bounds propagation. For example, in an airline-scheduling problem,
letâ€™s suppose there are two flights, F1 and F2 , for which the planes have capacities 165 and
385, respectively. The initial domains for the numbers of passengers on each flight are then
D1 = [0, 165] and

D2 = [0, 385] .

Now suppose we have the additional constraint that the two flights together must carry 420
people: F1 + F2 = 420. Propagating bounds constraints, we reduce the domains to
D1 = [35, 165] and
BOUNDS
CONSISTENT

D2 = [255, 385] .

We say that a CSP is bounds consistent if for every variable X, and for both the lowerbound and upper-bound values of X, there exists some value of Y that satisfies the constraint
between X and Y for every variable Y . This kind of bounds propagation is widely used in
practical constraint problems.

6.2.6 Sudoku example
SUDOKU

The popular Sudoku puzzle has introduced millions of people to constraint satisfaction problems, although they may not recognize it. A Sudoku board consists of 81 squares, some of
which are initially filled with digits from 1 to 9. The puzzle is to fill in all the remaining
squares such that no digit appears twice in any row, column, or 3 Ã— 3 box (see Figure 6.4). A
row, column, or box is called a unit.
The Sudoku puzzles that are printed in newspapers and puzzle books have the property
that there is exactly one solution. Although some can be tricky to solve by hand, taking tens
of minutes, even the hardest Sudoku problems yield to a CSP solver in less than 0.1 second.
A Sudoku puzzle can be considered a CSP with 81 variables, one for each square. We
use the variable names A1 through A9 for the top row (left to right), down to I1 through I9
for the bottom row. The empty squares have the domain {1, 2, 3, 4, 5, 6, 7, 8, 9} and the prefilled squares have a domain consisting of a single value. In addition, there are 27 different

Section 6.2.

Constraint Propagation: Inference in CSPs
1

2

9

C
D
E

G

I

5

2

3
1 8
8 1

6

7

8

8

5
6 4
2 9

A

1

B
C
D

8
6 7
8 2
2 6
9 5
2
3
5
1
3

1

2

3

4

5

6

7

8

9

4
9
2
5
7
1
3
8
6

8
6
5
4
2
3
7
1
9

3
7
1
8
9
6
2
4
5

9
3
8
1
5
7
6
2
4

2
4
7
3
6
9
8
5
1

1
5
6
2
4
8
9
3
7

6
8
4
9
1
2
5
7
3

5
2
9
7
3
4
1
6
8

7
1
3
6
8
5
4
9
2

9

6

7

F

H

4

3

A
B

3

213

E
F
G

9

H
I

(a)
Figure 6.4

(b)

(a) A Sudoku puzzle and (b) its solution.

Alldiï¬€ constraints: one for each row, column, and box of 9 squares.
Alldiï¬€ (A1, A2, A3, A4, A5, A6, A7, A8, A9)
Alldiï¬€ (B1, B2, B3, B4, B5, B6, B7, B8, B9)
Â·Â·Â·
Alldiï¬€ (A1, B1, C1, D1, E1, F 1, G1, H1, I1)
Alldiï¬€ (A2, B2, C2, D2, E2, F 2, G2, H2, I2)
Â·Â·Â·
Alldiï¬€ (A1, A2, A3, B1, B2, B3, C1, C2, C3)
Alldiï¬€ (A4, A5, A6, B4, B5, B6, C4, C5, C6)
Â·Â·Â·
Let us see how far arc consistency can take us. Assume that the Alldiï¬€ constraints have been
expanded into binary constraints (such as A1 = A2 ) so that we can apply the AC-3 algorithm
directly. Consider variable E6 from Figure 6.4(a)â€”the empty square between the 2 and the
8 in the middle box. From the constraints in the box, we can remove not only 2 and 8 but also
1 and 7 from E6 â€™s domain. From the constraints in its column, we can eliminate 5, 6, 2, 8,
9, and 3. That leaves E6 with a domain of {4}; in other words, we know the answer for E6 .
Now consider variable I6 â€”the square in the bottom middle box surrounded by 1, 3, and 3.
Applying arc consistency in its column, we eliminate 5, 6, 2, 4 (since we now know E6 must
be 4), 8, 9, and 3. We eliminate 1 by arc consistency with I5 , and we are left with only the
value 7 in the domain of I6 . Now there are 8 known values in column 6, so arc consistency
can infer that A6 must be 1. Inference continues along these lines, and eventually, AC-3 can
solve the entire puzzleâ€”all the variables have their domains reduced to a single value, as
shown in Figure 6.4(b).
Of course, Sudoku would soon lose its appeal if every puzzle could be solved by a

214

Chapter

6.

Constraint Satisfaction Problems

mechanical application of AC-3, and indeed AC-3 works only for the easiest Sudoku puzzles.
Slightly harder ones can be solved by PC-2, but at a greater computational cost: there are
255,960 different path constraints to consider in a Sudoku puzzle. To solve the hardest puzzles
and to make efficient progress, we will have to be more clever.
Indeed, the appeal of Sudoku puzzles for the human solver is the need to be resourceful
in applying more complex inference strategies. Aficionados give them colorful names, such
as â€œnaked triples.â€ That strategy works as follows: in any unit (row, column or box), find
three squares that each have a domain that contains the same three numbers or a subset of
those numbers. For example, the three domains might be {1, 8}, {3, 8}, and {1, 3, 8}. From
that we donâ€™t know which square contains 1, 3, or 8, but we do know that the three numbers
must be distributed among the three squares. Therefore we can remove 1, 3, and 8 from the
domains of every other square in the unit.
It is interesting to note how far we can go without saying much that is specific to Sudoku. We do of course have to say that there are 81 variables, that their domains are the digits
1 to 9, and that there are 27 Alldiï¬€ constraints. But beyond that, all the strategiesâ€”arc consistency, path consistency, etc.â€”apply generally to all CSPs, not just to Sudoku problems.
Even naked triples is really a strategy for enforcing consistency of Alldiï¬€ constraints and
has nothing to do with Sudoku per se. This is the power of the CSP formalism: for each new
problem area, we only need to define the problem in terms of constraints; then the general
constraint-solving mechanisms can take over.

6.3

BACKTRACKING S EARCH FOR CSP S

COMMUTATIVITY

Sudoku problems are designed to be solved by inference over constraints. But many other
CSPs cannot be solved by inference alone; there comes a time when we must search for a
solution. In this section we look at backtracking search algorithms that work on partial assignments; in the next section we look at local search algorithms over complete assignments.
We could apply a standard depth-limited search (from Chapter 3). A state would be a
partial assignment, and an action would be adding var = value to the assignment. But for a
CSP with n variables of domain size d, we quickly notice something terrible: the branching
factor at the top level is nd because any of d values can be assigned to any of n variables. At
the next level, the branching factor is (n âˆ’ 1)d, and so on for n levels. We generate a tree
with n! Â· dn leaves, even though there are only dn possible complete assignments!
Our seemingly reasonable but naive formulation ignores crucial property common to
all CSPs: commutativity. A problem is commutative if the order of application of any given
set of actions has no effect on the outcome. CSPs are commutative because when assigning
values to variables, we reach the same partial assignment regardless of order. Therefore, we
need only consider a single variable at each node in the search tree. For example, at the root
node of a search tree for coloring the map of Australia, we might make a choice between
SA = red , SA = green, and SA = blue, but we would never choose between SA = red and
WA = blue. With this restriction, the number of leaves is dn , as we would hope.

Section 6.3.

Backtracking Search for CSPs

215

function BACKTRACKING-S EARCH (csp) returns a solution, or failure
return BACKTRACK({ }, csp)
function BACKTRACK (assignment, csp) returns a solution, or failure
if assignment is complete then return assignment
var â† S ELECT-U NASSIGNED -VARIABLE(csp)
for each value in O RDER -D OMAIN -VALUES (var , assignment, csp) do
if value is consistent with assignment then
add {var = value} to assignment
inferences â† I NFERENCE(csp, var , value)
if inferences = failure then
add inferences to assignment
result â† BACKTRACK (assignment, csp)
if result = failure then
return result
remove {var = value} and inferences from assignment
return failure
Figure 6.5 A simple backtracking algorithm for constraint satisfaction problems. The algorithm is modeled on the recursive depth-first search of Chapter 3. By varying the functions
S ELECT-U NASSIGNED -VARIABLE and O RDER -D OMAIN -VALUES , we can implement the
general-purpose heuristics discussed in the text. The function I NFERENCE can optionally be
used to impose arc-, path-, or k-consistency, as desired. If a value choice leads to failure
(noticed either by I NFERENCE or by BACKTRACK ), then value assignments (including those
made by I NFERENCE) are removed from the current assignment and a new value is tried.
BACKTRACKING
SEARCH

The term backtracking search is used for a depth-first search that chooses values for
one variable at a time and backtracks when a variable has no legal values left to assign. The
algorithm is shown in Figure 6.5. It repeatedly chooses an unassigned variable, and then tries
all values in the domain of that variable in turn, trying to find a solution. If an inconsistency is
detected, then BACKTRACK returns failure, causing the previous call to try another value. Part
of the search tree for the Australia problem is shown in Figure 6.6, where we have assigned
variables in the order WA, NT , Q, . . .. Because the representation of CSPs is standardized,
there is no need to supply BACKTRACKING-S EARCH with a domain-specific initial state,
action function, transition model, or goal test.
Notice that BACKTRACKING-S EARCH keeps only a single representation of a state and
alters that representation rather than creating new ones, as described on page 87.
In Chapter 3 we improved the poor performance of uninformed search algorithms by
supplying them with domain-specific heuristic functions derived from our knowledge of the
problem. It turns out that we can solve CSPs efficiently without such domain-specific knowledge. Instead, we can add some sophistication to the unspecified functions in Figure 6.5,
using them to address the following questions:
1. Which variable should be assigned next (S ELECT-U NASSIGNED -VARIABLE ), and in
what order should its values be tried (O RDER -D OMAIN -VALUES )?

216

Chapter

WA=red

WA=red
NT=green

WA=red
NT=green
Q=red

Figure 6.6

6.

Constraint Satisfaction Problems

WA=green

WA=blue

WA=red
NT=blue

WA=red
NT=green
Q=blue

Part of the search tree for the map-coloring problem in Figure 6.1.

2. What inferences should be performed at each step in the search (I NFERENCE )?
3. When the search arrives at an assignment that violates a constraint, can the search avoid
repeating this failure?
The subsections that follow answer each of these questions in turn.

6.3.1 Variable and value ordering
The backtracking algorithm contains the line
var â† S ELECT-U NASSIGNED -VARIABLE(csp) .

MINIMUMREMAINING-VALUES

DEGREE HEURISTIC

The simplest strategy for S ELECT-U NASSIGNED -VARIABLE is to choose the next unassigned
variable in order, {X1 , X2 , . . .}. This static variable ordering seldom results in the most efficient search. For example, after the assignments for WA = red and NT = green in Figure 6.6,
there is only one possible value for SA, so it makes sense to assign SA = blue next rather than
assigning Q. In fact, after SA is assigned, the choices for Q, NSW , and V are all forced. This
intuitive ideaâ€”choosing the variable with the fewest â€œlegalâ€ valuesâ€”is called the minimumremaining-values (MRV) heuristic. It also has been called the â€œmost constrained variableâ€ or
â€œfail-firstâ€ heuristic, the latter because it picks a variable that is most likely to cause a failure
soon, thereby pruning the search tree. If some variable X has no legal values left, the MRV
heuristic will select X and failure will be detected immediatelyâ€”avoiding pointless searches
through other variables. The MRV heuristic usually performs better than a random or static
ordering, sometimes by a factor of 1,000 or more, although the results vary widely depending
on the problem.
The MRV heuristic doesnâ€™t help at all in choosing the first region to color in Australia,
because initially every region has three legal colors. In this case, the degree heuristic comes
in handy. It attempts to reduce the branching factor on future choices by selecting the variable that is involved in the largest number of constraints on other unassigned variables. In
Figure 6.1, SA is the variable with highest degree, 5; the other variables have degree 2 or 3,
except for T , which has degree 0. In fact, once SA is chosen, applying the degree heuristic solves the problem without any false stepsâ€”you can choose any consistent color at each
choice point and still arrive at a solution with no backtracking. The minimum-remaining-

Section 6.3.

LEASTCONSTRAININGVALUE

Backtracking Search for CSPs

217

values heuristic is usually a more powerful guide, but the degree heuristic can be useful as a
tie-breaker.
Once a variable has been selected, the algorithm must decide on the order in which to
examine its values. For this, the least-constraining-value heuristic can be effective in some
cases. It prefers the value that rules out the fewest choices for the neighboring variables in
the constraint graph. For example, suppose that in Figure 6.1 we have generated the partial
assignment with WA = red and NT = green and that our next choice is for Q. Blue would
be a bad choice because it eliminates the last legal value left for Qâ€™s neighbor, SA. The
least-constraining-value heuristic therefore prefers red to blue. In general, the heuristic is
trying to leave the maximum flexibility for subsequent variable assignments. Of course, if we
are trying to find all the solutions to a problem, not just the first one, then the ordering does
not matter because we have to consider every value anyway. The same holds if there are no
solutions to the problem.
Why should variable selection be fail-first, but value selection be fail-last? It turns out
that, for a wide variety of problems, a variable ordering that chooses a variable with the
minimum number of remaining values helps minimize the number of nodes in the search tree
by pruning larger parts of the tree earlier. For value orderi ng, the trick is that we only need
one solution; therefore it makes sense to look for the most likely values first. If we wanted to
enumerate all solutions rather than just find one, then value ordering would be irrelevant.

6.3.2 Interleaving search and inference

FORWARD
CHECKING

So far we have seen how AC-3 and other algorithms can infer reductions in the domain of
variables before we begin the search. But inference can be even more powerful in the course
of a search: every time we make a choice of a value for a variable, we have a brand-new
opportunity to infer new domain reductions on the neighboring variables.
One of the simplest forms of inference is called forward checking. Whenever a variable X is assigned, the forward-checking process establishes arc consistency for it: for each
unassigned variable Y that is connected to X by a constraint, delete from Y â€™s domain any
value that is inconsistent with the value chosen for X. Because forward checking only does
arc consistency inferences, there is no reason to do forward checking if we have already done
arc consistency as a preprocessing step.
Figure 6.7 shows the progress of backtracking search on the Australia CSP with forward checking. There are two important points to notice about this example. First, notice
that after WA = red and Q = green are assigned, the domains of NT and SA are reduced
to a single value; we have eliminated branching on these variables altogether by propagating information from WA and Q. A second point to notice is that after V = blue, the domain of SA is empty. Hence, forward checking has detected that the partial assignment
{WA = red , Q = green, V = blue} is inconsistent with the constraints of the problem, and
the algorithm will therefore backtrack immediately.
For many problems the search will be more effective if we combine the MRV heuristic with forward checking. Consider Figure 6.7 after assigning {WA = red }. Intuitively, it
seems that that assignment constrains its neighbors, NT and SA, so we should handle those

218

Chapter

WA

Initial domains
After WA=red
After Q=green
After V=blue

NT

Q

6.

Constraint Satisfaction Problems

NSW

V

SA

T

R G B R G B R G B R G B R G B R G B R G
G B R G B R G B R G B
R
G B R G
G
B
R
B R G B
B R G
R
G
B
R
B
R G
R

B
B
B
B

Figure 6.7 The progress of a map-coloring search with forward checking. WA = red
is assigned first; then forward checking deletes red from the domains of the neighboring
variables NT and SA. After Q = green is assigned, green is deleted from the domains of
NT , SA, and NSW . After V = blue is assigned, blue is deleted from the domains of NSW
and SA, leaving SA with no legal values.

MAINTAINING ARC
CONSISTENCY (MAC)

variables next, and then all the other variables will fall into place. Thatâ€™s exactly what happens with MRV: NT and SA have two values, so one of them is chosen first, then the other,
then Q, NSW , and V in order. Finally T still has three values, and any one of them works.
We can view forward checking as an efficient way to incrementally compute the information
that the MRV heuristic needs to do its job.
Although forward checking detects many inconsistencies, it does not detect all of them.
The problem is that it makes the current variable arc-consistent, but doesnâ€™t look ahead and
make all the other variables arc-consistent. For example, consider the third row of Figure 6.7.
It shows that when WA is red and Q is green, both NT and SA are forced to be blue. Forward
checking does not look far enough ahead to notice that this is an inconsistency: NT and SA
are adjacent and so cannot have the same value.
The algorithm called MAC (for Maintaining Arc Consistency (MAC)) detects this
inconsistency. After a variable Xi is assigned a value, the I NFERENCE procedure calls AC-3,
but instead of a queue of all arcs in the CSP, we start with only the arcs (Xj , Xi ) for all
Xj that are unassigned variables that are neighbors of Xi . From there, AC-3 does constraint
propagation in the usual way, and if any variable has its domain reduced to the empty set, the
call to AC-3 fails and we know to backtrack immediately. We can see that MAC is strictly
more powerful than forward checking because forward checking does the same thing as MAC
on the initial arcs in MACâ€™s queue; but unlike MAC, forward checking does not recursively
propagate constraints when changes are made to the domains of variables.

6.3.3 Intelligent backtracking: Looking backward

CHRONOLOGICAL
BACKTRACKING

The BACKTRACKING-S EARCH algorithm in Figure 6.5 has a very simple policy for what to
do when a branch of the search fails: back up to the preceding variable and try a different
value for it. This is called chronological backtracking because the most recent decision
point is revisited. In this subsection, we consider better possibilities.
Consider what happens when we apply simple backtracking in Figure 6.1 with a fixed
variable ordering Q, NSW , V , T , SA, WA, NT . Suppose we have generated the partial
assignment {Q = red , NSW = green, V = blue, T = red }. When we try the next variable,
SA, we see that every value violates a constraint. We back up to T and try a new color for

Section 6.3.

CONFLICT SET
BACKJUMPING

CONFLICT-DIRECTED
BACKJUMPING

Backtracking Search for CSPs

219

Tasmania! Obviously this is sillyâ€”recoloring Tasmania cannot possibly resolve the problem
with South Australia.
A more intelligent approach to backtracking is to backtrack to a variable that might fix
the problemâ€”a variable that was responsible for making one of the possible values of SA
impossible. To do this, we will keep track of a set of assignments that are in conflict with
some value for SA. The set (in this case {Q = red , NSW = green, V = blue, }), is called the
conflict set for SA. The backjumping method backtracks to the most recent assignment in
the conflict set; in this case, backjumping would jump over Tasmania and try a new value
for V . This method is easily implemented by a modification to BACKTRACK such that it
accumulates the conflict set while checking for a legal value to assign. If no legal value is
found, the algorithm should return the most recent element of the conflict set along with the
failure indicator.
The sharp-eyed reader will have noticed that forward checking can supply the conflict
set with no extra work: whenever forward checking based on an assignment X = x deletes a
value from Y â€™s domain, it should add X = x to Y â€™s conflict set. If the last value is deleted
from Y â€™s domain, then the assignments in the conflict set of Y are added to the conflict set
of X. Then, when we get to Y , we know immediately where to backtrack if needed.
The eagle-eyed reader will have noticed something odd: backjumping occurs when
every value in a domain is in conflict with the current assignment; but forward checking
detects this event and prevents the search from ever reaching such a node! In fact, it can be
shown that every branch pruned by backjumping is also pruned by forward checking. Hence,
simple backjumping is redundant in a forward-checking search or, indeed, in a search that
uses stronger consistency checking, such as MAC.
Despite the observations of the preceding paragraph, the idea behind backjumping remains a good one: to backtrack based on the reasons for failure. Backjumping notices failure
when a variableâ€™s domain becomes empty, but in many cases a branch is doomed long before
this occurs. Consider again the partial assignment {WA = red , NSW = red } (which, from
our earlier discussion, is inconsistent). Suppose we try T = red next and then assign NT , Q,
V , SA. We know that no assignment can work for these last four variables, so eventually we
run out of values to try at NT . Now, the question is, where to backtrack? Backjumping cannot
work, because NT does have values consistent with the preceding assigned variablesâ€”NT
doesnâ€™t have a complete conflict set of preceding variables that caused it to fail. We know,
however, that the four variables NT , Q, V , and SA, taken together, failed because of a set of
preceding variables, which must be those variables that directly conflict with the four. This
leads to a deeper notion of the conflict set for a variable such as NT : it is that set of preceding variables that caused NT , together with any subsequent variables, to have no consistent
solution. In this case, the set is WA and NSW , so the algorithm should backtrack to NSW
and skip over Tasmania. A backjumping algorithm that uses conflict sets defined in this way
is called conflict-directed backjumping.
We must now explain how these new conflict sets are computed. The method is in
fact quite simple. The â€œterminalâ€ failure of a branch of the search always occurs because a
variableâ€™s domain becomes empty; that variable has a standard conflict set. In our example,
SA fails, and its conflict set is (say) {WA, NT , Q}. We backjump to Q, and Q absorbs

220

Chapter

6.

Constraint Satisfaction Problems

the conflict set from SA (minus Q itself, of course) into its own direct conflict set, which is
{NT , NSW }; the new conflict set is {WA, NT , NSW }. That is, there is no solution from
Q onward, given the preceding assignment to {WA, NT , NSW }. Therefore, we backtrack
to NT , the most recent of these. NT absorbs {WA, NT , NSW } âˆ’ {NT } into its own
direct conflict set {WA}, giving {WA, NSW } (as stated in the previous paragraph). Now
the algorithm backjumps to NSW , as we would hope. To summarize: let Xj be the current
variable, and let conf (Xj ) be its conflict set. If every possible value for Xj fails, backjump
to the most recent variable Xi in conf (Xj ), and set
conf (Xi ) â† conf (Xi ) âˆª conf (Xj ) âˆ’ {Xi } .
When we reach a contradiction, backjumping can tell us how far to back up, so we donâ€™t
waste time changing variables that wonâ€™t fix the problem. But we would also like to avoid
running into the same problem again. When the search arrives at a contradiction, we know
that some subset of the conflict set is responsible for the problem. Constraint learning is the
idea of finding a minimum set of variables from the conflict set that causes the problem. This
set of variables, along with their corresponding values, is called a no-good. We then record
the no-good, either by adding a new constraint to the CSP or by keeping a separate cache of
no-goods.
For example, consider the state {WA = red , NT = green, Q = blue} in the bottom
row of Figure 6.6. Forward checking can tell us this state is a no-good because there is no
valid assignment to SA. In this particular case, recording the no-good would not help, because
once we prune this branch from the search tree, we will never encounter this combination
again. But suppose that the search tree in Figure 6.6 were actually part of a larger search tree
that started by first assigning values for V and T . Then it would be worthwhile to record
{WA = red , NT = green, Q = blue} as a no-good because we are going to run into the
same problem again for each possible set of assignments to V and T .
No-goods can be effectively used by forward checking or by backjumping. Constraint
learning is one of the most important techniques used by modern CSP solvers to achieve
efficiency on complex problems.

CONSTRAINT
LEARNING

NO-GOOD

6.4

L OCAL S EARCH FOR CSP S

MIN-CONFLICTS

Local search algorithms (see Section 4.1) turn out to be effective in solving many CSPs. They
use a complete-state formulation: the initial state assigns a value to every variable, and the
search changes the value of one variable at a time. For example, in the 8-queens problem (see
Figure 4.3), the initial state might be a random configuration of 8 queens in 8 columns, and
each step moves a single queen to a new position in its column. Typically, the initial guess
violates several constraints. The point of local search is to eliminate the violated constraints.2
In choosing a new value for a variable, the most obvious heuristic is to select the value
that results in the minimum number of conflicts with other variablesâ€”the min-conflicts
2

Local search can easily be extended to constraint optimization problems (COPs). In that case, all the techniques
for hill climbing and simulated annealing can be applied to optimize the objective function.

Section 6.4.

Local Search for CSPs

221

function M IN -C ONFLICTS (csp, max steps) returns a solution or failure
inputs: csp, a constraint satisfaction problem
max steps, the number of steps allowed before giving up
current â† an initial complete assignment for csp
for i = 1 to max steps do
if current is a solution for csp then return current
var â† a randomly chosen conflicted variable from csp.VARIABLES
value â† the value v for var that minimizes C ONFLICTS(var , v , current, csp)
set var = value in current
return failure
Figure 6.8 The M IN -C ONFLICTS algorithm for solving CSPs by local search. The initial
state may be chosen randomly or by a greedy assignment process that chooses a minimalconflict value for each variable in turn. The C ONFLICTS function counts the number of
constraints violated by a particular value, given the rest of the current assignment.

2

3

2

3

1
2

2

3

3

1

2

2

3
0

Figure 6.9 A two-step solution using min-conflicts for an 8-queens problem. At each
stage, a queen is chosen for reassignment in its column. The number of conflicts (in this
case, the number of attacking queens) is shown in each square. The algorithm moves the
queen to the min-conflicts square, breaking ties randomly.

heuristic. The algorithm is shown in Figure 6.8 and its application to an 8-queens problem is
diagrammed in Figure 6.9.
Min-conflicts is surprisingly effective for many CSPs. Amazingly, on the n-queens
problem, if you donâ€™t count the initial placement of queens, the run time of min-conflicts is
roughly independent of problem size. It solves even the million-queens problem in an average of 50 steps (after the initial assignment). This remarkable observation was the stimulus
leading to a great deal of research in the 1990s on local search and the distinction between
easy and hard problems, which we take up in Chapter 7. Roughly speaking, n-queens is
easy for local search because solutions are densely distributed throughout the state space.
Min-conflicts also works well for hard problems. For example, it has been used to schedule
observations for the Hubble Space Telescope, reducing the time taken to schedule a week of
observations from three weeks (!) to around 10 minutes.

222

Chapter

T HE S TRUCTURE OF P ROBLEMS

INDEPENDENT
SUBPROBLEMS
CONNECTED
COMPONENT

Constraint Satisfaction Problems

All the local search techniques from Section 4.1 are candidates for application to CSPs,
and some of those have proved especially effective. The landscape of a CSP under the minconflicts heuristic usually has a series of plateaux. There may be millions of variable assignments that are only one conflict away from a solution. Plateau searchâ€”allowing sideways moves to another state with the same scoreâ€”can help local search find its way off this
plateau. This wandering on the plateau can be directed with tabu search: keeping a small
list of recently visited states and forbidding the algorithm to return to those states. Simulated
annealing can also be used to escape from plateaux.
Another technique, called constraint weighting, can help concentrate the search on the
important constraints. Each constraint is given a numeric weight, Wi , initially all 1. At each
step of the search, the algorithm chooses a variable/value pair to change that will result in the
lowest total weight of all violated constraints. The weights are then adjusted by incrementing
the weight of each constraint that is violated by the current assignment. This has two benefits:
it adds topography to plateaux, making sure that it is possible to improve from the current
state, and it also, over time, adds weight to the constraints that are proving difficult to solve.
Another advantage of local search is that it can be used in an online setting when the
problem changes. This is particularly important in scheduling problems. A weekâ€™s airline
schedule may involve thousands of flights and tens of thousands of personnel assignments,
but bad weather at one airport can render the schedule infeasible. We would like to repair the
schedule with a minimum number of changes. This can be easily done with a local search
algorithm starting from the current schedule. A backtracking search with the new set of
constraints usually requires much more time and might find a solution with many changes
from the current schedule.

CONSTRAINT
WEIGHTING

6.5

6.

In this section, we examine ways in which the structure of the problem, as represented by
the constraint graph, can be used to find solutions quickly. Most of the approaches here also
apply to other problems besides CSPs, such as probabilistic reasoning. After all, the only way
we can possibly hope to deal with the real world is to decompose it into many subproblems.
Looking again at the constraint graph for Australia (Figure 6.1(b), repeated as Figure 6.12(a)),
one fact stands out: Tasmania is not connected to the mainland.3 Intuitively, it is obvious that
coloring Tasmania and coloring the mainland are independent subproblemsâ€”any solution
for the mainland combined with any solution for Tasmania yields a solution for the whole
map. Independence can be ascertained simply by finding connected components of the
constraint graph. Each component
corresponds 
to a subproblem CSP i . If assignment Si is

a solution of CSP i , then i Si is a solution of i CSP i . Why is this important? Consider
the following: suppose each CSP i has c variables from the total of n variables, where c is
a constant. Then there are n/c subproblems, each of which takes at most dc work to solve,
3

A careful cartographer or patriotic Tasmanian might object that Tasmania should not be colored the same as
its nearest mainland neighbor, to avoid the impression that it might be part of that state.

Section 6.5.

DIRECTED ARC
CONSISTENCY

TOPOLOGICAL SORT

The Structure of Problems

223

where d is the size of the domain. Hence, the total work is O(dc n/c), which is linear in n;
without the decomposition, the total work is O(dn ), which is exponential in n. Letâ€™s make
this more concrete: dividing a Boolean CSP with 80 variables into four subproblems reduces
the worst-case solution time from the lifetime of the universe down to less than a second.
Completely independent subproblems are delicious, then, but rare. Fortunately, some
other graph structures are also easy to solve. For example, a constraint graph is a tree when
any two variables are connected by only one path. We show that any tree-structured CSP can
be solved in time linear in the number of variables.4 The key is a new notion of consistency,
called directed arc consistency or DAC. A CSP is defined to be directed arc-consistent under
an ordering of variables X1 , X2 , . . . , Xn if and only if every Xi is arc-consistent with each
Xj for j > i.
To solve a tree-structured CSP, first pick any variable to be the root of the tree, and
choose an ordering of the variables such that each variable appears after its parent in the tree.
Such an ordering is called a topological sort. Figure 6.10(a) shows a sample tree and (b)
shows one possible ordering. Any tree with n nodes has nâˆ’1 arcs, so we can make this graph
directed arc-consistent in O(n) steps, each of which must compare up to d possible domain
values for two variables, for a total time of O(nd2 ). Once we have a directed arc-consistent
graph, we can just march down the list of variables and choose any remaining value. Since
each link from a parent to its child is arc consistent, we know that for any value we choose for
the parent, there will be a valid value left to choose for the child. That means we wonâ€™t have
to backtrack; we can move linearly through the variables. The complete algorithm is shown
in Figure 6.11.

A

E
B

C

A

D
(a)

F

B

C

D

E

F

(b)

Figure 6.10 (a) The constraint graph of a tree-structured CSP. (b) A linear ordering of the
variables consistent with the tree with A as the root. This is known as a topological sort of
the variables.

Now that we have an efficient algorithm for trees, we can consider whether more general
constraint graphs can be reduced to trees somehow. There are two primary ways to do this,
one based on removing nodes and one based on collapsing nodes together.
The first approach involves assigning values to some variables so that the remaining
variables form a tree. Consider the constraint graph for Australia, shown again in Figure 6.12(a). If we could delete South Australia, the graph would become a tree, as in (b).
Fortunately, we can do this (in the graph, not the continent) by fixing a value for SA and
4

Sadly, very few regions of the world have tree-structured maps, although Sulawesi comes close.

224

Chapter

6.

Constraint Satisfaction Problems

function T REE -CSP-S OLVER ( csp) returns a solution, or failure
inputs: csp, a CSP with components X, D, C
n â† number of variables in X
assignment â† an empty assignment
root â† any variable in X
X â† T OPOLOGICAL S ORT (X , root)
for j = n down to 2 do
M AKE -A RC -C ONSISTENT(PARENT(Xj ), Xj )
if it cannot be made consistent then return failure
for i = 1 to n do
assignment[Xi ] â† any consistent value from Di
if there is no consistent value then return failure
return assignment
Figure 6.11 The T REE -CSP-S OLVER algorithm for solving tree-structured CSPs. If the
CSP has a solution, we will find it in linear time; if not, we will detect a contradiction.

NT

NT
Q

Q

WA

WA
SA

NSW

NSW

V

V

T
(a)

T
(b)

Figure 6.12 (a) The original constraint graph from Figure 6.1. (b) The constraint graph
after the removal of SA.

deleting from the domains of the other variables any values that are inconsistent with the
value chosen for SA.
Now, any solution for the CSP after SA and its constraints are removed will be consistent with the value chosen for SA. (This works for binary CSPs; the situation is more
complicated with higher-order constraints.) Therefore, we can solve the remaining tree with
the algorithm given above and thus solve the whole problem. Of course, in the general case
(as opposed to map coloring), the value chosen for SA could be the wrong one, so we would
need to try each possible value. The general algorithm is as follows:

Section 6.5.

CYCLE CUTSET

CUTSET
CONDITIONING
TREE
DECOMPOSITION

TREE WIDTH

The Structure of Problems

225

1. Choose a subset S of the CSPâ€™s variables such that the constraint graph becomes a tree
after removal of S. S is called a cycle cutset.
2. For each possible assignment to the variables in S that satisfies all constraints on S,
(a) remove from the domains of the remaining variables any values that are inconsistent with the assignment for S, and
(b) If the remaining CSP has a solution, return it together with the assignment for S.
If the cycle cutset has size c, then the total run time is O(dc Â· (n âˆ’ c)d2 ): we have to try each
of the dc combinations of values for the variables in S, and for each combination we must
solve a tree problem of size n âˆ’ c. If the graph is â€œnearly a tree,â€ then c will be small and the
savings over straight backtracking will be huge. In the worst case, however, c can be as large
as (n âˆ’ 2). Finding the smallest cycle cutset is NP-hard, but several efficient approximation
algorithms are known. The overall algorithmic approach is called cutset conditioning; it
comes up again in Chapter 14, where it is used for reasoning about probabilities.
The second approach is based on constructing a tree decomposition of the constraint
graph into a set of connected subproblems. Each subproblem is solved independently, and the
resulting solutions are then combined. Like most divide-and-conquer algorithms, this works
well if no subproblem is too large. Figure 6.13 shows a tree decomposition of the mapcoloring problem into five subproblems. A tree decomposition must satisfy the following
three requirements:
â€¢ Every variable in the original problem appears in at least one of the subproblems.
â€¢ If two variables are connected by a constraint in the original problem, they must appear
together (along with the constraint) in at least one of the subproblems.
â€¢ If a variable appears in two subproblems in the tree, it must appear in every subproblem
along the path connecting those subproblems.
The first two conditions ensure that all the variables and constraints are represented in the
decomposition. The third condition seems rather technical, but simply reflects the constraint
that any given variable must have the same value in every subproblem in which it appears;
the links joining subproblems in the tree enforce this constraint. For example, SA appears in
all four of the connected subproblems in Figure 6.13. You can verify from Figure 6.12 that
this decomposition makes sense.
We solve each subproblem independently; if any one has no solution, we know the entire problem has no solution. If we can solve all the subproblems, then we attempt to construct
a global solution as follows. First, we view each subproblem as a â€œmega-variableâ€ whose domain is the set of all solutions for the subproblem. For example, the leftmost subproblem in
Figure 6.13 is a map-coloring problem with three variables and hence has six solutionsâ€”one
is {WA = red , SA = blue, NT = green}. Then, we solve the constraints connecting the
subproblems, using the efficient algorithm for trees given earlier. The constraints between
subproblems simply insist that the subproblem solutions agree on their shared variables. For
example, given the solution {WA = red , SA = blue, NT = green} for the first subproblem,
the only consistent solution for the next subproblem is {SA = blue, NT = green, Q = red }.
A given constraint graph admits many tree decompositions; in choosing a decomposition, the aim is to make the subproblems as small as possible. The tree width of a tree

226

Chapter

NT

6.

Constraint Satisfaction Problems

NT
Q

WA
SA

SA

Q

SA

NSW

SA

NSW

T

V

Figure 6.13

VALUE SYMMETRY
SYMMETRYBREAKING
CONSTRAINT

A tree decomposition of the constraint graph in Figure 6.12(a).

decomposition of a graph is one less than the size of the largest subproblem; the tree width
of the graph itself is defined to be the minimum tree width among all its tree decompositions.
If a graph has tree width w and we are given the corresponding tree decomposition, then the
problem can be solved in O(ndw+1 ) time. Hence, CSPs with constraint graphs of bounded
tree width are solvable in polynomial time. Unfortunately, finding the decomposition with
minimal tree width is NP-hard, but there are heuristic methods that work well in practice.
So far, we have looked at the structure of the constraint graph. There can be important
structure in the values of variables as well. Consider the map-coloring problem with n colors.
For every consistent solution, there is actually a set of n! solutions formed by permuting the
color names. For example, on the Australia map we know that WA, NT , and SA must all have
different colors, but there are 3! = 6 ways to assign the three colors to these three regions.
This is called value symmetry. We would like to reduce the search space by a factor of
n! by breaking the symmetry. We do this by introducing a symmetry-breaking constraint.
For our example, we might impose an arbitrary ordering constraint, NT < SA < WA, that
requires the three values to be in alphabetical order. This constraint ensures that only one of
the n! solutions is possible: {NT = blue, SA = green, WA = red }.
For map coloring, it was easy to find a constraint that eliminates the symmetry, and
in general it is possible to find constraints that eliminate all but one symmetric solution in
polynomial time, but it is NP-hard to eliminate all symmetry among intermediate sets of
values during search. In practice, breaking value symmetry has proved to be important and
effective on a wide range of problems.

Section 6.6.

6.6

Summary

227

S UMMARY
â€¢ Constraint satisfaction problems (CSPs) represent a state with a set of variable/value
pairs and represent the conditions for a solution by a set of constraints on the variables.
Many important real-world problems can be described as CSPs.
â€¢ A number of inference techniques use the constraints to infer which variable/value pairs
are consistent and which are not. These include node, arc, path, and k-consistency.
â€¢ Backtracking search, a form of depth-first search, is commonly used for solving CSPs.
Inference can be interwoven with search.
â€¢ The minimum-remaining-values and degree heuristics are domain-independent methods for deciding which variable to choose next in a backtracking search. The leastconstraining-value heuristic helps in deciding which value to try first for a given
variable. Backtracking occurs when no legal assignment can be found for a variable.
Conflict-directed backjumping backtracks directly to the source of the problem.
â€¢ Local search using the min-conflicts heuristic has also been applied to constraint satisfaction problems with great success.
â€¢ The complexity of solving a CSP is strongly related to the structure of its constraint
graph. Tree-structured problems can be solved in linear time. Cutset conditioning can
reduce a general CSP to a tree-structured one and is quite efficient if a small cutset can
be found. Tree decomposition techniques transform the CSP into a tree of subproblems
and are efficient if the tree width of the constraint graph is small.

B IBLIOGRAPHICAL

DIOPHANTINE
EQUATIONS

GRAPH COLORING

AND

H ISTORICAL N OTES

The earliest work related to constraint satisfaction dealt largely with numerical constraints.
Equational constraints with integer domains were studied by the Indian mathematician Brahmagupta in the seventh century; they are often called Diophantine equations, after the Greek
mathematician Diophantus (c. 200â€“284), who actually considered the domain of positive rationals. Systematic methods for solving linear equations by variable elimination were studied
by Gauss (1829); the solution of linear inequality constraints goes back to Fourier (1827).
Finite-domain constraint satisfaction problems also have a long history. For example,
graph coloring (of which map coloring is a special case) is an old problem in mathematics.
The four-color conjecture (that every planar graph can be colored with four or fewer colors)
was first made by Francis Guthrie, a student of De Morgan, in 1852. It resisted solutionâ€”
despite several published claims to the contraryâ€”until a proof was devised by Appel and
Haken (1977) (see the book Four Colors Suffice (Wilson, 2004)). Purists were disappointed
that part of the proof relied on a computer, so Georges Gonthier (2008), using the C OQ
theorem prover, derived a formal proof that Appel and Hakenâ€™s proof was correct.
Specific classes of constraint satisfaction problems occur throughout the history of
computer science. One of the most influential early examples was the S KETCHPAD sys-

228

Chapter

6.

Constraint Satisfaction Problems

tem (Sutherland, 1963), which solved geometric constraints in diagrams and was the forerunner of modern drawing programs and CAD tools. The identification of CSPs as a general
class is due to Ugo Montanari (1974). The reduction of higher-order CSPs to purely binary
CSPs with auxiliary variables (see Exercise 6.6) is due originally to the 19th-century logician
Charles Sanders Peirce. It was introduced into the CSP literature by Dechter (1990b) and
was elaborated by Bacchus and van Beek (1998). CSPs with preferences among solutions are
studied widely in the optimization literature; see Bistarelli et al. (1997) for a generalization
of the CSP framework to allow for preferences. The bucket-elimination algorithm (Dechter,
1999) can also be applied to optimization problems.
Constraint propagation methods were popularized by Waltzâ€™s (1975) success on polyhedral line-labeling problems for computer vision. Waltz showed that, in many problems,
propagation completely eliminates the need for backtracking. Montanari (1974) introduced
the notion of constraint networks and propagation by path consistency. Alan Mackworth
(1977) proposed the AC-3 algorithm for enforcing arc consistency as well as the general idea
of combining backtracking with some degree of consistency enforcement. AC-4, a more
efficient arc-consistency algorithm, was developed by Mohr and Henderson (1986). Soon after Mackworthâ€™s paper appeared, researchers began experimenting with the tradeoff between
the cost of consistency enforcement and the benefits in terms of search reduction. Haralick
and Elliot (1980) favored the minimal forward-checking algorithm described by McGregor
(1979), whereas Gaschnig (1979) suggested full arc-consistency checking after each variable assignmentâ€”an algorithm later called MAC by Sabin and Freuder (1994). The latter
paper provides somewhat convincing evidence that, on harder CSPs, full arc-consistency
checking pays off. Freuder (1978, 1982) investigated the notion of k-consistency and its
relationship to the complexity of solving CSPs. Apt (1999) describes a generic algorithmic
framework within which consistency propagation algorithms can be analyzed, and BessieÌ€re
(2006) presents a current survey.
Special methods for handling higher-order or global constraints were developed first
within the context of constraint logic programming. Marriott and Stuckey (1998) provide
excellent coverage of research in this area. The Alldiï¬€ constraint was studied by Regin
(1994), Stergiou and Walsh (1999), and van Hoeve (2001). Bounds constraints were incorporated into constraint logic programming by Van Hentenryck et al. (1998). A survey of global
constraints is provided by van Hoeve and Katriel (2006).
Sudoku has become the most widely known CSP and was described as such by Simonis
(2005). Agerbeck and Hansen (2008) describe some of the strategies and show that Sudoku
on an n2 Ã— n2 board is in the class of NP-hard problems. Reeson et al. (2007) show an
interactive solver based on CSP techniques.
The idea of backtracking search goes back to Golomb and Baumert (1965), and its
application to constraint satisfaction is due to Bitner and Reingold (1975), although they trace
the basic algorithm back to the 19th century. Bitner and Reingold also introduced the MRV
heuristic, which they called the most-constrained-variable heuristic. Brelaz (1979) used the
degree heuristic as a tiebreaker after applying the MRV heuristic. The resulting algorithm,
despite its simplicity, is still the best method for k-coloring arbitrary graphs. Haralick and
Elliot (1980) proposed the least-constraining-value heuristic.

Bibliographical and Historical Notes

DEPENDENCYDIRECTED
BACKTRACKING

BACKMARKING

DYNAMIC
BACKTRACKING

229

The basic backjumping method is due to John Gaschnig (1977, 1979). Kondrak and
van Beek (1997) showed that this algorithm is essentially subsumed by forward checking.
Conflict-directed backjumping was devised by Prosser (1993). The most general and powerful form of intelligent backtracking was actually developed very early on by Stallman and
Sussman (1977). Their technique of dependency-directed backtracking led to the development of truth maintenance systems (Doyle, 1979), which we discuss in Section 12.6.2. The
connection between the two areas is analyzed by de Kleer (1989).
The work of Stallman and Sussman also introduced the idea of constraint learning,
in which partial results obtained by search can be saved and reused later in the search. The
idea was formalized Dechter (1990a). Backmarking (Gaschnig, 1979) is a particularly simple method in which consistent and inconsistent pairwise assignments are saved and used
to avoid rechecking constraints. Backmarking can be combined with conflict-directed backjumping; Kondrak and van Beek (1997) present a hybrid algorithm that provably subsumes
either method taken separately. The method of dynamic backtracking (Ginsberg, 1993) retains successful partial assignments from later subsets of variables when backtracking over
an earlier choice that does not invalidate the later success.
Empirical studies of several randomized backtracking methods were done by Gomes
et al. (2000) and Gomes and Selman (2001). Van Beek (2006) surveys backtracking.
Local search in constraint satisfaction problems was popularized by the work of Kirkpatrick et al. (1983) on simulated annealing (see Chapter 4), which is widely used for scheduling problems. The min-conflicts heuristic was first proposed by Gu (1989) and was developed
independently by Minton et al. (1992). Sosic and Gu (1994) showed how it could be applied
to solve the 3,000,000 queens problem in less than a minute. The astounding success of
local search using min-conflicts on the n-queens problem led to a reappraisal of the nature
and prevalence of â€œeasyâ€ and â€œhardâ€ problems. Peter Cheeseman et al. (1991) explored the
difficulty of randomly generated CSPs and discovered that almost all such problems either
are trivially easy or have no solutions. Only if the parameters of the problem generator are
set in a certain narrow range, within which roughly half of the problems are solvable, do we
find â€œhardâ€ problem instances. We discuss this phenomenon further in Chapter 7. Konolige
(1994) showed that local search is inferior to backtracking search on problems with a certain
degree of local structure; this led to work that combined local search and inference, such as
that by Pinkas and Dechter (1995). Hoos and Tsang (2006) survey local search techniques.
Work relating the structure and complexity of CSPs originates with Freuder (1985), who
showed that search on arc consistent trees works without any backtracking. A similar result,
with extensions to acyclic hypergraphs, was developed in the database community (Beeri
et al., 1983). Bayardo and Miranker (1994) present an algorithm for tree-structured CSPs
that runs in linear time without any preprocessing.
Since those papers were published, there has been a great deal of progress in developing
more general results relating the complexity of solving a CSP to the structure of its constraint
graph. The notion of tree width was introduced by the graph theorists Robertson and Seymour
(1986). Dechter and Pearl (1987, 1989), building on the work of Freuder, applied a related
notion (which they called induced width) to constraint satisfaction problems and developed
the tree decomposition approach sketched in Section 6.5. Drawing on this work and on results

230

DISTRIBUTED
CONSTRAINT
SATISFACTION

Chapter

6.

Constraint Satisfaction Problems

from database theory, Gottlob et al. (1999a, 1999b) developed a notion, hypertree width, that
is based on the characterization of the CSP as a hypergraph. In addition to showing that any
CSP with hypertree width w can be solved in time O(nw+1 log n), they also showed that
hypertree width subsumes all previously defined measures of â€œwidthâ€ in the sense that there
are cases where the hypertree width is bounded and the other measures are unbounded.
Interest in look-back approaches to backtracking was rekindled by the work of Bayardo
and Schrag (1997), whose R ELSAT algorithm combined constraint learning and backjumping
and was shown to outperform many other algorithms of the time. This led to AND/OR
search algorithms applicable to both CSPs and probabilistic reasoning (Dechter and Mateescu, 2007). Brown et al. (1988) introduce the idea of symmetry breaking in CSPs, and
Gent et al. (2006) give a recent survey.
The field of distributed constraint satisfaction looks at solving CSPs when there is a
collection of agents, each of which controls a subset of the constraint variables. There have
been annual workshops on this problem since 2000, and good coverage elsewhere (Collin
et al., 1999; Pearce et al., 2008; Shoham and Leyton-Brown, 2009).
Comparing CSP algorithms is mostly an empirical science: few theoretical results show
that one algorithm dominates another on all problems; instead, we need to run experiments
to see which algorithms perform better on typical instances of problems. As Hooker (1995)
points out, we need to be careful to distinguish between competitive testingâ€”as occurs in
competitions among algorithms based on run timeâ€”and scientific testing, whose goal is to
identify the properties of an algorithm that determine its efficacy on a class of problems.
The recent textbooks by Apt (2003) and Dechter (2003), and the collection by Rossi
et al. (2006) are excellent resources on constraint processing. There are several good earlier
surveys, including those by Kumar (1992), Dechter and Frost (2002), and Bartak (2001); and
the encyclopedia articles by Dechter (1992) and Mackworth (1992). Pearson and Jeavons
(1997) survey tractable classes of CSPs, covering both structural decomposition methods
and methods that rely on properties of the domains or constraints themselves. Kondrak and
van Beek (1997) give an analytical survey of backtracking search algorithms, and Bacchus
and van Run (1995) give a more empirical survey. Constraint programming is covered in the
books by Apt (2003) and Fruhwirth and Abdennadher (2003). Several interesting applications
are described in the collection edited by Freuder and Mackworth (1994). Papers on constraint
satisfaction appear regularly in Artificial Intelligence and in the specialist journal Constraints.
The primary conference venue is the International Conference on Principles and Practice of
Constraint Programming, often called CP.

E XERCISES
6.1 How many solutions are there for the map-coloring problem in Figure 6.1? How many
solutions if four colors are allowed? Two colors?
6.2 Consider the problem of placing k knights on an n Ã— n chessboard such that no two
knights are attacking each other, where k is given and k â‰¤ n2 .

Exercises

231
a.
b.
c.
d.

Choose a CSP formulation. In your formulation, what are the variables?
What are the possible values of each variable?
What sets of variables are constrained, and how?
Now consider the problem of putting as many knights as possible on the board without any attacks. Explain how to solve this with local search by defining appropriate
ACTIONS and R ESULT functions and a sensible objective function.

6.3 Consider the problem of constructing (not solving) crossword puzzles:5 fitting words
into a rectangular grid. The grid, which is given as part of the problem, specifies which
squares are blank and which are shaded. Assume that a list of words (i.e., a dictionary)
is provided and that the task is to fill in the blank squares by using any subset of the list.
Formulate this problem precisely in two ways:
a. As a general search problem. Choose an appropriate search algorithm and specify a
heuristic function. Is it better to fill in blanks one letter at a time or one word at a time?
b. As a constraint satisfaction problem. Should the variables be words or letters?
Which formulation do you think will be better? Why?
6.4

Give precise formulations for each of the following as constraint satisfaction problems:

a. Rectilinear floor-planning: find non-overlapping places in a large rectangle for a number
of smaller rectangles.
b. Class scheduling: There is a fixed number of professors and classrooms, a list of classes
to be offered, and a list of possible time slots for classes. Each professor has a set of
classes that he or she can teach.
c. Hamiltonian tour: given a network of cities connected by roads, choose an order to visit
all cities in a country without repeating any.
6.5 Solve the cryptarithmetic problem in Figure 6.2 by hand, using the strategy of backtracking with forward checking and the MRV and least-constraining-value heuristics.
6.6 Show how a single ternary constraint such as â€œA + B = Câ€ can be turned into three
binary constraints by using an auxiliary variable. You may assume finite domains. (Hint:
Consider a new variable that takes on values that are pairs of other values, and consider
constraints such as â€œX is the first element of the pair Y .â€) Next, show how constraints with
more than three variables can be treated similarly. Finally, show how unary constraints can be
eliminated by altering the domains of variables. This completes the demonstration that any
CSP can be transformed into a CSP with only binary constraints.
6.7 Consider the following logic puzzle: In five houses, each with a different color, live five
persons of different nationalities, each of whom prefers a different brand of candy, a different
drink, and a different pet. Given the following facts, the questions to answer are â€œWhere does
the zebra live, and in which house do they drink water?â€
5

Ginsberg et al. (1990) discuss several methods for constructing crossword puzzles. Littman et al. (1999) tackle
the harder problem of solving them.

232

Chapter

6.

Constraint Satisfaction Problems

The Englishman lives in the red house.
The Spaniard owns the dog.
The Norwegian lives in the first house on the left.
The green house is immediately to the right of the ivory house.
The man who eats Hershey bars lives in the house next to the man with the fox.
Kit Kats are eaten in the yellow house.
The Norwegian lives next to the blue house.
The Smarties eater owns snails.
The Snickers eater drinks orange juice.
The Ukrainian drinks tea.
The Japanese eats Milky Ways.
Kit Kats are eaten in a house next to the house where the horse is kept.
Coffee is drunk in the green house.
Milk is drunk in the middle house.
Discuss different representations of this problem as a CSP. Why would one prefer one representation over another?
6.8 Consider the graph with 8 nodes A1 , A2 , A3 , A4 , H, T , F1 , F2 . Ai is connected to
Ai+1 for all i, each Ai is connected to H, H is connected to T , and T is connected to each
Fi . Find a 3-coloring of this graph by hand using the following strategy: backtracking with
conflict-directed backjumping, the variable order A1 , H, A4 , F1 , A2 , F2 , A3 , T , and the
value order R, G, B.
6.9 Explain why it is a good heuristic to choose the variable that is most constrained but the
value that is least constraining in a CSP search.
6.10 Generate random instances of map-coloring problems as follows: scatter n points on
the unit square; select a point X at random, connect X by a straight line to the nearest point
Y such that X is not already connected to Y and the line crosses no other line; repeat the
previous step until no more connections are possible. The points represent regions on the
map and the lines connect neighbors. Now try to find k-colorings of each map, for both
k = 3 and k = 4, using min-conflicts, backtracking, backtracking with forward checking, and
backtracking with MAC. Construct a table of average run times for each algorithm for values
of n up to the largest you can manage. Comment on your results.
6.11 Use the AC-3 algorithm to show that arc consistency can detect the inconsistency of
the partial assignment {WA = green, V = red } for the problem shown in Figure 6.1.
6.12

What is the worst-case complexity of running AC-3 on a tree-structured CSP?

6.13 AC-3 puts back on the queue every arc (Xk , Xi ) whenever any value is deleted from
the domain of Xi , even if each value of Xk is consistent with several remaining values of Xi .
Suppose that, for every arc (Xk , Xi ), we keep track of the number of remaining values of Xi
that are consistent with each value of Xk . Explain how to update these numbers efficiently
and hence show that arc consistency can be enforced in total time O(n2 d2 ).

Exercises

233
6.14 The T REE -CSP-S OLVER (Figure 6.10) makes arcs consistent starting at the leaves and
working backwards towards the root. Why does it do that? What would happen if it went in
the opposite direction?
6.15 We introduced Sudoku as a CSP to be solved by search over partial assignments because that is the way people generally undertake solving Sudoku problems. It is also possible,
of course, to attack these problems with local search over complete assignments. How well
would a local solver using the min-conflicts heuristic do on Sudoku problems?
6.16 Define in your own words the terms constraint, backtracking search, arc consistency,
backjumping, min-conflicts, and cycle cutset.
6.17 Suppose that a graph is known to have a cycle cutset of no more than k nodes. Describe
a simple algorithm for finding a minimal cycle cutset whose run time is not much more than
O(nk ) for a CSP with n variables. Search the literature for methods for finding approximately
minimal cycle cutsets in time that is polynomial in the size of the cutset. Does the existence
of such algorithms make the cycle cutset method practical?

7

LOGICAL AGENTS

In which we design agents that can form representations of a complex world, use a
process of inference to derive new representations about the world, and use these
new representations to deduce what to do.

REASONING
REPRESENTATION
KNOWLEDGE-BASED
AGENTS

LOGIC

Humans, it seems, know things; and what they know helps them do things. These are
not empty statements. They make strong claims about how the intelligence of humans is
achievedâ€”not by purely reflex mechanisms but by processes of reasoning that operate on
internal representations of knowledge. In AI, this approach to intelligence is embodied in
knowledge-based agents.
The problem-solving agents of Chapters 3 and 4 know things, but only in a very limited,
inflexible sense. For example, the transition model for the 8-puzzleâ€”knowledge of what the
actions doâ€”is hidden inside the domain-specific code of the R ESULT function. It can be
used to predict the outcome of actions but not to deduce that two tiles cannot occupy the
same space or that states with odd parity cannot be reached from states with even parity. The
atomic representations used by problem-solving agents are also very limiting. In a partially
observable environment, an agentâ€™s only choice for representing what it knows about the
current state is to list all possible concrete statesâ€”a hopeless prospect in large environments.
Chapter 6 introduced the idea of representing states as assignments of values to variables; this is a step in the right direction, enabling some parts of the agent to work in a
domain-independent way and allowing for more efficient algorithms. In this chapter and
those that follow, we take this step to its logical conclusion, so to speakâ€”we develop logic
as a general class of representations to support knowledge-based agents. Such agents can
combine and recombine information to suit myriad purposes. Often, this process can be quite
far removed from the needs of the momentâ€”as when a mathematician proves a theorem or
an astronomer calculates the earthâ€™s life expectancy. Knowledge-based agents can accept new
tasks in the form of explicitly described goals; they can achieve competence quickly by being
told or learning new knowledge about the environment; and they can adapt to changes in the
environment by updating the relevant knowledge.
We begin in Section 7.1 with the overall agent design. Section 7.2 introduces a simple new environment, the wumpus world, and illustrates the operation of a knowledge-based
agent without going into any technical detail. Then we explain the general principles of logic
234

Section 7.1.

Knowledge-Based Agents

235

in Section 7.3 and the specifics of propositional logic in Section 7.4. While less expressive
than first-order logic (Chapter 8), propositional logic illustrates all the basic concepts of
logic; it also comes with well-developed inference technologies, which we describe in sections 7.5 and 7.6. Finally, Section 7.7 combines the concept of knowledge-based agents with
the technology of propositional logic to build some simple agents for the wumpus world.

7.1

K NOWLEDGE -BASED AGENTS

KNOWLEDGE BASE
SENTENCE
KNOWLEDGE
REPRESENTATION
LANGUAGE
AXIOM

INFERENCE

BACKGROUND
KNOWLEDGE

The central component of a knowledge-based agent is its knowledge base, or KB. A knowledge base is a set of sentences. (Here â€œsentenceâ€ is used as a technical term. It is related
but not identical to the sentences of English and other natural languages.) Each sentence is
expressed in a language called a knowledge representation language and represents some
assertion about the world. Sometimes we dignify a sentence with the name axiom, when the
sentence is taken as given without being derived from other sentences.
There must be a way to add new sentences to the knowledge base and a way to query
what is known. The standard names for these operations are T ELL and A SK , respectively.
Both operations may involve inferenceâ€”that is, deriving new sentences from old. Inference
must obey the requirement that when one A SK s a question of the knowledge base, the answer
should follow from what has been told (or T ELL ed) to the knowledge base previously. Later
in this chapter, we will be more precise about the crucial word â€œfollow.â€ For now, take it to
mean that the inference process should not make things up as it goes along.
Figure 7.1 shows the outline of a knowledge-based agent program. Like all our agents,
it takes a percept as input and returns an action. The agent maintains a knowledge base, KB,
which may initially contain some background knowledge.
Each time the agent program is called, it does three things. First, it T ELL s the knowledge base what it perceives. Second, it A SK s the knowledge base what action it should
perform. In the process of answering this query, extensive reasoning may be done about
the current state of the world, about the outcomes of possible action sequences, and so on.
Third, the agent program T ELL s the knowledge base which action was chosen, and the agent
executes the action.
The details of the representation language are hidden inside three functions that implement the interface between the sensors and actuators on one side and the core representation
and reasoning system on the other. M AKE-P ERCEPT-S ENTENCE constructs a sentence asserting that the agent perceived the given percept at the given time. M AKE-ACTION -Q UERY
constructs a sentence that asks what action should be done at the current time. Finally,
M AKE-ACTION -S ENTENCE constructs a sentence asserting that the chosen action was executed. The details of the inference mechanisms are hidden inside T ELL and A SK . Later
sections will reveal these details.
The agent in Figure 7.1 appears quite similar to the agents with internal state described
in Chapter 2. Because of the definitions of T ELL and A SK , however, the knowledge-based
agent is not an arbitrary program for calculating actions. It is amenable to a description at

236

Chapter

7.

Logical Agents

function KB-AGENT( percept ) returns an action
persistent: KB , a knowledge base
t , a counter, initially 0, indicating time
T ELL(KB, M AKE -P ERCEPT-S ENTENCE( percept , t ))
action â† A SK(KB, M AKE -ACTION -Q UERY (t ))
T ELL(KB, M AKE -ACTION -S ENTENCE(action, t ))
t â†t + 1
return action
Figure 7.1 A generic knowledge-based agent. Given a percept, the agent adds the percept
to its knowledge base, asks the knowledge base for the best action, and tells the knowledge
base that it has in fact taken that action.

KNOWLEDGE LEVEL

IMPLEMENTATION
LEVEL

DECLARATIVE

7.2

the knowledge level, where we need specify only what the agent knows and what its goals
are, in order to fix its behavior. For example, an automated taxi might have the goal of
taking a passenger from San Francisco to Marin County and might know that the Golden
Gate Bridge is the only link between the two locations. Then we can expect it to cross the
Golden Gate Bridge because it knows that that will achieve its goal. Notice that this analysis
is independent of how the taxi works at the implementation level. It doesnâ€™t matter whether
its geographical knowledge is implemented as linked lists or pixel maps, or whether it reasons
by manipulating strings of symbols stored in registers or by propagating noisy signals in a
network of neurons.
A knowledge-based agent can be built simply by T ELL ing it what it needs to know.
Starting with an empty knowledge base, the agent designer can T ELL sentences one by one
until the agent knows how to operate in its environment. This is called the declarative approach to system building. In contrast, the procedural approach encodes desired behaviors
directly as program code. In the 1970s and 1980s, advocates of the two approaches engaged
in heated debates. We now understand that a successful agent often combines both declarative
and procedural elements in its design, and that declarative knowledge can often be compiled
into more efficient procedural code.
We can also provide a knowledge-based agent with mechanisms that allow it to learn
for itself. These mechanisms, which are discussed in Chapter 18, create general knowledge
about the environment from a series of percepts. A learning agent can be fully autonomous.

T HE W UMPUS W ORLD

WUMPUS WORLD

In this section we describe an environment in which knowledge-based agents can show their
worth. The wumpus world is a cave consisting of rooms connected by passageways. Lurking
somewhere in the cave is the terrible wumpus, a beast that eats anyone who enters its room.
The wumpus can be shot by an agent, but the agent has only one arrow. Some rooms contain

Section 7.2.

The Wumpus World

237

bottomless pits that will trap anyone who wanders into these rooms (except for the wumpus,
which is too big to fall in). The only mitigating feature of this bleak environment is the
possibility of finding a heap of gold. Although the wumpus world is rather tame by modern
computer game standards, it illustrates some important points about intelligence.
A sample wumpus world is shown in Figure 7.2. The precise definition of the task
environment is given, as suggested in Section 2.3, by the PEAS description:
â€¢ Performance measure: +1000 for climbing out of the cave with the gold, â€“1000 for
falling into a pit or being eaten by the wumpus, â€“1 for each action taken and â€“10 for
using up the arrow. The game ends either when the agent dies or when the agent climbs
out of the cave.
â€¢ Environment: A 4 Ã— 4 grid of rooms. The agent always starts in the square labeled
[1,1], facing to the right. The locations of the gold and the wumpus are chosen randomly, with a uniform distribution, from the squares other than the start square. In
addition, each square other than the start can be a pit, with probability 0.2.
â€¢ Actuators: The agent can move Forward, TurnLeft by 90â—¦ , or TurnRight by 90â—¦ . The
agent dies a miserable death if it enters a square containing a pit or a live wumpus. (It
is safe, albeit smelly, to enter a square with a dead wumpus.) If an agent tries to move
forward and bumps into a wall, then the agent does not move. The action Grab can be
used to pick up the gold if it is in the same square as the agent. The action Shoot can
be used to fire an arrow in a straight line in the direction the agent is facing. The arrow
continues until it either hits (and hence kills) the wumpus or hits a wall. The agent has
only one arrow, so only the first Shoot action has any effect. Finally, the action Climb
can be used to climb out of the cave, but only from square [1,1].
â€¢ Sensors: The agent has five sensors, each of which gives a single bit of information:
â€“ In the square containing the wumpus and in the directly (not diagonally) adjacent
squares, the agent will perceive a Stench.
â€“ In the squares directly adjacent to a pit, the agent will perceive a Breeze.
â€“ In the square where the gold is, the agent will perceive a Glitter.
â€“ When an agent walks into a wall, it will perceive a Bump.
â€“ When the wumpus is killed, it emits a woeful Scream that can be perceived anywhere in the cave.
The percepts will be given to the agent program in the form of a list of five symbols;
for example, if there is a stench and a breeze, but no glitter, bump, or scream, the agent
program will get [Stench, Breeze, None, None, None].
We can characterize the wumpus environment along the various dimensions given in Chapter 2. Clearly, it is discrete, static, and single-agent. (The wumpus doesnâ€™t move, fortunately.)
It is sequential, because rewards may come only after many actions are taken. It is partially
observable, because some aspects of the state are not directly perceivable: the agentâ€™s location, the wumpusâ€™s state of health, and the availability of an arrow. As for the locations
of the pits and the wumpus: we could treat them as unobserved parts of the state that happen to be immutableâ€”in which case, the transition model for the environment is completely

238

Chapter

4

Stench

Bree z e

PIT

PIT

Bree z e

7.

Logical Agents

Bree z e

3

Stench
Gold

2

Bree z e

Stench

Bree z e

1

PIT

Bree z e

3

4

START

1

Figure 7.2

2

A typical wumpus world. The agent is in the bottom left corner, facing right.

known; or we could say that the transition model itself is unknown because the agent doesnâ€™t
know which Forward actions are fatalâ€”in which case, discovering the locations of pits and
wumpus completes the agentâ€™s knowledge of the transition model.
For an agent in the environment, the main challenge is its initial ignorance of the configuration of the environment; overcoming this ignorance seems to require logical reasoning.
In most instances of the wumpus world, it is possible for the agent to retrieve the gold safely.
Occasionally, the agent must choose between going home empty-handed and risking death to
find the gold. About 21% of the environments are utterly unfair, because the gold is in a pit
or surrounded by pits.
Let us watch a knowledge-based wumpus agent exploring the environment shown in
Figure 7.2. We use an informal knowledge representation language consisting of writing
down symbols in a grid (as in Figures 7.3 and 7.4).
The agentâ€™s initial knowledge base contains the rules of the environment, as described
previously; in particular, it knows that it is in [1,1] and that [1,1] is a safe square; we denote
that with an â€œAâ€ and â€œOK,â€ respectively, in square [1,1].
The first percept is [None, None, None, None, None], from which the agent can conclude that its neighboring squares, [1,2] and [2,1], are free of dangersâ€”they are OK. Figure 7.3(a) shows the agentâ€™s state of knowledge at this point.
A cautious agent will move only into a square that it knows to be OK. Let us suppose
the agent decides to move forward to [2,1]. The agent perceives a breeze (denoted by â€œBâ€) in
[2,1], so there must be a pit in a neighboring square. The pit cannot be in [1,1], by the rules of
the game, so there must be a pit in [2,2] or [3,1] or both. The notation â€œP?â€ in Figure 7.3(b)
indicates a possible pit in those squares. At this point, there is only one known square that is
OK and that has not yet been visited. So the prudent agent will turn around, go back to [1,1],
and then proceed to [1,2].
The agent perceives a stench in [1,2], resulting in the state of knowledge shown in
Figure 7.4(a). The stench in [1,2] means that there must be a wumpus nearby. But the

Section 7.2.

The Wumpus World

239

1,4

2,4

3,4

4,4

1,3

2,3

3,3

4,3

1,2

2,2

3,2

2,1

3,1

A
B
G
OK
P
S
V
W

= Agent
= Breeze
= Glitter, Gold
= Safe square
= Pit
= Stench
= Visited
= Wumpus

1,4

2,4

3,4

4,4

1,3

2,3

3,3

4,3

4,2

1,2

2,2

3,2

4,2

4,1

1,1

OK
1,1

P?

OK

A
OK

2,1

A
B
OK

V
OK

OK

3,1

P?

4,1

(b)

(a)

Figure 7.3 The first step taken by the agent in the wumpus world. (a) The initial situation, after percept [None, None, None, None, None]. (b) After one move, with percept
[None, Breeze, None, None, None].
1,4

1,3

1,2

W!

A

2,4

3,4

4,4

2,3

3,3

4,3

2,2

3,2

4,2

S
OK
1,1

= Agent
= Breeze
= Glitter, Gold
= Safe square
= Pit
= Stench
= Visited
= Wumpus

1,4

2,4

1,3 W!

1,2

OK
2,1

V
OK

A
B
G
OK
P
S
V
W

B
V
OK

3,1

P!

4,1

S
V
OK

1,1

4,4

2,3

3,3 P?

4,3

2,2

3,2

4,2

A
S G
B

V
OK
2,1

V
OK

(a)

3,4

P?

B
V
OK

3,1

P!

4,1

(b)

Figure 7.4 Two later stages in the progress of the agent. (a) After the third move,
with percept [Stench, None, None, None, None]. (b) After the fifth move, with percept
[Stench, Breeze, Glitter , None, None].

wumpus cannot be in [1,1], by the rules of the game, and it cannot be in [2,2] (or the agent
would have detected a stench when it was in [2,1]). Therefore, the agent can infer that the
wumpus is in [1,3]. The notation W! indicates this inference. Moreover, the lack of a breeze
in [1,2] implies that there is no pit in [2,2]. Yet the agent has already inferred that there must
be a pit in either [2,2] or [3,1], so this means it must be in [3,1]. This is a fairly difficult
inference, because it combines knowledge gained at different times in different places and
relies on the lack of a percept to make one crucial step.

240

Chapter

7.

Logical Agents

The agent has now proved to itself that there is neither a pit nor a wumpus in [2,2], so it
is OK to move there. We do not show the agentâ€™s state of knowledge at [2,2]; we just assume
that the agent turns and moves to [2,3], giving us Figure 7.4(b). In [2,3], the agent detects a
glitter, so it should grab the gold and then return home.
Note that in each case for which the agent draws a conclusion from the available information, that conclusion is guaranteed to be correct if the available information is correct.
This is a fundamental property of logical reasoning. In the rest of this chapter, we describe
how to build logical agents that can represent information and draw conclusions such as those
described in the preceding paragraphs.

7.3

L OGIC

SYNTAX

SEMANTICS
TRUTH
POSSIBLE WORLD

MODEL

SATISFACTION

ENTAILMENT

This section summarizes the fundamental concepts of logical representation and reasoning.
These beautiful ideas are independent of any of logicâ€™s particular forms. We therefore postpone the technical details of those forms until the next section, using instead the familiar
example of ordinary arithmetic.
In Section 7.1, we said that knowledge bases consist of sentences. These sentences
are expressed according to the syntax of the representation language, which specifies all the
sentences that are well formed. The notion of syntax is clear enough in ordinary arithmetic:
â€œx + y = 4â€ is a well-formed sentence, whereas â€œx4y+ =â€ is not.
A logic must also define the semantics or meaning of sentences. The semantics defines
the truth of each sentence with respect to each possible world. For example, the semantics
for arithmetic specifies that the sentence â€œx + y = 4â€ is true in a world where x is 2 and y
is 2, but false in a world where x is 1 and y is 1. In standard logics, every sentence must be
either true or false in each possible worldâ€”there is no â€œin between.â€1
When we need to be precise, we use the term model in place of â€œpossible world.â€
Whereas possible worlds might be thought of as (potentially) real environments that the agent
might or might not be in, models are mathematical abstractions, each of which simply fixes
the truth or falsehood of every relevant sentence. Informally, we may think of a possible world
as, for example, having x men and y women sitting at a table playing bridge, and the sentence
x + y = 4 is true when there are four people in total. Formally, the possible models are just
all possible assignments of real numbers to the variables x and y. Each such assignment fixes
the truth of any sentence of arithmetic whose variables are x and y. If a sentence Î± is true in
model m, we say that m satisfies Î± or sometimes m is a model of Î±. We use the notation
M (Î±) to mean the set of all models of Î±.
Now that we have a notion of truth, we are ready to talk about logical reasoning. This
involves the relation of logical entailment between sentencesâ€”the idea that a sentence follows logically from another sentence. In mathematical notation, we write
Î± |= Î²
1

Fuzzy logic, discussed in Chapter 14, allows for degrees of truth.

Section 7.3.

Logic

241

PIT

2

PIT

2

2
2

1

KB

1

Î±1

2

3

2

KB

1

2

PIT

2

2

PIT

2
2

PIT

1

Breez e

2

PIT

2

3

1

2

3

PIT

2

PIT

1

1

1

PIT

PIT

3

(a)

3

Breez e

1

1

PIT

PIT

2

3

PIT

PIT

Breez e

2

Breez e

3
1

1

3
1

1
1

2

3

1

PIT

Breez e

1

3
1

2

Breez e

Breez e

1

Breez e

2

Breez e

PIT

1

PIT

1

3

2

PIT

3
1

2

2

3

2

1
2

2

PIT

2
Breez e

1

1

PIT
1

1

PIT

2

Breez e

1

Breez e

1

3

PIT

2

Breez e

1

PIT

2

Î±2

Breez e

1
Breez e

1

Breez e

PIT

2

3

2

3

(b)

Figure 7.5 Possible models for the presence of pits in squares [1,2], [2,2], and [3,1]. The
KB corresponding to the observations of nothing in [1,1] and a breeze in [2,1] is shown by
the solid line. (a) Dotted line shows models of Î±1 (no pit in [1,2]). (b) Dotted line shows
models of Î±2 (no pit in [2,2]).

to mean that the sentence Î± entails the sentence Î². The formal definition of entailment is this:
Î± |= Î² if and only if, in every model in which Î± is true, Î² is also true. Using the notation just
introduced, we can write
Î± |= Î² if and only if M (Î±) âŠ† M (Î²) .
(Note the direction of the âŠ† here: if Î± |= Î², then Î± is a stronger assertion than Î²: it rules out
more possible worlds.) The relation of entailment is familiar from arithmetic; we are happy
with the idea that the sentence x = 0 entails the sentence xy = 0. Obviously, in any model
where x is zero, it is the case that xy is zero (regardless of the value of y).
We can apply the same kind of analysis to the wumpus-world reasoning example given
in the preceding section. Consider the situation in Figure 7.3(b): the agent has detected
nothing in [1,1] and a breeze in [2,1]. These percepts, combined with the agentâ€™s knowledge
of the rules of the wumpus world, constitute the KB. The agent is interested (among other
things) in whether the adjacent squares [1,2], [2,2], and [3,1] contain pits. Each of the three
squares might or might not contain a pit, so (for the purposes of this example) there are 23 = 8
possible models. These eight models are shown in Figure 7.5.2
The KB can be thought of as a set of sentences or as a single sentence that asserts all
the individual sentences. The KB is false in models that contradict what the agent knowsâ€”
for example, the KB is false in any model in which [1,2] contains a pit, because there is
no breeze in [1,1]. There are in fact just three models in which the KB is true, and these are
2 Although the figure shows the models as partial wumpus worlds, they are really nothing more than assignments
of true and false to the sentences â€œthere is a pit in [1,2]â€ etc. Models, in the mathematical sense, do not need to
have â€™orrible â€™airy wumpuses in them.

242

Chapter

7.

Logical Agents

shown surrounded by a solid line in Figure 7.5. Now let us consider two possible conclusions:
Î±1 = â€œThere is no pit in [1,2].â€
Î±2 = â€œThere is no pit in [2,2].â€
We have surrounded the models of Î±1 and Î±2 with dotted lines in Figures 7.5(a) and 7.5(b),
respectively. By inspection, we see the following:
in every model in which KB is true, Î±1 is also true.
Hence, KB |= Î±1 : there is no pit in [1,2]. We can also see that
in some models in which KB is true, Î±2 is false.

LOGICAL INFERENCE
MODEL CHECKING

Hence, KB |= Î±2 : the agent cannot conclude that there is no pit in [2,2]. (Nor can it conclude
that there is a pit in [2,2].)3
The preceding example not only illustrates entailment but also shows how the definition
of entailment can be applied to derive conclusionsâ€”that is, to carry out logical inference.
The inference algorithm illustrated in Figure 7.5 is called model checking, because it enumerates all possible models to check that Î± is true in all models in which KB is true, that is,
that M (KB) âŠ† M (Î±).
In understanding entailment and inference, it might help to think of the set of all consequences of KB as a haystack and of Î± as a needle. Entailment is like the needle being in the
haystack; inference is like finding it. This distinction is embodied in some formal notation: if
an inference algorithm i can derive Î± from KB, we write
KB i Î± ,

SOUND
TRUTH-PRESERVING

COMPLETENESS

which is pronounced â€œÎ± is derived from KB by iâ€ or â€œi derives Î± from KB .â€
An inference algorithm that derives only entailed sentences is called sound or truthpreserving. Soundness is a highly desirable property. An unsound inference procedure essentially makes things up as it goes alongâ€”it announces the discovery of nonexistent needles.
It is easy to see that model checking, when it is applicable,4 is a sound procedure.
The property of completeness is also desirable: an inference algorithm is complete if
it can derive any sentence that is entailed. For real haystacks, which are finite in extent,
it seems obvious that a systematic examination can always decide whether the needle is in
the haystack. For many knowledge bases, however, the haystack of consequences is infinite,
and completeness becomes an important issue.5 Fortunately, there are complete inference
procedures for logics that are sufficiently expressive to handle many knowledge bases.
We have described a reasoning process whose conclusions are guaranteed to be true
in any world in which the premises are true; in particular, if KB is true in the real world,
then any sentence Î± derived from KB by a sound inference procedure is also true in the real
world. So, while an inference process operates on â€œsyntaxâ€â€”internal physical configurations
such as bits in registers or patterns of electrical blips in brainsâ€”the process corresponds
3

The agent can calculate the probability that there is a pit in [2,2]; Chapter 13 shows how.
Model checking works if the space of models is finiteâ€”for example, in wumpus worlds of fixed size. For
arithmetic, on the other hand, the space of models is infinite: even if we restrict ourselves to the integers, there
are infinitely many pairs of values for x and y in the sentence x + y = 4.
5 Compare with the case of infinite search spaces in Chapter 3, where depth-first search is not complete.
4

Section 7.4.

Propositional Logic: A Very Simple Logic

Sentences

Aspects of the
real world

Sentence
Entails

Follows

Semantics

World

Semantics

Representation

243

Aspect of the
real world

Figure 7.6 Sentences are physical configurations of the agent, and reasoning is a process
of constructing new physical configurations from old ones. Logical reasoning should ensure that the new configurations represent aspects of the world that actually follow from the
aspects that the old configurations represent.

to the real-world relationship whereby some aspect of the real world is the case6 by virtue
of other aspects of the real world being the case. This correspondence between world and
representation is illustrated in Figure 7.6.
The final issue to consider is groundingâ€”the connection between logical reasoning
processes and the real environment in which the agent exists. In particular, how do we know
that KB is true in the real world? (After all, KB is just â€œsyntaxâ€ inside the agentâ€™s head.)
This is a philosophical question about which many, many books have been written. (See
Chapter 26.) A simple answer is that the agentâ€™s sensors create the connection. For example,
our wumpus-world agent has a smell sensor. The agent program creates a suitable sentence
whenever there is a smell. Then, whenever that sentence is in the knowledge base, it is
true in the real world. Thus, the meaning and truth of percept sentences are defined by the
processes of sensing and sentence construction that produce them. What about the rest of the
agentâ€™s knowledge, such as its belief that wumpuses cause smells in adjacent squares? This
is not a direct representation of a single percept, but a general ruleâ€”derived, perhaps, from
perceptual experience but not identical to a statement of that experience. General rules like
this are produced by a sentence construction process called learning, which is the subject
of Part V. Learning is fallible. It could be the case that wumpuses cause smells except on
February 29 in leap years, which is when they take their baths. Thus, KB may not be true in
the real world, but with good learning procedures, there is reason for optimism.

GROUNDING

7.4

P ROPOSITIONAL L OGIC : A V ERY S IMPLE L OGIC

PROPOSITIONAL
LOGIC

We now present a simple but powerful logic called propositional logic. We cover the syntax
of propositional logic and its semanticsâ€”the way in which the truth of sentences is determined. Then we look at entailmentâ€”the relation between a sentence and another sentence
that follows from itâ€”and see how this leads to a simple algorithm for logical inference. Everything takes place, of course, in the wumpus world.
6

As Wittgenstein (1922) put it in his famous Tractatus: â€œThe world is everything that is the case.â€

244

Chapter

7.

Logical Agents

7.4.1 Syntax
ATOMIC SENTENCES
PROPOSITION
SYMBOL

COMPLEX
SENTENCES
LOGICAL
CONNECTIVES
NEGATION
LITERAL

CONJUNCTION
DISJUNCTION

IMPLICATION
PREMISE
CONCLUSION
RULES
BICONDITIONAL

The syntax of propositional logic defines the allowable sentences. The atomic sentences
consist of a single proposition symbol. Each such symbol stands for a proposition that can
be true or false. We use symbols that start with an uppercase letter and may contain other
letters or subscripts, for example: P , Q, R, W1,3 and North. The names are arbitrary but
are often chosen to have some mnemonic valueâ€”we use W1,3 to stand for the proposition
that the wumpus is in [1,3]. (Remember that symbols such as W1,3 are atomic, i.e., W , 1,
and 3 are not meaningful parts of the symbol.) There are two proposition symbols with fixed
meanings: True is the always-true proposition and False is the always-false proposition.
Complex sentences are constructed from simpler sentences, using parentheses and logical
connectives. There are five connectives in common use:
Â¬ (not). A sentence such as Â¬W1,3 is called the negation of W1,3 . A literal is either an
atomic sentence (a positive literal) or a negated atomic sentence (a negative literal).
âˆ§ (and). A sentence whose main connective is âˆ§, such as W1,3 âˆ§ P3,1 , is called a conjunction; its parts are the conjuncts. (The âˆ§ looks like an â€œAâ€ for â€œAnd.â€)
âˆ¨ (or). A sentence using âˆ¨, such as (W1,3 âˆ§ P3,1 )âˆ¨ W2,2 , is a disjunction of the disjuncts
(W1,3 âˆ§ P3,1 ) and W2,2 . (Historically, the âˆ¨ comes from the Latin â€œvel,â€ which means
â€œor.â€ For most people, it is easier to remember âˆ¨ as an upside-down âˆ§.)
â‡’ (implies). A sentence such as (W1,3 âˆ§ P3,1 ) â‡’ Â¬W2,2 is called an implication (or conditional). Its premise or antecedent is (W1,3 âˆ§ P3,1 ), and its conclusion or consequent
is Â¬W2,2 . Implications are also known as rules or ifâ€“then statements. The implication
symbol is sometimes written in other books as âŠƒ or â†’.
â‡” (if and only if). The sentence W1,3 â‡” Â¬W2,2 is a biconditional. Some other books
write this as â‰¡.
Sentence â†’ AtomicSentence | ComplexSentence
AtomicSentence â†’ True | False | P | Q | R | . . .
ComplexSentence â†’ ( Sentence ) | [ Sentence ]
| Â¬ Sentence

O PERATOR P RECEDENCE

|
|

Sentence âˆ§ Sentence
Sentence âˆ¨ Sentence

|
|

Sentence â‡’ Sentence
Sentence â‡” Sentence

:

Â¬, âˆ§, âˆ¨, â‡’, â‡”

Figure 7.7 A BNF (Backusâ€“Naur Form) grammar of sentences in propositional logic,
along with operator precedences, from highest to lowest.

Section 7.4.

Propositional Logic: A Very Simple Logic

245

Figure 7.7 gives a formal grammar of propositional logic; see page 1060 if you are not
familiar with the BNF notation. The BNF grammar by itself is ambiguous; a sentence with
several operators can be parsed by the grammar in multiple ways. To eliminate the ambiguity
we define a precedence for each operator. The â€œnotâ€ operator (Â¬) has the highest precedence,
which means that in the sentence Â¬A âˆ§ B the Â¬ binds most tightly, giving us the equivalent
of (Â¬A) âˆ§ B rather than Â¬(A âˆ§ B). (The notation for ordinary arithmetic is the same: âˆ’2 + 4
is 2, not â€“6.) When in doubt, use parentheses to make sure of the right interpretation. Square
brackets mean the same thing as parentheses; the choice of square brackets or parentheses is
solely to make it easier for a human to read a sentence.

7.4.2 Semantics

TRUTH VALUE

Having specified the syntax of propositional logic, we now specify its semantics. The semantics defines the rules for determining the truth of a sentence with respect to a particular
model. In propositional logic, a model simply fixes the truth valueâ€”true or falseâ€”for every proposition symbol. For example, if the sentences in the knowledge base make use of the
proposition symbols P1,2 , P2,2 , and P3,1 , then one possible model is
m1 = {P1,2 = false, P2,2 = false, P3,1 = true} .
With three proposition symbols, there are 23 = 8 possible modelsâ€”exactly those depicted
in Figure 7.5. Notice, however, that the models are purely mathematical objects with no
necessary connection to wumpus worlds. P1,2 is just a symbol; it might mean â€œthere is a pit
in [1,2]â€ or â€œIâ€™m in Paris today and tomorrow.â€
The semantics for propositional logic must specify how to compute the truth value of
any sentence, given a model. This is done recursively. All sentences are constructed from
atomic sentences and the five connectives; therefore, we need to specify how to compute the
truth of atomic sentences and how to compute the truth of sentences formed with each of the
five connectives. Atomic sentences are easy:
â€¢ True is true in every model and False is false in every model.
â€¢ The truth value of every other proposition symbol must be specified directly in the
model. For example, in the model m1 given earlier, P1,2 is false.
For complex sentences, we have five rules, which hold for any subsentences P and Q in any
model m (here â€œiffâ€ means â€œif and only ifâ€):
â€¢
â€¢
â€¢
â€¢
â€¢

TRUTH TABLE

Â¬P is true iff P is false in m.
P âˆ§ Q is true iff both P and Q are true in m.
P âˆ¨ Q is true iff either P or Q is true in m.
P â‡’ Q is true unless P is true and Q is false in m.
P â‡” Q is true iff P and Q are both true or both false in m.

The rules can also be expressed with truth tables that specify the truth value of a complex
sentence for each possible assignment of truth values to its components. Truth tables for the
five connectives are given in Figure 7.8. From these tables, the truth value of any sentence s
can be computed with respect to any model m by a simple recursive evaluation. For example,

246

Chapter

7.

Logical Agents

P

Q

Â¬P

P âˆ§Q

P âˆ¨Q

P â‡’ Q

P â‡” Q

false
false
true
true

false
true
false
true

true
true
false
false

false
false
false
true

false
true
true
true

true
true
false
true

true
false
false
true

Figure 7.8 Truth tables for the five logical connectives. To use the table to compute, for
example, the value of P âˆ¨ Q when P is true and Q is false, first look on the left for the row
where P is true and Q is false (the third row). Then look in that row under the P âˆ¨Q column
to see the result: true.

the sentence Â¬P1,2 âˆ§ (P2,2 âˆ¨ P3,1 ), evaluated in m1 , gives true âˆ§ (false âˆ¨ true) = true âˆ§
true = true. Exercise 7.3 asks you to write the algorithm PL-T RUE ?(s, m), which computes
the truth value of a propositional logic sentence s in a model m.
The truth tables for â€œand,â€ â€œor,â€ and â€œnotâ€ are in close accord with our intuitions about
the English words. The main point of possible confusion is that P âˆ¨ Q is true when P is true
or Q is true or both. A different connective, called â€œexclusive orâ€ (â€œxorâ€ for short), yields
false when both disjuncts are true.7 There is no consensus on the symbol for exclusive or;
some choices are âˆ¨Ë™ or = or âŠ•.
The truth table for â‡’ may not quite fit oneâ€™s intuitive understanding of â€œP implies Qâ€
or â€œif P then Q.â€ For one thing, propositional logic does not require any relation of causation
or relevance between P and Q. The sentence â€œ5 is odd implies Tokyo is the capital of Japanâ€
is a true sentence of propositional logic (under the normal interpretation), even though it is
a decidedly odd sentence of English. Another point of confusion is that any implication is
true whenever its antecedent is false. For example, â€œ5 is even implies Sam is smartâ€ is true,
regardless of whether Sam is smart. This seems bizarre, but it makes sense if you think of
â€œP â‡’ Qâ€ as saying, â€œIf P is true, then I am claiming that Q is true. Otherwise I am making
no claim.â€ The only way for this sentence to be false is if P is true but Q is false.
The biconditional, P â‡” Q, is true whenever both P â‡’ Q and Q â‡’ P are true. In
English, this is often written as â€œP if and only if Q.â€ Many of the rules of the wumpus world
are best written using â‡”. For example, a square is breezy if a neighboring square has a pit,
and a square is breezy only if a neighboring square has a pit. So we need a biconditional,
B1,1 â‡” (P1,2 âˆ¨ P2,1 ) ,
where B1,1 means that there is a breeze in [1,1].

7.4.3 A simple knowledge base
Now that we have defined the semantics for propositional logic, we can construct a knowledge
base for the wumpus world. We focus first on the immutable aspects of the wumpus world,
leaving the mutable aspects for a later section. For now, we need the following symbols for
each [x, y] location:
7

Latin has a separate word, aut, for exclusive or.

Section 7.4.

Propositional Logic: A Very Simple Logic

247

Px,y is true if there is a pit in [x, y].
Wx,y is true if there is a wumpus in [x, y], dead or alive.
Bx,y is true if the agent perceives a breeze in [x, y].
Sx,y is true if the agent perceives a stench in [x, y].
The sentences we write will suffice to derive Â¬P1,2 (there is no pit in [1,2]), as was done
informally in Section 7.3. We label each sentence Ri so that we can refer to them:
â€¢ There is no pit in [1,1]:
R1 :

Â¬P1,1 .

â€¢ A square is breezy if and only if there is a pit in a neighboring square. This has to be
stated for each square; for now, we include just the relevant squares:
R2 :

B1,1

â‡”

(P1,2 âˆ¨ P2,1 ) .

R3 :

B2,1

â‡”

(P1,1 âˆ¨ P2,2 âˆ¨ P3,1 ) .

â€¢ The preceding sentences are true in all wumpus worlds. Now we include the breeze
percepts for the first two squares visited in the specific world the agent is in, leading up
to the situation in Figure 7.3(b).
R4 : Â¬B1,1 .
R5 : B2,1 .

7.4.4 A simple inference procedure
Our goal now is to decide whether KB |= Î± for some sentence Î±. For example, is Â¬P1,2
entailed by our KB? Our first algorithm for inference is a model-checking approach that is a
direct implementation of the definition of entailment: enumerate the models, and check that
Î± is true in every model in which KB is true. Models are assignments of true or false to
every proposition symbol. Returning to our wumpus-world example, the relevant proposition symbols are B1,1 , B2,1 , P1,1 , P1,2 , P2,1 , P2,2 , and P3,1 . With seven symbols, there are
27 = 128 possible models; in three of these, KB is true (Figure 7.9). In those three models,
Â¬P1,2 is true, hence there is no pit in [1,2]. On the other hand, P2,2 is true in two of the three
models and false in one, so we cannot yet tell whether there is a pit in [2,2].
Figure 7.9 reproduces in a more precise form the reasoning illustrated in Figure 7.5. A
general algorithm for deciding entailment in propositional logic is shown in Figure 7.10. Like
the BACKTRACKING-S EARCH algorithm on page 215, TT-E NTAILS ? performs a recursive
enumeration of a finite space of assignments to symbols. The algorithm is sound because it
implements directly the definition of entailment, and complete because it works for any KB
and Î± and always terminatesâ€”there are only finitely many models to examine.
Of course, â€œfinitely manyâ€ is not always the same as â€œfew.â€ If KB and Î± contain n
symbols in all, then there are 2n models. Thus, the time complexity of the algorithm is
O(2n ). (The space complexity is only O(n) because the enumeration is depth-first.) Later in
this chapter we show algorithms that are much more efficient in many cases. Unfortunately,
propositional entailment is co-NP-complete (i.e., probably no easier than NP-completeâ€”see
Appendix A), so every known inference algorithm for propositional logic has a worst-case
complexity that is exponential in the size of the input.

248

Chapter

7.

Logical Agents

B1,1

B2,1

P1,1

P1,2

P2,1

P2,2

P3,1

R1

R2

R3

R4

R5

KB

false
false
..
.

false
false
..
.

false
false
..
.

false
false
..
.

false
false
..
.

false
false
..
.

false
true
..
.

true
true
..
.

true
true
..
.

true
false
..
.

true
true
..
.

false
false
..
.

false
false
..
.

false

true

false

false

false

false

false

true

true

false

true

true

false

false
false
false

true
true
true

false
false
false

false
false
false

false
false
false

false
true
true

true
false
true

true
true
true

true
true
true

true
true
true

true
true
true

true
true
true

true
true
true

false
..
.

true
..
.

false
..
.

false
..
.

true
..
.

false
..
.

false
..
.

true
..
.

false
..
.

false
..
.

true
..
.

true
..
.

false
..
.

true

true

true

true

true

true

true

false

true

true

false

true

false

Figure 7.9 A truth table constructed for the knowledge base given in the text. KB is true
if R1 through R5 are true, which occurs in just 3 of the 128 rows (the ones underlined in the
right-hand column). In all 3 rows, P1,2 is false, so there is no pit in [1,2]. On the other hand,
there might (or might not) be a pit in [2,2].

function TT-E NTAILS ?(KB , Î±) returns true or false
inputs: KB , the knowledge base, a sentence in propositional logic
Î±, the query, a sentence in propositional logic
symbols â† a list of the proposition symbols in KB and Î±
return TT-C HECK -A LL(KB , Î±, symbols, { })
function TT-C HECK -A LL(KB, Î±, symbols , model ) returns true or false
if E MPTY ?(symbols) then
if PL-T RUE ?(KB , model ) then return PL-T RUE ?(Î±, model )
else return true // when KB is false, always return true
else do
P â† F IRST (symbols)
rest â† R EST(symbols)
return (TT-C HECK -A LL(KB, Î±, rest , model âˆª {P = true})
and
TT-C HECK -A LL(KB , Î±, rest , model âˆª {P = false }))
Figure 7.10 A truth-table enumeration algorithm for deciding propositional entailment.
(TT stands for truth table.) PL-T RUE ? returns true if a sentence holds within a model. The
variable model represents a partial modelâ€”an assignment to some of the symbols. The keyword â€œandâ€ is used here as a logical operation on its two arguments, returning true or false.

Section 7.5.

Propositional Theorem Proving
(Î± âˆ§ Î²)
(Î± âˆ¨ Î²)
((Î± âˆ§ Î²) âˆ§ Î³)
((Î± âˆ¨ Î²) âˆ¨ Î³)
Â¬(Â¬Î±)
(Î± â‡’ Î²)
(Î± â‡’ Î²)
(Î± â‡” Î²)
Â¬(Î± âˆ§ Î²)
Â¬(Î± âˆ¨ Î²)
(Î± âˆ§ (Î² âˆ¨ Î³))
(Î± âˆ¨ (Î² âˆ§ Î³))

â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡

249

(Î² âˆ§ Î±) commutativity of âˆ§
(Î² âˆ¨ Î±) commutativity of âˆ¨
(Î± âˆ§ (Î² âˆ§ Î³)) associativity of âˆ§
(Î± âˆ¨ (Î² âˆ¨ Î³)) associativity of âˆ¨
Î± double-negation elimination
(Â¬Î² â‡’ Â¬Î±) contraposition
(Â¬Î± âˆ¨ Î²) implication elimination
((Î± â‡’ Î²) âˆ§ (Î² â‡’ Î±)) biconditional elimination
(Â¬Î± âˆ¨ Â¬Î²) De Morgan
(Â¬Î± âˆ§ Â¬Î²) De Morgan
((Î± âˆ§ Î²) âˆ¨ (Î± âˆ§ Î³)) distributivity of âˆ§ over âˆ¨
((Î± âˆ¨ Î²) âˆ§ (Î± âˆ¨ Î³)) distributivity of âˆ¨ over âˆ§

Figure 7.11 Standard logical equivalences. The symbols Î±, Î², and Î³ stand for arbitrary
sentences of propositional logic.

7.5

P ROPOSITIONAL T HEOREM P ROVING

THEOREM PROVING

LOGICAL
EQUIVALENCE

So far, we have shown how to determine entailment by model checking: enumerating models
and showing that the sentence must hold in all models. In this section, we show how entailment can be done by theorem provingâ€”applying rules of inference directly to the sentences
in our knowledge base to construct a proof of the desired sentence without consulting models.
If the number of models is large but the length of the proof is short, then theorem proving can
be more efficient than model checking.
Before we plunge into the details of theorem-proving algorithms, we will need some
additional concepts related to entailment. The first concept is logical equivalence: two sentences Î± and Î² are logically equivalent if they are true in the same set of models. We write
this as Î± â‰¡ Î². For example, we can easily show (using truth tables) that P âˆ§ Q and Q âˆ§ P
are logically equivalent; other equivalences are shown in Figure 7.11. These equivalences
play much the same role in logic as arithmetic identities do in ordinary mathematics. An
alternative definition of equivalence is as follows: any two sentences Î± and Î² are equivalent
only if each of them entails the other:
Î±â‰¡Î²

VALIDITY
TAUTOLOGY

DEDUCTION
THEOREM

if and only if Î± |= Î² and Î² |= Î± .

The second concept we will need is validity. A sentence is valid if it is true in all models. For
example, the sentence P âˆ¨ Â¬P is valid. Valid sentences are also known as tautologiesâ€”they
are necessarily true. Because the sentence True is true in all models, every valid sentence
is logically equivalent to True. What good are valid sentences? From our definition of
entailment, we can derive the deduction theorem, which was known to the ancient Greeks:
For any sentences Î± and Î², Î± |= Î² if and only if the sentence (Î± â‡’ Î²) is valid.
(Exercise 7.5 asks for a proof.) Hence, we can decide if Î± |= Î² by checking that (Î± â‡’ Î²) is
true in every modelâ€”which is essentially what the inference algorithm in Figure 7.10 doesâ€”

250

SATISFIABILITY

SAT

Chapter

7.

Logical Agents

or by proving that (Î± â‡’ Î²) is equivalent to True. Conversely, the deduction theorem states
that every valid implication sentence describes a legitimate inference.
The final concept we will need is satisfiability. A sentence is satisfiable if it is true
in, or satisfied by, some model. For example, the knowledge base given earlier, (R1 âˆ§ R2 âˆ§
R3 âˆ§ R4 âˆ§ R5 ), is satisfiable because there are three models in which it is true, as shown
in Figure 7.9. Satisfiability can be checked by enumerating the possible models until one is
found that satisfies the sentence. The problem of determining the satisfiability of sentences
in propositional logicâ€”the SAT problemâ€”was the first problem proved to be NP-complete.
Many problems in computer science are really satisfiability problems. For example, all the
constraint satisfaction problems in Chapter 6 ask whether the constraints are satisfiable by
some assignment.
Validity and satisfiability are of course connected: Î± is valid iff Â¬Î± is unsatisfiable;
contrapositively, Î± is satisfiable iff Â¬Î± is not valid. We also have the following useful result:
Î± |= Î² if and only if the sentence (Î± âˆ§ Â¬Î²) is unsatisfiable.

REDUCTIO AD
ABSURDUM
REFUTATION
CONTRADICTION

Proving Î² from Î± by checking the unsatisfiability of (Î± âˆ§ Â¬Î²) corresponds exactly to the
standard mathematical proof technique of reductio ad absurdum (literally, â€œreduction to an
absurd thingâ€). It is also called proof by refutation or proof by contradiction. One assumes a
sentence Î² to be false and shows that this leads to a contradiction with known axioms Î±. This
contradiction is exactly what is meant by saying that the sentence (Î± âˆ§ Â¬Î²) is unsatisfiable.

7.5.1 Inference and proofs
INFERENCE RULES
PROOF
MODUS PONENS

AND-ELIMINATION

This section covers inference rules that can be applied to derive a proofâ€”a chain of conclusions that leads to the desired goal. The best-known rule is called Modus Ponens (Latin for
mode that affirms) and is written
Î± â‡’ Î²,
Î±
.
Î²
The notation means that, whenever any sentences of the form Î± â‡’ Î² and Î± are given, then
the sentence Î² can be inferred. For example, if (WumpusAhead âˆ§ WumpusAlive) â‡’ Shoot
and (WumpusAhead âˆ§ WumpusAlive) are given, then Shoot can be inferred.
Another useful inference rule is And-Elimination, which says that, from a conjunction,
any of the conjuncts can be inferred:
Î±âˆ§Î²
.
Î±
For example, from (WumpusAhead âˆ§ WumpusAlive), WumpusAlive can be inferred.
By considering the possible truth values of Î± and Î², one can show easily that Modus
Ponens and And-Elimination are sound once and for all. These rules can then be used in
any particular instances where they apply, generating sound inferences without the need for
enumerating models.
All of the logical equivalences in Figure 7.11 can be used as inference rules. For example, the equivalence for biconditional elimination yields the two inference rules
(Î± â‡’ Î²) âˆ§ (Î² â‡’ Î±)
Î± â‡” Î²
and
.
(Î± â‡’ Î²) âˆ§ (Î² â‡’ Î±)
Î± â‡” Î²

Section 7.5.

Propositional Theorem Proving

251

Not all inference rules work in both directions like this. For example, we cannot run Modus
Ponens in the opposite direction to obtain Î± â‡’ Î² and Î± from Î².
Let us see how these inference rules and equivalences can be used in the wumpus world.
We start with the knowledge base containing R1 through R5 and show how to prove Â¬P1,2 ,
that is, there is no pit in [1,2]. First, we apply biconditional elimination to R2 to obtain
R6 :

(B1,1 â‡’ (P1,2 âˆ¨ P2,1 )) âˆ§ ((P1,2 âˆ¨ P2,1 ) â‡’ B1,1 ) .

Then we apply And-Elimination to R6 to obtain
R7 :

((P1,2 âˆ¨ P2,1 ) â‡’ B1,1 ) .

Logical equivalence for contrapositives gives
R8 :

(Â¬B1,1 â‡’ Â¬(P1,2 âˆ¨ P2,1 )) .

Now we can apply Modus Ponens with R8 and the percept R4 (i.e., Â¬B1,1 ), to obtain
R9 :

Â¬(P1,2 âˆ¨ P2,1 ) .

Finally, we apply De Morganâ€™s rule, giving the conclusion
R10 :

Â¬P1,2 âˆ§ Â¬P2,1 .

That is, neither [1,2] nor [2,1] contains a pit.
We found this proof by hand, but we can apply any of the search algorithms in Chapter 3
to find a sequence of steps that constitutes a proof. We just need to define a proof problem as
follows:
â€¢ I NITIAL S TATE: the initial knowledge base.
â€¢ ACTIONS: the set of actions consists of all the inference rules applied to all the sentences that match the top half of the inference rule.
â€¢ R ESULT: the result of an action is to add the sentence in the bottom half of the inference
rule.
â€¢ G OAL: the goal is a state that contains the sentence we are trying to prove.

MONOTONICITY

Thus, searching for proofs is an alternative to enumerating models. In many practical cases
finding a proof can be more efficient because the proof can ignore irrelevant propositions, no
matter how many of them there are. For example, the proof given earlier leading to Â¬P1,2 âˆ§
Â¬P2,1 does not mention the propositions B2,1 , P1,1 , P2,2 , or P3,1 . They can be ignored
because the goal proposition, P1,2 , appears only in sentence R2 ; the other propositions in R2
appear only in R4 and R2 ; so R1 , R3 , and R5 have no bearing on the proof. The same would
hold even if we added a million more sentences to the knowledge base; the simple truth-table
algorithm, on the other hand, would be overwhelmed by the exponential explosion of models.
One final property of logical systems is monotonicity, which says that the set of entailed sentences can only increase as information is added to the knowledge base.8 For any
sentences Î± and Î²,
if
8

KB |= Î±

then

KB âˆ§ Î² |= Î± .

Nonmonotonic logics, which violate the monotonicity property, capture a common property of human reasoning: changing oneâ€™s mind. They are discussed in Section 12.6.

252

Chapter

7.

Logical Agents

For example, suppose the knowledge base contains the additional assertion Î² stating that there
are exactly eight pits in the world. This knowledge might help the agent draw additional conclusions, but it cannot invalidate any conclusion Î± already inferredâ€”such as the conclusion
that there is no pit in [1,2]. Monotonicity means that inference rules can be applied whenever
suitable premises are found in the knowledge baseâ€”the conclusion of the rule must follow
regardless of what else is in the knowledge base.

7.5.2 Proof by resolution
We have argued that the inference rules covered so far are sound, but we have not discussed
the question of completeness for the inference algorithms that use them. Search algorithms
such as iterative deepening search (page 89) are complete in the sense that they will find
any reachable goal, but if the available inference rules are inadequate, then the goal is not
reachableâ€”no proof exists that uses only those inference rules. For example, if we removed
the biconditional elimination rule, the proof in the preceding section would not go through.
The current section introduces a single inference rule, resolution, that yields a complete
inference algorithm when coupled with any complete search algorithm.
We begin by using a simple version of the resolution rule in the wumpus world. Let us
consider the steps leading up to Figure 7.4(a): the agent returns from [2,1] to [1,1] and then
goes to [1,2], where it perceives a stench, but no breeze. We add the following facts to the
knowledge base:
R11 : Â¬B1,2 .
R12 : B1,2 â‡” (P1,1 âˆ¨ P2,2 âˆ¨ P1,3 ) .
By the same process that led to R10 earlier, we can now derive the absence of pits in [2,2]
and [1,3] (remember that [1,1] is already known to be pitless):
R13 : Â¬P2,2 .
R14 : Â¬P1,3 .
We can also apply biconditional elimination to R3 , followed by Modus Ponens with R5 , to
obtain the fact that there is a pit in [1,1], [2,2], or [3,1]:
R15 :
RESOLVENT

P1,1 âˆ¨ P2,2 âˆ¨ P3,1 .

Now comes the first application of the resolution rule: the literal Â¬P2,2 in R13 resolves with
the literal P2,2 in R15 to give the resolvent
R16 :

P1,1 âˆ¨ P3,1 .

In English; if thereâ€™s a pit in one of [1,1], [2,2], and [3,1] and itâ€™s not in [2,2], then itâ€™s in [1,1]
or [3,1]. Similarly, the literal Â¬P1,1 in R1 resolves with the literal P1,1 in R16 to give
R17 :
UNIT RESOLUTION

COMPLEMENTARY
LITERALS

P3,1 .

In English: if thereâ€™s a pit in [1,1] or [3,1] and itâ€™s not in [1,1], then itâ€™s in [3,1]. These last
two inference steps are examples of the unit resolution inference rule,
1 âˆ¨ Â· Â· Â· âˆ¨ k ,
m
,
1 âˆ¨ Â· Â· Â· âˆ¨ iâˆ’1 âˆ¨ i+1 âˆ¨ Â· Â· Â· âˆ¨ k
where each  is a literal and i and m are complementary literals (i.e., one is the negation

Section 7.5.
CLAUSE

UNIT CLAUSE
RESOLUTION

FACTORING

Propositional Theorem Proving

253

of the other). Thus, the unit resolution rule takes a clauseâ€”a disjunction of literalsâ€”and a
literal and produces a new clause. Note that a single literal can be viewed as a disjunction of
one literal, also known as a unit clause.
The unit resolution rule can be generalized to the full resolution rule,
1 âˆ¨ Â· Â· Â· âˆ¨ k ,
m1 âˆ¨ Â· Â· Â· âˆ¨ m n
,
1 âˆ¨ Â· Â· Â· âˆ¨ iâˆ’1 âˆ¨ i+1 âˆ¨ Â· Â· Â· âˆ¨ k âˆ¨ m1 âˆ¨ Â· Â· Â· âˆ¨ mjâˆ’1 âˆ¨ mj+1 âˆ¨ Â· Â· Â· âˆ¨ mn
where i and mj are complementary literals. This says that resolution takes two clauses and
produces a new clause containing all the literals of the two original clauses except the two
complementary literals. For example, we have
Â¬P1,1 âˆ¨ Â¬P2,2
P1,1 âˆ¨ P3,1 ,
.
P3,1 âˆ¨ Â¬P2,2
There is one more technical aspect of the resolution rule: the resulting clause should contain
only one copy of each literal.9 The removal of multiple copies of literals is called factoring.
For example, if we resolve (A âˆ¨ B) with (A âˆ¨ Â¬B), we obtain (A âˆ¨ A), which is reduced to
just A.
The soundness of the resolution rule can be seen easily by considering the literal i that
is complementary to literal mj in the other clause. If i is true, then mj is false, and hence
m1 âˆ¨ Â· Â· Â· âˆ¨ mjâˆ’1 âˆ¨ mj+1 âˆ¨ Â· Â· Â· âˆ¨ mn must be true, because m1 âˆ¨ Â· Â· Â· âˆ¨ mn is given. If i is
false, then 1 âˆ¨ Â· Â· Â· âˆ¨ iâˆ’1 âˆ¨ i+1 âˆ¨ Â· Â· Â· âˆ¨ k must be true because 1 âˆ¨ Â· Â· Â· âˆ¨ k is given. Now
i is either true or false, so one or other of these conclusions holdsâ€”exactly as the resolution
rule states.
What is more surprising about the resolution rule is that it forms the basis for a family
of complete inference procedures. A resolution-based theorem prover can, for any sentences
Î± and Î² in propositional logic, decide whether Î± |= Î². The next two subsections explain
how resolution accomplishes this.
Conjunctive normal form

CONJUNCTIVE
NORMAL FORM

The resolution rule applies only to clauses (that is, disjunctions of literals), so it would seem
to be relevant only to knowledge bases and queries consisting of clauses. How, then, can
it lead to a complete inference procedure for all of propositional logic? The answer is that
every sentence of propositional logic is logically equivalent to a conjunction of clauses. A
sentence expressed as a conjunction of clauses is said to be in conjunctive normal form or
CNF (see Figure 7.14). We now describe a procedure for converting to CNF. We illustrate
the procedure by converting the sentence B1,1 â‡” (P1,2 âˆ¨ P2,1 ) into CNF. The steps are as
follows:
1. Eliminate â‡”, replacing Î± â‡” Î² with (Î± â‡’ Î²) âˆ§ (Î² â‡’ Î±).
(B1,1 â‡’ (P1,2 âˆ¨ P2,1 )) âˆ§ ((P1,2 âˆ¨ P2,1 ) â‡’ B1,1 ) .
2. Eliminate â‡’, replacing Î± â‡’ Î² with Â¬Î± âˆ¨ Î²:
(Â¬B1,1 âˆ¨ P1,2 âˆ¨ P2,1 ) âˆ§ (Â¬(P1,2 âˆ¨ P2,1 ) âˆ¨ B1,1 ) .
9

If a clause is viewed as a set of literals, then this restriction is automatically respected. Using set notation for
clauses makes the resolution rule much cleaner, at the cost of introducing additional notation.

254

Chapter

7.

Logical Agents

3. CNF requires Â¬ to appear only in literals, so we â€œmove Â¬ inwardsâ€ by repeated application of the following equivalences from Figure 7.11:
Â¬(Â¬Î±) â‰¡ Î± (double-negation elimination)
Â¬(Î± âˆ§ Î²) â‰¡ (Â¬Î± âˆ¨ Â¬Î²) (De Morgan)
Â¬(Î± âˆ¨ Î²) â‰¡ (Â¬Î± âˆ§ Â¬Î²) (De Morgan)
In the example, we require just one application of the last rule:
(Â¬B1,1 âˆ¨ P1,2 âˆ¨ P2,1 ) âˆ§ ((Â¬P1,2 âˆ§ Â¬P2,1 ) âˆ¨ B1,1 ) .
4. Now we have a sentence containing nested âˆ§ and âˆ¨ operators applied to literals. We
apply the distributivity law from Figure 7.11, distributing âˆ¨ over âˆ§ wherever possible.
(Â¬B1,1 âˆ¨ P1,2 âˆ¨ P2,1 ) âˆ§ (Â¬P1,2 âˆ¨ B1,1 ) âˆ§ (Â¬P2,1 âˆ¨ B1,1 ) .
The original sentence is now in CNF, as a conjunction of three clauses. It is much harder to
read, but it can be used as input to a resolution procedure.
A resolution algorithm
Inference procedures based on resolution work by using the principle of proof by contradiction introduced on page 250. That is, to show that KB |= Î±, we show that (KB âˆ§ Â¬Î±) is
unsatisfiable. We do this by proving a contradiction.
A resolution algorithm is shown in Figure 7.12. First, (KB âˆ§ Â¬Î±) is converted into
CNF. Then, the resolution rule is applied to the resulting clauses. Each pair that contains
complementary literals is resolved to produce a new clause, which is added to the set if it is
not already present. The process continues until one of two things happens:
â€¢ there are no new clauses that can be added, in which case KB does not entail Î±; or,
â€¢ two clauses resolve to yield the empty clause, in which case KB entails Î±.
The empty clauseâ€”a disjunction of no disjunctsâ€”is equivalent to False because a disjunction
is true only if at least one of its disjuncts is true. Another way to see that an empty clause
represents a contradiction is to observe that it arises only from resolving two complementary
unit clauses such as P and Â¬P .
We can apply the resolution procedure to a very simple inference in the wumpus world.
When the agent is in [1,1], there is no breeze, so there can be no pits in neighboring squares.
The relevant knowledge base is
KB = R2 âˆ§ R4 = (B1,1 â‡” (P1,2 âˆ¨ P2,1 )) âˆ§ Â¬B1,1
and we wish to prove Î± which is, say, Â¬P1,2 . When we convert (KB âˆ§ Â¬Î±) into CNF, we
obtain the clauses shown at the top of Figure 7.13. The second row of the figure shows
clauses obtained by resolving pairs in the first row. Then, when P1,2 is resolved with Â¬P1,2 ,
we obtain the empty clause, shown as a small square. Inspection of Figure 7.13 reveals that
many resolution steps are pointless. For example, the clause B1,1 âˆ¨ Â¬B1,1 âˆ¨ P1,2 is equivalent
to True âˆ¨ P1,2 which is equivalent to True. Deducing that True is true is not very helpful.
Therefore, any clause in which two complementary literals appear can be discarded.

Section 7.5.

Propositional Theorem Proving

255

function PL-R ESOLUTION(KB, Î±) returns true or false
inputs: KB , the knowledge base, a sentence in propositional logic
Î±, the query, a sentence in propositional logic
clauses â† the set of clauses in the CNF representation of KB âˆ§ Â¬Î±
new â† { }
loop do
for each pair of clauses Ci , Cj in clauses do
resolvents â† PL-R ESOLVE(Ci , Cj )
if resolvents contains the empty clause then return true
new â† new âˆª resolvents
if new âŠ† clauses then return false
clauses â† clauses âˆª new
Figure 7.12 A simple resolution algorithm for propositional logic. The function
PL-R ESOLVE returns the set of all possible clauses obtained by resolving its two inputs.

Â¬B1,1

^

Â¬P2,1

P2,1

^

^

P2,1

Â¬P1,2

B1,1

P1,2

^

^

P1,2

P2,1

P2,1

Â¬B1,1

B1,1

^

^

B1,1

^

^

P1,2

Â¬B1,1 P1,2

^

^

Â¬B1,1

B1,1

^

Â¬P2,1

Â¬P1,2

Â¬P2,1

P1,2

Â¬P1,2

Figure 7.13 Partial application of PL-R ESOLUTION to a simple inference in the wumpus
world. Â¬P1,2 is shown to follow from the first four clauses in the top row.

Completeness of resolution
RESOLUTION
CLOSURE

GROUND
RESOLUTION
THEOREM

To conclude our discussion of resolution, we now show why PL-R ESOLUTION is complete.
To do this, we introduce the resolution closure RC (S) of a set of clauses S, which is the set
of all clauses derivable by repeated application of the resolution rule to clauses in S or their
derivatives. The resolution closure is what PL-R ESOLUTION computes as the final value of
the variable clauses. It is easy to see that RC (S) must be finite, because there are only finitely
many distinct clauses that can be constructed out of the symbols P1 , . . . , Pk that appear in S.
(Notice that this would not be true without the factoring step that removes multiple copies of
literals.) Hence, PL-R ESOLUTION always terminates.
The completeness theorem for resolution in propositional logic is called the ground
resolution theorem:
If a set of clauses is unsatisfiable, then the resolution closure of those clauses
contains the empty clause.
This theorem is proved by demonstrating its contrapositive: if the closure RC (S) does not

256

Chapter

7.

Logical Agents

contain the empty clause, then S is satisfiable. In fact, we can construct a model for S with
suitable truth values for P1 , . . . , Pk . The construction procedure is as follows:
For i from 1 to k,
â€“ If a clause in RC (S) contains the literal Â¬Pi and all its other literals are false under
the assignment chosen for P1 , . . . , Piâˆ’1 , then assign false to Pi .
â€“ Otherwise, assign true to Pi .
This assignment to P1 , . . . , Pk is a model of S. To see this, assume the oppositeâ€”that, at
some stage i in the sequence, assigning symbol Pi causes some clause C to become false.
For this to happen, it must be the case that all the other literals in C must already have been
falsified by assignments to P1 , . . . , Piâˆ’1 . Thus, C must now look like either (false âˆ¨ false âˆ¨
Â· Â· Â· false âˆ¨Pi ) or like (false âˆ¨false âˆ¨Â· Â· Â· false âˆ¨Â¬Pi ). If just one of these two is in RC(S), then
the algorithm will assign the appropriate truth value to Pi to make C true, so C can only be
falsified if both of these clauses are in RC(S). Now, since RC(S) is closed under resolution,
it will contain the resolvent of these two clauses, and that resolvent will have all of its literals
already falsified by the assignments to P1 , . . . , Piâˆ’1 . This contradicts our assumption that
the first falsified clause appears at stage i. Hence, we have proved that the construction never
falsifies a clause in RC(S); that is, it produces a model of RC(S) and thus a model of S
itself (since S is contained in RC(S)).

7.5.3 Horn clauses and definite clauses

DEFINITE CLAUSE

HORN CLAUSE

GOAL CLAUSES

BODY
HEAD
FACT

The completeness of resolution makes it a very important inference method. In many practical
situations, however, the full power of resolution is not needed. Some real-world knowledge
bases satisfy certain restrictions on the form of sentences they contain, which enables them
to use a more restricted and efficient inference algorithm.
One such restricted form is the definite clause, which is a disjunction of literals of
which exactly one is positive. For example, the clause (Â¬L1,1 âˆ¨ Â¬Breeze âˆ¨ B1,1 ) is a definite
clause, whereas (Â¬B1,1 âˆ¨ P1,2 âˆ¨ P2,1 ) is not.
Slightly more general is the Horn clause, which is a disjunction of literals of which at
most one is positive. So all definite clauses are Horn clauses, as are clauses with no positive
literals; these are called goal clauses. Horn clauses are closed under resolution: if you resolve
two Horn clauses, you get back a Horn clause.
Knowledge bases containing only definite clauses are interesting for three reasons:
1. Every definite clause can be written as an implication whose premise is a conjunction
of positive literals and whose conclusion is a single positive literal. (See Exercise 7.13.)
For example, the definite clause (Â¬L1,1 âˆ¨ Â¬Breeze âˆ¨ B1,1 ) can be written as the implication (L1,1 âˆ§ Breeze) â‡’ B1,1 . In the implication form, the sentence is easier to
understand: it says that if the agent is in [1,1] and there is a breeze, then [1,1] is breezy.
In Horn form, the premise is called the body and the conclusion is called the head. A
sentence consisting of a single positive literal, such as L1,1 , is called a fact. It too can
be written in implication form as True â‡’ L1,1 , but it is simpler to write just L1,1 .

Section 7.5.

Propositional Theorem Proving

257

CNFSentence â†’ Clause 1 âˆ§ Â· Â· Â· âˆ§ Clause n
Clause â†’ Literal 1 âˆ¨ Â· Â· Â· âˆ¨ Literal m
Literal â†’ Symbol | Â¬Symbol
Symbol â†’ P | Q | R | . . .
HornClauseForm â†’ Deï¬niteClauseForm | GoalClauseForm
Deï¬niteClauseForm â†’ (Symbol 1 âˆ§ Â· Â· Â· âˆ§ Symbol l ) â‡’ Symbol
GoalClauseForm â†’ (Symbol 1 âˆ§ Â· Â· Â· âˆ§ Symbol l ) â‡’ False

Figure 7.14 A grammar for conjunctive normal form, Horn clauses, and definite clauses.
A clause such as A âˆ§ B â‡’ C is still a definite clause when it is written as Â¬A âˆ¨ Â¬B âˆ¨ C,
but only the former is considered the canonical form for definite clauses. One more class is
the k-CNF sentence, which is a CNF sentence where each clause has at most k literals.

FORWARD-CHAINING
BACKWARDCHAINING

2. Inference with Horn clauses can be done through the forward-chaining and backwardchaining algorithms, which we explain next. Both of these algorithms are natural,
in that the inference steps are obvious and easy for humans to follow. This type of
inference is the basis for logic programming, which is discussed in Chapter 9.
3. Deciding entailment with Horn clauses can be done in time that is linear in the size of
the knowledge baseâ€”a pleasant surprise.

7.5.4 Forward and backward chaining
The forward-chaining algorithm PL-FC-E NTAILS ?(KB, q) determines if a single proposition symbol qâ€”the queryâ€”is entailed by a knowledge base of definite clauses. It begins
from known facts (positive literals) in the knowledge base. If all the premises of an implication are known, then its conclusion is added to the set of known facts. For example, if L1,1
and Breeze are known and (L1,1 âˆ§ Breeze) â‡’ B1,1 is in the knowledge base, then B1,1 can
be added. This process continues until the query q is added or until no further inferences can
be made. The detailed algorithm is shown in Figure 7.15; the main point to remember is that
it runs in linear time.
The best way to understand the algorithm is through an example and a picture. Figure 7.16(a) shows a simple knowledge base of Horn clauses with A and B as known facts.
Figure 7.16(b) shows the same knowledge base drawn as an ANDâ€“OR graph (see Chapter 4). In ANDâ€“OR graphs, multiple links joined by an arc indicate a conjunctionâ€”every
link must be provedâ€”while multiple links without an arc indicate a disjunctionâ€”any link
can be proved. It is easy to see how forward chaining works in the graph. The known leaves
(here, A and B) are set, and inference propagates up the graph as far as possible. Wherever a conjunction appears, the propagation waits until all the conjuncts are known before
proceeding. The reader is encouraged to work through the example in detail.

258

Chapter

7.

Logical Agents

function PL-FC-E NTAILS ?(KB, q) returns true or false
inputs: KB , the knowledge base, a set of propositional definite clauses
q, the query, a proposition symbol
count â† a table, where count[c] is the number of symbols in câ€™s premise
inferred â† a table, where inferred [s] is initially false for all symbols
agenda â† a queue of symbols, initially symbols known to be true in KB
while agenda is not empty do
p â† P OP(agenda)
if p = q then return true
if inferred [p] = false then
inferred [p] â† true
for each clause c in KB where p is in c.P REMISE do
decrement count[c]
if count[c] = 0 then add c.C ONCLUSION to agenda
return false
Figure 7.15 The forward-chaining algorithm for propositional logic. The agenda keeps
track of symbols known to be true but not yet â€œprocessed.â€ The count table keeps track of
how many premises of each implication are as yet unknown. Whenever a new symbol p from
the agenda is processed, the count is reduced by one for each implication in whose premise
p appears (easily identified in constant time with appropriate indexing.) If a count reaches
zero, all the premises of the implication are known, so its conclusion can be added to the
agenda. Finally, we need to keep track of which symbols have been processed; a symbol that
is already in the set of inferred symbols need not be added to the agenda again. This avoids
redundant work and prevents loops caused by implications such as P â‡’ Q and Q â‡’ P .

FIXED POINT

DATA-DRIVEN

It is easy to see that forward chaining is sound: every inference is essentially an application of Modus Ponens. Forward chaining is also complete: every entailed atomic sentence
will be derived. The easiest way to see this is to consider the final state of the inferred table
(after the algorithm reaches a fixed point where no new inferences are possible). The table
contains true for each symbol inferred during the process, and false for all other symbols.
We can view the table as a logical model; moreover, every definite clause in the original KB is
true in this model. To see this, assume the opposite, namely that some clause a1 âˆ§. . .âˆ§ak â‡’ b
is false in the model. Then a1 âˆ§ . . . âˆ§ ak must be true in the model and b must be false in
the model. But this contradicts our assumption that the algorithm has reached a fixed point!
We can conclude, therefore, that the set of atomic sentences inferred at the fixed point defines
a model of the original KB. Furthermore, any atomic sentence q that is entailed by the KB
must be true in all its models and in this model in particular. Hence, every entailed atomic
sentence q must be inferred by the algorithm.
Forward chaining is an example of the general concept of data-driven reasoningâ€”that
is, reasoning in which the focus of attention starts with the known data. It can be used within
an agent to derive conclusions from incoming percepts, often without a specific query in
mind. For example, the wumpus agent might T ELL its percepts to the knowledge base using

Section 7.6.

Effective Propositional Model Checking

259

Q
P â‡’ Q
Lâˆ§M â‡’ P

P

Bâˆ§L â‡’ M

M

Aâˆ§P â‡’ L
Aâˆ§B â‡’ L
A

L

B
(a)
Figure 7.16

GOAL-DIRECTED
REASONING

7.6

A
(b)

B

(a) A set of Horn clauses. (b) The corresponding AND â€“ OR graph.

an incremental forward-chaining algorithm in which new facts can be added to the agenda to
initiate new inferences. In humans, a certain amount of data-driven reasoning occurs as new
information arrives. For example, if I am indoors and hear rain starting to fall, it might occur
to me that the picnic will be canceled. Yet it will probably not occur to me that the seventeenth
petal on the largest rose in my neighborâ€™s garden will get wet; humans keep forward chaining
under careful control, lest they be swamped with irrelevant consequences.
The backward-chaining algorithm, as its name suggests, works backward from the
query. If the query q is known to be true, then no work is needed. Otherwise, the algorithm
finds those implications in the knowledge base whose conclusion is q. If all the premises of
one of those implications can be proved true (by backward chaining), then q is true. When
applied to the query Q in Figure 7.16, it works back down the graph until it reaches a set of
known facts, A and B, that forms the basis for a proof. The algorithm is essentially identical
to the A ND -O R -G RAPH -S EARCH algorithm in Figure 4.11. As with forward chaining, an
efficient implementation runs in linear time.
Backward chaining is a form of goal-directed reasoning. It is useful for answering
specific questions such as â€œWhat shall I do now?â€ and â€œWhere are my keys?â€ Often, the cost
of backward chaining is much less than linear in the size of the knowledge base, because the
process touches only relevant facts.

E FFECTIVE P ROPOSITIONAL M ODEL C HECKING
In this section, we describe two families of efficient algorithms for general propositional
inference based on model checking: One approach based on backtracking search, and one
on local hill-climbing search. These algorithms are part of the â€œtechnologyâ€ of propositional
logic. This section can be skimmed on a first reading of the chapter.

260

Chapter

7.

Logical Agents

The algorithms we describe are for checking satisfiability: the SAT problem. (As noted
earlier, testing entailment, Î± |= Î², can be done by testing unsatisfiability of Î± âˆ§ Â¬Î².) We
have already noted the connection between finding a satisfying model for a logical sentence
and finding a solution for a constraint satisfaction problem, so it is perhaps not surprising that
the two families of algorithms closely resemble the backtracking algorithms of Section 6.3
and the local search algorithms of Section 6.4. They are, however, extremely important in
their own right because so many combinatorial problems in computer science can be reduced
to checking the satisfiability of a propositional sentence. Any improvement in satisfiability
algorithms has huge consequences for our ability to handle complexity in general.

7.6.1 A complete backtracking algorithm
DAVISâ€“PUTNAM
ALGORITHM

PURE SYMBOL

The first algorithm we consider is often called the Davisâ€“Putnam algorithm, after the seminal paper by Martin Davis and Hilary Putnam (1960). The algorithm is in fact the version
described by Davis, Logemann, and Loveland (1962), so we will call it DPLL after the initials of all four authors. DPLL takes as input a sentence in conjunctive normal formâ€”a set
of clauses. Like BACKTRACKING-S EARCH and TT-E NTAILS ?, it is essentially a recursive,
depth-first enumeration of possible models. It embodies three improvements over the simple
scheme of TT-E NTAILS ?:
â€¢ Early termination: The algorithm detects whether the sentence must be true or false,
even with a partially completed model. A clause is true if any literal is true, even if
the other literals do not yet have truth values; hence, the sentence as a whole could be
judged true even before the model is complete. For example, the sentence (A âˆ¨ B) âˆ§
(A âˆ¨ C) is true if A is true, regardless of the values of B and C. Similarly, a sentence
is false if any clause is false, which occurs when each of its literals is false. Again, this
can occur long before the model is complete. Early termination avoids examination of
entire subtrees in the search space.
â€¢ Pure symbol heuristic: A pure symbol is a symbol that always appears with the same
â€œsignâ€ in all clauses. For example, in the three clauses (A âˆ¨ Â¬B), (Â¬B âˆ¨ Â¬C), and
(C âˆ¨ A), the symbol A is pure because only the positive literal appears, B is pure
because only the negative literal appears, and C is impure. It is easy to see that if
a sentence has a model, then it has a model with the pure symbols assigned so as to
make their literals true, because doing so can never make a clause false. Note that, in
determining the purity of a symbol, the algorithm can ignore clauses that are already
known to be true in the model constructed so far. For example, if the model contains
B = false, then the clause (Â¬B âˆ¨ Â¬C) is already true, and in the remaining clauses C
appears only as a positive literal; therefore C becomes pure.
â€¢ Unit clause heuristic: A unit clause was defined earlier as a clause with just one literal. In the context of DPLL, it also means clauses in which all literals but one are
already assigned false by the model. For example, if the model contains B = true,
then (Â¬B âˆ¨ Â¬C) simplifies to Â¬C, which is a unit clause. Obviously, for this clause
to be true, C must be set to false. The unit clause heuristic assigns all such symbols
before branching on the remainder. One important consequence of the heuristic is that

Section 7.6.

Effective Propositional Model Checking

261

function DPLL-S ATISFIABLE ?(s) returns true or false
inputs: s, a sentence in propositional logic
clauses â† the set of clauses in the CNF representation of s
symbols â† a list of the proposition symbols in s
return DPLL(clauses, symbols, { })
function DPLL(clauses, symbols, model ) returns true or false
if every clause in clauses is true in model then return true
if some clause in clauses is false in model then return false
P , value â† F IND -P URE -S YMBOL (symbols, clauses, model )
if P is non-null then return DPLL(clauses, symbols â€“ P , model âˆª {P =value})
P , value â† F IND -U NIT-C LAUSE(clauses, model )
if P is non-null then return DPLL(clauses, symbols â€“ P , model âˆª {P =value})
P â† F IRST(symbols); rest â† R EST(symbols)
return DPLL(clauses, rest, model âˆª {P =true}) or
DPLL(clauses, rest, model âˆª {P =false}))
Figure 7.17 The DPLL algorithm for checking satisfiability of a sentence in propositional
logic. The ideas behind F IND -P URE -S YMBOL and F IND -U NIT-C LAUSE are described in
the text; each returns a symbol (or null) and the truth value to assign to that symbol. Like
TT-E NTAILS ?, DPLL operates over partial models.

UNIT PROPAGATION

any attempt to prove (by refutation) a literal that is already in the knowledge base will
succeed immediately (Exercise 7.23). Notice also that assigning one unit clause can
create another unit clauseâ€”for example, when C is set to false, (C âˆ¨ A) becomes a
unit clause, causing true to be assigned to A. This â€œcascadeâ€ of forced assignments
is called unit propagation. It resembles the process of forward chaining with definite
clauses, and indeed, if the CNF expression contains only definite clauses then DPLL
essentially replicates forward chaining. (See Exercise 7.24.)
The DPLL algorithm is shown in Figure 7.17, which gives the the essential skeleton of the
search process.
What Figure 7.17 does not show are the tricks that enable SAT solvers to scale up to
large problems. It is interesting that most of these tricks are in fact rather general, and we
have seen them before in other guises:
1. Component analysis (as seen with Tasmania in CSPs): As DPLL assigns truth values
to variables, the set of clauses may become separated into disjoint subsets, called components, that share no unassigned variables. Given an efficient way to detect when this
occurs, a solver can gain considerable speed by working on each component separately.
2. Variable and value ordering (as seen in Section 6.3.1 for CSPs): Our simple implementation of DPLL uses an arbitrary variable ordering and always tries the value true
before false. The degree heuristic (see page 216) suggests choosing the variable that
appears most frequently over all remaining clauses.

262

Chapter

7.

Logical Agents

3. Intelligent backtracking (as seen in Section 6.3 for CSPs): Many problems that cannot be solved in hours of run time with chronological backtracking can be solved in
seconds with intelligent backtracking that backs up all the way to the relevant point of
conflict. All SAT solvers that do intelligent backtracking use some form of conflict
clause learning to record conflicts so that they wonâ€™t be repeated later in the search.
Usually a limited-size set of conflicts is kept, and rarely used ones are dropped.
4. Random restarts (as seen on page 124 for hill-climbing): Sometimes a run appears not
to be making progress. In this case, we can start over from the top of the search tree,
rather than trying to continue. After restarting, different random choices (in variable
and value selection) are made. Clauses that are learned in the first run are retained after
the restart and can help prune the search space. Restarting does not guarantee that a
solution will be found faster, but it does reduce the variance on the time to solution.
5. Clever indexing (as seen in many algorithms): The speedup methods used in DPLL
itself, as well as the tricks used in modern solvers, require fast indexing of such things
as â€œthe set of clauses in which variable Xi appears as a positive literal.â€ This task is
complicated by the fact that the algorithms are interested only in the clauses that have
not yet been satisfied by previous assignments to variables, so the indexing structures
must be updated dynamically as the computation proceeds.
With these enhancements, modern solvers can handle problems with tens of millions of variables. They have revolutionized areas such as hardware verification and security protocol
verification, which previously required laborious, hand-guided proofs.

7.6.2 Local search algorithms
We have seen several local search algorithms so far in this book, including H ILL -C LIMBING
(page 122) and S IMULATED -A NNEALING (page 126). These algorithms can be applied directly to satisfiability problems, provided that we choose the right evaluation function. Because the goal is to find an assignment that satisfies every clause, an evaluation function that
counts the number of unsatisfied clauses will do the job. In fact, this is exactly the measure
used by the M IN -C ONFLICTS algorithm for CSPs (page 221). All these algorithms take steps
in the space of complete assignments, flipping the truth value of one symbol at a time. The
space usually contains many local minima, to escape from which various forms of randomness are required. In recent years, there has been a great deal of experimentation to find a
good balance between greediness and randomness.
One of the simplest and most effective algorithms to emerge from all this work is called
WALK SAT (Figure 7.18). On every iteration, the algorithm picks an unsatisfied clause and
picks a symbol in the clause to flip. It chooses randomly between two ways to pick which
symbol to flip: (1) a â€œmin-conflictsâ€ step that minimizes the number of unsatisfied clauses in
the new state and (2) a â€œrandom walkâ€ step that picks the symbol randomly.
When WALK SAT returns a model, the input sentence is indeed satisfiable, but when
it returns failure, there are two possible causes: either the sentence is unsatisfiable or we
need to give the algorithm more time. If we set max ï¬‚ips = âˆž and p > 0, WALK SAT will
eventually return a model (if one exists), because the random-walk steps will eventually hit

Section 7.6.

Effective Propositional Model Checking

263

function WALK SAT(clauses, p, max ï¬‚ips) returns a satisfying model or failure
inputs: clauses, a set of clauses in propositional logic
p, the probability of choosing to do a â€œrandom walkâ€ move, typically around 0.5
max ï¬‚ips, number of flips allowed before giving up
model â† a random assignment of true/false to the symbols in clauses
for i = 1 to max ï¬‚ips do
if model satisfies clauses then return model
clause â† a randomly selected clause from clauses that is false in model
with probability p flip the value in model of a randomly selected symbol from clause
else flip whichever symbol in clause maximizes the number of satisfied clauses
return failure
Figure 7.18 The WALK SAT algorithm for checking satisfiability by randomly flipping
the values of variables. Many versions of the algorithm exist.

upon the solution. Alas, if max ï¬‚ips is infinity and the sentence is unsatisfiable, then the
algorithm never terminates!
For this reason, WALK SAT is most useful when we expect a solution to existâ€”for example, the problems discussed in Chapters 3 and 6 usually have solutions. On the other hand,
WALK SAT cannot always detect unsatisfiability, which is required for deciding entailment.
For example, an agent cannot reliably use WALK SAT to prove that a square is safe in the
wumpus world. Instead, it can say, â€œI thought about it for an hour and couldnâ€™t come up with
a possible world in which the square isnâ€™t safe.â€ This may be a good empirical indicator that
the square is safe, but itâ€™s certainly not a proof.

7.6.3 The landscape of random SAT problems

UNDERCONSTRAINED

Some SAT problems are harder than others. Easy problems can be solved by any old algorithm, but because we know that SAT is NP-complete, at least some problem instances must
require exponential run time. In Chapter 6, we saw some surprising discoveries about certain
kinds of problems. For example, the n-queens problemâ€”thought to be quite tricky for backtracking search algorithmsâ€”turned out to be trivially easy for local search methods, such as
min-conflicts. This is because solutions are very densely distributed in the space of assignments, and any initial assignment is guaranteed to have a solution nearby. Thus, n-queens is
easy because it is underconstrained.
When we look at satisfiability problems in conjunctive normal form, an underconstrained problem is one with relatively few clauses constraining the variables. For example,
here is a randomly generated 3-CNF sentence with five symbols and five clauses:
(Â¬D âˆ¨ Â¬B âˆ¨ C) âˆ§ (B âˆ¨ Â¬A âˆ¨ Â¬C) âˆ§ (Â¬C âˆ¨ Â¬B âˆ¨ E)
âˆ§ (E âˆ¨ Â¬D âˆ¨ B) âˆ§ (B âˆ¨ E âˆ¨ Â¬C) .
Sixteen of the 32 possible assignments are models of this sentence, so, on average, it would
take just two random guesses to find a model. This is an easy satisfiability problem, as are

264

7.

Logical Agents

most such underconstrained problems. On the other hand, an overconstrained problem has
many clauses relative to the number of variables and is likely to have no solutions.
To go beyond these basic intuitions, we must define exactly how random sentences
are generated. The notation CN Fk (m, n) denotes a k-CNF sentence with m clauses and n
symbols, where the clauses are chosen uniformly, independently, and without replacement
from among all clauses with k different literals, which are positive or negative at random. (A
symbol may not appear twice in a clause, nor may a clause appear twice in a sentence.)
Given a source of random sentences, we can measure the probability of satisfiability.
Figure 7.19(a) plots the probability for CN F3 (m, 50), that is, sentences with 50 variables
and 3 literals per clause, as a function of the clause/symbol ratio, m/n. As we expect, for
small m/n the probability of satisfiability is close to 1, and at large m/n the probability
is close to 0. The probability drops fairly sharply around m/n = 4.3. Empirically, we find
that the â€œcliffâ€ stays in roughly the same place (for k = 3) and gets sharper and sharper as n
increases. Theoretically, the satisfiability threshold conjecture says that for every k â‰¥ 3,
there is a threshold ratio rk such that, as n goes to infinity, the probability that CN Fk (n, rn)
is satisfiable becomes 1 for all values of r below the threshold, and 0 for all values above.
The conjecture remains unproven.
1

Runtime

0.8
P(satisfiable)

SATISFIABILITY
THRESHOLD
CONJECTURE

Chapter

0.6
0.4
0.2
0
0

1

2
3
4
5
6
Clause/symbol ratio m/n

(a)

7

8

2000
1800
1600
1400
1200
1000
800
600
400
200
0

DPLL
WalkSAT

0

1

2
3
4
5
6
Clause/symbol ratio m/n

7

8

(b)

Figure 7.19 (a) Graph showing the probability that a random 3-CNF sentence with n = 50
symbols is satisfiable, as a function of the clause/symbol ratio m/n. (b) Graph of the median
run time (measured in number of recursive calls to DPLL, a good proxy) on random 3-CNF
sentences. The most difficult problems have a clause/symbol ratio of about 4.3.

Now that we have a good idea where the satisfiable and unsatisfiable problems are, the
next question is, where are the hard problems? It turns out that they are also often at the
threshold value. Figure 7.19(b) shows that 50-symbol problems at the threshold value of 4.3
are about 20 times more difficult to solve than those at a ratio of 3.3. The underconstrained
problems are easiest to solve (because it is so easy to guess a solution); the overconstrained
problems are not as easy as the underconstrained, but still are much easier than the ones right
at the threshold.

Section 7.7.

7.7

Agents Based on Propositional Logic

265

AGENTS BASED ON P ROPOSITIONAL L OGIC
In this section, we bring together what we have learned so far in order to construct wumpus
world agents that use propositional logic. The first step is to enable the agent to deduce, to the
extent possible, the state of the world given its percept history. This requires writing down a
complete logical model of the effects of actions. We also show how the agent can keep track of
the world efficiently without going back into the percept history for each inference. Finally,
we show how the agent can use logical inference to construct plans that are guaranteed to
achieve its goals.

7.7.1 The current state of the world
As stated at the beginning of the chapter, a logical agent operates by deducing what to do
from a knowledge base of sentences about the world. The knowledge base is composed of
axiomsâ€”general knowledge about how the world worksâ€”and percept sentences obtained
from the agentâ€™s experience in a particular world. In this section, we focus on the problem of
deducing the current state of the wumpus worldâ€”where am I, is that square safe, and so on.
We began collecting axioms in Section 7.4.3. The agent knows that the starting square
contains no pit (Â¬P1,1 ) and no wumpus (Â¬W1,1 ). Furthermore, for each square, it knows that
the square is breezy if and only if a neighboring square has a pit; and a square is smelly if and
only if a neighboring square has a wumpus. Thus, we include a large collection of sentences
of the following form:
B1,1 â‡” (P1,2 âˆ¨ P2,1 )
S1,1 â‡” (W1,2 âˆ¨ W2,1 )
Â·Â·Â·
The agent also knows that there is exactly one wumpus. This is expressed in two parts. First,
we have to say that there is at least one wumpus:
W1,1 âˆ¨ W1,2 âˆ¨ Â· Â· Â· âˆ¨ W4,3 âˆ¨ W4,4 .
Then, we have to say that there is at most one wumpus. For each pair of locations, we add a
sentence saying that at least one of them must be wumpus-free:
Â¬W1,1 âˆ¨ Â¬W1,2
Â¬W1,1 âˆ¨ Â¬W1,3
Â·Â·Â·
Â¬W4,3 âˆ¨ Â¬W4,4 .
So far, so good. Now letâ€™s consider the agentâ€™s percepts. If there is currently a stench, one
might suppose that a proposition Stench should be added to the knowledge base. This is not
quite right, however: if there was no stench at the previous time step, then Â¬Stench would already be asserted, and the new assertion would simply result in a contradiction. The problem
is solved when we realize that a percept asserts something only about the current time. Thus,
if the time step (as supplied to M AKE-P ERCEPT-S ENTENCE in Figure 7.1) is 4, then we add

266

FLUENT

ATEMPORAL
VARIABLE

EFFECT AXIOM

Chapter

Logical Agents

Stench 4 to the knowledge base, rather than Stenchâ€”neatly avoiding any contradiction with
Â¬Stench 3 . The same goes for the breeze, bump, glitter, and scream percepts.
The idea of associating propositions with time steps extends to any aspect of the world
that changes over time. For example, the initial knowledge base includes L01,1 â€”the agent is in
square [1, 1] at time 0â€”as well as FacingEast 0 , HaveArrow 0 , and WumpusAlive 0 . We use
the word fluent (from the Latin fluens, flowing) to refer an aspect of the world that changes.
â€œFluentâ€ is a synonym for â€œstate variable,â€ in the sense described in the discussion of factored
representations in Section 2.4.7 on page 57. Symbols associated with permanent aspects of
the world do not need a time superscript and are sometimes called atemporal variables.
We can connect stench and breeze percepts directly to the properties of the squares
where they are experienced through the location fluent as follows.10 For any time step t and
any square [x, y], we assert
Ltx,y â‡’ (Breezet â‡” Bx,y )
Ltx,y â‡’ (Stencht â‡” Sx,y ) .
Now, of course, we need axioms that allow the agent to keep track of fluents such as Ltx,y .
These fluents change as the result of actions taken by the agent, so, in the terminology of
Chapter 3, we need to write down the transition model of the wumpus world as a set of
logical sentences.
First, we need proposition symbols for the occurrences of actions. As with percepts,
these symbols are indexed by time; thus, Forward 0 means that the agent executes the Forward
action at time 0. By convention, the percept for a given time step happens first, followed by
the action for that time step, followed by a transition to the next time step.
To describe how the world changes, we can try writing effect axioms that specify the
outcome of an action at the next time step. For example, if the agent is at location [1, 1] facing
east at time 0 and goes Forward , the result is that the agent is in square [2, 1] and no longer
is in [1, 1]:
L01,1 âˆ§ FacingEast 0 âˆ§ Forward 0 â‡’ (L12,1 âˆ§ Â¬L11,1 ) .

FRAME PROBLEM

7.

(7.1)

We would need one such sentence for each possible time step, for each of the 16 squares,
and each of the four orientations. We would also need similar sentences for the other actions:
Grab, Shoot, Climb, TurnLeft, and TurnRight.
Let us suppose that the agent does decide to move Forward at time 0 and asserts this
fact into its knowledge base. Given the effect axiom in Equation (7.1), combined with the
initial assertions about the state at time 0, the agent can now deduce that it is in [2, 1]. That
is, A SK (KB , L12,1 ) = true. So far, so good. Unfortunately, the news elsewhere is less good:
if we A SK (KB , HaveArrow 1 ), the answer is false, that is, the agent cannot prove it still
has the arrow; nor can it prove it doesnâ€™t have it! The information has been lost because the
effect axiom fails to state what remains unchanged as the result of an action. The need to do
this gives rise to the frame problem.11 One possible solution to the frame problem would
10

Section 7.4.3 conveniently glossed over this requirement.
The name â€œframe problemâ€ comes from â€œframe of referenceâ€ in physicsâ€”the assumed stationary background
with respect to which motion is measured. It also has an analogy to the frames of a movie, in which normally
most of the background stays constant while changes occur in the foreground.
11

Section 7.7.
FRAME AXIOM

Agents Based on Propositional Logic

267

be to add frame axioms explicitly asserting all the propositions that remain the same. For
example, for each time t we would have
Forward t â‡’ (HaveArrow t â‡” HaveArrow t+1 )
Forward t â‡’ (WumpusAlive t â‡” WumpusAlive t+1 )
Â·Â·Â·

REPRESENTATIONAL
FRAME PROBLEM

LOCALITY

INFERENTIAL FRAME
PROBLEM

SUCCESSOR-STATE
AXIOM

where we explicitly mention every proposition that stays unchanged from time t to time
t + 1 under the action Forward . Although the agent now knows that it still has the arrow
after moving forward and that the wumpus hasnâ€™t died or come back to life, the proliferation
of frame axioms seems remarkably inefficient. In a world with m different actions and n
fluents, the set of frame axioms will be of size O(mn). This specific manifestation of the
frame problem is sometimes called the representational frame problem. Historically, the
problem was a significant one for AI researchers; we explore it further in the notes at the end
of the chapter.
The representational frame problem is significant because the real world has very many
fluents, to put it mildly. Fortunately for us humans, each action typically changes no more
than some small number k of those fluentsâ€”the world exhibits locality. Solving the representational frame problem requires defining the transition model with a set of axioms of size
O(mk) rather than size O(mn). There is also an inferential frame problem: the problem
of projecting forward the results of a t step plan of action in time O(kt) rather than O(nt).
The solution to the problem involves changing oneâ€™s focus from writing axioms about
actions to writing axioms about fluents. Thus, for each fluent F , we will have an axiom that
defines the truth value of F t+1 in terms of fluents (including F itself) at time t and the actions
that may have occurred at time t. Now, the truth value of F t+1 can be set in one of two ways:
either the action at time t causes F to be true at t + 1, or F was already true at time t and the
action at time t does not cause it to be false. An axiom of this form is called a successor-state
axiom and has this schema:
F t+1 â‡” ActionCausesF t âˆ¨ (F t âˆ§ Â¬ActionCausesNotF t ) .
One of the simplest successor-state axioms is the one for HaveArrow . Because there is no
action for reloading, the ActionCausesF t part goes away and we are left with
HaveArrow t+1 â‡” (HaveArrow t âˆ§ Â¬Shoot t ) .

(7.2)

For the agentâ€™s location, the successor-state axioms are more elaborate. For example, Lt+1
1,1
is true if either (a) the agent moved Forward from [1, 2] when facing south, or from [2, 1]
when facing west; or (b) Lt1,1 was already true and the action did not cause movement (either
because the action was not Forward or because the action bumped into a wall). Written out
in propositional logic, this becomes
Lt+1
1,1

â‡”

(Lt1,1 âˆ§ (Â¬Forward t âˆ¨ Bump t+1 ))

âˆ¨ (Lt1,2 âˆ§ (South t âˆ§ Forward t ))
âˆ¨

(Lt2,1

âˆ§ (West âˆ§ Forward )) .
t

t

Exercise 7.26 asks you to write out axioms for the remaining wumpus world fluents.

(7.3)

268

Chapter

7.

Logical Agents

Given a complete set of successor-state axioms and the other axioms listed at the beginning of this section, the agent will be able to A SK and answer any answerable question about
the current state of the world. For example, in Section 7.2 the initial sequence of percepts and
actions is
Â¬Stench 0 âˆ§ Â¬Breeze 0 âˆ§ Â¬Glitter 0 âˆ§ Â¬Bump 0 âˆ§ Â¬Scream 0 ; Forward 0
Â¬Stench 1 âˆ§ Breeze 1 âˆ§ Â¬Glitter 1 âˆ§ Â¬Bump 1 âˆ§ Â¬Scream 1 ; TurnRight 1
Â¬Stench 2 âˆ§ Breeze 2 âˆ§ Â¬Glitter 2 âˆ§ Â¬Bump 2 âˆ§ Â¬Scream 2 ; TurnRight 2
Â¬Stench 3 âˆ§ Breeze 3 âˆ§ Â¬Glitter 3 âˆ§ Â¬Bump 3 âˆ§ Â¬Scream 3 ; Forward 3
Â¬Stench 4 âˆ§ Â¬Breeze 4 âˆ§ Â¬Glitter 4 âˆ§ Â¬Bump 4 âˆ§ Â¬Scream 4 ; TurnRight 4
Â¬Stench 5 âˆ§ Â¬Breeze 5 âˆ§ Â¬Glitter 5 âˆ§ Â¬Bump 5 âˆ§ Â¬Scream 5 ; Forward 5
Stench 6 âˆ§ Â¬Breeze 6 âˆ§ Â¬Glitter 6 âˆ§ Â¬Bump 6 âˆ§ Â¬Scream 6
At this point, we have A SK (KB, L61,2 ) = true, so the agent knows where it is. Moreover,
A SK (KB , W1,3 ) = true and A SK (KB, P3,1 ) = true, so the agent has found the wumpus and
one of the pits. The most important question for the agent is whether a square is OK to move
into, that is, the square contains no pit nor live wumpus. Itâ€™s convenient to add axioms for
this, having the form
OK tx,y â‡” Â¬Px,y âˆ§ Â¬(Wx,y âˆ§ WumpusAlive t ) .

QUALIFICATION
PROBLEM

Finally, A SK (KB , OK 62,2 ) = true, so the square [2, 2] is OK to move into. In fact, given a
sound and complete inference algorithm such as DPLL, the agent can answer any answerable
question about which squares are OKâ€”and can do so in just a few milliseconds for small-tomedium wumpus worlds.
Solving the representational and inferential frame problems is a big step forward, but
a pernicious problem remains: we need to confirm that all the necessary preconditions of an
action hold for it to have its intended effect. We said that the Forward action moves the agent
ahead unless there is a wall in the way, but there are many other unusual exceptions that could
cause the action to fail: the agent might trip and fall, be stricken with a heart attack, be carried
away by giant bats, etc. Specifying all these exceptions is called the qualification problem.
There is no complete solution within logic; system designers have to use good judgment in
deciding how detailed they want to be in specifying their model, and what details they want
to leave out. We will see in Chapter 13 that probability theory allows us to summarize all the
exceptions without explicitly naming them.

7.7.2 A hybrid agent

HYBRID AGENT

The ability to deduce various aspects of the state of the world can be combined fairly straightforwardly with conditionâ€“action rules and with problem-solving algorithms from Chapters 3
and 4 to produce a hybrid agent for the wumpus world. Figure 7.20 shows one possible way
to do this. The agent program maintains and updates a knowledge base as well as a current
plan. The initial knowledge base contains the atemporal axiomsâ€”those that donâ€™t depend
on t, such as the axiom relating the breeziness of squares to the presence of pits. At each
time step, the new percept sentence is added along with all the axioms that depend on t, such

Section 7.7.

Agents Based on Propositional Logic

269

as the successor-state axioms. (The next section explains why the agent doesnâ€™t need axioms
for future time steps.) Then, the agent uses logical inference, by A SK ing questions of the
knowledge base, to work out which squares are safe and which have yet to be visited.
The main body of the agent program constructs a plan based on a decreasing priority of
goals. First, if there is a glitter, the program constructs a plan to grab the gold, follow a route
back to the initial location, and climb out of the cave. Otherwise, if there is no current plan,
the program plans a route to the closest safe square that it has not visited yet, making sure
the route goes through only safe squares. Route planning is done with Aâˆ— search, not with
A SK . If there are no safe squares to explore, the next stepâ€”if the agent still has an arrowâ€”is
to try to make a safe square by shooting at one of the possible wumpus locations. These are
determined by asking where A SK (KB, Â¬Wx,y ) is falseâ€”that is, where it is not known that
there is not a wumpus. The function P LAN -S HOT (not shown) uses P LAN -ROUTE to plan a
sequence of actions that will line up this shot. If this fails, the program looks for a square to
explore that is not provably unsafeâ€”that is, a square for which A SK (KB, Â¬OK tx,y ) returns
false. If there is no such square, then the mission is impossible and the agent retreats to [1, 1]
and climbs out of the cave.

7.7.3 Logical state estimation

CACHING

The agent program in Figure 7.20 works quite well, but it has one major weakness: as time
goes by, the computational expense involved in the calls to A SK goes up and up. This happens
mainly because the required inferences have to go back further and further in time and involve
more and more proposition symbols. Obviously, this is unsustainableâ€”we cannot have an
agent whose time to process each percept grows in proportion to the length of its life! What
we really need is a constant update timeâ€”that is, independent of t. The obvious answer is to
save, or cache, the results of inference, so that the inference process at the next time step can
build on the results of earlier steps instead of having to start again from scratch.
As we saw in Section 4.4, the past history of percepts and all their ramifications can
be replaced by the belief stateâ€”that is, some representation of the set of all possible current
states of the world.12 The process of updating the belief state as new percepts arrive is called
state estimation. Whereas in Section 4.4 the belief state was an explicit list of states, here
we can use a logical sentence involving the proposition symbols associated with the current
time step, as well as the atemporal symbols. For example, the logical sentence
WumpusAlive 1 âˆ§ L12,1 âˆ§ B2,1 âˆ§ (P3,1 âˆ¨ P2,2 )

(7.4)

represents the set of all states at time 1 in which the wumpus is alive, the agent is at [2, 1],
that square is breezy, and there is a pit in [3, 1] or [2, 2] or both.
Maintaining an exact belief state as a logical formula turns out not to be easy. If there
are n fluent symbols for time t, then there are 2n possible statesâ€”that is, assignments of truth
values to those symbols. Now, the set of belief states is the powerset (set of all subsets) of the
n
set of physical states. There are 2n physical states, hence 22 belief states. Even if we used
the most compact possible encoding of logical formulas, with each belief state represented
12

We can think of the percept history itself as a representation of the belief state, but one that makes inference
increasingly expensive as the history gets longer.

270

Chapter

7.

Logical Agents

function H YBRID -W UMPUS -AGENT ( percept ) returns an action
inputs: percept , a list, [stench,breeze,glitter ,bump,scream]
persistent: KB , a knowledge base, initially the atemporal â€œwumpus physicsâ€
t , a counter, initially 0, indicating time
plan, an action sequence, initially empty
T ELL(KB, M AKE -P ERCEPT-S ENTENCE( percept , t ))
T ELL the KB the temporal â€œphysicsâ€ sentences for time t
safe â† {[x , y] : A SK (KB , OK tx,y ) = true}
if A SK(KB , Glitter t ) = true then
plan â† [Grab] + P LAN -ROUTE(current, {[1,1]}, safe) + [Climb]
if plan is empty then

unvisited â† {[x , y] : A SK(KB , Ltx,y ) = false for all t â‰¤ t}
plan â† P LAN -ROUTE(current, unvisited âˆ© safe, safe)
if plan is empty and A SK (KB , HaveArrow t ) = true then
possible wumpus â† {[x , y] : A SK(KB , Â¬ Wx,y ) = false}
plan â† P LAN -S HOT(current, possible wumpus, safe)
if plan is empty then // no choice but to take a risk
not unsafe â† {[x , y] : A SK(KB , Â¬ OK tx,y ) = false}
plan â† P LAN -ROUTE(current, unvisited âˆ© not unsafe, safe)
if plan is empty then
plan â† P LAN -ROUTE(current, {[1, 1]}, safe) + [Climb]
action â† P OP(plan)
T ELL(KB, M AKE -ACTION -S ENTENCE(action, t ))
t â†t + 1
return action
function P LAN -ROUTE(current,goals,allowed ) returns an action sequence
inputs: current, the agentâ€™s current position
goals, a set of squares; try to plan a route to one of them
allowed , a set of squares that can form part of the route
problem â† ROUTE -P ROBLEM(current, goals,allowed )
return A*-G RAPH -S EARCH(problem)
Figure 7.20 A hybrid agent program for the wumpus world. It uses a propositional knowledge base to infer the state of the world, and a combination of problem-solving search and
domain-specific code to decide what actions to take.
n

by a unique binary number, we would need numbers with log2 (22 ) = 2n bits to label the
current belief state. That is, exact state estimation may require logical formulas whose size is
exponential in the number of symbols.
One very common and natural scheme for approximate state estimation is to represent
belief states as conjunctions of literals, that is, 1-CNF formulas. To do this, the agent program
simply tries to prove X t and Â¬X t for each symbol X t (as well as each atemporal symbol
whose truth value is not yet known), given the belief state at t âˆ’ 1. The conjunction of

Section 7.7.

Agents Based on Propositional Logic

271

Figure 7.21 Depiction of a 1-CNF belief state (bold outline) as a simply representable,
conservative approximation to the exact (wiggly) belief state (shaded region with dashed
outline). Each possible world is shown as a circle; the shaded ones are consistent with all the
percepts.

CONSERVATIVE
APPROXIMATION

provable literals becomes the new belief state, and the previous belief state is discarded.
It is important to understand that this scheme may lose some information as time goes
along. For example, if the sentence in Equation (7.4) were the true belief state, then neither
P3,1 nor P2,2 would be provable individually and neither would appear in the 1-CNF belief
state. (Exercise 7.27 explores one possible solution to this problem.) On the other hand,
because every literal in the 1-CNF belief state is proved from the previous belief state, and
the initial belief state is a true assertion, we know that entire 1-CNF belief state must be
true. Thus, the set of possible states represented by the 1-CNF belief state includes all states
that are in fact possible given the full percept history. As illustrated in Figure 7.21, the 1CNF belief state acts as a simple outer envelope, or conservative approximation, around the
exact belief state. We see this idea of conservative approximations to complicated sets as a
recurring theme in many areas of AI.

7.7.4 Making plans by propositional inference
The agent in Figure 7.20 uses logical inference to determine which squares are safe, but uses
Aâˆ— search to make plans. In this section, we show how to make plans by logical inference.
The basic idea is very simple:
1. Construct a sentence that includes
(a) Init 0 , a collection of assertions about the initial state;
(b) Transition 1 , . . . , Transition t , the successor-state axioms for all possible actions
at each time up to some maximum time t;
(c) the assertion that the goal is achieved at time t: HaveGold t âˆ§ ClimbedOut t .

272

Chapter

7.

Logical Agents

2. Present the whole sentence to a SAT solver. If the solver finds a satisfying model, then
the goal is achievable; if the sentence is unsatisfiable, then the planning problem is
impossible.
3. Assuming a model is found, extract from the model those variables that represent actions and are assigned true. Together they represent a plan to achieve the goals.
A propositional planning procedure, SATP LAN , is shown in Figure 7.22. It implements the
basic idea just given, with one twist. Because the agent does not know how many steps it
will take to reach the goal, the algorithm tries each possible number of steps t, up to some
maximum conceivable plan length Tmax . In this way, it is guaranteed to find the shortest plan
if one exists. Because of the way SATP LAN searches for a solution, this approach cannot
be used in a partially observable environment; SATP LAN would just set the unobservable
variables to the values it needs to create a solution.
function SAT PLAN( init , transition, goal , T max ) returns solution or failure
inputs: init , transition, goal , constitute a description of the problem
T max , an upper limit for plan length
for t = 0 to T max do
cnf â† T RANSLATE -T O -SAT( init , transition, goal , t )
model â† SAT-S OLVER (cnf )
if model is not null then
return E XTRACT-S OLUTION(model )
return failure
Figure 7.22 The SATP LAN algorithm. The planning problem is translated into a CNF
sentence in which the goal is asserted to hold at a fixed time step t and axioms are included
for each time step up to t. If the satisfiability algorithm finds a model, then a plan is extracted
by looking at those proposition symbols that refer to actions and are assigned true in the
model. If no model exists, then the process is repeated with the goal moved one step later.

The key step in using SATP LAN is the construction of the knowledge base. It might
seem, on casual inspection, that the wumpus world axioms in Section 7.7.1 suffice for steps
1(a) and 1(b) above. There is, however, a significant difference between the requirements for
entailment (as tested by A SK ) and those for satisfiability. Consider, for example, the agentâ€™s
location, initially [1, 1], and suppose the agentâ€™s unambitious goal is to be in [2, 1] at time 1.
The initial knowledge base contains L01,1 and the goal is L12,1 . Using A SK , we can prove L12,1
if Forward0 is asserted, and, reassuringly, we cannot prove L12,1 if, say, Shoot0 is asserted
instead. Now, SATP LAN will find the plan [Forward0 ]; so far, so good. Unfortunately,
SATP LAN also finds the plan [Shoot0 ]. How could this be? To find out, we inspect the model
that SATP LAN constructs: it includes the assignment L02,1 , that is, the agent can be in [2, 1]
at time 1 by being there at time 0 and shooting. One might ask, â€œDidnâ€™t we say the agent is in
[1, 1] at time 0?â€ Yes, we did, but we didnâ€™t tell the agent that it canâ€™t be in two places at once!
For entailment, L02,1 is unknown and cannot, therefore, be used in a proof; for satisfiability,

Section 7.7.

PRECONDITION
AXIOMS

Agents Based on Propositional Logic

273

on the other hand, L02,1 is unknown and can, therefore, be set to whatever value helps to
make the goal true. For this reason, SATP LAN is a good debugging tool for knowledge bases
because it reveals places where knowledge is missing. In this particular case, we can fix the
knowledge base by asserting that, at each time step, the agent is in exactly one location, using
a collection of sentences similar to those used to assert the existence of exactly one wumpus.
Alternatively, we can assert Â¬L0x,y for all locations other than [1, 1]; the successor-state axiom
for location takes care of subsequent time steps. The same fixes also work to make sure the
agent has only one orientation.
SATP LAN has more surprises in store, however. The first is that it finds models with
impossible actions, such as shooting with no arrow. To understand why, we need to look more
carefully at what the successor-state axioms (such as Equation (7.3)) say about actions whose
preconditions are not satisfied. The axioms do predict correctly that nothing will happen when
such an action is executed (see Exercise 10.14), but they do not say that the action cannot be
executed! To avoid generating plans with illegal actions, we must add precondition axioms
stating that an action occurrence requires the preconditions to be satisfied.13 For example, we
need to say, for each time t, that
Shoot t â‡’ HaveArrow t .

ACTION EXCLUSION
AXIOM

This ensures that if a plan selects the Shoot action at any time, it must be the case that the
agent has an arrow at that time.
SATP LAN â€™s second surprise is the creation of plans with multiple simultaneous actions.
For example, it may come up with a model in which both Forward 0 and Shoot 0 are true,
which is not allowed. To eliminate this problem, we introduce action exclusion axioms: for
every pair of actions Ati and Atj we add the axiom
Â¬Ati âˆ¨ Â¬Atj .
It might be pointed out that walking forward and shooting at the same time is not so hard to
do, whereas, say, shooting and grabbing at the same time is rather impractical. By imposing
action exclusion axioms only on pairs of actions that really do interfere with each other, we
can allow for plans that include multiple simultaneous actionsâ€”and because SATP LAN finds
the shortest legal plan, we can be sure that it will take advantage of this capability.
To summarize, SATP LAN finds models for a sentence containing the initial state, the
goal, the successor-state axioms, the precondition axioms, and the action exclusion axioms.
It can be shown that this collection of axioms is sufficient, in the sense that there are no
longer any spurious â€œsolutions.â€ Any model satisfying the propositional sentence will be a
valid plan for the original problem. Modern SAT-solving technology makes the approach
quite practical. For example, a DPLL-style solver has no difficulty in generating the 11-step
solution for the wumpus world instance shown in Figure 7.2.
This section has described a declarative approach to agent construction: the agent works
by a combination of asserting sentences in the knowledge base and performing logical inference. This approach has some weaknesses hidden in phrases such as â€œfor each time tâ€ and
13

Notice that the addition of precondition axioms means that we need not include preconditions for actions in
the successor-state axioms.

274

Chapter

7.

Logical Agents

â€œfor each square [x, y].â€ For any practical agent, these phrases have to be implemented by
code that generates instances of the general sentence schema automatically for insertion into
the knowledge base. For a wumpus world of reasonable sizeâ€”one comparable to a smallish
computer gameâ€”we might need a 100 Ã— 100 board and 1000 time steps, leading to knowledge bases with tens or hundreds of millions of sentences. Not only does this become rather
impractical, but it also illustrates a deeper problem: we know something about the wumpus worldâ€”namely, that the â€œphysicsâ€ works the same way across all squares and all time
stepsâ€”that we cannot express directly in the language of propositional logic. To solve this
problem, we need a more expressive language, one in which phrases like â€œfor each time tâ€
and â€œfor each square [x, y]â€ can be written in a natural way. First-order logic, described in
Chapter 8, is such a language; in first-order logic a wumpus world of any size and duration
can be described in about ten sentences rather than ten million or ten trillion.

7.8

S UMMARY
We have introduced knowledge-based agents and have shown how to define a logic with
which such agents can reason about the world. The main points are as follows:
â€¢ Intelligent agents need knowledge about the world in order to reach good decisions.
â€¢ Knowledge is contained in agents in the form of sentences in a knowledge representation language that are stored in a knowledge base.
â€¢ A knowledge-based agent is composed of a knowledge base and an inference mechanism. It operates by storing sentences about the world in its knowledge base, using the
inference mechanism to infer new sentences, and using these sentences to decide what
action to take.
â€¢ A representation language is defined by its syntax, which specifies the structure of
sentences, and its semantics, which defines the truth of each sentence in each possible
world or model.
â€¢ The relationship of entailment between sentences is crucial to our understanding of
reasoning. A sentence Î± entails another sentence Î² if Î² is true in all worlds where
Î± is true. Equivalent definitions include the validity of the sentence Î± â‡’ Î² and the
unsatisfiability of the sentence Î± âˆ§ Â¬Î².
â€¢ Inference is the process of deriving new sentences from old ones. Sound inference algorithms derive only sentences that are entailed; complete algorithms derive all sentences
that are entailed.
â€¢ Propositional logic is a simple language consisting of proposition symbols and logical
connectives. It can handle propositions that are known true, known false, or completely
unknown.
â€¢ The set of possible models, given a fixed propositional vocabulary, is finite, so entailment can be checked by enumerating models. Efficient model-checking inference
algorithms for propositional logic include backtracking and local search methods and
can often solve large problems quickly.

Bibliographical and Historical Notes

275

â€¢ Inference rules are patterns of sound inference that can be used to find proofs. The
resolution rule yields a complete inference algorithm for knowledge bases that are
expressed in conjunctive normal form. Forward chaining and backward chaining
are very natural reasoning algorithms for knowledge bases in Horn form.
â€¢ Local search methods such as WALK SAT can be used to find solutions. Such algorithms are sound but not complete.
â€¢ Logical state estimation involves maintaining a logical sentence that describes the set
of possible states consistent with the observation history. Each update step requires
inference using the transition model of the environment, which is built from successorstate axioms that specify how each fluent changes.
â€¢ Decisions within a logical agent can be made by SAT solving: finding possible models
specifying future action sequences that reach the goal. This approach works only for
fully observable or sensorless environments.
â€¢ Propositional logic does not scale to environments of unbounded size because it lacks
the expressive power to deal concisely with time, space, and universal patterns of relationships among objects.

B IBLIOGRAPHICAL

SYLLOGISM

AND

H ISTORICAL N OTES

John McCarthyâ€™s paper â€œPrograms with Common Senseâ€ (McCarthy, 1958, 1968) promulgated the notion of agents that use logical reasoning to mediate between percepts and actions.
It also raised the flag of declarativism, pointing out that telling an agent what it needs to know
is an elegant way to build software. Allen Newellâ€™s (1982) article â€œThe Knowledge Levelâ€
makes the case that rational agents can be described and analyzed at an abstract level defined
by the knowledge they possess rather than the programs they run. The declarative and procedural approaches to AI are analyzed in depth by Boden (1977). The debate was revived by,
among others, Brooks (1991) and Nilsson (1991), and continues to this day (Shaparau et al.,
2008). Meanwhile, the declarative approach has spread into other areas of computer science
such as networking (Loo et al., 2006).
Logic itself had its origins in ancient Greek philosophy and mathematics. Various logical principlesâ€”principles connecting the syntactic structure of sentences with their truth
and falsity, with their meaning, or with the validity of arguments in which they figureâ€”are
scattered in the works of Plato. The first known systematic study of logic was carried out
by Aristotle, whose work was assembled by his students after his death in 322 B . C . as a
treatise called the Organon. Aristotleâ€™s syllogisms were what we would now call inference
rules. Although the syllogisms included elements of both propositional and first-order logic,
the system as a whole lacked the compositional properties required to handle sentences of
arbitrary complexity.
The closely related Megarian and Stoic schools (originating in the fifth century B . C .
and continuing for several centuries thereafter) began the systematic study of the basic logical
connectives. The use of truth tables for defining connectives is due to Philo of Megara. The

276

Chapter

7.

Logical Agents

Stoics took five basic inference rules as valid without proof, including the rule we now call
Modus Ponens. They derived a number of other rules from these five, using, among other
principles, the deduction theorem (page 249) and were much clearer about the notion of
proof than was Aristotle. A good account of the history of Megarian and Stoic logic is given
by Benson Mates (1953).
The idea of reducing logical inference to a purely mechanical process applied to a formal language is due to Wilhelm Leibniz (1646â€“1716), although he had limited success in implementing the ideas. George Boole (1847) introduced the first comprehensive and workable
system of formal logic in his book The Mathematical Analysis of Logic. Booleâ€™s logic was
closely modeled on the ordinary algebra of real numbers and used substitution of logically
equivalent expressions as its primary inference method. Although Booleâ€™s system still fell
short of full propositional logic, it was close enough that other mathematicians could quickly
fill in the gaps. SchroÌˆder (1877) described conjunctive normal form, while Horn form was
introduced much later by Alfred Horn (1951). The first comprehensive exposition of modern
propositional logic (and first-order logic) is found in Gottlob Fregeâ€™s (1879) Begriffschrift
(â€œConcept Writingâ€ or â€œConceptual Notationâ€).
The first mechanical device to carry out logical inferences was constructed by the third
Earl of Stanhope (1753â€“1816). The Stanhope Demonstrator could handle syllogisms and
certain inferences in the theory of probability. William Stanley Jevons, one of those who
improved upon and extended Booleâ€™s work, constructed his â€œlogical pianoâ€ in 1869 to perform inferences in Boolean logic. An entertaining and instructive history of these and other
early mechanical devices for reasoning is given by Martin Gardner (1968). The first published computer program for logical inference was the Logic Theorist of Newell, Shaw,
and Simon (1957). This program was intended to model human thought processes. Martin Davis (1957) had actually designed a program that came up with a proof in 1954, but the
Logic Theoristâ€™s results were published slightly earlier.
Truth tables as a method of testing validity or unsatisfiability in propositional logic were
introduced independently by Emil Post (1921) and Ludwig Wittgenstein (1922). In the 1930s,
a great deal of progress was made on inference methods for first-order logic. In particular,
GoÌˆdel (1930) showed that a complete procedure for inference in first-order logic could be
obtained via a reduction to propositional logic, using Herbrandâ€™s theorem (Herbrand, 1930).
We take up this history again in Chapter 9; the important point here is that the development
of efficient propositional algorithms in the 1960s was motivated largely by the interest of
mathematicians in an effective theorem prover for first-order logic. The Davisâ€“Putnam algorithm (Davis and Putnam, 1960) was the first effective algorithm for propositional resolution
but was in most cases much less efficient than the DPLL backtracking algorithm introduced
two years later (1962). The full resolution rule and a proof of its completeness appeared in a
seminal paper by J. A. Robinson (1965), which also showed how to do first-order reasoning
without resort to propositional techniques.
Stephen Cook (1971) showed that deciding satisfiability of a sentence in propositional
logic (the SAT problem) is NP-complete. Since deciding entailment is equivalent to deciding unsatisfiability, it is co-NP-complete. Many subsets of propositional logic are known for
which the satisfiability problem is polynomially solvable; Horn clauses are one such subset.

Bibliographical and Historical Notes

277

The linear-time forward-chaining algorithm for Horn clauses is due to Dowling and Gallier
(1984), who describe their algorithm as a dataflow process similar to the propagation of signals in a circuit.
Early theoretical investigations showed that DPLL has polynomial average-case complexity for certain natural distributions of problems. This potentially exciting fact became
less exciting when Franco and Paull (1983) showed that the same problems could be solved
in constant time simply by guessing random assignments. The random-generation method
described in the chapter produces much harder problems. Motivated by the empirical success
of local search on these problems, Koutsoupias and Papadimitriou (1992) showed that a simple hill-climbing algorithm can solve almost all satisfiability problem instances very quickly,
suggesting that hard problems are rare. Moreover, SchoÌˆning (1999) exhibited a randomized
hill-climbing algorithm whose worst-case expected run time on 3-SAT problems (that is, satisfiability of 3-CNF sentences) is O(1.333n )â€”still exponential, but substantially faster than
previous worst-case bounds. The current record is O(1.324n ) (Iwama and Tamaki, 2004).
Achlioptas et al. (2004) and Alekhnovich et al. (2005) exhibit families of 3-SAT instances
for which all known DPLL-like algorithms require exponential running time.
On the practical side, efficiency gains in propositional solvers have been marked. Given
ten minutes of computing time, the original DPLL algorithm in 1962 could only solve problems with no more than 10 or 15 variables. By 1995 the S ATZ solver (Li and Anbulagan,
1997) could handle 1,000 variables, thanks to optimized data structures for indexing variables. Two crucial contributions were the watched literal indexing technique of Zhang and
Stickel (1996), which makes unit propagation very efficient, and the introduction of clause
(i.e., constraint) learning techniques from the CSP community by Bayardo and Schrag (1997).
Using these ideas, and spurred by the prospect of solving industrial-scale circuit verification
problems, Moskewicz et al. (2001) developed the C HAFF solver, which could handle problems with millions of variables. Beginning in 2002, SAT competitions have been held regularly; most of the winning entries have either been descendants of C HAFF or have used the
same general approach. RS AT (Pipatsrisawat and Darwiche, 2007), the 2007 winner, falls in
the latter category. Also noteworthy is M INI SAT (Een and SoÌˆrensson, 2003), an open-source
implementation available at http://minisat.se that is designed to be easily modified
and improved. The current landscape of solvers is surveyed by Gomes et al. (2008).
Local search algorithms for satisfiability were tried by various authors throughout the
1980s; all of the algorithms were based on the idea of minimizing the number of unsatisfied
clauses (Hansen and Jaumard, 1990). A particularly effective algorithm was developed by
Gu (1989) and independently by Selman et al. (1992), who called it GSAT and showed that
it was capable of solving a wide range of very hard problems very quickly. The WALK SAT
algorithm described in the chapter is due to Selman et al. (1996).
The â€œphase transitionâ€ in satisfiability of random k-SAT problems was first observed
by Simon and Dubois (1989) and has given rise to a great deal of theoretical and empirical
researchâ€”due, in part, to the obvious connection to phase transition phenomena in statistical
physics. Cheeseman et al. (1991) observed phase transitions in several CSPs and conjecture
that all NP-hard problems have a phase transition. Crawford and Auton (1993) located the
3-SAT transition at a clause/variable ratio of around 4.26, noting that this coincides with a

278

SATISFIABILITY
THRESHOLD
CONJECTURE

SURVEY
PROPAGATION

TEMPORALPROJECTION

Chapter

7.

Logical Agents

sharp peak in the run time of their SAT solver. Cook and Mitchell (1997) provide an excellent
summary of the early literature on the problem.
The current state of theoretical understanding is summarized by Achlioptas (2009).
The satisfiability threshold conjecture states that, for each k, there is a sharp satisfiability
threshold rk , such that as the number of variables n â†’ âˆž, instances below the threshold are
satisfiable with probability 1, while those above the threshold are unsatisfiable with probability 1. The conjecture was not quite proved by Friedgut (1999): a sharp threshold exists but
its location might depend on n even as n â†’ âˆž. Despite significant progress in asymptotic
analysis of the threshold location for large k (Achlioptas and Peres, 2004; Achlioptas et al.,
2007), all that can be proved for k = 3 is that it lies in the range [3.52,4.51]. Current theory
suggests that a peak in the run time of a SAT solver is not necessarily related to the satisfiability threshold, but instead to a phase transition in the solution distribution and structure of
SAT instances. Empirical results due to Coarfa et al. (2003) support this view. In fact, algorithms such as survey propagation (Parisi and Zecchina, 2002; Maneva et al., 2007) take
advantage of special properties of random SAT instances near the satisfiability threshold and
greatly outperform general SAT solvers on such instances.
The best sources for information on satisfiability, both theoretical and practical, are the
Handbook of Satisfiability (Biere et al., 2009) and the regular International Conferences on
Theory and Applications of Satisfiability Testing, known as SAT.
The idea of building agents with propositional logic can be traced back to the seminal
paper of McCulloch and Pitts (1943), which initiated the field of neural networks. Contrary to popular supposition, the paper was concerned with the implementation of a Boolean
circuit-based agent design in the brain. Circuit-based agents, which perform computation by
propagating signals in hardware circuits rather than running algorithms in general-purpose
computers, have received little attention in AI, however. The most notable exception is the
work of Stan Rosenschein (Rosenschein, 1985; Kaelbling and Rosenschein, 1990), who developed ways to compile circuit-based agents from declarative descriptions of the task environment. (Rosenscheinâ€™s approach is described at some length in the second edition of this
book.) The work of Rod Brooks (1986, 1989) demonstrates the effectiveness of circuit-based
designs for controlling robotsâ€”a topic we take up in Chapter 25. Brooks (1991) argues
that circuit-based designs are all that is needed for AIâ€”that representation and reasoning
are cumbersome, expensive, and unnecessary. In our view, neither approach is sufficient by
itself. Williams et al. (2003) show how a hybrid agent design not too different from our
wumpus agent has been used to control NASA spacecraft, planning sequences of actions and
diagnosing and recovering from faults.
The general problem of keeping track of a partially observable environment was introduced for state-based representations in Chapter 4. Its instantiation for propositional representations was studied by Amir and Russell (2003), who identified several classes of environments that admit efficient state-estimation algorithms and showed that for several other
classes the problem is intractable. The temporal-projection problem, which involves determining what propositions hold true after an action sequence is executed, can be seen as a
special case of state estimation with empty percepts. Many authors have studied this problem
because of its importance in planning; some important hardness results were established by

Exercises

279
Liberatore (1997). The idea of representing a belief state with propositions can be traced to
Wittgenstein (1922).
Logical state estimation, of course, requires a logical representation of the effects of
actionsâ€”a key problem in AI since the late 1950s. The dominant proposal has been the situation calculus formalism (McCarthy, 1963), which is couched within first-order logic. We
discuss situation calculus, and various extensions and alternatives, in Chapters 10 and 12. The
approach taken in this chapterâ€”using temporal indices on propositional variablesâ€”is more
restrictive but has the benefit of simplicity. The general approach embodied in the SATP LAN
algorithm was proposed by Kautz and Selman (1992). Later generations of SATP LAN were
able to take advantage of the advances in SAT solvers, described earlier, and remain among
the most effective ways of solving difficult problems (Kautz, 2006).
The frame problem was first recognized by McCarthy and Hayes (1969). Many researchers considered the problem unsolvable within first-order logic, and it spurred a great
deal of research into nonmonotonic logics. Philosophers from Dreyfus (1972) to Crockett
(1994) have cited the frame problem as one symptom of the inevitable failure of the entire
AI enterprise. The solution of the frame problem with successor-state axioms is due to Ray
Reiter (1991). Thielscher (1999) identifies the inferential frame problem as a separate idea
and provides a solution. In retrospect, one can see that Rosenscheinâ€™s (1985) agents were
using circuits that implemented successor-state axioms, but Rosenschein did not notice that
the frame problem was thereby largely solved. Foo (2001) explains why the discrete-event
control theory models typically used by engineers do not have to explicitly deal with the
frame problem: because they are dealing with prediction and control, not with explanation
and reasoning about counterfactual situations.
Modern propositional solvers have wide applicability in industrial applications. The application of propositional inference in the synthesis of computer hardware is now a standard
technique having many large-scale deployments (Nowick et al., 1993). The SATMC satisfiability checker was used to detect a previously unknown vulnerability in a Web browser user
sign-on protocol (Armando et al., 2008).
The wumpus world was invented by Gregory Yob (1975). Ironically, Yob developed it
because he was bored with games played on a rectangular grid: the topology of his original
wumpus world was a dodecahedron, and we put it back in the boring old grid. Michael
Genesereth was the first to suggest that the wumpus world be used as an agent testbed.

E XERCISES
7.1 Suppose the agent has progressed to the point shown in Figure 7.4(a), page 239, having
perceived nothing in [1,1], a breeze in [2,1], and a stench in [1,2], and is now concerned with
the contents of [1,3], [2,2], and [3,1]. Each of these can contain a pit, and at most one can
contain a wumpus. Following the example of Figure 7.5, construct the set of possible worlds.
(You should find 32 of them.) Mark the worlds in which the KB is true and those in which

280

Chapter

7.

Logical Agents

each of the following sentences is true:
Î±2 = â€œThere is no pit in [2,2].â€
Î±3 = â€œThere is a wumpus in [1,3].â€
Hence show that KB |= Î±2 and KB |= Î±3 .
7.2 (Adapted from Barwise and Etchemendy (1993).) Given the following, can you prove
that the unicorn is mythical? How about magical? Horned?
If the unicorn is mythical, then it is immortal, but if it is not mythical, then it is a
mortal mammal. If the unicorn is either immortal or a mammal, then it is horned.
The unicorn is magical if it is horned.
7.3 Consider the problem of deciding whether a propositional logic sentence is true in a
given model.
a. Write a recursive algorithm PL-T RUE ?(s, m) that returns true if and only if the sentence s is true in the model m (where m assigns a truth value for every symbol in s).
The algorithm should run in time linear in the size of the sentence. (Alternatively, use a
version of this function from the online code repository.)
b. Give three examples of sentences that can be determined to be true or false in a partial
model that does not specify a truth value for some of the symbols.
c. Show that the truth value (if any) of a sentence in a partial model cannot be determined
efficiently in general.
d. Modify your PL-T RUE ? algorithm so that it can sometimes judge truth from partial
models, while retaining its recursive structure and linear run time. Give three examples
of sentences whose truth in a partial model is not detected by your algorithm.
e. Investigate whether the modified algorithm makes TT-E NTAILS ? more efficient.
7.4
a.
b.
c.
d.
e.
f.
g.
h.
i.
j.
k.
l.

Which of the following are correct?
False |= True.
True |= False.
(A âˆ§ B) |= (A â‡” B).
A â‡” B |= A âˆ¨ B.
A â‡” B |= Â¬A âˆ¨ B.
(A âˆ§ B) â‡’ C |= (A â‡’ C) âˆ¨ (B â‡’ C).
(C âˆ¨ (Â¬A âˆ§ Â¬B)) â‰¡ ((A â‡’ C) âˆ§ (B â‡’ C)).
(A âˆ¨ B) âˆ§ (Â¬C âˆ¨ Â¬D âˆ¨ E) |= (A âˆ¨ B).
(A âˆ¨ B) âˆ§ (Â¬C âˆ¨ Â¬D âˆ¨ E) |= (A âˆ¨ B) âˆ§ (Â¬D âˆ¨ E).
(A âˆ¨ B) âˆ§ Â¬(A â‡’ B) is satisfiable.
(A â‡” B) âˆ§ (Â¬A âˆ¨ B) is satisfiable.
(A â‡” B) â‡” C has the same number of models as (A â‡” B) for any fixed set of
proposition symbols that includes A, B, C.

Exercises

281
7.5
a.
b.
c.
d.
e.
7.6

Prove each of the following assertions:
Î± is valid if and only if True |= Î±.
For any Î±, False |= Î±.
Î± |= Î² if and only if the sentence (Î± â‡’ Î²) is valid.
Î± â‰¡ Î² if and only if the sentence (Î± â‡” Î²) is valid.
Î± |= Î² if and only if the sentence (Î± âˆ§ Â¬Î²) is unsatisfiable.
Prove, or find a counterexample to, each of the following assertions:

a. If Î± |= Î³ or Î² |= Î³ (or both) then (Î± âˆ§ Î²) |= Î³
b. If Î± |= (Î² âˆ§ Î³) then Î± |= Î² and Î± |= Î³.
c. If Î± |= (Î² âˆ¨ Î³) then Î± |= Î² or Î± |= Î³ (or both).
7.7 Consider a vocabulary with only four propositions, A, B, C, and D. How many models
are there for the following sentences?
a. B âˆ¨ C.
b. Â¬A âˆ¨ Â¬B âˆ¨ Â¬C âˆ¨ Â¬D.
c. (A â‡’ B) âˆ§ A âˆ§ Â¬B âˆ§ C âˆ§ D.
7.8

We have defined four binary logical connectives.

a. Are there any others that might be useful?
b. How many binary connectives can there be?
c. Why are some of them not very useful?
7.9

Using a method of your choice, verify each of the equivalences in Figure 7.11 (page 249).

7.10 Decide whether each of the following sentences is valid, unsatisfiable, or neither. Verify your decisions using truth tables or the equivalence rules of Figure 7.11 (page 249).
a.
b.
c.
d.
e.
f.
g.

Smoke â‡’ Smoke
Smoke â‡’ Fire
(Smoke â‡’ Fire) â‡’ (Â¬Smoke â‡’ Â¬Fire)
Smoke âˆ¨ Fire âˆ¨ Â¬Fire
((Smoke âˆ§ Heat) â‡’ Fire) â‡” ((Smoke â‡’ Fire) âˆ¨ (Heat â‡’ Fire))
(Smoke â‡’ Fire) â‡’ ((Smoke âˆ§ Heat) â‡’ Fire)
Big âˆ¨ Dumb âˆ¨ (Big â‡’ Dumb)

7.11 Any propositional logic sentence is logically equivalent to the assertion that each possible world in which it would be false is not the case. From this observation, prove that any
sentence can be written in CNF.
7.12

Use resolution to prove the sentence Â¬Aâˆ§ Â¬B from the clauses in Exercise 7.20.

7.13

This exercise looks into the relationship between clauses and implication sentences.

282

IMPLICATIVE
NORMAL FORM

Chapter

7.

Logical Agents

a. Show that the clause (Â¬P1 âˆ¨ Â· Â· Â· âˆ¨ Â¬Pm âˆ¨ Q) is logically equivalent to the implication
sentence (P1 âˆ§ Â· Â· Â· âˆ§ Pm ) â‡’ Q.
b. Show that every clause (regardless of the number of positive literals) can be written in
the form (P1 âˆ§ Â· Â· Â· âˆ§ Pm ) â‡’ (Q1 âˆ¨ Â· Â· Â· âˆ¨ Qn ), where the P s and Qs are proposition
symbols. A knowledge base consisting of such sentences is in implicative normal
form or Kowalski form (Kowalski, 1979).
c. Write down the full resolution rule for sentences in implicative normal form.
7.14 According to some political pundits, a person who is radical (R) is electable (E) if
he/she is conservative (C), but otherwise is not electable.
a. Which of the following are correct representations of this assertion?
(i) (R âˆ§ E) â‡â‡’ C
(ii) R â‡’ (E â‡â‡’ C)
(iii) R â‡’ ((C â‡’ E) âˆ¨ Â¬E)
b. Which of the sentences in (a) can be expressed in Horn form?
7.15

This question considers representing satisfiability (SAT) problems as CSPs.

a. Draw the constraint graph corresponding to the SAT problem
(Â¬X1 âˆ¨ X2 ) âˆ§ (Â¬X2 âˆ¨ X3 ) âˆ§ . . . âˆ§ (Â¬Xnâˆ’1 âˆ¨ Xn )
for the particular case n = 5.
b. How many solutions are there for this general SAT problem as a function of n?
c. Suppose we apply BACKTRACKING-S EARCH (page 215) to find all solutions to a SAT
CSP of the type given in (a). (To find all solutions to a CSP, we simply modify the
basic algorithm so it continues searching after each solution is found.) Assume that
variables are ordered X1 , . . . , Xn and false is ordered before true. How much time
will the algorithm take to terminate? (Write an O(Â·) expression as a function of n.)
d. We know that SAT problems in Horn form can be solved in linear time by forward
chaining (unit propagation). We also know that every tree-structured binary CSP with
discrete, finite domains can be solved in time linear in the number of variables (Section 6.5). Are these two facts connected? Discuss.
7.16 Explain why every nonempty propositional clause, by itself, is satisfiable. Prove rigorously that every set of five 3-SAT clauses is satisfiable, provided that each clause mentions
exactly three distinct variables. What is the smallest set of such clauses that is unsatisfiable?
Construct such a set.
7.17 A propositional 2-CNF expression is a conjunction of clauses, each containing exactly
2 literals, e.g.,
(A âˆ¨ B) âˆ§ (Â¬A âˆ¨ C) âˆ§ (Â¬B âˆ¨ D) âˆ§ (Â¬C âˆ¨ G) âˆ§ (Â¬D âˆ¨ G) .
a. Prove using resolution that the above sentence entails G.

Exercises

283
b. Two clauses are semantically distinct if they are not logically equivalent. How many
semantically distinct 2-CNF clauses can be constructed from n proposition symbols?
c. Using your answer to (b), prove that propositional resolution always terminates in time
polynomial in n given a 2-CNF sentence containing no more than n distinct symbols.
d. Explain why your argument in (c) does not apply to 3-CNF.
7.18

Consider the following sentence:
[(Food â‡’ Party) âˆ¨ (Drinks â‡’ Party)] â‡’ [(Food âˆ§ Drinks) â‡’ Party] .

a. Determine, using enumeration, whether this sentence is valid, satisfiable (but not valid),
or unsatisfiable.
b. Convert the left-hand and right-hand sides of the main implication into CNF, showing
each step, and explain how the results confirm your answer to (a).
c. Prove your answer to (a) using resolution.
DISJUNCTIVE
NORMAL FORM

7.19 A sentence is in disjunctive normal form (DNF) if it is the disjunction of conjunctions
of literals. For example, the sentence (A âˆ§ B âˆ§ Â¬C) âˆ¨ (Â¬A âˆ§ C) âˆ¨ (B âˆ§ Â¬C) is in DNF.
a. Any propositional logic sentence is logically equivalent to the assertion that some possible world in which it would be true is in fact the case. From this observation, prove
that any sentence can be written in DNF.
b. Construct an algorithm that converts any sentence in propositional logic into DNF.
(Hint: The algorithm is similar to the algorithm for conversion to CNF given in Section 7.5.2.)
c. Construct a simple algorithm that takes as input a sentence in DNF and returns a satisfying assignment if one exists, or reports that no satisfying assignment exists.
d. Apply the algorithms in (b) and (c) to the following set of sentences:
A â‡’ B
B â‡’ C
C â‡’ Â¬A .
e. Since the algorithm in (b) is very similar to the algorithm for conversion to CNF, and
since the algorithm in (c) is much simpler than any algorithm for solving a set of sentences in CNF, why is this technique not used in automated reasoning?
7.20

Convert the following set of sentences to clausal form.
S1:
S2:
S3:
S4:
S5:
S6:

A â‡” (B âˆ¨ E).
E â‡’ D.
C âˆ§ F â‡’ Â¬B.
E â‡’ B.
B â‡’ F.
B â‡’ C

Give a trace of the execution of DPLL on the conjunction of these clauses.

284

Chapter

7.

Logical Agents

7.21 Is a randomly generated 4-CNF sentence with n symbols and m clauses more or less
likely to be solvable than a randomly generated 3-CNF sentence with n symbols and m
clauses? Explain.
7.22 Minesweeper, the well-known computer game, is closely related to the wumpus world.
A minesweeper world is a rectangular grid of N squares with M invisible mines scattered
among them. Any square may be probed by the agent; instant death follows if a mine is
probed. Minesweeper indicates the presence of mines by revealing, in each probed square,
the number of mines that are directly or diagonally adjacent. The goal is to probe every
unmined square.
a. Let Xi,j be true iff square [i, j] contains a mine. Write down the assertion that exactly
two mines are adjacent to [1,1] as a sentence involving some logical combination of
Xi,j propositions.
b. Generalize your assertion from (a) by explaining how to construct a CNF sentence
asserting that k of n neighbors contain mines.
c. Explain precisely how an agent can use DPLL to prove that a given square does (or
does not) contain a mine, ignoring the global constraint that there are exactly M mines
in all.
d. Suppose that the global constraint is constructed from your method from part (b). How
does the number of clauses depend on M and N ? Suggest a way to modify DPLL so
that the global constraint does not need to be represented explicitly.
e. Are any conclusions derived by the method in part (c) invalidated when the global
constraint is taken into account?
f. Give examples of configurations of probe values that induce long-range dependencies
such that the contents of a given unprobed square would give information about the
contents of a far-distant square. (Hint: consider an N Ã— 1 board.)
7.23 How long does it take to prove KB |= Î± using DPLL when Î± is a literal already
contained in KB? Explain.
7.24 Trace the behavior of DPLL on the knowledge base in Figure 7.16 when trying to
prove Q, and compare this behavior with that of the forward-chaining algorithm.
7.25 Write a successor-state axiom for the Locked predicate, which applies to doors, assuming the only actions available are Lock and Unlock .
7.26 Section 7.7.1 provides some of the successor-state axioms required for the wumpus
world. Write down axioms for all remaining fluent symbols.
7.27 Modify the H YBRID -W UMPUS -AGENT to use the 1-CNF logical state estimation
method described on page 271. We noted on that page that such an agent will not be able
to acquire, maintain, and use more complex beliefs such as the disjunction P3,1 âˆ¨ P2,2 . Suggest a method for overcoming this problem by defining additional proposition symbols, and
try it out in the wumpus world. Does it improve the performance of the agent?

8

FIRST-ORDER LOGIC

In which we notice that the world is blessed with many objects, some of which are
related to other objects, and in which we endeavor to reason about them.

FIRST-ORDER LOGIC

8.1

In Chapter 7, we showed how a knowledge-based agent could represent the world in which it
operates and deduce what actions to take. We used propositional logic as our representation
language because it sufficed to illustrate the basic concepts of logic and knowledge-based
agents. Unfortunately, propositional logic is too puny a language to represent knowledge
of complex environments in a concise way. In this chapter, we examine first-order logic,1
which is sufficiently expressive to represent a good deal of our commonsense knowledge.
It also either subsumes or forms the foundation of many other representation languages and
has been studied intensively for many decades. We begin in Section 8.1 with a discussion of
representation languages in general; Section 8.2 covers the syntax and semantics of first-order
logic; Sections 8.3 and 8.4 illustrate the use of first-order logic for simple representations.

R EPRESENTATION R EVISITED
In this section, we discuss the nature of representation languages. Our discussion motivates
the development of first-order logic, a much more expressive language than the propositional
logic introduced in Chapter 7. We look at propositional logic and at other kinds of languages
to understand what works and what fails. Our discussion will be cursory, compressing centuries of thought, trial, and error into a few paragraphs.
Programming languages (such as C++ or Java or Lisp) are by far the largest class of
formal languages in common use. Programs themselves represent, in a direct sense, only
computational processes. Data structures within programs can represent facts; for example,
a program could use a 4 Ã— 4 array to represent the contents of the wumpus world. Thus, the
programming language statement World [2,2] â† Pit is a fairly natural way to assert that there
is a pit in square [2,2]. (Such representations might be considered ad hoc; database systems
were developed precisely to provide a more general, domain-independent way to store and
1

Also called first-order predicate calculus, sometimes abbreviated as FOL or FOPC.

285

286

COMPOSITIONALITY

Chapter

8.

First-Order Logic

retrieve facts.) What programming languages lack is any general mechanism for deriving
facts from other facts; each update to a data structure is done by a domain-specific procedure
whose details are derived by the programmer from his or her own knowledge of the domain.
This procedural approach can be contrasted with the declarative nature of propositional logic,
in which knowledge and inference are separate, and inference is entirely domain independent.
A second drawback of data structures in programs (and of databases, for that matter)
is the lack of any easy way to say, for example, â€œThere is a pit in [2,2] or [3,1]â€ or â€œIf the
wumpus is in [1,1] then he is not in [2,2].â€ Programs can store a single value for each variable,
and some systems allow the value to be â€œunknown,â€ but they lack the expressiveness required
to handle partial information.
Propositional logic is a declarative language because its semantics is based on a truth
relation between sentences and possible worlds. It also has sufficient expressive power to
deal with partial information, using disjunction and negation. Propositional logic has a third
property that is desirable in representation languages, namely, compositionality. In a compositional language, the meaning of a sentence is a function of the meaning of its parts. For
example, the meaning of â€œS1,4 âˆ§ S1,2 â€ is related to the meanings of â€œS1,4 â€ and â€œS1,2 .â€ It
would be very strange if â€œS1,4 â€ meant that there is a stench in square [1,4] and â€œS1,2 â€ meant
that there is a stench in square [1,2], but â€œS1,4 âˆ§ S1,2 â€ meant that France and Poland drew 1â€“1
in last weekâ€™s ice hockey qualifying match. Clearly, noncompositionality makes life much
more difficult for the reasoning system.
As we saw in Chapter 7, however, propositional logic lacks the expressive power to
concisely describe an environment with many objects. For example, we were forced to write
a separate rule about breezes and pits for each square, such as
B1,1 â‡” (P1,2 âˆ¨ P2,1 ) .
In English, on the other hand, it seems easy enough to say, once and for all, â€œSquares adjacent
to pits are breezy.â€ The syntax and semantics of English somehow make it possible to describe
the environment concisely.

8.1.1 The language of thought
Natural languages (such as English or Spanish) are very expressive indeed. We managed to
write almost this whole book in natural language, with only occasional lapses into other languages (including logic, mathematics, and the language of diagrams). There is a long tradition in linguistics and the philosophy of language that views natural language as a declarative
knowledge representation language. If we could uncover the rules for natural language, we
could use it in representation and reasoning systems and gain the benefit of the billions of
pages that have been written in natural language.
The modern view of natural language is that it serves a as a medium for communication
rather than pure representation. When a speaker points and says, â€œLook!â€ the listener comes
to know that, say, Superman has finally appeared over the rooftops. Yet we would not want
to say that the sentence â€œLook!â€ represents that fact. Rather, the meaning of the sentence
depends both on the sentence itself and on the context in which the sentence was spoken.
Clearly, one could not store a sentence such as â€œLook!â€ in a knowledge base and expect to

Section 8.1.

AMBIGUITY

Representation Revisited

287

recover its meaning without also storing a representation of the contextâ€”which raises the
question of how the context itself can be represented. Natural languages also suffer from
ambiguity, a problem for a representation language. As Pinker (1995) puts it: â€œWhen people
think about spring, surely they are not confused as to whether they are thinking about a season
or something that goes boingâ€”and if one word can correspond to two thoughts, thoughts
canâ€™t be words.â€
The famous Sapirâ€“Whorf hypothesis claims that our understanding of the world is
strongly influenced by the language we speak. Whorf (1956) wrote â€œWe cut nature up, organize it into concepts, and ascribe significances as we do, largely because we are parties to an
agreement to organize it this wayâ€”an agreement that holds throughout our speech community and is codified in the patterns of our language.â€ It is certainly true that different speech
communities divide up the world differently. The French have two words â€œchaiseâ€ and â€œfauteuil,â€ for a concept that English speakers cover with one: â€œchair.â€ But English speakers
can easily recognize the category fauteuil and give it a nameâ€”roughly â€œopen-arm chairâ€â€”so
does language really make a difference? Whorf relied mainly on intuition and speculation,
but in the intervening years we actually have real data from anthropological, psychological
and neurological studies.
For example, can you remember which of the following two phrases formed the opening
of Section 8.1?
â€œIn this section, we discuss the nature of representation languages . . .â€
â€œThis section covers the topic of knowledge representation languages . . .â€
Wanner (1974) did a similar experiment and found that subjects made the right choice at
chance levelâ€”about 50% of the timeâ€”but remembered the content of what they read with
better than 90% accuracy. This suggests that people process the words to form some kind of
nonverbal representation.
More interesting is the case in which a concept is completely absent in a language.
Speakers of the Australian aboriginal language Guugu Yimithirr have no words for relative
directions, such as front, back, right, or left. Instead they use absolute directions, saying,
for example, the equivalent of â€œI have a pain in my north arm.â€ This difference in language
makes a difference in behavior: Guugu Yimithirr speakers are better at navigating in open
terrain, while English speakers are better at placing the fork to the right of the plate.
Language also seems to influence thought through seemingly arbitrary grammatical
features such as the gender of nouns. For example, â€œbridgeâ€ is masculine in Spanish and
feminine in German. Boroditsky (2003) asked subjects to choose English adjectives to describe a photograph of a particular bridge. Spanish speakers chose big, dangerous, strong,
and towering, whereas German speakers chose beautiful, elegant, fragile, and slender. Words
can serve as anchor points that affect how we perceive the world. Loftus and Palmer (1974)
showed experimental subjects a movie of an auto accident. Subjects who were asked â€œHow
fast were the cars going when they contacted each other?â€ reported an average of 32 mph,
while subjects who were asked the question with the word â€œsmashedâ€ instead of â€œcontactedâ€
reported 41mph for the same cars in the same movie.

288

Chapter

8.

First-Order Logic

In a first-order logic reasoning system that uses CNF, we can see that the linguistic form
â€œÂ¬(A âˆ¨ B)â€ and â€œÂ¬A âˆ§ Â¬Bâ€ are the same because we can look inside the system and see
that the two sentences are stored as the same canonical CNF form. Can we do that with the
human brain? Until recently the answer was â€œno,â€ but now it is â€œmaybe.â€ Mitchell et al.
(2008) put subjects in an fMRI (functional magnetic resonance imaging) machine, showed
them words such as â€œcelery,â€ and imaged their brains. The researchers were then able to train
a computer program to predict, from a brain image, what word the subject had been presented
with. Given two choices (e.g., â€œceleryâ€ or â€œairplaneâ€), the system predicts correctly 77% of
the time. The system can even predict at above-chance levels for words it has never seen
an fMRI image of before (by considering the images of related words) and for people it has
never seen before (proving that fMRI reveals some level of common representation across
people). This type of work is still in its infancy, but fMRI (and other imaging technology
such as intracranial electrophysiology (Sahin et al., 2009)) promises to give us much more
concrete ideas of what human knowledge representations are like.
From the viewpoint of formal logic, representing the same knowledge in two different
ways makes absolutely no difference; the same facts will be derivable from either representation. In practice, however, one representation might require fewer steps to derive a conclusion, meaning that a reasoner with limited resources could get to the conclusion using one
representation but not the other. For nondeductive tasks such as learning from experience,
outcomes are necessarily dependent on the form of the representations used. We show in
Chapter 18 that when a learning program considers two possible theories of the world, both
of which are consistent with all the data, the most common way of breaking the tie is to choose
the most succinct theoryâ€”and that depends on the language used to represent theories. Thus,
the influence of language on thought is unavoidable for any agent that does learning.

8.1.2 Combining the best of formal and natural languages

OBJECT
RELATION
FUNCTION

PROPERTY

We can adopt the foundation of propositional logicâ€”a declarative, compositional semantics
that is context-independent and unambiguousâ€”and build a more expressive logic on that
foundation, borrowing representational ideas from natural language while avoiding its drawbacks. When we look at the syntax of natural language, the most obvious elements are nouns
and noun phrases that refer to objects (squares, pits, wumpuses) and verbs and verb phrases
that refer to relations among objects (is breezy, is adjacent to, shoots). Some of these relations are functionsâ€”relations in which there is only one â€œvalueâ€ for a given â€œinput.â€ It is
easy to start listing examples of objects, relations, and functions:
â€¢ Objects: people, houses, numbers, theories, Ronald McDonald, colors, baseball games,
wars, centuries . . .
â€¢ Relations: these can be unary relations or properties such as red, round, bogus, prime,
multistoried . . ., or more general n-ary relations such as brother of, bigger than, inside,
part of, has color, occurred after, owns, comes between, . . .
â€¢ Functions: father of, best friend, third inning of, one more than, beginning of . . .
Indeed, almost any assertion can be thought of as referring to objects and properties or relations. Some examples follow:

Section 8.1.

Representation Revisited

289

â€¢ â€œOne plus two equals three.â€
Objects: one, two, three, one plus two; Relation: equals; Function: plus. (â€œOne plus
twoâ€ is a name for the object that is obtained by applying the function â€œplusâ€ to the
objects â€œoneâ€ and â€œtwo.â€ â€œThreeâ€ is another name for this object.)
â€¢ â€œSquares neighboring the wumpus are smelly.â€
Objects: wumpus, squares; Property: smelly; Relation: neighboring.
â€¢ â€œEvil King John ruled England in 1200.â€
Objects: John, England, 1200; Relation: ruled; Properties: evil, king.

ONTOLOGICAL
COMMITMENT

TEMPORAL LOGIC

HIGHER-ORDER
LOGIC

EPISTEMOLOGICAL
COMMITMENT

The language of first-order logic, whose syntax and semantics we define in the next section,
is built around objects and relations. It has been so important to mathematics, philosophy, and
artificial intelligence precisely because those fieldsâ€”and indeed, much of everyday human
existenceâ€”can be usefully thought of as dealing with objects and the relations among them.
First-order logic can also express facts about some or all of the objects in the universe. This
enables one to represent general laws or rules, such as the statement â€œSquares neighboring
the wumpus are smelly.â€
The primary difference between propositional and first-order logic lies in the ontological commitment made by each languageâ€”that is, what it assumes about the nature of reality.
Mathematically, this commitment is expressed through the nature of the formal models with
respect to which the truth of sentences is defined. For example, propositional logic assumes
that there are facts that either hold or do not hold in the world. Each fact can be in one
of two states: true or false, and each model assigns true or false to each proposition symbol (see Section 7.4.2).2 First-order logic assumes more; namely, that the world consists of
objects with certain relations among them that do or do not hold. The formal models are
correspondingly more complicated than those for propositional logic. Special-purpose logics
make still further ontological commitments; for example, temporal logic assumes that facts
hold at particular times and that those times (which may be points or intervals) are ordered.
Thus, special-purpose logics give certain kinds of objects (and the axioms about them) â€œfirst
classâ€ status within the logic, rather than simply defining them within the knowledge base.
Higher-order logic views the relations and functions referred to by first-order logic as objects in themselves. This allows one to make assertions about all relationsâ€”for example, one
could wish to define what it means for a relation to be transitive. Unlike most special-purpose
logics, higher-order logic is strictly more expressive than first-order logic, in the sense that
some sentences of higher-order logic cannot be expressed by any finite number of first-order
logic sentences.
A logic can also be characterized by its epistemological commitmentsâ€”the possible
states of knowledge that it allows with respect to each fact. In both propositional and firstorder logic, a sentence represents a fact and the agent either believes the sentence to be true,
believes it to be false, or has no opinion. These logics therefore have three possible states
of knowledge regarding any sentence. Systems using probability theory, on the other hand,
2

In contrast, facts in fuzzy logic have a degree of truth between 0 and 1. For example, the sentence â€œVienna is
a large cityâ€ might be true in our world only to degree 0.6 in fuzzy logic.

290

Chapter

8.

First-Order Logic

can have any degree of belief, ranging from 0 (total disbelief) to 1 (total belief).3 For example, a probabilistic wumpus-world agent might believe that the wumpus is in [1,3] with
probability 0.75. The ontological and epistemological commitments of five different logics
are summarized in Figure 8.1.
Language

Ontological Commitment
(What exists in the world)

Epistemological Commitment
(What an agent believes about facts)

Propositional logic
First-order logic
Temporal logic
Probability theory
Fuzzy logic

facts
facts, objects, relations
facts, objects, relations, times
facts
facts with degree of truth âˆˆ [0, 1]

true/false/unknown
true/false/unknown
true/false/unknown
degree of belief âˆˆ [0, 1]
known interval value

Figure 8.1

Formal languages and their ontological and epistemological commitments.

In the next section, we will launch into the details of first-order logic. Just as a student of
physics requires some familiarity with mathematics, a student of AI must develop a talent for
working with logical notation. On the other hand, it is also important not to get too concerned
with the specifics of logical notationâ€”after all, there are dozens of different versions. The
main things to keep hold of are how the language facilitates concise representations and how
its semantics leads to sound reasoning procedures.

8.2

S YNTAX AND S EMANTICS OF F IRST-O RDER L OGIC
We begin this section by specifying more precisely the way in which the possible worlds
of first-order logic reflect the ontological commitment to objects and relations. Then we
introduce the various elements of the language, explaining their semantics as we go along.

8.2.1 Models for first-order logic

DOMAIN
DOMAIN ELEMENTS

Recall from Chapter 7 that the models of a logical language are the formal structures that
constitute the possible worlds under consideration. Each model links the vocabulary of the
logical sentences to elements of the possible world, so that the truth of any sentence can
be determined. Thus, models for propositional logic link proposition symbols to predefined
truth values. Models for first-order logic are much more interesting. First, they have objects
in them! The domain of a model is the set of objects or domain elements it contains. The domain is required to be nonemptyâ€”every possible world must contain at least one object. (See
Exercise 8.7 for a discussion of empty worlds.) Mathematically speaking, it doesnâ€™t matter
what these objects areâ€”all that matters is how many there are in each particular modelâ€”but
for pedagogical purposes weâ€™ll use a concrete example. Figure 8.2 shows a model with five
3

It is important not to confuse the degree of belief in probability theory with the degree of truth in fuzzy logic.
Indeed, some fuzzy systems allow uncertainty (degree of belief) about degrees of truth.

Section 8.2.

TUPLE

Syntax and Semantics of First-Order Logic

291

objects: Richard the Lionheart, King of England from 1189 to 1199; his younger brother, the
evil King John, who ruled from 1199 to 1215; the left legs of Richard and John; and a crown.
The objects in the model may be related in various ways. In the figure, Richard and
John are brothers. Formally speaking, a relation is just the set of tuples of objects that are
related. (A tuple is a collection of objects arranged in a fixed order and is written with angle
brackets surrounding the objects.) Thus, the brotherhood relation in this model is the set
{ Richard the Lionheart, King John, King John, Richard the Lionheart } .

(8.1)

(Here we have named the objects in English, but you may, if you wish, mentally substitute the
pictures for the names.) The crown is on King Johnâ€™s head, so the â€œon headâ€ relation contains
just one tuple, the crown, King John. The â€œbrotherâ€ and â€œon headâ€ relations are binary
relationsâ€”that is, they relate pairs of objects. The model also contains unary relations, or
properties: the â€œpersonâ€ property is true of both Richard and John; the â€œkingâ€ property is true
only of John (presumably because Richard is dead at this point); and the â€œcrownâ€ property is
true only of the crown.
Certain kinds of relationships are best considered as functions, in that a given object
must be related to exactly one object in this way. For example, each person has one left leg,
so the model has a unary â€œleft legâ€ function that includes the following mappings:
Richard the Lionheart â†’ Richardâ€™s left leg
King John â†’ Johnâ€™s left leg .
TOTAL FUNCTIONS

(8.2)

Strictly speaking, models in first-order logic require total functions, that is, there must be a
value for every input tuple. Thus, the crown must have a left leg and so must each of the left
legs. There is a technical solution to this awkward problem involving an additional â€œinvisibleâ€
crown

on head

brother

person
king

person
brother

R

$
left leg

J
left leg

Figure 8.2 A model containing five objects, two binary relations, three unary relations
(indicated by labels on the objects), and one unary function, left-leg.

292

Chapter

8.

First-Order Logic

object that is the left leg of everything that has no left leg, including itself. Fortunately, as
long as one makes no assertions about the left legs of things that have no left legs, these
technicalities are of no import.
So far, we have described the elements that populate models for first-order logic. The
other essential part of a model is the link between those elements and the vocabulary of the
logical sentences, which we explain next.

8.2.2 Symbols and interpretations

CONSTANT SYMBOL
PREDICATE SYMBOL
FUNCTION SYMBOL

ARITY

INTERPRETATION

INTENDED
INTERPRETATION

We turn now to the syntax of first-order logic. The impatient reader can obtain a complete
description from the formal grammar in Figure 8.3.
The basic syntactic elements of first-order logic are the symbols that stand for objects,
relations, and functions. The symbols, therefore, come in three kinds: constant symbols,
which stand for objects; predicate symbols, which stand for relations; and function symbols, which stand for functions. We adopt the convention that these symbols will begin with
uppercase letters. For example, we might use the constant symbols Richard and John; the
predicate symbols Brother , OnHead , Person, King, and Crown; and the function symbol
LeftLeg . As with proposition symbols, the choice of names is entirely up to the user. Each
predicate and function symbol comes with an arity that fixes the number of arguments.
As in propositional logic, every model must provide the information required to determine if any given sentence is true or false. Thus, in addition to its objects, relations, and
functions, each model includes an interpretation that specifies exactly which objects, relations and functions are referred to by the constant, predicate, and function symbols. One
possible interpretation for our exampleâ€”which a logician would call the intended interpretationâ€”is as follows:
â€¢ Richard refers to Richard the Lionheart and John refers to the evil King John.
â€¢ Brother refers to the brotherhood relation, that is, the set of tuples of objects given in
Equation (8.1); OnHead refers to the â€œon headâ€ relation that holds between the crown
and King John; Person, King, and Crown refer to the sets of objects that are persons,
kings, and crowns.
â€¢ LeftLeg refers to the â€œleft legâ€ function, that is, the mapping given in Equation (8.2).
There are many other possible interpretations, of course. For example, one interpretation
maps Richard to the crown and John to King Johnâ€™s left leg. There are five objects in
the model, so there are 25 possible interpretations just for the constant symbols Richard
and John. Notice that not all the objects need have a nameâ€”for example, the intended
interpretation does not name the crown or the legs. It is also possible for an object to have
several names; there is an interpretation under which both Richard and John refer to the
crown.4 If you find this possibility confusing, remember that, in propositional logic, it is
perfectly possible to have a model in which Cloudy and Sunny are both true; it is the job of
the knowledge base to rule out models that are inconsistent with our knowledge.
4

Later, in Section 8.2.8, we examine a semantics in which every object has exactly one name.

Section 8.2.

Syntax and Semantics of First-Order Logic

293

Sentence â†’ AtomicSentence | ComplexSentence
AtomicSentence â†’ Predicate | Predicate(Term, . . .) | Term = Term
ComplexSentence â†’ ( Sentence ) | [ Sentence ]
| Â¬ Sentence
|
|

Sentence âˆ§ Sentence
Sentence âˆ¨ Sentence

|
|

Sentence â‡’ Sentence
Sentence â‡” Sentence

|

Quantiï¬er Variable, . . . Sentence

Term â†’ Function(Term, . . .)
|
|

Constant
Variable

Quantiï¬er â†’ âˆ€ | âˆƒ
Constant â†’ A | X1 | John | Â· Â· Â·
Variable â†’ a | x | s | Â· Â· Â·
Predicate â†’ True | False | After | Loves | Raining | Â· Â· Â·
Function â†’ Mother | LeftLeg | Â· Â· Â·
O PERATOR P RECEDENCE

:

Â¬, =, âˆ§, âˆ¨, â‡’, â‡”

Figure 8.3 The syntax of first-order logic with equality, specified in Backusâ€“Naur form
(see page 1060 if you are not familiar with this notation). Operator precedences are specified,
from highest to lowest. The precedence of quantifiers is such that a quantifier holds over
everything to the right of it.

R

J

R

J

R

J

R

J

R

...

J

R

...

J

...

Figure 8.4 Some members of the set of all models for a language with two constant symbols, R and J, and one binary relation symbol. The interpretation of each constant symbol is
shown by a gray arrow. Within each model, the related objects are connected by arrows.

294

Chapter

8.

First-Order Logic

In summary, a model in first-order logic consists of a set of objects and an interpretation
that maps constant symbols to objects, predicate symbols to relations on those objects, and
function symbols to functions on those objects. Just as with propositional logic, entailment,
validity, and so on are defined in terms of all possible models. To get an idea of what the
set of all possible models looks like, see Figure 8.4. It shows that models vary in how many
objects they containâ€”from one up to infinityâ€”and in the way the constant symbols map
to objects. If there are two constant symbols and one object, then both symbols must refer
to the same object; but this can still happen even with more objects. When there are more
objects than constant symbols, some of the objects will have no names. Because the number
of possible models is unbounded, checking entailment by the enumeration of all possible
models is not feasible for first-order logic (unlike propositional logic). Even if the number of
objects is restricted, the number of combinations can be very large. (See Exercise 8.5.) For
the example in Figure 8.4, there are 137,506,194,466 models with six or fewer objects.

8.2.3 Terms
TERM

A term is a logical expression that refers to an object. Constant symbols are therefore terms,
but it is not always convenient to have a distinct symbol to name every object. For example,
in English we might use the expression â€œKing Johnâ€™s left legâ€ rather than giving a name
to his leg. This is what function symbols are for: instead of using a constant symbol, we
use LeftLeg(John). In the general case, a complex term is formed by a function symbol
followed by a parenthesized list of terms as arguments to the function symbol. It is important
to remember that a complex term is just a complicated kind of name. It is not a â€œsubroutine
callâ€ that â€œreturns a value.â€ There is no LeftLeg subroutine that takes a person as input and
returns a leg. We can reason about left legs (e.g., stating the general rule that everyone has one
and then deducing that John must have one) without ever providing a definition of LeftLeg.
This is something that cannot be done with subroutines in programming languages.5
The formal semantics of terms is straightforward. Consider a term f (t1 , . . . , tn ). The
function symbol f refers to some function in the model (call it F ); the argument terms refer
to objects in the domain (call them d1 , . . . , dn ); and the term as a whole refers to the object
that is the value of the function F applied to d1 , . . . , dn . For example, suppose the LeftLeg
function symbol refers to the function shown in Equation (8.2) and John refers to King John,
then LeftLeg(John) refers to King Johnâ€™s left leg. In this way, the interpretation fixes the
referent of every term.

8.2.4 Atomic sentences
Now that we have both terms for referring to objects and predicate symbols for referring to
relations, we can put them together to make atomic sentences that state facts. An atomic
5 Î»-expressions provide a useful notation in which new function symbols are constructed â€œon the fly.â€ For
example, the function that squares its argument can be written as (Î»x x Ã— x) and can be applied to arguments
just like any other function symbol. A Î»-expression can also be defined and used as a predicate symbol. (See
Chapter 22.) The lambda operator in Lisp plays exactly the same role. Notice that the use of Î» in this way does
not increase the formal expressive power of first-order logic, because any sentence that includes a Î»-expression
can be rewritten by â€œplugging inâ€ its arguments to yield an equivalent sentence.

Section 8.2.
ATOMIC SENTENCE
ATOM

Syntax and Semantics of First-Order Logic

295

sentence (or atom for short) is formed from a predicate symbol optionally followed by a
parenthesized list of terms, such as
Brother (Richard , John).
This states, under the intended interpretation given earlier, that Richard the Lionheart is the
brother of King John.6 Atomic sentences can have complex terms as arguments. Thus,
Married (Father (Richard ), Mother (John))
states that Richard the Lionheartâ€™s father is married to King Johnâ€™s mother (again, under a
suitable interpretation).
An atomic sentence is true in a given model if the relation referred to by the predicate
symbol holds among the objects referred to by the arguments.

8.2.5 Complex sentences
We can use logical connectives to construct more complex sentences, with the same syntax
and semantics as in propositional calculus. Here are four sentences that are true in the model
of Figure 8.2 under our intended interpretation:
Â¬Brother (LeftLeg (Richard ), John)
Brother (Richard , John) âˆ§ Brother (John, Richard )
King(Richard ) âˆ¨ King(John)
Â¬King(Richard ) â‡’ King(John) .

8.2.6 Quantifiers
QUANTIFIER

Once we have a logic that allows objects, it is only natural to want to express properties of
entire collections of objects, instead of enumerating the objects by name. Quantifiers let us
do this. First-order logic contains two standard quantifiers, called universal and existential.
Universal quantification (âˆ€)
Recall the difficulty we had in Chapter 7 with the expression of general rules in propositional logic. Rules such as â€œSquares neighboring the wumpus are smellyâ€ and â€œAll kings
are personsâ€ are the bread and butter of first-order logic. We deal with the first of these in
Section 8.3. The second rule, â€œAll kings are persons,â€ is written in first-order logic as
âˆ€ x King(x) â‡’ Person(x) .

VARIABLE

GROUND TERM

EXTENDED
INTERPRETATION

âˆ€ is usually pronounced â€œFor all . . .â€. (Remember that the upside-down A stands for â€œall.â€)
Thus, the sentence says, â€œFor all x, if x is a king, then x is a person.â€ The symbol x is called
a variable. By convention, variables are lowercase letters. A variable is a term all by itself,
and as such can also serve as the argument of a functionâ€”for example, LeftLeg (x). A term
with no variables is called a ground term.
Intuitively, the sentence âˆ€ x P , where P is any logical expression, says that P is true
for every object x. More precisely, âˆ€ x P is true in a given model if P is true in all possible
extended interpretations constructed from the interpretation given in the model, where each
6

We usually follow the argument-ordering convention that P (x, y) is read as â€œx is a P of y.â€

296

Chapter

8.

First-Order Logic

extended interpretation specifies a domain element to which x refers.
This sounds complicated, but it is really just a careful way of stating the intuitive meaning of universal quantification. Consider the model shown in Figure 8.2 and the intended
interpretation that goes with it. We can extend the interpretation in five ways:
x â†’ Richard the Lionheart,
x â†’ King John,
x â†’ Richardâ€™s left leg,
x â†’ Johnâ€™s left leg,
x â†’ the crown.
The universally quantified sentence âˆ€ x King(x) â‡’ Person(x) is true in the original model
if the sentence King(x) â‡’ Person(x) is true under each of the five extended interpretations. That is, the universally quantified sentence is equivalent to asserting the following five
sentences:
Richard the Lionheart is a king â‡’ Richard the Lionheart is a person.
King John is a king â‡’ King John is a person.
Richardâ€™s left leg is a king â‡’ Richardâ€™s left leg is a person.
Johnâ€™s left leg is a king â‡’ Johnâ€™s left leg is a person.
The crown is a king â‡’ the crown is a person.
Let us look carefully at this set of assertions. Since, in our model, King John is the only
king, the second sentence asserts that he is a person, as we would hope. But what about
the other four sentences, which appear to make claims about legs and crowns? Is that part
of the meaning of â€œAll kings are personsâ€? In fact, the other four assertions are true in the
model, but make no claim whatsoever about the personhood qualifications of legs, crowns,
or indeed Richard. This is because none of these objects is a king. Looking at the truth table
for â‡’ (Figure 7.8 on page 246), we see that the implication is true whenever its premise is
falseâ€”regardless of the truth of the conclusion. Thus, by asserting the universally quantified
sentence, which is equivalent to asserting a whole list of individual implications, we end
up asserting the conclusion of the rule just for those objects for whom the premise is true
and saying nothing at all about those individuals for whom the premise is false. Thus, the
truth-table definition of â‡’ turns out to be perfect for writing general rules with universal
quantifiers.
A common mistake, made frequently even by diligent readers who have read this paragraph several times, is to use conjunction instead of implication. The sentence
âˆ€ x King(x) âˆ§ Person(x)
would be equivalent to asserting
Richard the Lionheart is a king âˆ§ Richard the Lionheart is a person,
King John is a king âˆ§ King John is a person,
Richardâ€™s left leg is a king âˆ§ Richardâ€™s left leg is a person,
and so on. Obviously, this does not capture what we want.

Section 8.2.

Syntax and Semantics of First-Order Logic

297

Existential quantification (âˆƒ)
Universal quantification makes statements about every object. Similarly, we can make a statement about some object in the universe without naming it, by using an existential quantifier.
To say, for example, that King John has a crown on his head, we write
âˆƒ x Crown(x) âˆ§ OnHead (x, John) .
âˆƒx is pronounced â€œThere exists an x such that . . .â€ or â€œFor some x . . .â€.
Intuitively, the sentence âˆƒ x P says that P is true for at least one object x. More
precisely, âˆƒ x P is true in a given model if P is true in at least one extended interpretation
that assigns x to a domain element. That is, at least one of the following is true:
Richard the Lionheart is a crown âˆ§ Richard the Lionheart is on Johnâ€™s head;
King John is a crown âˆ§ King John is on Johnâ€™s head;
Richardâ€™s left leg is a crown âˆ§ Richardâ€™s left leg is on Johnâ€™s head;
Johnâ€™s left leg is a crown âˆ§ Johnâ€™s left leg is on Johnâ€™s head;
The crown is a crown âˆ§ the crown is on Johnâ€™s head.
The fifth assertion is true in the model, so the original existentially quantified sentence is
true in the model. Notice that, by our definition, the sentence would also be true in a model
in which King John was wearing two crowns. This is entirely consistent with the original
sentence â€œKing John has a crown on his head.â€ 7
Just as â‡’ appears to be the natural connective to use with âˆ€, âˆ§ is the natural connective
to use with âˆƒ. Using âˆ§ as the main connective with âˆ€ led to an overly strong statement in
the example in the previous section; using â‡’ with âˆƒ usually leads to a very weak statement,
indeed. Consider the following sentence:
âˆƒ x Crown(x) â‡’ OnHead(x, John) .
On the surface, this might look like a reasonable rendition of our sentence. Applying the
semantics, we see that the sentence says that at least one of the following assertions is true:
Richard the Lionheart is a crown â‡’ Richard the Lionheart is on Johnâ€™s head;
King John is a crown â‡’ King John is on Johnâ€™s head;
Richardâ€™s left leg is a crown â‡’ Richardâ€™s left leg is on Johnâ€™s head;
and so on. Now an implication is true if both premise and conclusion are true, or if its premise
is false. So if Richard the Lionheart is not a crown, then the first assertion is true and the
existential is satisfied. So, an existentially quantified implication sentence is true whenever
any object fails to satisfy the premise; hence such sentences really do not say much at all.
Nested quantifiers
We will often want to express more complex sentences using multiple quantifiers. The simplest case is where the quantifiers are of the same type. For example, â€œBrothers are siblingsâ€
can be written as
âˆ€ x âˆ€ y Brother (x, y) â‡’ Sibling(x, y) .
There is a variant of the existential quantifier, usually written âˆƒ1 or âˆƒ!, that means â€œThere exists exactly one.â€
The same meaning can be expressed using equality statements.

7

298

Chapter

8.

First-Order Logic

Consecutive quantifiers of the same type can be written as one quantifier with several variables. For example, to say that siblinghood is a symmetric relationship, we can write
âˆ€ x, y Sibling(x, y) â‡” Sibling(y, x) .
In other cases we will have mixtures. â€œEverybody loves somebodyâ€ means that for every
person, there is someone that person loves:
âˆ€ x âˆƒ y Loves(x, y) .
On the other hand, to say â€œThere is someone who is loved by everyone,â€ we write
âˆƒ y âˆ€ x Loves(x, y) .
The order of quantification is therefore very important. It becomes clearer if we insert parentheses. âˆ€ x (âˆƒ y Loves(x, y)) says that everyone has a particular property, namely, the property that they love someone. On the other hand, âˆƒ y (âˆ€ x Loves(x, y)) says that someone in
the world has a particular property, namely the property of being loved by everybody.
Some confusion can arise when two quantifiers are used with the same variable name.
Consider the sentence
âˆ€ x (Crown(x) âˆ¨ (âˆƒ x Brother (Richard , x))) .
Here the x in Brother (Richard , x) is existentially quantified. The rule is that the variable
belongs to the innermost quantifier that mentions it; then it will not be subject to any other
quantification. Another way to think of it is this: âˆƒ x Brother (Richard , x) is a sentence
about Richard (that he has a brother), not about x; so putting a âˆ€ x outside it has no effect. It
could equally well have been written âˆƒ z Brother (Richard , z). Because this can be a source
of confusion, we will always use different variable names with nested quantifiers.
Connections between âˆ€ and âˆƒ
The two quantifiers are actually intimately connected with each other, through negation. Asserting that everyone dislikes parsnips is the same as asserting there does not exist someone
who likes them, and vice versa:
âˆ€ x Â¬Likes(x, Parsnips ) is equivalent to

Â¬âˆƒ x Likes(x, Parsnips) .

We can go one step further: â€œEveryone likes ice creamâ€ means that there is no one who does
not like ice cream:
âˆ€ x Likes(x, IceCream) is equivalent to Â¬âˆƒ x Â¬Likes(x, IceCream) .
Because âˆ€ is really a conjunction over the universe of objects and âˆƒ is a disjunction, it should
not be surprising that they obey De Morganâ€™s rules. The De Morgan rules for quantified and
unquantified sentences are as follows:
âˆ€ x Â¬P
Â¬âˆ€ x P
âˆ€x P
âˆƒx P

â‰¡
â‰¡
â‰¡
â‰¡

Â¬âˆƒ x P
âˆƒ x Â¬P
Â¬âˆƒ x Â¬P
Â¬âˆ€ x Â¬P

Â¬(P âˆ¨ Q)
Â¬(P âˆ§ Q)
P âˆ§Q
P âˆ¨Q

â‰¡
â‰¡
â‰¡
â‰¡

Â¬P âˆ§ Â¬Q
Â¬P âˆ¨ Â¬Q
Â¬(Â¬P âˆ¨ Â¬Q)
Â¬(Â¬P âˆ§ Â¬Q) .

Thus, we do not really need both âˆ€ and âˆƒ, just as we do not really need both âˆ§ and âˆ¨. Still,
readability is more important than parsimony, so we will keep both of the quantifiers.

Section 8.2.

Syntax and Semantics of First-Order Logic

299

8.2.7 Equality
EQUALITY SYMBOL

First-order logic includes one more way to make atomic sentences, other than using a predicate and terms as described earlier. We can use the equality symbol to signify that two terms
refer to the same object. For example,
Father (John) = Henry
says that the object referred to by Father (John) and the object referred to by Henry are the
same. Because an interpretation fixes the referent of any term, determining the truth of an
equality sentence is simply a matter of seeing that the referents of the two terms are the same
object.
The equality symbol can be used to state facts about a given function, as we just did for
the Father symbol. It can also be used with negation to insist that two terms are not the same
object. To say that Richard has at least two brothers, we would write
âˆƒ x, y Brother (x, Richard ) âˆ§ Brother (y, Richard ) âˆ§ Â¬(x = y) .
The sentence
âˆƒ x, y Brother (x, Richard ) âˆ§ Brother (y, Richard )
does not have the intended meaning. In particular, it is true in the model of Figure 8.2, where
Richard has only one brother. To see this, consider the extended interpretation in which both
x and y are assigned to King John. The addition of Â¬(x = y) rules out such models. The
notation x = y is sometimes used as an abbreviation for Â¬(x = y).

8.2.8 An alternative semantics?

UNIQUE-NAMES
ASSUMPTION
CLOSED-WORLD
ASSUMPTION
DOMAIN CLOSURE

Continuing the example from the previous section, suppose that we believe that Richard has
two brothers, John and Geoffrey. 8 Can we capture this state of affairs by asserting
Brother (John, Richard ) âˆ§ Brother (Geoï¬€rey , Richard ) ?
(8.3)
Not quite. First, this assertion is true in a model where Richard has only one brotherâ€”
we need to add John = Geoï¬€rey. Second, the sentence doesnâ€™t rule out models in which
Richard has many more brothers besides John and Geoffrey. Thus, the correct translation of
â€œRichardâ€™s brothers are John and Geoffreyâ€ is as follows:
Brother (John, Richard ) âˆ§ Brother (Geoï¬€rey , Richard ) âˆ§ John = Geoï¬€rey
âˆ§ âˆ€ x Brother (x, Richard ) â‡’ (x = John âˆ¨ x = Geoï¬€rey) .
For many purposes, this seems much more cumbersome than the corresponding naturallanguage expression. As a consequence, humans may make mistakes in translating their
knowledge into first-order logic, resulting in unintuitive behaviors from logical reasoning
systems that use the knowledge. Can we devise a semantics that allows a more straightforward logical expression?
One proposal that is very popular in database systems works as follows. First, we insist
that every constant symbol refer to a distinct objectâ€”the so-called unique-names assumption. Second, we assume that atomic sentences not known to be true are in fact falseâ€”the
closed-world assumption. Finally, we invoke domain closure, meaning that each model
8

Actually he had four, the others being William and Henry.

300

Chapter

R

J

R

R

J

R
J

R

J

R
J

R

J

R
J

J

8.

First-Order Logic

R

...

J

R
J

Figure 8.5 Some members of the set of all models for a language with two constant symbols, R and J, and one binary relation symbol, under database semantics. The interpretation
of the constant symbols is fixed, and there is a distinct object for each constant symbol.

DATABASE
SEMANTICS

8.3

DOMAIN

contains no more domain elements than those named by the constant symbols. Under the
resulting semantics, which we call database semantics to distinguish it from the standard
semantics of first-order logic, the sentence Equation (8.3) does indeed state that Richardâ€™s
two brothers are John and Geoffrey. Database semantics is also used in logic programming
systems, as explained in Section 9.4.5.
It is instructive to consider the set of all possible models under database semantics for
the same case as shown in Figure 8.4. Figure 8.5 shows some of the models, ranging from
the model with no tuples satisfying the relation to the model with all tuples satisfying the
relation. With two objects, there are four possible two-element tuples, so there are 24 = 16
different subsets of tuples that can satisfy the relation. Thus, there are 16 possible models in
allâ€”a lot fewer than the infinitely many models for the standard first-order semantics. On the
other hand, the database semantics requires definite knowledge of what the world contains.
This example brings up an important point: there is no one â€œcorrectâ€ semantics for
logic. The usefulness of any proposed semantics depends on how concise and intuitive it
makes the expression of the kinds of knowledge we want to write down, and on how easy
and natural it is to develop the corresponding rules of inference. Database semantics is most
useful when we are certain about the identity of all the objects described in the knowledge
base and when we have all the facts at hand; in other cases, it is quite awkward. For the rest
of this chapter, we assume the standard semantics while noting instances in which this choice
leads to cumbersome expressions.

U SING F IRST-O RDER L OGIC
Now that we have defined an expressive logical language, it is time to learn how to use it. The
best way to do this is through examples. We have seen some simple sentences illustrating the
various aspects of logical syntax; in this section, we provide more systematic representations
of some simple domains. In knowledge representation, a domain is just some part of the
world about which we wish to express some knowledge.
We begin with a brief description of the T ELL /A SK interface for first-order knowledge
bases. Then we look at the domains of family relationships, numbers, sets, and lists, and at

Section 8.3.

Using First-Order Logic

301

the wumpus world. The next section contains a more substantial example (electronic circuits)
and Chapter 12 covers everything in the universe.

8.3.1 Assertions and queries in first-order logic
ASSERTION

Sentences are added to a knowledge base using T ELL , exactly as in propositional logic. Such
sentences are called assertions. For example, we can assert that John is a king, Richard is a
person, and all kings are persons:
T ELL (KB , King(John)) .
T ELL (KB , Person(Richard )) .
T ELL (KB , âˆ€ x King(x) â‡’ Person(x)) .
We can ask questions of the knowledge base using A SK . For example,
A SK (KB , King(John))

QUERY
GOAL

returns true. Questions asked with A SK are called queries or goals. Generally speaking, any
query that is logically entailed by the knowledge base should be answered affirmatively. For
example, given the two preceding assertions, the query
A SK (KB , Person(John))
should also return true. We can ask quantified queries, such as
A SK (KB , âˆƒ x Person(x)) .
The answer is true, but this is perhaps not as helpful as we would like. It is rather like
answering â€œCan you tell me the time?â€ with â€œYes.â€ If we want to know what value of x
makes the sentence true, we will need a different function, A SK VARS , which we call with
A SK VARS (KB , Person(x))

SUBSTITUTION
BINDING LIST

and which yields a stream of answers. In this case there will be two answers: {x/John} and
{x/Richard }. Such an answer is called a substitution or binding list. A SK VARS is usually
reserved for knowledge bases consisting solely of Horn clauses, because in such knowledge
bases every way of making the query true will bind the variables to specific values. That is
not the case with first-order logic; if KB has been told King(John) âˆ¨ King(Richard ), then
there is no binding to x for the query âˆƒ x King(x), even though the query is true.

8.3.2 The kinship domain
The first example we consider is the domain of family relationships, or kinship. This domain
includes facts such as â€œElizabeth is the mother of Charlesâ€ and â€œCharles is the father of
Williamâ€ and rules such as â€œOneâ€™s grandmother is the mother of oneâ€™s parent.â€
Clearly, the objects in our domain are people. We have two unary predicates, Male and
Female. Kinship relationsâ€”parenthood, brotherhood, marriage, and so onâ€”are represented
by binary predicates: Parent , Sibling, Brother , Sister , Child , Daughter , Son, Spouse,
Wife, Husband , Grandparent , Grandchild , Cousin, Aunt, and Uncle. We use functions
for Mother and Father , because every person has exactly one of each of these (at least
according to natureâ€™s design).

302

Chapter

8.

First-Order Logic

We can go through each function and predicate, writing down what we know in terms
of the other symbols. For example, oneâ€™s mother is oneâ€™s female parent:
âˆ€ m, c Mother (c) = m â‡” Female(m) âˆ§ Parent(m, c) .
Oneâ€™s husband is oneâ€™s male spouse:
âˆ€ w, h Husband (h, w) â‡” Male(h) âˆ§ Spouse(h, w) .
Male and female are disjoint categories:
âˆ€ x Male(x) â‡” Â¬Female(x) .
Parent and child are inverse relations:
âˆ€ p, c Parent(p, c) â‡” Child (c, p) .
A grandparent is a parent of oneâ€™s parent:
âˆ€ g, c Grandparent (g, c) â‡” âˆƒ p Parent (g, p) âˆ§ Parent(p, c) .
A sibling is another child of oneâ€™s parents:
âˆ€ x, y Sibling(x, y) â‡” x = y âˆ§ âˆƒ p Parent (p, x) âˆ§ Parent(p, y) .

DEFINITION

THEOREM

We could go on for several more pages like this, and Exercise 8.14 asks you to do just that.
Each of these sentences can be viewed as an axiom of the kinship domain, as explained
in Section 7.1. Axioms are commonly associated with purely mathematical domainsâ€”we
will see some axioms for numbers shortlyâ€”but they are needed in all domains. They provide
the basic factual information from which useful conclusions can be derived. Our kinship
axioms are also definitions; they have the form âˆ€ x, y P (x, y) â‡” . . .. The axioms define
the Mother function and the Husband , Male, Parent, Grandparent , and Sibling predicates
in terms of other predicates. Our definitions â€œbottom outâ€ at a basic set of predicates (Child ,
Spouse, and Female) in terms of which the others are ultimately defined. This is a natural
way in which to build up the representation of a domain, and it is analogous to the way in
which software packages are built up by successive definitions of subroutines from primitive
library functions. Notice that there is not necessarily a unique set of primitive predicates;
we could equally well have used Parent , Spouse, and Male. In some domains, as we show,
there is no clearly identifiable basic set.
Not all logical sentences about a domain are axioms. Some are theoremsâ€”that is, they
are entailed by the axioms. For example, consider the assertion that siblinghood is symmetric:
âˆ€ x, y Sibling(x, y) â‡” Sibling(y, x) .
Is this an axiom or a theorem? In fact, it is a theorem that follows logically from the axiom
that defines siblinghood. If we A SK the knowledge base this sentence, it should return true.
From a purely logical point of view, a knowledge base need contain only axioms and
no theorems, because the theorems do not increase the set of conclusions that follow from
the knowledge base. From a practical point of view, theorems are essential to reduce the
computational cost of deriving new sentences. Without them, a reasoning system has to start
from first principles every time, rather like a physicist having to rederive the rules of calculus
for every new problem.

Section 8.3.

Using First-Order Logic

303

Not all axioms are definitions. Some provide more general information about certain
predicates without constituting a definition. Indeed, some predicates have no complete definition because we do not know enough to characterize them fully. For example, there is no
obvious definitive way to complete the sentence
âˆ€ x Person(x) â‡” . . .
Fortunately, first-order logic allows us to make use of the Person predicate without completely defining it. Instead, we can write partial specifications of properties that every person
has and properties that make something a person:
âˆ€ x Person(x) â‡’ . . .
âˆ€ x . . . â‡’ Person(x) .
Axioms can also be â€œjust plain facts,â€ such as Male(Jim) and Spouse(Jim, Laura).
Such facts form the descriptions of specific problem instances, enabling specific questions
to be answered. The answers to these questions will then be theorems that follow from
the axioms. Often, one finds that the expected answers are not forthcomingâ€”for example,
from Spouse(Jim, Laura) one expects (under the laws of many countries) to be able to infer
Â¬Spouse(George, Laura); but this does not follow from the axioms given earlierâ€”even after
we add Jim = George as suggested in Section 8.2.8. This is a sign that an axiom is missing.
Exercise 8.8 asks the reader to supply it.

8.3.3 Numbers, sets, and lists
NATURAL NUMBERS

PEANO AXIOMS

INFIX

Numbers are perhaps the most vivid example of how a large theory can be built up from
a tiny kernel of axioms. We describe here the theory of natural numbers or non-negative
integers. We need a predicate NatNum that will be true of natural numbers; we need one
constant symbol, 0; and we need one function symbol, S (successor). The Peano axioms
define natural numbers and addition.9 Natural numbers are defined recursively:
NatNum(0) .
âˆ€ n NatNum(n) â‡’ NatNum(S(n)) .
That is, 0 is a natural number, and for every object n, if n is a natural number, then S(n) is
a natural number. So the natural numbers are 0, S(0), S(S(0)), and so on. (After reading
Section 8.2.8, you will notice that these axioms allow for other natural numbers besides the
usual ones; see Exercise 8.12.) We also need axioms to constrain the successor function:
âˆ€ n 0 = S(n) .
âˆ€ m, n m = n â‡’ S(m) = S(n) .
Now we can define addition in terms of the successor function:
âˆ€ m NatNum(m) â‡’ + (0, m) = m .
âˆ€ m, n NatNum(m) âˆ§ NatNum(n) â‡’ + (S(m), n) = S(+(m, n)) .
The first of these axioms says that adding 0 to any natural number m gives m itself. Notice
the use of the binary function symbol â€œ+â€ in the term +(m, 0); in ordinary mathematics, the
term would be written m + 0 using infix notation. (The notation we have used for first-order
9

The Peano axioms also include the principle of induction, which is a sentence of second-order logic rather
than of first-order logic. The importance of this distinction is explained in Chapter 9.

304
PREFIX

Chapter

8.

First-Order Logic

logic is called prefix.) To make our sentences about numbers easier to read, we allow the use
of infix notation. We can also write S(n) as n + 1, so the second axiom becomes
âˆ€ m, n NatNum(m) âˆ§ NatNum(n) â‡’ (m + 1) + n = (m + n) + 1 .

SYNTACTIC SUGAR

SET

This axiom reduces addition to repeated application of the successor function.
The use of infix notation is an example of syntactic sugar, that is, an extension to or
abbreviation of the standard syntax that does not change the semantics. Any sentence that
uses sugar can be â€œdesugaredâ€ to produce an equivalent sentence in ordinary first-order logic.
Once we have addition, it is straightforward to define multiplication as repeated addition, exponentiation as repeated multiplication, integer division and remainders, prime numbers, and so on. Thus, the whole of number theory (including cryptography) can be built up
from one constant, one function, one predicate and four axioms.
The domain of sets is also fundamental to mathematics as well as to commonsense
reasoning. (In fact, it is possible to define number theory in terms of set theory.) We want to
be able to represent individual sets, including the empty set. We need a way to build up sets
by adding an element to a set or taking the union or intersection of two sets. We will want
to know whether an element is a member of a set and we will want to distinguish sets from
objects that are not sets.
We will use the normal vocabulary of set theory as syntactic sugar. The empty set is a
constant written as { }. There is one unary predicate, Set, which is true of sets. The binary
predicates are x âˆˆ s (x is a member of set s) and s1 âŠ† s2 (set s1 is a subset, not necessarily
proper, of set s2 ). The binary functions are s1 âˆ© s2 (the intersection of two sets), s1 âˆª s2
(the union of two sets), and {x|s} (the set resulting from adjoining element x to set s). One
possible set of axioms is as follows:
1. The only sets are the empty set and those made by adjoining something to a set:
âˆ€ s Set(s) â‡” (s = { }) âˆ¨ (âˆƒ x, s2 Set(s2 ) âˆ§ s = {x|s2 }) .
2. The empty set has no elements adjoined into it. In other words, there is no way to
decompose { } into a smaller set and an element:
Â¬âˆƒ x, s {x|s} = { } .
3. Adjoining an element already in the set has no effect:
âˆ€ x, s x âˆˆ s â‡” s = {x|s} .
4. The only members of a set are the elements that were adjoined into it. We express
this recursively, saying that x is a member of s if and only if s is equal to some set s2
adjoined with some element y, where either y is the same as x or x is a member of s2 :
âˆ€ x, s x âˆˆ s â‡” âˆƒ y, s2 (s = {y|s2 } âˆ§ (x = y âˆ¨ x âˆˆ s2 )) .
5. A set is a subset of another set if and only if all of the first setâ€™s members are members
of the second set:
âˆ€ s1 , s2 s1 âŠ† s2 â‡” (âˆ€ x x âˆˆ s1 â‡’ x âˆˆ s2 ) .
6. Two sets are equal if and only if each is a subset of the other:
âˆ€ s1 , s2 (s1 = s2 ) â‡” (s1 âŠ† s2 âˆ§ s2 âŠ† s1 ) .

Section 8.3.

Using First-Order Logic

305

7. An object is in the intersection of two sets if and only if it is a member of both sets:
âˆ€ x, s1 , s2 x âˆˆ (s1 âˆ© s2 ) â‡” (x âˆˆ s1 âˆ§ x âˆˆ s2 ) .
8. An object is in the union of two sets if and only if it is a member of either set:
âˆ€ x, s1 , s2 x âˆˆ (s1 âˆª s2 ) â‡” (x âˆˆ s1 âˆ¨ x âˆˆ s2 ) .
LIST

Lists are similar to sets. The differences are that lists are ordered and the same element can
appear more than once in a list. We can use the vocabulary of Lisp for lists: Nil is the constant
list with no elements; Cons, Append , First, and Rest are functions; and Find is the predicate that does for lists what Member does for sets. List? is a predicate that is true only of
lists. As with sets, it is common to use syntactic sugar in logical sentences involving lists. The
empty list is [ ]. The term Cons(x, y), where y is a nonempty list, is written [x|y]. The term
Cons(x, Nil ) (i.e., the list containing the element x) is written as [x]. A list of several elements, such as [A, B, C], corresponds to the nested term Cons(A, Cons(B, Cons(C, Nil ))).
Exercise 8.16 asks you to write out the axioms for lists.

8.3.4 The wumpus world
Some propositional logic axioms for the wumpus world were given in Chapter 7. The firstorder axioms in this section are much more concise, capturing in a natural way exactly what
we want to say.
Recall that the wumpus agent receives a percept vector with five elements. The corresponding first-order sentence stored in the knowledge base must include both the percept and
the time at which it occurred; otherwise, the agent will get confused about when it saw what.
We use integers for time steps. A typical percept sentence would be
Percept ([Stench, Breeze, Glitter , None, None], 5) .
Here, Percept is a binary predicate, and Stench and so on are constants placed in a list. The
actions in the wumpus world can be represented by logical terms:
Turn(Right), Turn(Left ), Forward , Shoot , Grab, Climb .
To determine which is best, the agent program executes the query
A SK VARS (âˆƒ a BestAction(a, 5)) ,
which returns a binding list such as {a/Grab}. The agent program can then return Grab as
the action to take. The raw percept data implies certain facts about the current state. For
example:
âˆ€ t, s, g, m, c Percept ([s, Breeze, g, m, c], t) â‡’ Breeze(t) ,
âˆ€ t, s, b, m, c Percept([s, b, Glitter , m, c], t) â‡’ Glitter (t) ,
and so on. These rules exhibit a trivial form of the reasoning process called perception, which
we study in depth in Chapter 24. Notice the quantification over time t. In propositional logic,
we would need copies of each sentence for each time step.
Simple â€œreflexâ€ behavior can also be implemented by quantified implication sentences.
For example, we have
âˆ€ t Glitter (t) â‡’ BestAction(Grab, t) .

306

Chapter

8.

First-Order Logic

Given the percept and rules from the preceding paragraphs, this would yield the desired conclusion BestAction(Grab, 5)â€”that is, Grab is the right thing to do.
We have represented the agentâ€™s inputs and outputs; now it is time to represent the
environment itself. Let us begin with objects. Obvious candidates are squares, pits, and the
wumpus. We could name each squareâ€”Square 1,2 and so onâ€”but then the fact that Square 1,2
and Square 1,3 are adjacent would have to be an â€œextraâ€ fact, and we would need one such
fact for each pair of squares. It is better to use a complex term in which the row and column
appear as integers; for example, we can simply use the list term [1, 2]. Adjacency of any two
squares can be defined as
âˆ€ x, y, a, b Adjacent([x, y], [a, b]) â‡”
(x = a âˆ§ (y = b âˆ’ 1 âˆ¨ y = b + 1)) âˆ¨ (y = b âˆ§ (x = a âˆ’ 1 âˆ¨ x = a + 1)) .
We could name each pit, but this would be inappropriate for a different reason: there is no
reason to distinguish among pits.10 It is simpler to use a unary predicate Pit that is true of
squares containing pits. Finally, since there is exactly one wumpus, a constant Wumpus is
just as good as a unary predicate (and perhaps more dignified from the wumpusâ€™s viewpoint).
The agentâ€™s location changes over time, so we write At(Agent, s, t) to mean that the
agent is at square s at time t. We can fix the wumpusâ€™s location with âˆ€t At(Wumpus, [2, 2], t).
We can then say that objects can only be at one location at a time:
âˆ€ x, s1 , s2 , t At(x, s1 , t) âˆ§ At(x, s2 , t) â‡’ s1 = s2 .
Given its current location, the agent can infer properties of the square from properties of its
current percept. For example, if the agent is at a square and perceives a breeze, then that
square is breezy:
âˆ€ s, t At(Agent, s, t) âˆ§ Breeze(t) â‡’ Breezy(s) .
It is useful to know that a square is breezy because we know that the pits cannot move about.
Notice that Breezy has no time argument.
Having discovered which places are breezy (or smelly) and, very important, not breezy
(or not smelly), the agent can deduce where the pits are (and where the wumpus is). Whereas
propositional logic necessitates a separate axiom for each square (see R2 and R3 on page 247)
and would need a different set of axioms for each geographical layout of the world, first-order
logic just needs one axiom:
âˆ€ s Breezy(s) â‡” âˆƒ r Adjacent (r, s) âˆ§ Pit(r) .

(8.4)

Similarly, in first-order logic we can quantify over time, so we need just one successor-state
axiom for each predicate, rather than a different copy for each time step. For example, the
axiom for the arrow (Equation (7.2) on page 267) becomes
âˆ€ t HaveArrow (t + 1) â‡” (HaveArrow (t) âˆ§ Â¬Action(Shoot , t)) .
From these two example sentences, we can see that the first-order logic formulation is no
less concise than the original English-language description given in Chapter 7. The reader
10 Similarly, most of us do not name each bird that flies overhead as it migrates to warmer regions in winter. An
ornithologist wishing to study migration patterns, survival rates, and so on does name each bird, by means of a
ring on its leg, because individual birds must be tracked.

Section 8.4.

Knowledge Engineering in First-Order Logic

307

is invited to construct analogous axioms for the agentâ€™s location and orientation; in these
cases, the axioms quantify over both space and time. As in the case of propositional state
estimation, an agent can use logical inference with axioms of this kind to keep track of aspects
of the world that are not directly observed. Chapter 10 goes into more depth on the subject of
first-order successor-state axioms and their uses for constructing plans.

8.4

K NOWLEDGE E NGINEERING IN F IRST-O RDER L OGIC

KNOWLEDGE
ENGINEERING

The preceding section illustrated the use of first-order logic to represent knowledge in three
simple domains. This section describes the general process of knowledge-base constructionâ€”
a process called knowledge engineering. A knowledge engineer is someone who investigates
a particular domain, learns what concepts are important in that domain, and creates a formal
representation of the objects and relations in the domain. We illustrate the knowledge engineering process in an electronic circuit domain that should already be fairly familiar, so that
we can concentrate on the representational issues involved. The approach we take is suitable
for developing special-purpose knowledge bases whose domain is carefully circumscribed
and whose range of queries is known in advance. General-purpose knowledge bases, which
cover a broad range of human knowledge and are intended to support tasks such as natural
language understanding, are discussed in Chapter 12.

8.4.1 The knowledge-engineering process
Knowledge engineering projects vary widely in content, scope, and difficulty, but all such
projects include the following steps:
1. Identify the task. The knowledge engineer must delineate the range of questions that
the knowledge base will support and the kinds of facts that will be available for each
specific problem instance. For example, does the wumpus knowledge base need to be
able to choose actions or is it required to answer questions only about the contents
of the environment? Will the sensor facts include the current location? The task will
determine what knowledge must be represented in order to connect problem instances to
answers. This step is analogous to the PEAS process for designing agents in Chapter 2.

KNOWLEDGE
ACQUISITION

2. Assemble the relevant knowledge. The knowledge engineer might already be an expert
in the domain, or might need to work with real experts to extract what they knowâ€”a
process called knowledge acquisition. At this stage, the knowledge is not represented
formally. The idea is to understand the scope of the knowledge base, as determined by
the task, and to understand how the domain actually works.
For the wumpus world, which is defined by an artificial set of rules, the relevant
knowledge is easy to identify. (Notice, however, that the definition of adjacency was
not supplied explicitly in the wumpus-world rules.) For real domains, the issue of
relevance can be quite difficultâ€”for example, a system for simulating VLSI designs
might or might not need to take into account stray capacitances and skin effects.

308

ONTOLOGY

Chapter

8.

First-Order Logic

3. Decide on a vocabulary of predicates, functions, and constants. That is, translate the
important domain-level concepts into logic-level names. This involves many questions
of knowledge-engineering style. Like programming style, this can have a significant
impact on the eventual success of the project. For example, should pits be represented
by objects or by a unary predicate on squares? Should the agentâ€™s orientation be a
function or a predicate? Should the wumpusâ€™s location depend on time? Once the
choices have been made, the result is a vocabulary that is known as the ontology of
the domain. The word ontology means a particular theory of the nature of being or
existence. The ontology determines what kinds of things exist, but does not determine
their specific properties and interrelationships.
4. Encode general knowledge about the domain. The knowledge engineer writes down
the axioms for all the vocabulary terms. This pins down (to the extent possible) the
meaning of the terms, enabling the expert to check the content. Often, this step reveals
misconceptions or gaps in the vocabulary that must be fixed by returning to step 3 and
iterating through the process.
5. Encode a description of the specific problem instance. If the ontology is well thought
out, this step will be easy. It will involve writing simple atomic sentences about instances of concepts that are already part of the ontology. For a logical agent, problem
instances are supplied by the sensors, whereas a â€œdisembodiedâ€ knowledge base is supplied with additional sentences in the same way that traditional programs are supplied
with input data.
6. Pose queries to the inference procedure and get answers. This is where the reward is:
we can let the inference procedure operate on the axioms and problem-specific facts to
derive the facts we are interested in knowing. Thus, we avoid the need for writing an
application-specific solution algorithm.
7. Debug the knowledge base. Alas, the answers to queries will seldom be correct on
the first try. More precisely, the answers will be correct for the knowledge base as
written, assuming that the inference procedure is sound, but they will not be the ones
that the user is expecting. For example, if an axiom is missing, some queries will not be
answerable from the knowledge base. A considerable debugging process could ensue.
Missing axioms or axioms that are too weak can be easily identified by noticing places
where the chain of reasoning stops unexpectedly. For example, if the knowledge base
includes a diagnostic rule (see Exercise 8.13) for finding the wumpus,
âˆ€ s Smelly(s) â‡’ Adjacent (Home(Wumpus), s) ,
instead of the biconditional, then the agent will never be able to prove the absence of
wumpuses. Incorrect axioms can be identified because they are false statements about
the world. For example, the sentence
âˆ€ x NumOfLegs(x, 4) â‡’ Mammal (x)
is false for reptiles, amphibians, and, more importantly, tables. The falsehood of this
sentence can be determined independently of the rest of the knowledge base. In contrast,

Section 8.4.

Knowledge Engineering in First-Order Logic

309

a typical error in a program looks like this:
offset = position + 1 .
It is impossible to tell whether this statement is correct without looking at the rest of the
program to see whether, for example, offset is used to refer to the current position,
or to one beyond the current position, or whether the value of position is changed
by another statement and so offset should also be changed again.
To understand this seven-step process better, we now apply it to an extended exampleâ€”the
domain of electronic circuits.

8.4.2 The electronic circuits domain
We will develop an ontology and knowledge base that allow us to reason about digital circuits
of the kind shown in Figure 8.6. We follow the seven-step process for knowledge engineering.
Identify the task
There are many reasoning tasks associated with digital circuits. At the highest level, one
analyzes the circuitâ€™s functionality. For example, does the circuit in Figure 8.6 actually add
properly? If all the inputs are high, what is the output of gate A2? Questions about the
circuitâ€™s structure are also interesting. For example, what are all the gates connected to the
first input terminal? Does the circuit contain feedback loops? These will be our tasks in this
section. There are more detailed levels of analysis, including those related to timing delays,
circuit area, power consumption, production cost, and so on. Each of these levels would
require additional knowledge.
Assemble the relevant knowledge
What do we know about digital circuits? For our purposes, they are composed of wires and
gates. Signals flow along wires to the input terminals of gates, and each gate produces a
C1
1
2

X1

3

X2

1

A2
A1

O1

2

Figure 8.6 A digital circuit C1, purporting to be a one-bit full adder. The first two inputs
are the two bits to be added, and the third input is a carry bit. The first output is the sum, and
the second output is a carry bit for the next adder. The circuit contains two XOR gates, two
AND gates, and one OR gate.

310

Chapter

8.

First-Order Logic

signal on the output terminal that flows along another wire. To determine what these signals
will be, we need to know how the gates transform their input signals. There are four types
of gates: AND, OR, and XOR gates have two input terminals, and NOT gates have one. All
gates have one output terminal. Circuits, like gates, have input and output terminals.
To reason about functionality and connectivity, we do not need to talk about the wires
themselves, the paths they take, or the junctions where they come together. All that matters
is the connections between terminalsâ€”we can say that one output terminal is connected to
another input terminal without having to say what actually connects them. Other factors such
as the size, shape, color, or cost of the various components are irrelevant to our analysis.
If our purpose were something other than verifying designs at the gate level, the ontology would be different. For example, if we were interested in debugging faulty circuits, then
it would probably be a good idea to include the wires in the ontology, because a faulty wire
can corrupt the signal flowing along it. For resolving timing faults, we would need to include
gate delays. If we were interested in designing a product that would be profitable, then the
cost of the circuit and its speed relative to other products on the market would be important.
Decide on a vocabulary
We now know that we want to talk about circuits, terminals, signals, and gates. The next step
is to choose functions, predicates, and constants to represent them. First, we need to be able
to distinguish gates from each other and from other objects. Each gate is represented as an
object named by a constant, about which we assert that it is a gate with, say, Gate(X1 ). The
behavior of each gate is determined by its type: one of the constants AN D, OR, XOR, or
N OT . Because a gate has exactly one type, a function is appropriate: Type(X1 ) = XOR.
Circuits, like gates, are identified by a predicate: Circuit(C1 ).
Next we consider terminals, which are identified by the predicate Terminal (x). A gate
or circuit can have one or more input terminals and one or more output terminals. We use the
function In(1, X1 ) to denote the first input terminal for gate X1 . A similar function Out is
used for output terminals. The function Arity(c, i, j) says that circuit c has i input and j output terminals. The connectivity between gates can be represented by a predicate, Connected ,
which takes two terminals as arguments, as in Connected (Out(1, X1 ), In(1, X2 )).
Finally, we need to know whether a signal is on or off. One possibility is to use a unary
predicate, On(t), which is true when the signal at a terminal is on. This makes it a little
difficult, however, to pose questions such as â€œWhat are all the possible values of the signals
at the output terminals of circuit C1 ?â€ We therefore introduce as objects two signal values, 1
and 0, and a function Signal (t) that denotes the signal value for the terminal t.
Encode general knowledge of the domain
One sign that we have a good ontology is that we require only a few general rules, which can
be stated clearly and concisely. These are all the axioms we will need:
1. If two terminals are connected, then they have the same signal:
âˆ€ t1 , t2 Terminal (t1 ) âˆ§ Terminal (t2 ) âˆ§ Connected (t1 , t2 ) â‡’
Signal (t1 ) = Signal (t2 ) .

Section 8.4.

Knowledge Engineering in First-Order Logic

311

2. The signal at every terminal is either 1 or 0:
âˆ€ t Terminal (t) â‡’ Signal (t) = 1 âˆ¨ Signal (t) = 0 .
3. Connected is commutative:
âˆ€ t1 , t2 Connected (t1 , t2 ) â‡” Connected (t2 , t1 ) .
4. There are four types of gates:
âˆ€ g Gate(g) âˆ§ k = Type(g) â‡’ k = AND âˆ¨ k = OR âˆ¨ k = XOR âˆ¨ k = NOT .
5. An AND gateâ€™s output is 0 if and only if any of its inputs is 0:
âˆ€ g Gate(g) âˆ§ Type(g) = AND â‡’
Signal (Out(1, g)) = 0 â‡” âˆƒ n Signal (In(n, g)) = 0 .
6. An OR gateâ€™s output is 1 if and only if any of its inputs is 1:
âˆ€ g Gate(g) âˆ§ Type(g) = OR â‡’
Signal (Out(1, g)) = 1 â‡” âˆƒ n Signal (In(n, g)) = 1 .
7. An XOR gateâ€™s output is 1 if and only if its inputs are different:
âˆ€ g Gate(g) âˆ§ Type(g) = XOR â‡’
Signal (Out(1, g)) = 1 â‡” Signal (In(1, g)) = Signal (In(2, g)) .
8. A NOT gateâ€™s output is different from its input:
âˆ€ g Gate(g) âˆ§ (Type(g) = NOT ) â‡’
Signal (Out(1, g)) = Signal (In(1, g)) .
9. The gates (except for NOT) have two inputs and one output.
âˆ€ g Gate(g) âˆ§ Type(g) = NOT â‡’ Arity(g, 1, 1) .
âˆ€ g Gate(g) âˆ§ k = Type(g) âˆ§ (k = AND âˆ¨ k = OR âˆ¨ k = XOR) â‡’
Arity(g, 2, 1)
10. A circuit has terminals, up to its input and output arity, and nothing beyond its arity:
âˆ€ c, i, j Circuit(c) âˆ§ Arity(c, i, j) â‡’
âˆ€ n (n â‰¤ i â‡’ Terminal (In(c, n))) âˆ§ (n > i â‡’ In(c, n) = Nothing) âˆ§
âˆ€ n (n â‰¤ j â‡’ Terminal (Out(c, n))) âˆ§ (n > j â‡’ Out(c, n) = Nothing)
11. Gates, terminals, signals, gate types, and Nothing are all distinct.
âˆ€ g, t Gate(g) âˆ§ Terminal (t) â‡’
g = t = 1 = 0 = OR = AND = XOR = NOT = Nothing .
12. Gates are circuits.
âˆ€ g Gate(g) â‡’ Circuit(g)
Encode the specific problem instance
The circuit shown in Figure 8.6 is encoded as circuit C1 with the following description. First,
we categorize the circuit and its component gates:
Circuit(C1 ) âˆ§ Arity(C1 , 3, 2)
Gate(X1 ) âˆ§ Type(X1 ) = XOR
Gate(X2 ) âˆ§ Type(X2 ) = XOR
Gate(A1 ) âˆ§ Type(A1 ) = AND
Gate(A2 ) âˆ§ Type(A2 ) = AND
Gate(O1 ) âˆ§ Type(O1 ) = OR .

312

Chapter

8.

First-Order Logic

Then, we show the connections between them:
Connected (Out(1, X1 ), In(1, X2 )) Connected (In(1, C1 ), In(1, X1 ))
Connected (Out(1, X1 ), In(2, A2 )) Connected (In(1, C1 ), In(1, A1 ))
Connected (Out(1, A2 ), In(1, O1 )) Connected (In(2, C1 ), In(2, X1 ))
Connected (Out(1, A1 ), In(2, O1 )) Connected (In(2, C1 ), In(2, A1 ))
Connected (Out(1, X2 ), Out(1, C1 )) Connected (In(3, C1 ), In(2, X2 ))
Connected (Out(1, O1 ), Out(2, C1 )) Connected (In(3, C1 ), In(1, A2 )) .
Pose queries to the inference procedure
What combinations of inputs would cause the first output of C1 (the sum bit) to be 0 and the
second output of C1 (the carry bit) to be 1?
âˆƒ i1 , i2 , i3 Signal (In(1, C1 )) = i1 âˆ§ Signal (In(2, C1 )) = i2 âˆ§ Signal (In(3, C1 )) = i3
âˆ§ Signal (Out(1, C1 )) = 0 âˆ§ Signal (Out(2, C1 )) = 1 .

The answers are substitutions for the variables i1 , i2 , and i3 such that the resulting sentence
is entailed by the knowledge base. A SK VARS will give us three such substitutions:
{i1 /1, i2 /1, i3 /0}

{i1 /1, i2 /0, i3 /1}

{i1 /0, i2 /1, i3 /1} .

What are the possible sets of values of all the terminals for the adder circuit?
âˆƒ i1 , i2 , i3 , o1 , o2 Signal (In(1, C1 )) = i1 âˆ§ Signal (In(2, C1 )) = i2
âˆ§ Signal (In(3, C1 )) = i3 âˆ§ Signal (Out(1, C1 )) = o1 âˆ§ Signal (Out(2, C1 )) = o2 .

CIRCUIT
VERIFICATION

This final query will return a complete inputâ€“output table for the device, which can be used
to check that it does in fact add its inputs correctly. This is a simple example of circuit
verification. We can also use the definition of the circuit to build larger digital systems, for
which the same kind of verification procedure can be carried out. (See Exercise 8.26.) Many
domains are amenable to the same kind of structured knowledge-base development, in which
more complex concepts are defined on top of simpler concepts.
Debug the knowledge base
We can perturb the knowledge base in various ways to see what kinds of erroneous behaviors
emerge. For example, suppose we fail to read Section 8.2.8 and hence forget to assert that
1 = 0. Suddenly, the system will be unable to prove any outputs for the circuit, except for
the input cases 000 and 110. We can pinpoint the problem by asking for the outputs of each
gate. For example, we can ask
âˆƒ i1 , i2 , o Signal (In(1, C1 )) = i1 âˆ§ Signal (In(2, C1 )) = i2 âˆ§ Signal (Out(1, X1 )) ,
which reveals that no outputs are known at X1 for the input cases 10 and 01. Then, we look
at the axiom for XOR gates, as applied to X1 :
Signal (Out(1, X1 )) = 1 â‡” Signal (In(1, X1 )) = Signal (In(2, X1 )) .
If the inputs are known to be, say, 1 and 0, then this reduces to
Signal (Out(1, X1 )) = 1 â‡” 1 = 0 .
Now the problem is apparent: the system is unable to infer that Signal (Out(1, X1 )) = 1, so
we need to tell it that 1 = 0.

Section 8.5.

8.5

Summary

313

S UMMARY
This chapter has introduced first-order logic, a representation language that is far more powerful than propositional logic. The important points are as follows:
â€¢ Knowledge representation languages should be declarative, compositional, expressive,
context independent, and unambiguous.
â€¢ Logics differ in their ontological commitments and epistemological commitments.
While propositional logic commits only to the existence of facts, first-order logic commits to the existence of objects and relations and thereby gains expressive power.
â€¢ The syntax of first-order logic builds on that of propositional logic. It adds terms to
represent objects, and has universal and existential quantifiers to construct assertions
about all or some of the possible values of the quantified variables.
â€¢ A possible world, or model, for first-order logic includes a set of objects and an interpretation that maps constant symbols to objects, predicate symbols to relations among
objects, and function symbols to functions on objects.
â€¢ An atomic sentence is true just when the relation named by the predicate holds between
the objects named by the terms. Extended interpretations, which map quantifier variables to objects in the model, define the truth of quantified sentences.
â€¢ Developing a knowledge base in first-order logic requires a careful process of analyzing
the domain, choosing a vocabulary, and encoding the axioms required to support the
desired inferences.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Although Aristotleâ€™s logic deals with generalizations over objects, it fell far short of the expressive power of first-order logic. A major barrier to its further development was its concentration on one-place predicates to the exclusion of many-place relational predicates. The first
systematic treatment of relations was given by Augustus De Morgan (1864), who cited the
following example to show the sorts of inferences that Aristotleâ€™s logic could not handle: â€œAll
horses are animals; therefore, the head of a horse is the head of an animal.â€ This inference
is inaccessible to Aristotle because any valid rule that can support this inference must first
analyze the sentence using the two-place predicate â€œx is the head of y.â€ The logic of relations
was studied in depth by Charles Sanders Peirce (1870, 2004).
True first-order logic dates from the introduction of quantifiers in Gottlob Fregeâ€™s (1879)
Begriffschrift (â€œConcept Writingâ€ or â€œConceptual Notationâ€). Peirce (1883) also developed
first-order logic independently of Frege, although slightly later. Fregeâ€™s ability to nest quantifiers was a big step forward, but he used an awkward notation. The present notation for
first-order logic is due substantially to Giuseppe Peano (1889), but the semantics is virtually
identical to Fregeâ€™s. Oddly enough, Peanoâ€™s axioms were due in large measure to Grassmann
(1861) and Dedekind (1888).

314

Chapter

8.

First-Order Logic

Leopold LoÌˆwenheim (1915) gave a systematic treatment of model theory for first-order
logic, including the first proper treatment of the equality symbol. LoÌˆwenheimâ€™s results were
further extended by Thoralf Skolem (1920). Alfred Tarski (1935, 1956) gave an explicit
definition of truth and model-theoretic satisfaction in first-order logic, using set theory.
McCarthy (1958) was primarily responsible for the introduction of first-order logic as a
tool for building AI systems. The prospects for logic-based AI were advanced significantly by
Robinsonâ€™s (1965) development of resolution, a complete procedure for first-order inference
described in Chapter 9. The logicist approach took root at Stanford University. Cordell Green
(1969a, 1969b) developed a first-order reasoning system, QA3, leading to the first attempts to
build a logical robot at SRI (Fikes and Nilsson, 1971). First-order logic was applied by Zohar
Manna and Richard Waldinger (1971) for reasoning about programs and later by Michael
Genesereth (1984) for reasoning about circuits. In Europe, logic programming (a restricted
form of first-order reasoning) was developed for linguistic analysis (Colmerauer et al., 1973)
and for general declarative systems (Kowalski, 1974). Computational logic was also well
entrenched at Edinburgh through the LCF (Logic for Computable Functions) project (Gordon
et al., 1979). These developments are chronicled further in Chapters 9 and 12.
Practical applications built with first-order logic include a system for evaluating the
manufacturing requirements for electronic products (Mannion, 2002), a system for reasoning
about policies for file access and digital rights management (Halpern and Weissman, 2008),
and a system for the automated composition of Web services (McIlraith and Zeng, 2001).
Reactions to the Whorf hypothesis (Whorf, 1956) and the problem of language and
thought in general, appear in several recent books (Gumperz and Levinson, 1996; Bowerman
and Levinson, 2001; Pinker, 2003; Gentner and Goldin-Meadow, 2003). The â€œtheoryâ€ theory
(Gopnik and Glymour, 2002; Tenenbaum et al., 2007) views childrenâ€™s learning about the
world as analogous to the construction of scientific theories. Just as the predictions of a
machine learning algorithm depend strongly on the vocabulary supplied to it, so will the
childâ€™s formulation of theories depend on the linguistic environment in which learning occurs.
There are a number of good introductory texts on first-order logic, including some by
leading figures in the history of logic: Alfred Tarski (1941), Alonzo Church (1956), and
W.V. Quine (1982) (which is one of the most readable). Enderton (1972) gives a more mathematically oriented perspective. A highly formal treatment of first-order logic, along with
many more advanced topics in logic, is provided by Bell and Machover (1977). Manna and
Waldinger (1985) give a readable introduction to logic from a computer science perspective, as do Huth and Ryan (2004), who concentrate on program verification. Barwise and
Etchemendy (2002) take an approach similar to the one used here. Smullyan (1995) presents
results concisely, using the tableau format. Gallier (1986) provides an extremely rigorous
mathematical exposition of first-order logic, along with a great deal of material on its use in
automated reasoning. Logical Foundations of Artificial Intelligence (Genesereth and Nilsson,
1987) is both a solid introduction to logic and the first systematic treatment of logical agents
with percepts and actions, and there are two good handbooks: van Bentham and ter Meulen
(1997) and Robinson and Voronkov (2001). The journal of record for the field of pure mathematical logic is the Journal of Symbolic Logic, whereas the Journal of Applied Logic deals
with concerns closer to those of artificial intelligence.

Exercises

315

E XERCISES
8.1 A logical knowledge base represents the world using a set of sentences with no explicit
structure. An analogical representation, on the other hand, has physical structure that corresponds directly to the structure of the thing represented. Consider a road map of your country
as an analogical representation of facts about the countryâ€”it represents facts with a map language. The two-dimensional structure of the map corresponds to the two-dimensional surface
of the area.
a. Give five examples of symbols in the map language.
b. An explicit sentence is a sentence that the creator of the representation actually writes
down. An implicit sentence is a sentence that results from explicit sentences because
of properties of the analogical representation. Give three examples each of implicit and
explicit sentences in the map language.
c. Give three examples of facts about the physical structure of your country that cannot be
represented in the map language.
d. Give two examples of facts that are much easier to express in the map language than in
first-order logic.
e. Give two other examples of useful analogical representations. What are the advantages
and disadvantages of each of these languages?
8.2 Consider a knowledge base containing just two sentences: P (a) and P (b). Does this
knowledge base entail âˆ€ x P (x)? Explain your answer in terms of models.
8.3

Is the sentence âˆƒ x, y x = y valid? Explain.

8.4 Write down a logical sentence such that every world in which it is true contains exactly
one object.
8.5 Consider a symbol vocabulary that contains c constant symbols, pk predicate symbols
of each arity k, and fk function symbols of each arity k, where 1 â‰¤ k â‰¤ A. Let the domain
size be fixed at D. For any given model, each predicate or function symbol is mapped onto a
relation or function, respectively, of the same arity. You may assume that the functions in the
model allow some input tuples to have no value for the function (i.e., the value is the invisible
object). Derive a formula for the number of possible models for a domain with D elements.
Donâ€™t worry about eliminating redundant combinations.
8.6

Which of the following are valid (necessarily true) sentences?

a. (âˆƒx x = x) â‡’ (âˆ€ y âˆƒz y = z).
b. âˆ€ x P (x) âˆ¨ Â¬P (x).
c. âˆ€ x Smart (x) âˆ¨ (x = x).
8.7 Consider a version of the semantics for first-order logic in which models with empty
domains are allowed. Give at least two examples of sentences that are valid according to the

316

Chapter

8.

First-Order Logic

standard semantics but not according to the new semantics. Discuss which outcome makes
more intuitive sense for your examples.
8.8 Does the fact Â¬Spouse(George, Laura ) follow from the facts Jim = George and
Spouse(Jim, Laura)? If so, give a proof; if not, supply additional axioms as needed. What
happens if we use Spouse as a unary function symbol instead of a binary predicate?
8.9 This exercise uses the function MapColor and predicates In(x, y), Borders (x, y), and
Country(x), whose arguments are geographical regions, along with constant symbols for
various regions. In each of the following we give an English sentence and a number of candidate logical expressions. For each of the logical expressions, state whether it (1) correctly
expresses the English sentence; (2) is syntactically invalid and therefore meaningless; or (3)
is syntactically valid but does not express the meaning of the English sentence.
a. Paris and Marseilles are both in France.
(i) In(Paris âˆ§ Marseilles, France ).
(ii) In(Paris, France ) âˆ§ In(Marseilles, France ).
(iii) In(Paris, France ) âˆ¨ In(Marseilles, France ).
b. There is a country that borders both Iraq and Pakistan.
(i)
(ii)
(iii)
(iv)

âˆƒc
âˆƒc
[âˆƒ c
âˆƒc

Country (c) âˆ§ Border (c, Iraq ) âˆ§ Border (c, Pakistan).
Country (c) â‡’ [Border (c, Iraq ) âˆ§ Border (c, Pakistan)].
Country(c)] â‡’ [Border (c, Iraq ) âˆ§ Border (c, Pakistan)].
Border (Country(c), Iraq âˆ§ Pakistan).

c. All countries that border Ecuador are in South America.
(i)
(ii)
(iii)
(iv)

âˆ€c
âˆ€c
âˆ€c
âˆ€c

Country(c) âˆ§ Border (c, Ecuador ) â‡’ In(c, SouthAmerica).
Country(c) â‡’ [Border (c, Ecuador ) â‡’ In(c, SouthAmerica)].
[Country(c) â‡’ Border (c, Ecuador )] â‡’ In(c, SouthAmerica).
Country(c) âˆ§ Border (c, Ecuador ) âˆ§ In(c, SouthAmerica).

d. No region in South America borders any region in Europe.
(i)
(ii)
(iii)
(iv)

Â¬[âˆƒ c, d In(c, SouthAmerica ) âˆ§ In(d, Europe ) âˆ§ Borders(c, d)].
âˆ€ c, d [In(c, SouthAmerica) âˆ§ In(d, Europe )] â‡’ Â¬Borders(c, d)].
Â¬âˆ€ c In(c, SouthAmerica ) â‡’ âˆƒ d In(d, Europe ) âˆ§ Â¬Borders(c, d).
âˆ€ c In(c, SouthAmerica) â‡’ âˆ€ d In(d, Europe ) â‡’ Â¬Borders(c, d).

e. No two adjacent countries have the same map color.
(i) âˆ€ x, y Â¬Country(x) âˆ¨ Â¬Country(y) âˆ¨ Â¬Borders(x, y) âˆ¨
Â¬(MapColor (x) = MapColor (y)).
(ii) âˆ€ x, y (Country(x) âˆ§ Country(y) âˆ§ Borders (x, y) âˆ§ Â¬(x = y)) â‡’
Â¬(MapColor (x) = MapColor (y)).
(iii) âˆ€ x, y Country(x) âˆ§ Country(y) âˆ§ Borders(x, y) âˆ§
Â¬(MapColor (x) = MapColor (y)).
(iv) âˆ€ x, y (Country(x) âˆ§ Country(y) âˆ§ Borders (x, y)) â‡’ MapColor (x = y).

Exercises

317
8.10

Consider a vocabulary with the following symbols:
Occupation(p, o): Predicate. Person p has occupation o.
Customer (p1, p2): Predicate. Person p1 is a customer of person p2.
Boss(p1, p2): Predicate. Person p1 is a boss of person p2.
Doctor , Surgeon, Lawyer , Actor : Constants denoting occupations.
Emily, Joe: Constants denoting people.

Use these symbols to write the following assertions in first-order logic:
a.
b.
c.
d.
e.
f.
g.
8.11

Emily is either a surgeon or a lawyer.
Joe is an actor, but he also holds another job.
All surgeons are doctors.
Joe does not have a lawyer (i.e., is not a customer of any lawyer).
Emily has a boss who is a lawyer.
There exists a lawyer all of whose customers are doctors.
Every surgeon has a lawyer.
Complete the following exercises about logical senntences:

a. Translate into good, natural English (no xs or ys!):
âˆ€ x, y, l SpeaksLanguage (x, l) âˆ§ SpeaksLanguage (y, l)
â‡’ Understands (x, y) âˆ§ Understands(y, x).
b. Explain why this sentence is entailed by the sentence
âˆ€ x, y, l SpeaksLanguage (x, l) âˆ§ SpeaksLanguage (y, l)
â‡’ Understands (x, y).
c. Translate into first-order logic the following sentences:
(i) Understanding leads to friendship.
(ii) Friendship is transitive.
Remember to define all predicates, functions, and constants you use.
8.12 Rewrite the first two Peano axioms in Section 8.3.3 as a single axiom that defines
NatNum(x) so as to exclude the possibility of natural numbers except for those generated by
the successor function.
8.13 Equation (8.4) on page 306 defines the conditions under which a square is breezy. Here
we consider two other ways to describe this aspect of the wumpus world.
DIAGNOSTIC RULE

CAUSAL RULE

a. We can write diagnostic rules leading from observed effects to hidden causes. For finding pits, the obvious diagnostic rules say that if a square is breezy, some adjacent square
must contain a pit; and if a square is not breezy, then no adjacent square contains a pit.
Write these two rules in first-order logic and show that their conjunction is logically
equivalent to Equation (8.4).
b. We can write causal rules leading from cause to effect. One obvious causal rule is that
a pit causes all adjacent squares to be breezy. Write this rule in first-order logic, explain
why it is incomplete compared to Equation (8.4), and supply the missing axiom.

318

Chapter
George

Spencer

Kydd

Diana

Charles

William Harry

Figure 8.7
children.

Elizabeth

Anne

Peter

Mark

8.

First-Order Logic

Mum

Philip

Andrew

Margaret

Sarah

Zara Beatrice Eugenie

Edward

Louise

Sophie

James

A typical family tree. The symbol â€œ
â€ connects spouses and arrows point to

8.14 Write axioms describing the predicates Grandchild , Greatgrandparent , Ancestor ,
Brother , Sister , Daughter , Son, FirstCousin, BrotherInLaw , SisterInLaw , Aunt, and
Uncle. Find out the proper definition of mth cousin n times removed, and write the definition in first-order logic. Now write down the basic facts depicted in the family tree in
Figure 8.7. Using a suitable logical reasoning system, T ELL it all the sentences you have
written down, and A SK it who are Elizabethâ€™s grandchildren, Dianaâ€™s brothers-in-law, Zaraâ€™s
great-grandparents, and Eugenieâ€™s ancestors.
8.15 Explain what is wrong with the following proposed definition of the set membership
predicate âˆˆ :
âˆ€ x, s x âˆˆ {x|s}
âˆ€ x, s x âˆˆ s â‡’ âˆ€ y x âˆˆ {y|s} .
8.16 Using the set axioms as examples, write axioms for the list domain, including all the
constants, functions, and predicates mentioned in the chapter.
8.17 Explain what is wrong with the following proposed definition of adjacent squares in
the wumpus world:
âˆ€ x, y Adjacent([x, y], [x + 1, y]) âˆ§ Adjacent ([x, y], [x, y + 1]) .
8.18 Write out the axioms required for reasoning about the wumpusâ€™s location, using a
constant symbol Wumpus and a binary predicate At(Wumpus, Location ). Remember that
there is only one wumpus.
8.19 Assuming predicates Parent(p, q) and Female(p) and constants Joan and Kevin,
with the obvious meanings, express each of the following sentences in first-order logic. (You
may use the abbreviation âˆƒ1 to mean â€œthere exists exactly one.â€)
a.
b.
c.
d.
e.

Joan has a daughter (possibly more than one, and possibly sons as well).
Joan has exactly one daughter (but may have sons as well).
Joan has exactly one child, a daughter.
Joan and Kevin have exactly one child together.
Joan has at least one child with Kevin, and no children with anyone else.

Exercises

319
8.20 Arithmetic assertions can be written in first-order logic with the predicate symbol <,
the function symbols + and Ã—, and the constant symbols 0 and 1. Additional predicates can
also be defined with biconditionals.
a. Represent the property â€œx is an even number.â€
b. Represent the property â€œx is prime.â€
c. Goldbachâ€™s conjecture is the conjecture (unproven as yet) that every even number is
equal to the sum of two primes. Represent this conjecture as a logical sentence.
8.21 In Chapter 6, we used equality to indicate the relation between a variable and its value.
For instance, we wrote WA = red to mean that Western Australia is colored red. Representing this in first-order logic, we must write more verbosely ColorOf (WA) = red . What
incorrect inference could be drawn if we wrote sentences such as WA = red directly as logical
assertions?
8.22 Write in first-order logic the assertion that every key and at least one of every pair of
socks will eventually be lost forever, using only the following vocabulary: Key(x), x is a key;
Sock (x), x is a sock; Pair (x, y), x and y are a pair; Now , the current time; Before(t1 , t2 ),
time t1 comes before time t2 ; Lost (x, t), object x is lost at time t.
8.23 For each of the following sentences in English, decide if the accompanying first-order
logic sentence is a good translation. If not, explain why not and correct it. (Some sentences
may have more than one error!)
a. No two people have the same social security number.
Â¬âˆƒ x, y, n Person(x) âˆ§ Person(y) â‡’ [HasSS #(x, n) âˆ§ HasSS #(y, n)].
b. Johnâ€™s social security number is the same as Maryâ€™s.
âˆƒ n HasSS #(John, n) âˆ§ HasSS #(Mary, n).
c. Everyoneâ€™s social security number has nine digits.
âˆ€ x, n Person(x) â‡’ [HasSS #(x, n) âˆ§ Digits(n, 9)].
d. Rewrite each of the above (uncorrected) sentences using a function symbol SS # instead
of the predicate HasSS #.
8.24 Represent the following sentences in first-order logic, using a consistent vocabulary
(which you must define):
a.
b.
c.
d.
e.
f.
g.

Some students took French in spring 2001.
Every student who takes French passes it.
Only one student took Greek in spring 2001.
The best score in Greek is always higher than the best score in French.
Every person who buys a policy is smart.
No person buys an expensive policy.
There is an agent who sells policies only to people who are not insured.

320

Chapter

X0
Y0
X1
Y1

X3
Y3

Figure 8.8

First-Order Logic

Z0

Ad0

X3 X2 X1 X0

Z1

Ad1

+
X2
Y2

8.

Z2

Ad2

Ad3

Y3 Y2 Y 1 Y0

Z4 Z3 Z2 Z 1 Z0

Z3
Z4

A four-bit adder. Each Ad i is a one-bit adder, as in Figure 8.6 on page 309.

h. There is a barber who shaves all men in town who do not shave themselves.
i. A person born in the UK, each of whose parents is a UK citizen or a UK resident, is a
UK citizen by birth.
j. A person born outside the UK, one of whose parents is a UK citizen by birth, is a UK
citizen by descent.
k. Politicians can fool some of the people all of the time, and they can fool all of the people
some of the time, but they canâ€™t fool all of the people all of the time.
l. All Greeks speak the same language. (Use Speaks(x, l) to mean that person x speaks
language l.)
8.25 Write a general set of facts and axioms to represent the assertion â€œWellington heard
about Napoleonâ€™s deathâ€ and to correctly answer the question â€œDid Napoleon hear about
Wellingtonâ€™s death?â€
8.26 Extend the vocabulary from Section 8.4 to define addition for n-bit binary numbers.
Then encode the description of the four-bit adder in Figure 8.8, and pose the queries needed
to verify that it is in fact correct.
8.27 Obtain a passport application for your country, identify the rules determining eligibility for a passport, and translate them into first-order logic, following the steps outlined in
Section 8.4.
8.28 Consider a first-order logical knowledge base that describes worlds containing people,
songs, albums (e.g., â€œMeet the Beatlesâ€) and disks (i.e., particular physical instances of CDs).
The vocabulary contains the following symbols:
CopyOf (d, a): Predicate. Disk d is a copy of album a.
Owns(p, d): Predicate. Person p owns disk d.
Sings(p, s, a): Album a includes a recording of song s sung by person p.
Wrote(p, s): Person p wrote song s.
McCartney, Gershwin, BHoliday, Joe, EleanorRigby, TheManILove, Revolver :
Constants with the obvious meanings.

Exercises

321
Express the following statements in first-order logic:
a.
b.
c.
d.
e.
f.
g.
h.
i.
j.
k.
l.

Gershwin wrote â€œThe Man I Love.â€
Gershwin did not write â€œEleanor Rigby.â€
Either Gershwin or McCartney wrote â€œThe Man I Love.â€
Joe has written at least one song.
Joe owns a copy of Revolver.
Every song that McCartney sings on Revolver was written by McCartney.
Gershwin did not write any of the songs on Revolver.
Every song that Gershwin wrote has been recorded on some album. (Possibly different
songs are recorded on different albums.)
There is a single album that contains every song that Joe has written.
Joe owns a copy of an album that has Billie Holiday singing â€œThe Man I Love.â€
Joe owns a copy of every album that has a song sung by McCartney. (Of course, each
different album is instantiated in a different physical CD.)
Joe owns a copy of every album on which all the songs are sung by Billie Holiday.

9

INFERENCE IN
FIRST-ORDER LOGIC

In which we define effective procedures for answering questions posed in firstorder logic.

Chapter 7 showed how sound and complete inference can be achieved for propositional logic.
In this chapter, we extend those results to obtain algorithms that can answer any answerable question stated in first-order logic. Section 9.1 introduces inference rules for quantifiers
and shows how to reduce first-order inference to propositional inference, albeit at potentially
great expense. Section 9.2 describes the idea of unification, showing how it can be used
to construct inference rules that work directly with first-order sentences. We then discuss
three major families of first-order inference algorithms. Forward chaining and its applications to deductive databases and production systems are covered in Section 9.3; backward
chaining and logic programming systems are developed in Section 9.4. Forward and backward chaining can be very efficient, but are applicable only to knowledge bases that can
be expressed as sets of Horn clauses. General first-order sentences require resolution-based
theorem proving, which is described in Section 9.5.

9.1

P ROPOSITIONAL VS . F IRST-O RDER I NFERENCE
This section and the next introduce the ideas underlying modern logical inference systems.
We begin with some simple inference rules that can be applied to sentences with quantifiers
to obtain sentences without quantifiers. These rules lead naturally to the idea that first-order
inference can be done by converting the knowledge base to propositional logic and using
propositional inference, which we already know how to do. The next section points out an
obvious shortcut, leading to inference methods that manipulate first-order sentences directly.

9.1.1 Inference rules for quantifiers
Let us begin with universal quantifiers. Suppose our knowledge base contains the standard
folkloric axiom stating that all greedy kings are evil:
âˆ€ x King(x) âˆ§ Greedy (x) â‡’ Evil(x) .
322

Section 9.1.

Propositional vs. First-Order Inference

323

Then it seems quite permissible to infer any of the following sentences:
King(John) âˆ§ Greedy (John) â‡’ Evil(John)
King(Richard ) âˆ§ Greedy (Richard ) â‡’ Evil(Richard )
King(Father (John)) âˆ§ Greedy (Father (John)) â‡’ Evil(Father (John)) .
..
.
UNIVERSAL
INSTANTIATION
GROUND TERM

EXISTENTIAL
INSTANTIATION

The rule of Universal Instantiation (UI for short) says that we can infer any sentence obtained by substituting a ground term (a term without variables) for the variable. 1 To write
out the inference rule formally, we use the notion of substitutions introduced in Section 8.3.
Let S UBST (Î¸, Î±) denote the result of applying the substitution Î¸ to the sentence Î±. Then the
rule is written
âˆ€v Î±
S UBST ({v/g}, Î±)
for any variable v and ground term g. For example, the three sentences given earlier are
obtained with the substitutions {x/John}, {x/Richard }, and {x/Father (John)}.
In the rule for Existential Instantiation, the variable is replaced by a single new constant symbol. The formal statement is as follows: for any sentence Î±, variable v, and constant
symbol k that does not appear elsewhere in the knowledge base,
âˆƒv Î±
.
S UBST ({v/k}, Î±)
For example, from the sentence
âˆƒ x Crown(x) âˆ§ OnHead (x, John)
we can infer the sentence
Crown(C1 ) âˆ§ OnHead (C1 , John)

SKOLEM CONSTANT

INFERENTIAL
EQUIVALENCE

as long as C1 does not appear elsewhere in the knowledge base. Basically, the existential
sentence says there is some object satisfying a condition, and applying the existential instantiation rule just gives a name to that object. Of course, that name must not already belong
to another object. Mathematics provides a nice example: suppose we discover that there is a
number that is a little bigger than 2.71828 and that satisfies the equation d(xy )/dy = xy for x.
We can give this number a name, such as e, but it would be a mistake to give it the name of
an existing object, such as Ï€. In logic, the new name is called a Skolem constant. Existential Instantiation is a special case of a more general process called skolemization, which we
cover in Section 9.5.
Whereas Universal Instantiation can be applied many times to produce many different
consequences, Existential Instantiation can be applied once, and then the existentially quantified sentence can be discarded. For example, we no longer need âˆƒ x Kill(x, Victim) once
we have added the sentence Kill (Murderer , Victim). Strictly speaking, the new knowledge
base is not logically equivalent to the old, but it can be shown to be inferentially equivalent
in the sense that it is satisfiable exactly when the original knowledge base is satisfiable.
1 Do not confuse these substitutions with the extended interpretations used to define the semantics of quantifiers.
The substitution replaces a variable with a term (a piece of syntax) to produce a new sentence, whereas an
interpretation maps a variable to an object in the domain.

324

Chapter

9.

Inference in First-Order Logic

9.1.2 Reduction to propositional inference
Once we have rules for inferring nonquantified sentences from quantified sentences, it becomes possible to reduce first-order inference to propositional inference. In this section we
give the main ideas; the details are given in Section 9.5.
The first idea is that, just as an existentially quantified sentence can be replaced by
one instantiation, a universally quantified sentence can be replaced by the set of all possible
instantiations. For example, suppose our knowledge base contains just the sentences
âˆ€ x King(x) âˆ§ Greedy (x) â‡’ Evil(x)
King(John)
Greedy (John)
Brother (Richard , John) .

(9.1)

Then we apply UI to the first sentence using all possible ground-term substitutions from the
vocabulary of the knowledge baseâ€”in this case, {x/John} and {x/Richard }. We obtain
King(John) âˆ§ Greedy (John) â‡’ Evil(John)
King(Richard ) âˆ§ Greedy (Richard ) â‡’ Evil(Richard ) ,
and we discard the universally quantified sentence. Now, the knowledge base is essentially
propositional if we view the ground atomic sentencesâ€”King (John), Greedy (John), and
so onâ€”as proposition symbols. Therefore, we can apply any of the complete propositional
algorithms in Chapter 7 to obtain conclusions such as Evil(John).
This technique of propositionalization can be made completely general, as we show
in Section 9.5; that is, every first-order knowledge base and query can be propositionalized
in such a way that entailment is preserved. Thus, we have a complete decision procedure
for entailment . . . or perhaps not. There is a problem: when the knowledge base includes
a function symbol, the set of possible ground-term substitutions is infinite! For example, if
the knowledge base mentions the Father symbol, then infinitely many nested terms such as
Father (Father (Father (John))) can be constructed. Our propositional algorithms will have
difficulty with an infinitely large set of sentences.
Fortunately, there is a famous theorem due to Jacques Herbrand (1930) to the effect
that if a sentence is entailed by the original, first-order knowledge base, then there is a proof
involving just a finite subset of the propositionalized knowledge base. Since any such subset
has a maximum depth of nesting among its ground terms, we can find the subset by first
generating all the instantiations with constant symbols (Richard and John), then all terms of
depth 1 (Father (Richard ) and Father (John)), then all terms of depth 2, and so on, until we
are able to construct a propositional proof of the entailed sentence.
We have sketched an approach to first-order inference via propositionalization that is
completeâ€”that is, any entailed sentence can be proved. This is a major achievement, given
that the space of possible models is infinite. On the other hand, we do not know until the
proof is done that the sentence is entailed! What happens when the sentence is not entailed?
Can we tell? Well, for first-order logic, it turns out that we cannot. Our proof procedure can
go on and on, generating more and more deeply nested terms, but we will not know whether
it is stuck in a hopeless loop or whether the proof is just about to pop out. This is very much

Section 9.2.

Unification and Lifting

325

like the halting problem for Turing machines. Alan Turing (1936) and Alonzo Church (1936)
both proved, in rather different ways, the inevitability of this state of affairs. The question of
entailment for first-order logic is semidecidableâ€”that is, algorithms exist that say yes to every
entailed sentence, but no algorithm exists that also says no to every nonentailed sentence.

9.2

U NIFICATION AND L IFTING
The preceding section described the understanding of first-order inference that existed up
to the early 1960s. The sharp-eyed reader (and certainly the computational logicians of the
early 1960s) will have noticed that the propositionalization approach is rather inefficient. For
example, given the query Evil(x) and the knowledge base in Equation (9.1), it seems perverse to generate sentences such as King(Richard ) âˆ§ Greedy (Richard ) â‡’ Evil(Richard ).
Indeed, the inference of Evil(John) from the sentences
âˆ€ x King(x) âˆ§ Greedy (x) â‡’ Evil(x)
King(John)
Greedy (John)
seems completely obvious to a human being. We now show how to make it completely
obvious to a computer.

9.2.1 A first-order inference rule
The inference that John is evilâ€”that is, that {x/John} solves the query Evil(x)â€”works like
this: to use the rule that greedy kings are evil, find some x such that x is a king and x is
greedy, and then infer that this x is evil. More generally, if there is some substitution Î¸ that
makes each of the conjuncts of the premise of the implication identical to sentences already
in the knowledge base, then we can assert the conclusion of the implication, after applying Î¸.
In this case, the substitution Î¸ = {x/John} achieves that aim.
We can actually make the inference step do even more work. Suppose that instead of
knowing Greedy (John), we know that everyone is greedy:
âˆ€ y Greedy (y) .

GENERALIZED
MODUS PONENS

(9.2)

Then we would still like to be able to conclude that Evil(John), because we know that
John is a king (given) and John is greedy (because everyone is greedy). What we need for
this to work is to find a substitution both for the variables in the implication sentence and
for the variables in the sentences that are in the knowledge base. In this case, applying the
substitution {x/John, y/John} to the implication premises King(x) and Greedy (x) and the
knowledge-base sentences King(John) and Greedy (y) will make them identical. Thus, we
can infer the conclusion of the implication.
This inference process can be captured as a single inference rule that we call Generalized Modus Ponens:2 For atomic sentences pi , pi  , and q, where there is a substitution Î¸

326

Chapter

9.

Inference in First-Order Logic

such that S UBST (Î¸, pi  ) = S UBST (Î¸, pi ), for all i,
p1  , p2  , . . . , pn  , (p1 âˆ§ p2 âˆ§ . . . âˆ§ pn â‡’ q)
.
S UBST (Î¸, q)
There are n + 1 premises to this rule: the n atomic sentences pi  and the one implication. The
conclusion is the result of applying the substitution Î¸ to the consequent q. For our example:
p1 is King(x)
p1  is King(John)

p2 is Greedy (x)
p2 is Greedy (y)
Î¸ is {x/John, y/John}
q is Evil(x)
S UBST (Î¸, q) is Evil(John) .
It is easy to show that Generalized Modus Ponens is a sound inference rule. First, we observe
that, for any sentence p (whose variables are assumed to be universally quantified) and for
any substitution Î¸,
p |= S UBST (Î¸, p)
holds by Universal Instantiation. It holds in particular for a Î¸ that satisfies the conditions of
the Generalized Modus Ponens rule. Thus, from p1  , . . . , pn  we can infer
S UBST (Î¸, p1  ) âˆ§ . . . âˆ§ S UBST (Î¸, pn  )
and from the implication p1 âˆ§ . . . âˆ§ pn â‡’ q we can infer
S UBST (Î¸, p1 ) âˆ§ . . . âˆ§ S UBST (Î¸, pn ) â‡’ S UBST (Î¸, q) .

LIFTING

Now, Î¸ in Generalized Modus Ponens is defined so that S UBST (Î¸, pi  ) = S UBST (Î¸, pi ), for
all i; therefore the first of these two sentences matches the premise of the second exactly.
Hence, S UBST (Î¸, q) follows by Modus Ponens.
Generalized Modus Ponens is a lifted version of Modus Ponensâ€”it raises Modus Ponens from ground (variable-free) propositional logic to first-order logic. We will see in the
rest of this chapter that we can develop lifted versions of the forward chaining, backward
chaining, and resolution algorithms introduced in Chapter 7. The key advantage of lifted
inference rules over propositionalization is that they make only those substitutions that are
required to allow particular inferences to proceed.

9.2.2 Unification
UNIFICATION
UNIFIER

Lifted inference rules require finding substitutions that make different logical expressions
look identical. This process is called unification and is a key component of all first-order
inference algorithms. The U NIFY algorithm takes two sentences and returns a unifier for
them if one exists:
U NIFY (p, q) = Î¸ where S UBST(Î¸, p) = S UBST(Î¸, q) .
Let us look at some examples of how U NIFY should behave. Suppose we have a query
AskVars(Knows(John, x)): whom does John know? Answers to this query can be found
2 Generalized Modus Ponens is more general than Modus Ponens (page 249) in the sense that the known facts
and the premise of the implication need match only up to a substitution, rather than exactly. On the other hand,
Modus Ponens allows any sentence Î± as the premise, rather than just a conjunction of atomic sentences.

Section 9.2.

Unification and Lifting

327

by finding all sentences in the knowledge base that unify with Knows(John, x). Here are the
results of unification with four different sentences that might be in the knowledge base:
U NIFY (Knows(John, x),
U NIFY (Knows(John, x),
U NIFY (Knows(John, x),
U NIFY (Knows(John, x),

STANDARDIZING
APART

Knows(John, Jane)) = {x/Jane}
Knows(y, Bill )) = {x/Bill , y/John}
Knows(y, Mother (y))) = {y/John, x/Mother (John)}
Knows(x, Elizabeth)) = fail .

The last unification fails because x cannot take on the values John and Elizabeth at the
same time. Now, remember that Knows(x, Elizabeth) means â€œEveryone knows Elizabeth,â€
so we should be able to infer that John knows Elizabeth. The problem arises only because
the two sentences happen to use the same variable name, x. The problem can be avoided
by standardizing apart one of the two sentences being unified, which means renaming its
variables to avoid name clashes. For example, we can rename x in Knows(x, Elizabeth) to
x17 (a new variable name) without changing its meaning. Now the unification will work:
U NIFY (Knows(John, x), Knows(x17 , Elizabeth)) = {x/Elizabeth, x17 /John} .

MOST GENERAL
UNIFIER

OCCUR CHECK

Exercise 9.12 delves further into the need for standardizing apart.
There is one more complication: we said that U NIFY should return a substitution
that makes the two arguments look the same. But there could be more than one such unifier. For example, U NIFY (Knows(John, x), Knows(y, z)) could return {y/John, x/z} or
{y/John, x/John, z/John}. The first unifier gives Knows(John, z) as the result of unification, whereas the second gives Knows(John, John). The second result could be obtained
from the first by an additional substitution {z/John}; we say that the first unifier is more
general than the second, because it places fewer restrictions on the values of the variables. It
turns out that, for every unifiable pair of expressions, there is a single most general unifier (or
MGU) that is unique up to renaming and substitution of variables. (For example, {x/John}
and {y/John} are considered equivalent, as are {x/John, y/John} and {x/John, y/x}.) In
this case it is {y/John, x/z}.
An algorithm for computing most general unifiers is shown in Figure 9.1. The process
is simple: recursively explore the two expressions simultaneously â€œside by side,â€ building up
a unifier along the way, but failing if two corresponding points in the structures do not match.
There is one expensive step: when matching a variable against a complex term, one must
check whether the variable itself occurs inside the term; if it does, the match fails because no
consistent unifier can be constructed. For example, S(x) canâ€™t unify with S(S(x)). This socalled occur check makes the complexity of the entire algorithm quadratic in the size of the
expressions being unified. Some systems, including all logic programming systems, simply
omit the occur check and sometimes make unsound inferences as a result; other systems use
more complex algorithms with linear-time complexity.

9.2.3 Storage and retrieval
Underlying the T ELL and A SK functions used to inform and interrogate a knowledge base
are the more primitive S TORE and F ETCH functions. S TORE (s) stores a sentence s into the
knowledge base and F ETCH (q) returns all unifiers such that the query q unifies with some

328

Chapter

9.

Inference in First-Order Logic

function U NIFY(x , y, Î¸) returns a substitution to make x and y identical
inputs: x , a variable, constant, list, or compound expression
y, a variable, constant, list, or compound expression
Î¸, the substitution built up so far (optional, defaults to empty)
if Î¸ = failure then return failure
else if x = y then return Î¸
else if VARIABLE ?(x ) then return U NIFY-VAR(x , y, Î¸)
else if VARIABLE ?(y) then return U NIFY-VAR(y, x , Î¸)
else if C OMPOUND ?(x ) and C OMPOUND ?(y) then
return U NIFY (x .A RGS, y.A RGS , U NIFY (x .O P, y.O P , Î¸))
else if L IST ?(x ) and L IST ?(y) then
return U NIFY (x .R EST, y.R EST , U NIFY (x .F IRST, y.F IRST , Î¸))
else return failure
function U NIFY-VAR (var , x , Î¸) returns a substitution
if {var /val } âˆˆ Î¸ then return U NIFY(val , x , Î¸)
else if {x/val } âˆˆ Î¸ then return U NIFY (var , val , Î¸)
else if O CCUR -C HECK ?(var , x ) then return failure
else return add {var /x } to Î¸
Figure 9.1 The unification algorithm. The algorithm works by comparing the structures
of the inputs, element by element. The substitution Î¸ that is the argument to U NIFY is built
up along the way and is used to make sure that later comparisons are consistent with bindings
that were established earlier. In a compound expression such as F (A, B), the O P field picks
out the function symbol F and the A RGS field picks out the argument list (A, B).

INDEXING
PREDICATE
INDEXING

sentence in the knowledge base. The problem we used to illustrate unificationâ€”finding all
facts that unify with Knows(John, x)â€”is an instance of F ETCH ing.
The simplest way to implement S TORE and F ETCH is to keep all the facts in one long
list and unify each query against every element of the list. Such a process is inefficient, but
it works, and itâ€™s all you need to understand the rest of the chapter. The remainder of this
section outlines ways to make retrieval more efficient; it can be skipped on first reading.
We can make F ETCH more efficient by ensuring that unifications are attempted only
with sentences that have some chance of unifying. For example, there is no point in trying
to unify Knows(John, x) with Brother (Richard , John). We can avoid such unifications by
indexing the facts in the knowledge base. A simple scheme called predicate indexing puts
all the Knows facts in one bucket and all the Brother facts in another. The buckets can be
stored in a hash table for efficient access.
Predicate indexing is useful when there are many predicate symbols but only a few
clauses for each symbol. Sometimes, however, a predicate has many clauses. For example,
suppose that the tax authorities want to keep track of who employs whom, using a predicate Employs(x, y). This would be a very large bucket with perhaps millions of employers

Section 9.2.

Unification and Lifting

329

Employs(x,y)

Employs(x,Richard)

Employs(x,y)

Employs(IBM,y)

Employs(IBM,Richard)

Employs(x,John)

Employs(x,x)

Employs(John,y)

Employs(John,John)

(a)

(b)

Figure 9.2 (a) The subsumption lattice whose lowest node is Employs(IBM , Richard ).
(b) The subsumption lattice for the sentence Employs (John, John).

and tens of millions of employees. Answering a query such as Employs(x, Richard ) with
predicate indexing would require scanning the entire bucket.
For this particular query, it would help if facts were indexed both by predicate and by
second argument, perhaps using a combined hash table key. Then we could simply construct
the key from the query and retrieve exactly those facts that unify with the query. For other
queries, such as Employs(IBM , y), we would need to have indexed the facts by combining
the predicate with the first argument. Therefore, facts can be stored under multiple index
keys, rendering them instantly accessible to various queries that they might unify with.
Given a sentence to be stored, it is possible to construct indices for all possible queries
that unify with it. For the fact Employs(IBM , Richard ), the queries are
Employs(IBM , Richard )
Employs(x, Richard )
Employs(IBM , y)
Employs(x, y)
SUBSUMPTION
LATTICE

Does IBM employ Richard?
Who employs Richard?
Whom does IBM employ?
Who employs whom?

These queries form a subsumption lattice, as shown in Figure 9.2(a). The lattice has some
interesting properties. For example, the child of any node in the lattice is obtained from its
parent by a single substitution; and the â€œhighestâ€ common descendant of any two nodes is
the result of applying their most general unifier. The portion of the lattice above any ground
fact can be constructed systematically (Exercise 9.5). A sentence with repeated constants has
a slightly different lattice, as shown in Figure 9.2(b). Function symbols and variables in the
sentences to be stored introduce still more interesting lattice structures.
The scheme we have described works very well whenever the lattice contains a small
number of nodes. For a predicate with n arguments, however, the lattice contains O(2n )
nodes. If function symbols are allowed, the number of nodes is also exponential in the size
of the terms in the sentence to be stored. This can lead to a huge number of indices. At some
point, the benefits of indexing are outweighed by the costs of storing and maintaining all
the indices. We can respond by adopting a fixed policy, such as maintaining indices only on
keys composed of a predicate plus each argument, or by using an adaptive policy that creates
indices to meet the demands of the kinds of queries being asked. For most AI systems, the
number of facts to be stored is small enough that efficient indexing is considered a solved
problem. For commercial databases, where facts number in the billions, the problem has
been the subject of intensive study and technology development..

330

9.3

Chapter

9.

Inference in First-Order Logic

F ORWARD C HAINING
A forward-chaining algorithm for propositional definite clauses was given in Section 7.5.
The idea is simple: start with the atomic sentences in the knowledge base and apply Modus
Ponens in the forward direction, adding new atomic sentences, until no further inferences
can be made. Here, we explain how the algorithm is applied to first-order definite clauses.
Definite clauses such as Situation â‡’ Response are especially useful for systems that make
inferences in response to newly arrived information. Many systems can be defined this way,
and forward chaining can be implemented very efficiently.

9.3.1 First-order definite clauses
First-order definite clauses closely resemble propositional definite clauses (page 256): they
are disjunctions of literals of which exactly one is positive. A definite clause either is atomic
or is an implication whose antecedent is a conjunction of positive literals and whose consequent is a single positive literal. The following are first-order definite clauses:
King(x) âˆ§ Greedy (x) â‡’ Evil(x) .
King(John) .
Greedy (y) .
Unlike propositional literals, first-order literals can include variables, in which case those
variables are assumed to be universally quantified. (Typically, we omit universal quantifiers
when writing definite clauses.) Not every knowledge base can be converted into a set of
definite clauses because of the single-positive-literal restriction, but many can. Consider the
following problem:
The law says that it is a crime for an American to sell weapons to hostile nations. The
country Nono, an enemy of America, has some missiles, and all of its missiles were sold
to it by Colonel West, who is American.

We will prove that West is a criminal. First, we will represent these facts as first-order definite
clauses. The next section shows how the forward-chaining algorithm solves the problem.
â€œ. . . it is a crime for an American to sell weapons to hostile nationsâ€:
American(x) âˆ§ Weapon(y) âˆ§ Sells(x, y, z) âˆ§ Hostile(z) â‡’ Criminal (x) .

(9.3)

â€œNono . . . has some missiles.â€ The sentence âˆƒ x Owns(Nono, x)âˆ§Missile(x) is transformed
into two definite clauses by Existential Instantiation, introducing a new constant M1 :
Owns(Nono, M1 )

(9.4)

.
Missile(M
1)

(9.5)

â€œAll of its missiles were sold to it by Colonel Westâ€:
Missile(x) âˆ§ Owns(Nono, x) â‡’ Sells(West, x, Nono) .

(9.6)

We will also need to know that missiles are weapons:
Missile(x) â‡’ Weapon(x)

(9.7)

Section 9.3.

Forward Chaining

331

and we must know that an enemy of America counts as â€œhostileâ€:
Enemy(x, America) â‡’ Hostile(x) .

(9.8)

â€œWest, who is American . . .â€:
American(West) .

(9.9)

â€œThe country Nono, an enemy of America . . .â€:
Enemy(Nono, America) .
DATALOG

(9.10)

This knowledge base contains no function symbols and is therefore an instance of the class
of Datalog knowledge bases. Datalog is a language that is restricted to first-order definite
clauses with no function symbols. Datalog gets its name because it can represent the type of
statements typically made in relational databases. We will see that the absence of function
symbols makes inference much easier.

9.3.2 A simple forward-chaining algorithm

RENAMING

The first forward-chaining algorithm we consider is a simple one, shown in Figure 9.3. Starting from the known facts, it triggers all the rules whose premises are satisfied, adding their
conclusions to the known facts. The process repeats until the query is answered (assuming
that just one answer is required) or no new facts are added. Notice that a fact is not â€œnewâ€
if it is just a renaming of a known fact. One sentence is a renaming of another if they
are identical except for the names of the variables. For example, Likes(x, IceCream) and
Likes(y, IceCream) are renamings of each other because they differ only in the choice of x
or y; their meanings are identical: everyone likes ice cream.
We use our crime problem to illustrate how FOL-FC-A SK works. The implication
sentences are (9.3), (9.6), (9.7), and (9.8). Two iterations are required:
â€¢ On the first iteration, rule (9.3) has unsatisfied premises.
Rule (9.6) is satisfied with {x/M1 }, and Sells(West, M1 , Nono) is added.
Rule (9.7) is satisfied with {x/M1 }, and Weapon(M1 ) is added.
Rule (9.8) is satisfied with {x/Nono}, and Hostile(Nono) is added.
â€¢ On the second iteration, rule (9.3) is satisfied with {x/West , y/M1 , z/Nono}, and
Criminal (West) is added.
Figure 9.4 shows the proof tree that is generated. Notice that no new inferences are possible
at this point because every sentence that could be concluded by forward chaining is already
contained explicitly in the KB. Such a knowledge base is called a fixed point of the inference
process. Fixed points reached by forward chaining with first-order definite clauses are similar
to those for propositional forward chaining (page 258); the principal difference is that a firstorder fixed point can include universally quantified atomic sentences.
FOL-FC-A SK is easy to analyze. First, it is sound, because every inference is just an
application of Generalized Modus Ponens, which is sound. Second, it is complete for definite
clause knowledge bases; that is, it answers every query whose answers are entailed by any
knowledge base of definite clauses. For Datalog knowledge bases, which contain no function
symbols, the proof of completeness is fairly easy. We begin by counting the number of

332

Chapter

9.

Inference in First-Order Logic

function FOL-FC-A SK (KB, Î±) returns a substitution or false
inputs: KB , the knowledge base, a set of first-order definite clauses
Î±, the query, an atomic sentence
local variables: new , the new sentences inferred on each iteration
repeat until new is empty
new â† { }
for each rule in KB do
(p1 âˆ§ . . . âˆ§ pn â‡’ q) â† S TANDARDIZE -VARIABLES(rule)
for each Î¸ such that S UBST (Î¸, p1 âˆ§ . . . âˆ§ pn ) = S UBST (Î¸, p1 âˆ§ . . . âˆ§ pn )
for some p1 , . . . , pn in KB

q â† S UBST (Î¸, q)
if q  does not unify with some sentence already in KB or new then
add q  to new
Ï† â† U NIFY (q  , Î±)
if Ï† is not fail then return Ï†
add new to KB
return false
Figure 9.3 A conceptually straightforward, but very inefficient, forward-chaining algorithm. On each iteration, it adds to KB all the atomic sentences that can be inferred in one
step from the implication sentences and the atomic sentences already in KB . The function
S TANDARDIZE -VARIABLES replaces all variables in its arguments with new ones that have
not been used before.
Criminal(West)

Weapon(M1)

American(West)

Missile(M1)

Sells(West,M1,Nono)

Owns(Nono,M1)

Hostile(Nono)

Enemy(Nono,America)

Figure 9.4 The proof tree generated by forward chaining on the crime example. The initial
facts appear at the bottom level, facts inferred on the first iteration in the middle level, and
facts inferred on the second iteration at the top level.

possible facts that can be added, which determines the maximum number of iterations. Let k
be the maximum arity (number of arguments) of any predicate, p be the number of predicates,
and n be the number of constant symbols. Clearly, there can be no more than pnk distinct
ground facts, so after this many iterations the algorithm must have reached a fixed point. Then
we can make an argument very similar to the proof of completeness for propositional forward

Section 9.3.

Forward Chaining

333

chaining. (See page 258.) The details of how to make the transition from propositional to
first-order completeness are given for the resolution algorithm in Section 9.5.
For general definite clauses with function symbols, FOL-FC-A SK can generate infinitely many new facts, so we need to be more careful. For the case in which an answer to
the query sentence q is entailed, we must appeal to Herbrandâ€™s theorem to establish that the
algorithm will find a proof. (See Section 9.5 for the resolution case.) If the query has no
answer, the algorithm could fail to terminate in some cases. For example, if the knowledge
base includes the Peano axioms
NatNum(0)
âˆ€ n NatNum(n) â‡’ NatNum(S(n)) ,
then forward chaining adds NatNum(S(0)), NatNum(S(S(0))), NatNum(S(S(S(0)))),
and so on. This problem is unavoidable in general. As with general first-order logic, entailment with definite clauses is semidecidable.

9.3.3 Efficient forward chaining

PATTERN MATCHING

The forward-chaining algorithm in Figure 9.3 is designed for ease of understanding rather
than for efficiency of operation. There are three possible sources of inefficiency. First, the
â€œinner loopâ€ of the algorithm involves finding all possible unifiers such that the premise of
a rule unifies with a suitable set of facts in the knowledge base. This is often called pattern
matching and can be very expensive. Second, the algorithm rechecks every rule on every
iteration to see whether its premises are satisfied, even if very few additions are made to the
knowledge base on each iteration. Finally, the algorithm might generate many facts that are
irrelevant to the goal. We address each of these issues in turn.
Matching rules against known facts
The problem of matching the premise of a rule against the facts in the knowledge base might
seem simple enough. For example, suppose we want to apply the rule
Missile(x) â‡’ Weapon(x) .
Then we need to find all the facts that unify with Missile(x); in a suitably indexed knowledge
base, this can be done in constant time per fact. Now consider a rule such as
Missile(x) âˆ§ Owns(Nono, x) â‡’ Sells(West, x, Nono) .

CONJUNCT
ORDERING

Again, we can find all the objects owned by Nono in constant time per object; then, for each
object, we could check whether it is a missile. If the knowledge base contains many objects
owned by Nono and very few missiles, however, it would be better to find all the missiles first
and then check whether they are owned by Nono. This is the conjunct ordering problem:
find an ordering to solve the conjuncts of the rule premise so that the total cost is minimized.
It turns out that finding the optimal ordering is NP-hard, but good heuristics are available.
For example, the minimum-remaining-values (MRV) heuristic used for CSPs in Chapter 6
would suggest ordering the conjuncts to look for missiles first if fewer missiles than objects
are owned by Nono.

334

Chapter

NT

9.

Inference in First-Order Logic

Diï¬€ (wa, nt ) âˆ§ Diï¬€ (wa, sa) âˆ§

Q

Diï¬€ (nt, q) âˆ§ Diï¬€ (nt , sa) âˆ§

WA

Diï¬€ (q, nsw ) âˆ§ Diï¬€ (q, sa) âˆ§

SA

NSW
V

Diï¬€ (nsw , v) âˆ§ Diï¬€ (nsw , sa) âˆ§
Diï¬€ (v, sa) â‡’ Colorable()
Diï¬€ (Red , Blue) Diï¬€ (Red , Green)
Diï¬€ (Green, Red ) Diï¬€ (Green, Blue)

T
(a)

Diï¬€ (Blue, Red ) Diï¬€ (Blue, Green)
(b)

Figure 9.5 (a) Constraint graph for coloring the map of Australia. (b) The map-coloring
CSP expressed as a single definite clause. Each map region is represented as a variable whose
value can be one of the constants Red, Green or Blue.

The connection between pattern matching and constraint satisfaction is actually very
close. We can view each conjunct as a constraint on the variables that it containsâ€”for example, Missile(x) is a unary constraint on x. Extending this idea, we can express every
finite-domain CSP as a single definite clause together with some associated ground facts.
Consider the map-coloring problem from Figure 6.1, shown again in Figure 9.5(a). An equivalent formulation as a single definite clause is given in Figure 9.5(b). Clearly, the conclusion
Colorable() can be inferred only if the CSP has a solution. Because CSPs in general include
3-SAT problems as special cases, we can conclude that matching a definite clause against a
set of facts is NP-hard.
It might seem rather depressing that forward chaining has an NP-hard matching problem
in its inner loop. There are three ways to cheer ourselves up:

DATA COMPLEXITY

â€¢ We can remind ourselves that most rules in real-world knowledge bases are small and
simple (like the rules in our crime example) rather than large and complex (like the
CSP formulation in Figure 9.5). It is common in the database world to assume that
both the sizes of rules and the arities of predicates are bounded by a constant and to
worry only about data complexityâ€”that is, the complexity of inference as a function
of the number of ground facts in the knowledge base. It is easy to show that the data
complexity of forward chaining is polynomial.
â€¢ We can consider subclasses of rules for which matching is efficient. Essentially every
Datalog clause can be viewed as defining a CSP, so matching will be tractable just
when the corresponding CSP is tractable. Chapter 6 describes several tractable families
of CSPs. For example, if the constraint graph (the graph whose nodes are variables
and whose links are constraints) forms a tree, then the CSP can be solved in linear
time. Exactly the same result holds for rule matching. For instance, if we remove South

Section 9.3.

Forward Chaining

335

Australia from the map in Figure 9.5, the resulting clause is
Diï¬€ (wa, nt) âˆ§ Diï¬€ (nt, q) âˆ§ Diï¬€ (q, nsw) âˆ§ Diï¬€ (nsw , v) â‡’ Colorable ()

which corresponds to the reduced CSP shown in Figure 6.12 on page 224. Algorithms
for solving tree-structured CSPs can be applied directly to the problem of rule matching.
â€¢ We can try to to eliminate redundant rule-matching attempts in the forward-chaining
algorithm, as described next.
Incremental forward chaining
When we showed how forward chaining works on the crime example, we cheated; in particular, we omitted some of the rule matching done by the algorithm shown in Figure 9.3. For
example, on the second iteration, the rule
Missile(x) â‡’ Weapon(x)
matches against Missile(M1 ) (again), and of course the conclusion Weapon(M1 ) is already
known so nothing happens. Such redundant rule matching can be avoided if we make the
following observation: Every new fact inferred on iteration t must be derived from at least
one new fact inferred on iteration t âˆ’ 1. This is true because any inference that does not
require a new fact from iteration t âˆ’ 1 could have been done at iteration t âˆ’ 1 already.
This observation leads naturally to an incremental forward-chaining algorithm where,
at iteration t, we check a rule only if its premise includes a conjunct pi that unifies with a fact
pi newly inferred at iteration t âˆ’ 1. The rule-matching step then fixes pi to match with pi , but
allows the other conjuncts of the rule to match with facts from any previous iteration. This
algorithm generates exactly the same facts at each iteration as the algorithm in Figure 9.3, but
is much more efficient.
With suitable indexing, it is easy to identify all the rules that can be triggered by any
given fact, and indeed many real systems operate in an â€œupdateâ€ mode wherein forward chaining occurs in response to each new fact that is T ELL ed to the system. Inferences cascade
through the set of rules until the fixed point is reached, and then the process begins again for
the next new fact.
Typically, only a small fraction of the rules in the knowledge base are actually triggered
by the addition of a given fact. This means that a great deal of redundant work is done in
repeatedly constructing partial matches that have some unsatisfied premises. Our crime example is rather too small to show this effectively, but notice that a partial match is constructed
on the first iteration between the rule
American(x) âˆ§ Weapon(y) âˆ§ Sells(x, y, z) âˆ§ Hostile(z) â‡’ Criminal (x)

RETE

and the fact American(West). This partial match is then discarded and rebuilt on the second
iteration (when the rule succeeds). It would be better to retain and gradually complete the
partial matches as new facts arrive, rather than discarding them.
The rete algorithm3 was the first to address this problem. The algorithm preprocesses
the set of rules in the knowledge base to construct a sort of dataflow network in which each
3

Rete is Latin for net. The English pronunciation rhymes with treaty.

336

PRODUCTION
SYSTEM

COGNITIVE
ARCHITECTURES

Chapter

9.

Inference in First-Order Logic

node is a literal from a rule premise. Variable bindings flow through the network and are
filtered out when they fail to match a literal. If two literals in a rule share a variableâ€”for
example, Sells(x, y, z) âˆ§ Hostile(z) in the crime exampleâ€”then the bindings from each
literal are filtered through an equality node. A variable binding reaching a node for an nary literal such as Sells(x, y, z) might have to wait for bindings for the other variables to be
established before the process can continue. At any given point, the state of a rete network
captures all the partial matches of the rules, avoiding a great deal of recomputation.
Rete networks, and various improvements thereon, have been a key component of socalled production systems, which were among the earliest forward-chaining systems in
widespread use.4 The X CON system (originally called R1; McDermott, 1982) was built
with a production-system architecture. X CON contained several thousand rules for designing
configurations of computer components for customers of the Digital Equipment Corporation.
It was one of the first clear commercial successes in the emerging field of expert systems.
Many other similar systems have been built with the same underlying technology, which has
been implemented in the general-purpose language O PS -5.
Production systems are also popular in cognitive architecturesâ€”that is, models of human reasoningâ€”such as ACT (Anderson, 1983) and S OAR (Laird et al., 1987). In such systems, the â€œworking memoryâ€ of the system models human short-term memory, and the productions are part of long-term memory. On each cycle of operation, productions are matched
against the working memory of facts. A production whose conditions are satisfied can add or
delete facts in working memory. In contrast to the typical situation in databases, production
systems often have many rules and relatively few facts. With suitably optimized matching
technology, some modern systems can operate in real time with tens of millions of rules.
Irrelevant facts

DEDUCTIVE
DATABASES

MAGIC SET

The final source of inefficiency in forward chaining appears to be intrinsic to the approach
and also arises in the propositional context. Forward chaining makes all allowable inferences
based on the known facts, even if they are irrelevant to the goal at hand. In our crime example,
there were no rules capable of drawing irrelevant conclusions, so the lack of directedness was
not a problem. In other cases (e.g., if many rules describe the eating habits of Americans and
the prices of missiles), FOL-FC-A SK will generate many irrelevant conclusions.
One way to avoid drawing irrelevant conclusions is to use backward chaining, as described in Section 9.4. Another solution is to restrict forward chaining to a selected subset of
rules, as in PL-FC-E NTAILS ? (page 258). A third approach has emerged in the field of deductive databases, which are large-scale databases, like relational databases, but which use
forward chaining as the standard inference tool rather than SQL queries. The idea is to rewrite
the rule set, using information from the goal, so that only relevant variable bindingsâ€”those
belonging to a so-called magic setâ€”are considered during forward inference. For example, if
the goal is Criminal (West), the rule that concludes Criminal (x) will be rewritten to include
an extra conjunct that constrains the value of x:
Magic(x) âˆ§ American(x) âˆ§ Weapon(y) âˆ§ Sells(x, y, z) âˆ§ Hostile(z) â‡’ Criminal (x) .
4

The word production in production systems denotes a conditionâ€“action rule.

Section 9.4.

Backward Chaining

337

The fact Magic(West) is also added to the KB. In this way, even if the knowledge base
contains facts about millions of Americans, only Colonel West will be considered during the
forward inference process. The complete process for defining magic sets and rewriting the
knowledge base is too complex to go into here, but the basic idea is to perform a sort of
â€œgenericâ€ backward inference from the goal in order to work out which variable bindings
need to be constrained. The magic sets approach can therefore be thought of as a kind of
hybrid between forward inference and backward preprocessing.

9.4

BACKWARD C HAINING
The second major family of logical inference algorithms uses the backward chaining approach introduced in Section 7.5 for definite clauses. These algorithms work backward from
the goal, chaining through rules to find known facts that support the proof. We describe
the basic algorithm, and then we describe how it is used in logic programming, which is the
most widely used form of automated reasoning. We also see that backward chaining has some
disadvantages compared with forward chaining, and we look at ways to overcome them. Finally, we look at the close connection between logic programming and constraint satisfaction
problems.

9.4.1 A backward-chaining algorithm

GENERATOR

Figure 9.6 shows a backward-chaining algorithm for definite clauses. FOL-BC-A SK (KB,
goal ) will be proved if the knowledge base contains a clause of the form lhs â‡’ goal , where
lhs (left-hand side) is a list of conjuncts. An atomic fact like American(West) is considered
as a clause whose lhs is the empty list. Now a query that contains variables might be proved
in multiple ways. For example, the query Person(x) could be proved with the substitution
{x/John} as well as with {x/Richard }. So we implement FOL-BC-A SK as a generatorâ€”
a function that returns multiple times, each time giving one possible result.
Backward chaining is a kind of AND / OR searchâ€”the OR part because the goal query
can be proved by any rule in the knowledge base, and the AND part because all the conjuncts
in the lhs of a clause must be proved. FOL-BC-O R works by fetching all clauses that might
unify with the goal, standardizing the variables in the clause to be brand-new variables, and
then, if the rhs of the clause does indeed unify with the goal, proving every conjunct in the
lhs, using FOL-BC-A ND . That function in turn works by proving each of the conjuncts in
turn, keeping track of the accumulated substitution as we go. Figure 9.7 is the proof tree for
deriving Criminal (West) from sentences (9.3) through (9.10).
Backward chaining, as we have written it, is clearly a depth-first search algorithm.
This means that its space requirements are linear in the size of the proof (neglecting, for
now, the space required to accumulate the solutions). It also means that backward chaining
(unlike forward chaining) suffers from problems with repeated states and incompleteness. We
will discuss these problems and some potential solutions, but first we show how backward
chaining is used in logic programming systems.

338

Chapter

9.

Inference in First-Order Logic

function FOL-BC-A SK (KB , query) returns a generator of substitutions
return FOL-BC-O R (KB, query, { })
generator FOL-BC-O R (KB, goal , Î¸) yields a substitution
for each rule (lhs â‡’ rhs) in F ETCH -RULES -F OR -G OAL(KB , goal ) do
(lhs, rhs) â† S TANDARDIZE -VARIABLES((lhs, rhs))
for each Î¸ in FOL-BC-A ND (KB , lhs, U NIFY (rhs, goal , Î¸)) do
yield Î¸
generator FOL-BC-A ND (KB, goals, Î¸) yields a substitution
if Î¸ = failure then return
else if L ENGTH(goals) = 0 then yield Î¸
else do
ï¬rst,rest â† F IRST (goals), R EST(goals)
for each Î¸ in FOL-BC-O R (KB, S UBST (Î¸, ï¬rst), Î¸) do
for each Î¸ in FOL-BC-A ND (KB, rest , Î¸ ) do
yield Î¸
Figure 9.6

A simple backward-chaining algorithm for first-order knowledge bases.

Criminal(West)

American(West)

Weapon(y)

Sells(West,M1,z)

Hostile(Nono)

{z/Nono}

{}

Missile(y)

Missile(M1)

Owns(Nono,M1)

Enemy(Nono,America)

{y/M1}

{}

{}

{}

Figure 9.7 Proof tree constructed by backward chaining to prove that West is a criminal.
The tree should be read depth first, left to right. To prove Criminal (West ), we have to prove
the four conjuncts below it. Some of these are in the knowledge base, and others require
further backward chaining. Bindings for each successful unification are shown next to the
corresponding subgoal. Note that once one subgoal in a conjunction succeeds, its substitution
is applied to subsequent subgoals. Thus, by the time FOL-BC-A SK gets to the last conjunct,
originally Hostile(z), z is already bound to Nono.

Section 9.4.

Backward Chaining

339

9.4.2 Logic programming
Logic programming is a technology that comes fairly close to embodying the declarative
ideal described in Chapter 7: that systems should be constructed by expressing knowledge in
a formal language and that problems should be solved by running inference processes on that
knowledge. The ideal is summed up in Robert Kowalskiâ€™s equation,
Algorithm = Logic + Control .
PROLOG

Prolog is the most widely used logic programming language. It is used primarily as a rapidprototyping language and for symbol-manipulation tasks such as writing compilers (Van Roy,
1990) and parsing natural language (Pereira and Warren, 1980). Many expert systems have
been written in Prolog for legal, medical, financial, and other domains.
Prolog programs are sets of definite clauses written in a notation somewhat different
from standard first-order logic. Prolog uses uppercase letters for variables and lowercase for
constantsâ€”the opposite of our convention for logic. Commas separate conjuncts in a clause,
and the clause is written â€œbackwardsâ€ from what we are used to; instead of A âˆ§ B â‡’ C in
Prolog we have C :- A, B. Here is a typical example:
criminal(X) :- american(X), weapon(Y), sells(X,Y,Z), hostile(Z).

The notation [E|L] denotes a list whose first element is E and whose rest is L. Here is a
Prolog program for append(X,Y,Z), which succeeds if list Z is the result of appending
lists X and Y:
append([],Y,Y).
append([A|X],Y,[A|Z]) :- append(X,Y,Z).
In English, we can read these clauses as (1) appending an empty list with a list Y produces
the same list Y and (2) [A|Z] is the result of appending [A|X] onto Y, provided that Z is
the result of appending X onto Y. In most high-level languages we can write a similar recursive function that describes how to append two lists. The Prolog definition is actually much
more powerful, however, because it describes a relation that holds among three arguments,
rather than a function computed from two arguments. For example, we can ask the query
append(X,Y,[1,2]): what two lists can be appended to give [1,2]? We get back the
solutions
X=[]
Y=[1,2];
X=[1]
Y=[2];
X=[1,2] Y=[]
The execution of Prolog programs is done through depth-first backward chaining, where
clauses are tried in the order in which they are written in the knowledge base. Some aspects
of Prolog fall outside standard logical inference:
â€¢ Prolog uses the database semantics of Section 8.2.8 rather than first-order semantics,
and this is apparent in its treatment of equality and negation (see Section 9.4.5).
â€¢ There is a set of built-in functions for arithmetic. Literals using these function symbols
are â€œprovedâ€ by executing code rather than doing further inference. For example, the

340

Chapter

9.

Inference in First-Order Logic

goal â€œX is 4+3â€ succeeds with X bound to 7. On the other hand, the goal â€œ5 is X+Yâ€
fails, because the built-in functions do not do arbitrary equation solving.5
â€¢ There are built-in predicates that have side effects when executed. These include inputâ€“
output predicates and the assert/retract predicates for modifying the knowledge
base. Such predicates have no counterpart in logic and can produce confusing resultsâ€”
for example, if facts are asserted in a branch of the proof tree that eventually fails.
â€¢ The occur check is omitted from Prologâ€™s unification algorithm. This means that some
unsound inferences can be made; these are almost never a problem in practice.
â€¢ Prolog uses depth-first backward-chaining search with no checks for infinite recursion.
This makes it very fast when given the right set of axioms, but incomplete when given
the wrong ones.
Prologâ€™s design represents a compromise between declarativeness and execution efficiencyâ€”
inasmuch as efficiency was understood at the time Prolog was designed.

9.4.3 Efficient implementation of logic programs

CHOICE POINT

TRAIL

The execution of a Prolog program can happen in two modes: interpreted and compiled.
Interpretation essentially amounts to running the FOL-BC-A SK algorithm from Figure 9.6,
with the program as the knowledge base. We say â€œessentiallyâ€ because Prolog interpreters
contain a variety of improvements designed to maximize speed. Here we consider only two.
First, our implementation had to explicitly manage the iteration over possible results
generated by each of the subfunctions. Prolog interpreters have a global data structure,
a stack of choice points, to keep track of the multiple possibilities that we considered in
FOL-BC-O R . This global stack is more efficient, and it makes debugging easier, because
the debugger can move up and down the stack.
Second, our simple implementation of FOL-BC-A SK spends a good deal of time generating substitutions. Instead of explicitly constructing substitutions, Prolog has logic variables
that remember their current binding. At any point in time, every variable in the program either is unbound or is bound to some value. Together, these variables and values implicitly
define the substitution for the current branch of the proof. Extending the path can only add
new variable bindings, because an attempt to add a different binding for an already bound
variable results in a failure of unification. When a path in the search fails, Prolog will back
up to a previous choice point, and then it might have to unbind some variables. This is done
by keeping track of all the variables that have been bound in a stack called the trail. As each
new variable is bound by U NIFY-VAR , the variable is pushed onto the trail. When a goal fails
and it is time to back up to a previous choice point, each of the variables is unbound as it is
removed from the trail.
Even the most efficient Prolog interpreters require several thousand machine instructions per inference step because of the cost of index lookup, unification, and building the
recursive call stack. In effect, the interpreter always behaves as if it has never seen the program before; for example, it has to find clauses that match the goal. A compiled Prolog
5

Note that if the Peano axioms are provided, such goals can be solved by inference within a Prolog program.

Section 9.4.

Backward Chaining

341

procedure A PPEND(ax , y, az , continuation)
trail â† G LOBAL -T RAIL -P OINTER()
if ax = [ ] and U NIFY (y, az ) then C ALL(continuation)
R ESET-T RAIL(trail)
a, x , z â† N EW-VARIABLE(), N EW-VARIABLE(), N EW-VARIABLE()
if U NIFY(ax , [a | x ]) and U NIFY(az , [a | z ]) then A PPEND(x , y, z , continuation)
Figure 9.8 Pseudocode representing the result of compiling the Append predicate. The
function N EW-VARIABLE returns a new variable, distinct from all other variables used so far.
The procedure C ALL(continuation) continues execution with the specified continuation.

OPEN-CODE

CONTINUATION

program, on the other hand, is an inference procedure for a specific set of clauses, so it knows
what clauses match the goal. Prolog basically generates a miniature theorem prover for each
different predicate, thereby eliminating much of the overhead of interpretation. It is also possible to open-code the unification routine for each different call, thereby avoiding explicit
analysis of term structure. (For details of open-coded unification, see Warren et al. (1977).)
The instruction sets of todayâ€™s computers give a poor match with Prologâ€™s semantics,
so most Prolog compilers compile into an intermediate language rather than directly into machine language. The most popular intermediate language is the Warren Abstract Machine,
or WAM, named after David H. D. Warren, one of the implementers of the first Prolog compiler. The WAM is an abstract instruction set that is suitable for Prolog and can be either
interpreted or translated into machine language. Other compilers translate Prolog into a highlevel language such as Lisp or C and then use that languageâ€™s compiler to translate to machine
language. For example, the definition of the Append predicate can be compiled into the code
shown in Figure 9.8. Several points are worth mentioning:
â€¢ Rather than having to search the knowledge base for Append clauses, the clauses become a procedure and the inferences are carried out simply by calling the procedure.
â€¢ As described earlier, the current variable bindings are kept on a trail. The first step of the
procedure saves the current state of the trail, so that it can be restored by R ESET-T RAIL
if the first clause fails. This will undo any bindings generated by the first call to U NIFY .
â€¢ The trickiest part is the use of continuations to implement choice points. You can think
of a continuation as packaging up a procedure and a list of arguments that together
define what should be done next whenever the current goal succeeds. It would not
do just to return from a procedure like A PPEND when the goal succeeds, because it
could succeed in several ways, and each of them has to be explored. The continuation
argument solves this problem because it can be called each time the goal succeeds. In
the A PPEND code, if the first argument is empty and the second argument unifies with
the third, then the A PPEND predicate has succeeded. We then C ALL the continuation,
with the appropriate bindings on the trail, to do whatever should be done next. For
example, if the call to A PPEND were at the top level, the continuation would print the
bindings of the variables.

342

OR-PARALLELISM

AND-PARALLELISM

Chapter

9.

Inference in First-Order Logic

Before Warrenâ€™s work on the compilation of inference in Prolog, logic programming was
too slow for general use. Compilers by Warren and others allowed Prolog code to achieve
speeds that are competitive with C on a variety of standard benchmarks (Van Roy, 1990).
Of course, the fact that one can write a planner or natural language parser in a few dozen
lines of Prolog makes it somewhat more desirable than C for prototyping most small-scale AI
research projects.
Parallelization can also provide substantial speedup. There are two principal sources of
parallelism. The first, called OR-parallelism, comes from the possibility of a goal unifying
with many different clauses in the knowledge base. Each gives rise to an independent branch
in the search space that can lead to a potential solution, and all such branches can be solved
in parallel. The second, called AND-parallelism, comes from the possibility of solving
each conjunct in the body of an implication in parallel. AND-parallelism is more difficult to
achieve, because solutions for the whole conjunction require consistent bindings for all the
variables. Each conjunctive branch must communicate with the other branches to ensure a
global solution.

9.4.4 Redundant inference and infinite loops
We now turn to the Achilles heel of Prolog: the mismatch between depth-first search and
search trees that include repeated states and infinite paths. Consider the following logic program that decides if a path exists between two points on a directed graph:
path(X,Z) :- link(X,Z).
path(X,Z) :- path(X,Y), link(Y,Z).
A simple three-node graph, described by the facts link(a,b) and link(b,c), is shown
in Figure 9.9(a). With this program, the query path(a,c) generates the proof tree shown
in Figure 9.10(a). On the other hand, if we put the two clauses in the order
path(X,Z) :- path(X,Y), link(Y,Z).
path(X,Z) :- link(X,Z).

DYNAMIC
PROGRAMMING

then Prolog follows the infinite path shown in Figure 9.10(b). Prolog is therefore incomplete
as a theorem prover for definite clausesâ€”even for Datalog programs, as this example showsâ€”
because, for some knowledge bases, it fails to prove sentences that are entailed. Notice that
forward chaining does not suffer from this problem: once path(a,b), path(b,c), and
path(a,c) are inferred, forward chaining halts.
Depth-first backward chaining also has problems with redundant computations. For
example, when finding a path from A1 to J4 in Figure 9.9(b), Prolog performs 877 inferences,
most of which involve finding all possible paths to nodes from which the goal is unreachable.
This is similar to the repeated-state problem discussed in Chapter 3. The total amount of
inference can be exponential in the number of ground facts that are generated. If we apply
forward chaining instead, at most n2 path(X,Y) facts can be generated linking n nodes.
For the problem in Figure 9.9(b), only 62 inferences are needed.
Forward chaining on graph search problems is an example of dynamic programming,
in which the solutions to subproblems are constructed incrementally from those of smaller

Section 9.4.

Backward Chaining

343
A1

A

B

C

(a)

J4

(b)

Figure 9.9 (a) Finding a path from A to C can lead Prolog into an infinite loop. (b) A
graph in which each node is connected to two random successors in the next layer. Finding a
path from A1 to J4 requires 877 inferences.

path(a,c)

path(a,c)

path(a,Y)

link(a,c)

path(a,Y)

fail

link(Y,c)

link(b,c)
{}

path(a,Yâ€™)

link(Yâ€™,Y)

link(a,Y)
{ Y / b}

(a)

(b)

Figure 9.10 (a) Proof that a path exists from A to C. (b) Infinite proof tree generated
when the clauses are in the â€œwrongâ€ order.

TABLED LOGIC
PROGRAMMING

subproblems and are cached to avoid recomputation. We can obtain a similar effect in a
backward chaining system using memoizationâ€”that is, caching solutions to subgoals as
they are found and then reusing those solutions when the subgoal recurs, rather than repeating the previous computation. This is the approach taken by tabled logic programming systems, which use efficient storage and retrieval mechanisms to perform memoization. Tabled
logic programming combines the goal-directedness of backward chaining with the dynamicprogramming efficiency of forward chaining. It is also complete for Datalog knowledge
bases, which means that the programmer need worry less about infinite loops. (It is still possible to get an infinite loop with predicates like father(X,Y) that refer to a potentially
unbounded number of objects.)

9.4.5 Database semantics of Prolog
Prolog uses database semantics, as discussed in Section 8.2.8. The unique names assumption
says that every Prolog constant and every ground term refers to a distinct object, and the
closed world assumption says that the only sentences that are true are those that are entailed

344

Chapter

9.

Inference in First-Order Logic

by the knowledge base. There is no way to assert that a sentence is false in Prolog. This makes
Prolog less expressive than first-order logic, but it is part of what makes Prolog more efficient
and more concise. Consider the following Prolog assertions about some course offerings:
Course(CS , 101), Course(CS , 102), Course(CS , 106), Course(EE , 101). (9.11)
Under the unique names assumption, CS and EE are different (as are 101, 102, and 106),
so this means that there are four distinct courses. Under the closed-world assumption there
are no other courses, so there are exactly four courses. But if these were assertions in FOL
rather than in Prolog, then all we could say is that there are somewhere between one and
infinity courses. Thatâ€™s because the assertions (in FOL) do not deny the possibility that other
unmentioned courses are also offered, nor do they say that the courses mentioned are different
from each other. If we wanted to translate Equation (9.11) into FOL, we would get this:
Course(d, n)

â‡”

(d = CS âˆ§ n = 101) âˆ¨ (d = CS âˆ§ n = 102)
âˆ¨ (d = CS âˆ§ n = 106) âˆ¨ (d = EE âˆ§ n = 101) .

COMPLETION

(9.12)

This is called the completion of Equation (9.11). It expresses in FOL the idea that there are
at most four courses. To express in FOL the idea that there are at least four courses, we need
to write the completion of the equality predicate:
x=y

â‡”

(x = CS âˆ§ y = CS ) âˆ¨ (x = EE âˆ§ y = EE ) âˆ¨ (x = 101 âˆ§ y = 101)
âˆ¨ (x = 102 âˆ§ y = 102) âˆ¨ (x = 106 âˆ§ y = 106) .

The completion is useful for understanding database semantics, but for practical purposes, if
your problem can be described with database semantics, it is more efficient to reason with
Prolog or some other database semantics system, rather than translating into FOL and reasoning with a full FOL theorem prover.

9.4.6 Constraint logic programming
In our discussion of forward chaining (Section 9.3), we showed how constraint satisfaction
problems (CSPs) can be encoded as definite clauses. Standard Prolog solves such problems
in exactly the same way as the backtracking algorithm given in Figure 6.5.
Because backtracking enumerates the domains of the variables, it works only for finitedomain CSPs. In Prolog terms, there must be a finite number of solutions for any goal
with unbound variables. (For example, the goal diff(Q,SA), which says that Queensland
and South Australia must be different colors, has six solutions if three colors are allowed.)
Infinite-domain CSPsâ€”for example, with integer or real-valued variablesâ€”require quite different algorithms, such as bounds propagation or linear programming.
Consider the following example. We define triangle(X,Y,Z) as a predicate that
holds if the three arguments are numbers that satisfy the triangle inequality:
triangle(X,Y,Z) :X>0, Y>0, Z>0, X+Y>=Z, Y+Z>=X, X+Z>=Y.
If we ask Prolog the query triangle(3,4,5), it succeeds. On the other hand, if we
ask triangle(3,4,Z), no solution will be found, because the subgoal Z>=0 cannot be
handled by Prolog; we canâ€™t compare an unbound value to 0.

Section 9.5.
CONSTRAINT LOGIC
PROGRAMMING

METARULE

9.5

Resolution

345

Constraint logic programming (CLP) allows variables to be constrained rather than
bound. A CLP solution is the most specific set of constraints on the query variables that can
be derived from the knowledge base. For example, the solution to the triangle(3,4,Z)
query is the constraint 7 >= Z >= 1. Standard logic programs are just a special case of
CLP in which the solution constraints must be equality constraintsâ€”that is, bindings.
CLP systems incorporate various constraint-solving algorithms for the constraints allowed in the language. For example, a system that allows linear inequalities on real-valued
variables might include a linear programming algorithm for solving those constraints. CLP
systems also adopt a much more flexible approach to solving standard logic programming
queries. For example, instead of depth-first, left-to-right backtracking, they might use any of
the more efficient algorithms discussed in Chapter 6, including heuristic conjunct ordering,
backjumping, cutset conditioning, and so on. CLP systems therefore combine elements of
constraint satisfaction algorithms, logic programming, and deductive databases.
Several systems that allow the programmer more control over the search order for inference have been defined. The MRS language (Genesereth and Smith, 1981; Russell, 1985)
allows the programmer to write metarules to determine which conjuncts are tried first. The
user could write a rule saying that the goal with the fewest variables should be tried first or
could write domain-specific rules for particular predicates.

R ESOLUTION
The last of our three families of logical systems is based on resolution. We saw on page 250
that propositional resolution using refutation is a complete inference procedure for propositional logic. In this section, we describe how to extend resolution to first-order logic.

9.5.1 Conjunctive normal form for first-order logic
As in the propositional case, first-order resolution requires that sentences be in conjunctive
normal form (CNF)â€”that is, a conjunction of clauses, where each clause is a disjunction of
literals.6 Literals can contain variables, which are assumed to be universally quantified. For
example, the sentence
âˆ€ x American(x) âˆ§ Weapon(y) âˆ§ Sells(x, y, z) âˆ§ Hostile(z) â‡’ Criminal (x)
becomes, in CNF,
Â¬American(x) âˆ¨ Â¬Weapon(y) âˆ¨ Â¬Sells(x, y, z) âˆ¨ Â¬Hostile(z) âˆ¨ Criminal (x) .
Every sentence of first-order logic can be converted into an inferentially equivalent CNF
sentence. In particular, the CNF sentence will be unsatisfiable just when the original sentence
is unsatisfiable, so we have a basis for doing proofs by contradiction on the CNF sentences.
6 A clause can also be represented as an implication with a conjunction of atoms in the premise and a disjunction
of atoms in the conclusion (Exercise 7.13). This is called implicative normal form or Kowalski form (especially
when written with a right-to-left implication symbol (Kowalski, 1979)) and is often much easier to read.

346

Chapter

9.

Inference in First-Order Logic

The procedure for conversion to CNF is similar to the propositional case, which we saw
on page 253. The principal difference arises from the need to eliminate existential quantifiers.
We illustrate the procedure by translating the sentence â€œEveryone who loves all animals is
loved by someone,â€ or
âˆ€ x [âˆ€ y Animal(y) â‡’ Loves(x, y)] â‡’ [âˆƒ y Loves(y, x)] .
The steps are as follows:
â€¢ Eliminate implications:
âˆ€ x [Â¬âˆ€ y Â¬Animal(y) âˆ¨ Loves(x, y)] âˆ¨ [âˆƒ y Loves(y, x)] .
â€¢ Move Â¬ inwards: In addition to the usual rules for negated connectives, we need rules
for negated quantifiers. Thus, we have
Â¬âˆ€ x p
becomes
âˆƒ x Â¬p
Â¬âˆƒ x p
becomes
âˆ€ x Â¬p .
Our sentence goes through the following transformations:
âˆ€ x [âˆƒ y Â¬(Â¬Animal(y) âˆ¨ Loves(x, y))] âˆ¨ [âˆƒ y Loves(y, x)] .
âˆ€ x [âˆƒ y Â¬Â¬Animal(y) âˆ§ Â¬Loves(x, y)] âˆ¨ [âˆƒ y Loves(y, x)] .
âˆ€ x [âˆƒ y Animal (y) âˆ§ Â¬Loves(x, y)] âˆ¨ [âˆƒ y Loves(y, x)] .
Notice how a universal quantifier (âˆ€ y) in the premise of the implication has become
an existential quantifier. The sentence now reads â€œEither there is some animal that x
doesnâ€™t love, or (if this is not the case) someone loves x.â€ Clearly, the meaning of the
original sentence has been preserved.
â€¢ Standardize variables: For sentences like (âˆƒ x P (x)) âˆ¨ (âˆƒ x Q(x)) which use the same
variable name twice, change the name of one of the variables. This avoids confusion
later when we drop the quantifiers. Thus, we have
SKOLEMIZATION

âˆ€ x [âˆƒ y Animal (y) âˆ§ Â¬Loves(x, y)] âˆ¨ [âˆƒ z Loves(z, x)] .
â€¢ Skolemize: Skolemization is the process of removing existential quantifiers by elimination. In the simple case, it is just like the Existential Instantiation rule of Section 9.1:
translate âˆƒ x P (x) into P (A), where A is a new constant. However, we canâ€™t apply Existential Instantiation to our sentence above because it doesnâ€™t match the pattern âˆƒ v Î±;
only parts of the sentence match the pattern. If we blindly apply the rule to the two
matching parts we get
âˆ€ x [Animal (A) âˆ§ Â¬Loves(x, A)] âˆ¨ Loves(B, x) ,
which has the wrong meaning entirely: it says that everyone either fails to love a particular animal A or is loved by some particular entity B. In fact, our original sentence
allows each person to fail to love a different animal or to be loved by a different person.
Thus, we want the Skolem entities to depend on x and z:

SKOLEM FUNCTION

âˆ€ x [Animal (F (x)) âˆ§ Â¬Loves(x, F (x))] âˆ¨ Loves(G(z), x) .
Here F and G are Skolem functions. The general rule is that the arguments of the
Skolem function are all the universally quantified variables in whose scope the existential quantifier appears. As with Existential Instantiation, the Skolemized sentence is
satisfiable exactly when the original sentence is satisfiable.

Section 9.5.

Resolution

347

â€¢ Drop universal quantifiers: At this point, all remaining variables must be universally
quantified. Moreover, the sentence is equivalent to one in which all the universal quantifiers have been moved to the left. We can therefore drop the universal quantifiers:
[Animal (F (x)) âˆ§ Â¬Loves(x, F (x))] âˆ¨ Loves(G(z), x) .
â€¢ Distribute âˆ¨ over âˆ§:
[Animal (F (x)) âˆ¨ Loves(G(z), x)] âˆ§ [Â¬Loves(x, F (x)) âˆ¨ Loves(G(z), x)] .
This step may also require flattening out nested conjunctions and disjunctions.
The sentence is now in CNF and consists of two clauses. It is quite unreadable. (It may
help to explain that the Skolem function F (x) refers to the animal potentially unloved by x,
whereas G(z) refers to someone who might love x.) Fortunately, humans seldom need look
at CNF sentencesâ€”the translation process is easily automated.

9.5.2 The resolution inference rule
The resolution rule for first-order clauses is simply a lifted version of the propositional resolution rule given on page 253. Two clauses, which are assumed to be standardized apart so
that they share no variables, can be resolved if they contain complementary literals. Propositional literals are complementary if one is the negation of the other; first-order literals are
complementary if one unifies with the negation of the other. Thus, we have
m1 âˆ¨ Â· Â· Â· âˆ¨ m n
1 âˆ¨ Â· Â· Â· âˆ¨  k ,
S UBST (Î¸, 1 âˆ¨ Â· Â· Â· âˆ¨ iâˆ’1 âˆ¨ i+1 âˆ¨ Â· Â· Â· âˆ¨ k âˆ¨ m1 âˆ¨ Â· Â· Â· âˆ¨ mjâˆ’1 âˆ¨ mj+1 âˆ¨ Â· Â· Â· âˆ¨ mn )
where U NIFY (i , Â¬mj ) = Î¸. For example, we can resolve the two clauses
[Animal (F (x)) âˆ¨ Loves(G(x), x)]

and

[Â¬Loves(u, v) âˆ¨ Â¬Kills(u, v)]

by eliminating the complementary literals Loves(G(x), x) and Â¬Loves(u, v), with unifier
Î¸ = {u/G(x), v/x}, to produce the resolvent clause
[Animal (F (x)) âˆ¨ Â¬Kills(G(x), x)] .
BINARY RESOLUTION

This rule is called the binary resolution rule because it resolves exactly two literals. The
binary resolution rule by itself does not yield a complete inference procedure. The full resolution rule resolves subsets of literals in each clause that are unifiable. An alternative approach
is to extend factoringâ€”the removal of redundant literalsâ€”to the first-order case. Propositional factoring reduces two literals to one if they are identical; first-order factoring reduces
two literals to one if they are unifiable. The unifier must be applied to the entire clause. The
combination of binary resolution and factoring is complete.

9.5.3 Example proofs
Resolution proves that KB |= Î± by proving KB âˆ§ Â¬Î± unsatisfiable, that is, by deriving the
empty clause. The algorithmic approach is identical to the propositional case, described in

Chapter

^

Enemy(Nono, America)

Figure 9.11
are in bold.

Â¬Owns(Nono,M1) Â¬Hostile(Nono)

Â¬Owns(Nono,M1)

^

Â¬Enemy(x,America) Hostile(x)

Â¬Missile(M1)

^

^

Owns(Nono, M1)

Â¬Sells(West,M1,z) Â¬Hostile(z)

^

^

Missile(M1)

Â¬Missile(y) Â¬Sells(West,y,z) Â¬Hostile(z)

^

Sells(West,x, Nono)

^

Â¬Missile(x) Â¬Owns(Nono,x)

Â¬Sells(West,y,z) Â¬Hostile(z)

^

^

Missile(M1)

Â¬Weapon(y)

^

^

Â¬Missile(x) Weapon(x)

Â¬American(West) Â¬Weapon(y) Â¬Sells(West,y,z) Â¬Hostile(z)

^

^

American(West)

Â¬Criminal(West)

^

Â¬Sells(x,y,z) Â¬Hostile(z) Criminal(x)

Inference in First-Order Logic

^

^

Â¬Weapon(y)

^

Â¬American(x)

9.

^

348

Â¬Hostile(Nono)

Â¬Hostile(Nono)
Â¬Enemy(Nono, America)

A resolution proof that West is a criminal. At each step, the literals that unify

Figure 7.12, so we need not repeat it here. Instead, we give two example proofs. The first is
the crime example from Section 9.3. The sentences in CNF are
Â¬American(x) âˆ¨ Â¬Weapon(y) âˆ¨ Â¬Sells(x, y, z) âˆ¨ Â¬Hostile(z) âˆ¨ Criminal (x)
Â¬Missile(x) âˆ¨ Â¬Owns(Nono, x) âˆ¨ Sells(West, x, Nono)
Â¬Enemy(x, America) âˆ¨ Hostile(x)
Â¬Missile(x) âˆ¨ Weapon(x)
Missile(M1 )
Owns(Nono, M1 )
American(West)
Enemy(Nono, America) .
We also include the negated goal Â¬Criminal (West). The resolution proof is shown in Figure 9.11. Notice the structure: single â€œspineâ€ beginning with the goal clause, resolving against
clauses from the knowledge base until the empty clause is generated. This is characteristic
of resolution on Horn clause knowledge bases. In fact, the clauses along the main spine
correspond exactly to the consecutive values of the goals variable in the backward-chaining
algorithm of Figure 9.6. This is because we always choose to resolve with a clause whose
positive literal unified with the leftmost literal of the â€œcurrentâ€ clause on the spine; this is
exactly what happens in backward chaining. Thus, backward chaining is just a special case
of resolution with a particular control strategy to decide which resolution to perform next.
Our second example makes use of Skolemization and involves clauses that are not definite clauses. This results in a somewhat more complex proof structure. In English, the
problem is as follows:
Everyone who loves all animals is loved by someone.
Anyone who kills an animal is loved by no one.
Jack loves all animals.
Either Jack or Curiosity killed the cat, who is named Tuna.
Did Curiosity kill the cat?

Resolution

349

First, we express the original sentences, some background knowledge, and the negated goal
G in first-order logic:
A.

âˆ€ x [âˆ€ y Animal (y) â‡’ Loves(x, y)] â‡’ [âˆƒ y Loves(y, x)]

B.

âˆ€ x [âˆƒ z Animal (z) âˆ§ Kills(x, z)] â‡’ [âˆ€ y Â¬Loves(y, x)]

C.

âˆ€ x Animal(x) â‡’ Loves(Jack , x)

D.

Kills(Jack , Tuna) âˆ¨ Kills(Curiosity, Tuna)

E.

Cat(Tuna)

F.

âˆ€ x Cat(x) â‡’ Animal (x)

Â¬G.

Â¬Kills(Curiosity, Tuna)

Now we apply the conversion procedure to convert each sentence to CNF:
A1.

Animal(F (x)) âˆ¨ Loves(G(x), x)

A2.

Â¬Loves(x, F (x)) âˆ¨ Loves(G(x), x)

B.

Â¬Loves(y, x) âˆ¨ Â¬Animal (z) âˆ¨ Â¬Kills(x, z)

C.

Â¬Animal(x) âˆ¨ Loves(Jack , x)

D.

Kills(Jack , Tuna) âˆ¨ Kills(Curiosity, Tuna)

E.

Cat(Tuna)

F.

Â¬Cat(x) âˆ¨ Animal (x)

Â¬G.

Â¬Kills(Curiosity, Tuna)

The resolution proof that Curiosity killed the cat is given in Figure 9.12. In English, the proof
could be paraphrased as follows:
Suppose Curiosity did not kill Tuna. We know that either Jack or Curiosity did; thus
Jack must have. Now, Tuna is a cat and cats are animals, so Tuna is an animal. Because
anyone who kills an animal is loved by no one, we know that no one loves Jack. On the
other hand, Jack loves all animals, so someone loves him; so we have a contradiction.
Therefore, Curiosity killed the cat.

Â¬Kills(x, Tuna)

^

Â¬Loves(y, Jack)

Kills(Jack, Tuna)

Â¬Kills(Curiosity, Tuna)

Â¬Loves(x, F(x))

Â¬Animal(F(Jack))

^

Â¬Kills(x, z)

Kills(Curiosity, Tuna)

Loves(G(x), x)

Loves(G(Jack), Jack)

Â¬Animal(x)

Animal(F(x))

^

^

Â¬Loves(y, x)

Â¬Animal(z)

^

Â¬Loves(y, x)

Kills(Jack, Tuna)

^

Animal(x)

^

Animal(Tuna)

Â¬Cat(x)

^

Cat(Tuna)

^

Section 9.5.

Loves(Jack, x)

Loves(G(x), x)

Loves(G(Jack), Jack)

Figure 9.12 A resolution proof that Curiosity killed the cat. Notice the use of factoring
in the derivation of the clause Loves(G(Jack ), Jack ). Notice also in the upper right, the
unification of Loves(x, F (x)) and Loves(Jack, x) can only succeed after the variables have
been standardized apart.

350

NONCONSTRUCTIVE
PROOF

ANSWER LITERAL

Chapter

9.

Inference in First-Order Logic

The proof answers the question â€œDid Curiosity kill the cat?â€ but often we want to pose more
general questions, such as â€œWho killed the cat?â€ Resolution can do this, but it takes a little
more work to obtain the answer. The goal is âˆƒ w Kills(w, Tuna), which, when negated,
becomes Â¬Kills(w, Tuna) in CNF. Repeating the proof in Figure 9.12 with the new negated
goal, we obtain a similar proof tree, but with the substitution {w/Curiosity } in one of the
steps. So, in this case, finding out who killed the cat is just a matter of keeping track of the
bindings for the query variables in the proof.
Unfortunately, resolution can produce nonconstructive proofs for existential goals.
For example, Â¬Kills(w, Tuna) resolves with Kills(Jack , Tuna) âˆ¨ Kills(Curiosity , Tuna)
to give Kills(Jack , Tuna), which resolves again with Â¬Kills(w, Tuna) to yield the empty
clause. Notice that w has two different bindings in this proof; resolution is telling us that,
yes, someone killed Tunaâ€”either Jack or Curiosity. This is no great surprise! One solution is to restrict the allowed resolution steps so that the query variables can be bound
only once in a given proof; then we need to be able to backtrack over the possible bindings. Another solution is to add a special answer literal to the negated goal, which becomes Â¬Kills(w, Tuna) âˆ¨ Answer (w). Now, the resolution process generates an answer
whenever a clause is generated containing just a single answer literal. For the proof in Figure 9.12, this is Answer(Curiosity ). The nonconstructive proof would generate the clause
Answer (Curiosity) âˆ¨ Answer(Jack ), which does not constitute an answer.

9.5.4 Completeness of resolution

REFUTATION
COMPLETENESS

This section gives a completeness proof of resolution. It can be safely skipped by those who
are willing to take it on faith.
We show that resolution is refutation-complete, which means that if a set of sentences
is unsatisfiable, then resolution will always be able to derive a contradiction. Resolution
cannot be used to generate all logical consequences of a set of sentences, but it can be used
to establish that a given sentence is entailed by the set of sentences. Hence, it can be used to
find all answers to a given question, Q(x), by proving that KB âˆ§ Â¬Q(x) is unsatisfiable.
We take it as given that any sentence in first-order logic (without equality) can be rewritten as a set of clauses in CNF. This can be proved by induction on the form of the sentence,
using atomic sentences as the base case (Davis and Putnam, 1960). Our goal therefore is to
prove the following: if S is an unsatisfiable set of clauses, then the application of a finite
number of resolution steps to S will yield a contradiction.
Our proof sketch follows Robinsonâ€™s original proof with some simplifications from
Genesereth and Nilsson (1987). The basic structure of the proof (Figure 9.13) is as follows:
1. First, we observe that if S is unsatisfiable, then there exists a particular set of ground
instances of the clauses of S such that this set is also unsatisfiable (Herbrandâ€™s theorem).
2. We then appeal to the ground resolution theorem given in Chapter 7, which states that
propositional resolution is complete for ground sentences.
3. We then use a lifting lemma to show that, for any propositional resolution proof using
the set of ground sentences, there is a corresponding first-order resolution proof using
the first-order sentences from which the ground sentences were obtained.

Section 9.5.

Resolution

351

Any set of sentences S is representable in clausal form

Assume S is unsatisfiable, and in clausal form
Herbrandâ€™s theorem
Some set S' of ground instances is unsatisfiable
Ground resolution
theorem
Resolution can find a contradiction in S'
Lifting lemma
There is a resolution proof for the contradiction in S'

Figure 9.13

Structure of a completeness proof for resolution.

To carry out the first step, we need three new concepts:
HERBRAND
UNIVERSE

â€¢ Herbrand universe: If S is a set of clauses, then HS , the Herbrand universe of S, is
the set of all ground terms constructable from the following:
a. The function symbols in S, if any.
b. The constant symbols in S, if any; if none, then the constant symbol A.
For example, if S contains just the clause Â¬P (x, F (x, A)) âˆ¨ Â¬Q(x, A) âˆ¨ R(x, B), then
HS is the following infinite set of ground terms:
{A, B, F (A, A), F (A, B), F (B, A), F (B, B), F (A, F (A, A)), . . .} .

SATURATION

HERBRAND BASE

HERBRANDâ€™S
THEOREM

â€¢ Saturation: If S is a set of clauses and P is a set of ground terms, then P (S), the
saturation of S with respect to P , is the set of all ground clauses obtained by applying
all possible consistent substitutions of ground terms in P with variables in S.
â€¢ Herbrand base: The saturation of a set S of clauses with respect to its Herbrand universe is called the Herbrand base of S, written as HS (S). For example, if S contains
solely the clause just given, then HS (S) is the infinite set of clauses
{Â¬P (A, F (A, A)) âˆ¨ Â¬Q(A, A) âˆ¨ R(A, B),
Â¬P (B, F (B, A)) âˆ¨ Â¬Q(B, A) âˆ¨ R(B, B),
Â¬P (F (A, A), F (F (A, A), A)) âˆ¨ Â¬Q(F (A, A), A) âˆ¨ R(F (A, A), B),
Â¬P (F (A, B), F (F (A, B), A)) âˆ¨ Â¬Q(F (A, B), A) âˆ¨ R(F (A, B), B), . . . }
These definitions allow us to state a form of Herbrandâ€™s theorem (Herbrand, 1930):
If a set S of clauses is unsatisfiable, then there exists a finite subset of HS (S) that
is also unsatisfiable.
Let S  be this finite subset of ground sentences. Now, we can appeal to the ground resolution
theorem (page 255) to show that the resolution closure RC (S  ) contains the empty clause.
That is, running propositional resolution to completion on S  will derive a contradiction.
Now that we have established that there is always a resolution proof involving some
finite subset of the Herbrand base of S, the next step is to show that there is a resolution

352

Chapter

9.

Inference in First-Order Logic

G OÌˆDELâ€™ S I NCOMPLETENESS T HEOREM
By slightly extending the language of first-order logic to allow for the mathematical induction schema in arithmetic, Kurt GoÌˆdel was able to show, in his incompleteness theorem, that there are true arithmetic sentences that cannot be proved.
The proof of the incompleteness theorem is somewhat beyond the scope of
this book, occupying, as it does, at least 30 pages, but we can give a hint here. We
begin with the logical theory of numbers. In this theory, there is a single constant,
0, and a single function, S (the successor function). In the intended model, S(0)
denotes 1, S(S(0)) denotes 2, and so on; the language therefore has names for all
the natural numbers. The vocabulary also includes the function symbols +, Ã—, and
Expt (exponentiation) and the usual set of logical connectives and quantifiers. The
first step is to notice that the set of sentences that we can write in this language can
be enumerated. (Imagine defining an alphabetical order on the symbols and then
arranging, in alphabetical order, each of the sets of sentences of length 1, 2, and
so on.) We can then number each sentence Î± with a unique natural number #Î±
(the GoÌˆdel number). This is crucial: number theory contains a name for each of
its own sentences. Similarly, we can number each possible proof P with a GoÌˆdel
number G(P ), because a proof is simply a finite sequence of sentences.
Now suppose we have a recursively enumerable set A of sentences that are
true statements about the natural numbers. Recalling that A can be named by a
given set of integers, we can imagine writing in our language a sentence Î±(j, A) of
the following sort:
âˆ€ i i is not the GoÌˆdel number of a proof of the sentence whose GoÌˆdel
number is j, where the proof uses only premises in A.
Then let Ïƒ be the sentence Î±(#Ïƒ, A), that is, a sentence that states its own unprovability from A. (That this sentence always exists is true but not entirely obvious.)
Now we make the following ingenious argument: Suppose that Ïƒ is provable
from A; then Ïƒ is false (because Ïƒ says it cannot be proved). But then we have a
false sentence that is provable from A, so A cannot consist of only true sentencesâ€”
a violation of our premise. Therefore, Ïƒ is not provable from A. But this is exactly
what Ïƒ itself claims; hence Ïƒ is a true sentence.
So, we have shown (barring 29 21 pages) that for any set of true sentences of
number theory, and in particular any set of basic axioms, there are other true sentences that cannot be proved from those axioms. This establishes, among other
things, that we can never prove all the theorems of mathematics within any given
system of axioms. Clearly, this was an important discovery for mathematics. Its
significance for AI has been widely debated, beginning with speculations by GoÌˆdel
himself. We take up the debate in Chapter 26.

Section 9.5.

Resolution

353

proof using the clauses of S itself, which are not necessarily ground clauses. We start by
considering a single application of the resolution rule. Robinson stated this lemma:
Let C1 and C2 be two clauses with no shared variables, and let C1 and C2 be
ground instances of C1 and C2 . If C  is a resolvent of C1 and C2 , then there exists
a clause C such that (1) C is a resolvent of C1 and C2 and (2) C  is a ground
instance of C.
LIFTING LEMMA

This is called a lifting lemma, because it lifts a proof step from ground clauses up to general
first-order clauses. In order to prove his basic lifting lemma, Robinson had to invent unification and derive all of the properties of most general unifiers. Rather than repeat the proof
here, we simply illustrate the lemma:
C1 = Â¬P (x, F (x, A)) âˆ¨ Â¬Q(x, A) âˆ¨ R(x, B)
C2 = Â¬N (G(y), z) âˆ¨ P (H(y), z)
C1 = Â¬P (H(B), F (H(B), A)) âˆ¨ Â¬Q(H(B), A) âˆ¨ R(H(B), B)
C2 = Â¬N (G(B), F (H(B), A)) âˆ¨ P (H(B), F (H(B), A))
C  = Â¬N (G(B), F (H(B), A)) âˆ¨ Â¬Q(H(B), A) âˆ¨ R(H(B), B)
C = Â¬N (G(y), F (H(y), A)) âˆ¨ Â¬Q(H(y), A) âˆ¨ R(H(y), B) .
We see that indeed C  is a ground instance of C. In general, for C1 and C2 to have any
resolvents, they must be constructed by first applying to C1 and C2 the most general unifier
of a pair of complementary literals in C1 and C2 . From the lifting lemma, it is easy to derive
a similar statement about any sequence of applications of the resolution rule:
For any clause C  in the resolution closure of S  there is a clause C in the resolution closure of S such that C  is a ground instance of C and the derivation of C is
the same length as the derivation of C  .
From this fact, it follows that if the empty clause appears in the resolution closure of S  , it
must also appear in the resolution closure of S. This is because the empty clause cannot be a
ground instance of any other clause. To recap: we have shown that if S is unsatisfiable, then
there is a finite derivation of the empty clause using the resolution rule.
The lifting of theorem proving from ground clauses to first-order clauses provides a vast
increase in power. This increase comes from the fact that the first-order proof need instantiate
variables only as far as necessary for the proof, whereas the ground-clause methods were
required to examine a huge number of arbitrary instantiations.

9.5.5 Equality
None of the inference methods described so far in this chapter handle an assertion of the form
x = y. Three distinct approaches can be taken. The first approach is to axiomatize equalityâ€”
to write down sentences about the equality relation in the knowledge base. We need to say that
equality is reflexive, symmetric, and transitive, and we also have to say that we can substitute
equals for equals in any predicate or function. So we need three basic axioms, and then one

354

Chapter

9.

Inference in First-Order Logic

for each predicate and function:
âˆ€x x=x
âˆ€ x, y x = y â‡’ y = x
âˆ€ x, y, z x = y âˆ§ y = z â‡’ x = z
âˆ€ x, y x = y â‡’ (P1 (x) â‡” P1 (y))
âˆ€ x, y x = y â‡’ (P2 (x) â‡” P2 (y))
..
.
âˆ€ w, x, y, z w = y âˆ§ x = z â‡’ (F1 (w, x) = F1 (y, z))
âˆ€ w, x, y, z w = y âˆ§ x = z â‡’ (F2 (w, x) = F2 (y, z))
..
.
Given these sentences, a standard inference procedure such as resolution can perform tasks
requiring equality reasoning, such as solving mathematical equations. However, these axioms
will generate a lot of conclusions, most of them not helpful to a proof. So there has been a
search for more efficient ways of handling equality. One alternative is to add inference rules
rather than axioms. The simplest rule, demodulation, takes a unit clause x = y and some
clause Î± that contains the term x, and yields a new clause formed by substituting y for x
within Î±. It works if the term within Î± unifies with x; it need not be exactly equal to x.
Note that demodulation is directional; given x = y, the x always gets replaced with y, never
vice versa. That means that demodulation can be used for simplifying expressions using
demodulators such as x + 0 = x or x1 = x. As another example, given
Father (Father (x)) = PaternalGrandfather (x)
Birthdate (Father (Father (Bella)), 1926)
we can conclude by demodulation
Birthdate (PaternalGrandfather (Bella), 1926) .
More formally, we have
DEMODULATION

â€¢ Demodulation: For any terms x, y, and z, where z appears somewhere in literal mi
and where U NIFY (x, z) = Î¸,
x = y,
m1 âˆ¨ Â· Â· Â· âˆ¨ mn
.
S UB(S UBST (Î¸, x), S UBST (Î¸, y), m1 âˆ¨ Â· Â· Â· âˆ¨ mn )
where S UBST is the usual substitution of a binding list, and S UB(x, y, m) means to
replace x with y everywhere that x occurs within m.
The rule can also be extended to handle non-unit clauses in which an equality literal appears:

PARAMODULATION

â€¢ Paramodulation: For any terms x, y, and z, where z appears somewhere in literal mi ,
and where U NIFY (x, z) = Î¸,
m1 âˆ¨ Â· Â· Â· âˆ¨ mn
1 âˆ¨ Â· Â· Â· âˆ¨ k âˆ¨ x = y,
.
S UB(S UBST (Î¸, x), S UBST (Î¸, y), S UBST (Î¸, 1 âˆ¨ Â· Â· Â· âˆ¨ k âˆ¨ m1 âˆ¨ Â· Â· Â· âˆ¨ mn )
For example, from
P (F (x, B), x) âˆ¨ Q(x)

and

F (A, y) = y âˆ¨ R(y)

Section 9.5.

Resolution

355

we have Î¸ = U NIFY (F (A, y), F (x, B)) = {x/A, y/B}, and we can conclude by paramodulation the sentence
P (B, A) âˆ¨ Q(A) âˆ¨ R(B) .

EQUATIONAL
UNIFICATION

Paramodulation yields a complete inference procedure for first-order logic with equality.
A third approach handles equality reasoning entirely within an extended unification
algorithm. That is, terms are unifiable if they are provably equal under some substitution,
where â€œprovablyâ€ allows for equality reasoning. For example, the terms 1 + 2 and 2 + 1
normally are not unifiable, but a unification algorithm that knows that x + y = y + x could
unify them with the empty substitution. Equational unification of this kind can be done with
efficient algorithms designed for the particular axioms used (commutativity, associativity, and
so on) rather than through explicit inference with those axioms. Theorem provers using this
technique are closely related to the CLP systems described in Section 9.4.

9.5.6 Resolution strategies
We know that repeated applications of the resolution inference rule will eventually find a
proof if one exists. In this subsection, we examine strategies that help find proofs efficiently.
UNIT PREFERENCE

Unit preference: This strategy prefers to do resolutions where one of the sentences is a single
literal (also known as a unit clause). The idea behind the strategy is that we are trying to
produce an empty clause, so it might be a good idea to prefer inferences that produce shorter
clauses. Resolving a unit sentence (such as P ) with any other sentence (such as Â¬P âˆ¨Â¬Qâˆ¨R)
always yields a clause (in this case, Â¬Q âˆ¨ R) that is shorter than the other clause. When
the unit preference strategy was first tried for propositional inference in 1964, it led to a
dramatic speedup, making it feasible to prove theorems that could not be handled without the
preference. Unit resolution is a restricted form of resolution in which every resolution step
must involve a unit clause. Unit resolution is incomplete in general, but complete for Horn
clauses. Unit resolution proofs on Horn clauses resemble forward chaining.
The OTTER theorem prover (Organized Techniques for Theorem-proving and Effective
Research, McCune, 1992), uses a form of best-first search. Its heuristic function measures
the â€œweightâ€ of each clause, where lighter clauses are preferred. The exact choice of heuristic
is up to the user, but generally, the weight of a clause should be correlated with its size or
difficulty. Unit clauses are treated as light; the search can thus be seen as a generalization of
the unit preference strategy.

SET OF SUPPORT

Set of support: Preferences that try certain resolutions first are helpful, but in general it is
more effective to try to eliminate some potential resolutions altogether. For example, we can
insist that every resolution step involve at least one element of a special set of clausesâ€”the
set of support. The resolvent is then added into the set of support. If the set of support is
small relative to the whole knowledge base, the search space will be reduced dramatically.
We have to be careful with this approach because a bad choice for the set of support
will make the algorithm incomplete. However, if we choose the set of support S so that the
remainder of the sentences are jointly satisfiable, then set-of-support resolution is complete.
For example, one can use the negated query as the set of support, on the assumption that the

356

Chapter

9.

Inference in First-Order Logic

original knowledge base is consistent. (After all, if it is not consistent, then the fact that the
query follows from it is vacuous.) The set-of-support strategy has the additional advantage of
generating goal-directed proof trees that are often easy for humans to understand.
INPUT RESOLUTION

LINEAR RESOLUTION

SUBSUMPTION

Input resolution: In this strategy, every resolution combines one of the input sentences (from
the KB or the query) with some other sentence. The proof in Figure 9.11 on page 348 uses
only input resolutions and has the characteristic shape of a single â€œspineâ€ with single sentences combining onto the spine. Clearly, the space of proof trees of this shape is smaller
than the space of all proof graphs. In Horn knowledge bases, Modus Ponens is a kind of
input resolution strategy, because it combines an implication from the original KB with some
other sentences. Thus, it is no surprise that input resolution is complete for knowledge bases
that are in Horn form, but incomplete in the general case. The linear resolution strategy is a
slight generalization that allows P and Q to be resolved together either if P is in the original
KB or if P is an ancestor of Q in the proof tree. Linear resolution is complete.
Subsumption: The subsumption method eliminates all sentences that are subsumed by (that
is, more specific than) an existing sentence in the KB. For example, if P (x) is in the KB, then
there is no sense in adding P (A) and even less sense in adding P (A) âˆ¨ Q(B). Subsumption
helps keep the KB small and thus helps keep the search space small.
Practical uses of resolution theorem provers

SYNTHESIS
VERIFICATION

DEDUCTIVE
SYNTHESIS

Theorem provers can be applied to the problems involved in the synthesis and verification
of both hardware and software. Thus, theorem-proving research is carried out in the fields of
hardware design, programming languages, and software engineeringâ€”not just in AI.
In the case of hardware, the axioms describe the interactions between signals and circuit elements. (See Section 8.4.2 on page 309 for an example.) Logical reasoners designed
specially for verification have been able to verify entire CPUs, including their timing properties (Srivas and Bickford, 1990). The A URA theorem prover has been applied to design
circuits that are more compact than any previous design (Wojciechowski and Wojcik, 1983).
In the case of software, reasoning about programs is quite similar to reasoning about
actions, as in Chapter 7: axioms describe the preconditions and effects of each statement.
The formal synthesis of algorithms was one of the first uses of theorem provers, as outlined
by Cordell Green (1969a), who built on earlier ideas by Herbert Simon (1963). The idea
is to constructively prove a theorem to the effect that â€œthere exists a program p satisfying a
certain specification.â€ Although fully automated deductive synthesis, as it is called, has not
yet become feasible for general-purpose programming, hand-guided deductive synthesis has
been successful in designing several novel and sophisticated algorithms. Synthesis of specialpurpose programs, such as scientific computing code, is also an active area of research.
Similar techniques are now being applied to software verification by systems such as the
S PIN model checker (Holzmann, 1997). For example, the Remote Agent spacecraft control
program was verified before and after flight (Havelund et al., 2000). The RSA public key
encryption algorithm and the Boyerâ€“Moore string-matching algorithm have been verified this
way (Boyer and Moore, 1984).

Section 9.6.

9.6

Summary

357

S UMMARY
We have presented an analysis of logical inference in first-order logic and a number of algorithms for doing it.
â€¢ A first approach uses inference rules (universal instantiation and existential instantiation) to propositionalize the inference problem. Typically, this approach is slow,
unless the domain is small.
â€¢ The use of unification to identify appropriate substitutions for variables eliminates the
instantiation step in first-order proofs, making the process more efficient in many cases.
â€¢ A lifted version of Modus Ponens uses unification to provide a natural and powerful
inference rule, generalized Modus Ponens. The forward-chaining and backwardchaining algorithms apply this rule to sets of definite clauses.
â€¢ Generalized Modus Ponens is complete for definite clauses, although the entailment
problem is semidecidable. For Datalog knowledge bases consisting of function-free
definite clauses, entailment is decidable.
â€¢ Forward chaining is used in deductive databases, where it can be combined with relational database operations. It is also used in production systems, which perform
efficient updates with very large rule sets. Forward chaining is complete for Datalog
and runs in polynomial time.
â€¢ Backward chaining is used in logic programming systems, which employ sophisticated compiler technology to provide very fast inference. Backward chaining suffers
from redundant inferences and infinite loops; these can be alleviated by memoization.
â€¢ Prolog, unlike first-order logic, uses a closed world with the unique names assumption
and negation as failure. These make Prolog a more practical programming language,
but bring it further from pure logic.
â€¢ The generalized resolution inference rule provides a complete proof system for firstorder logic, using knowledge bases in conjunctive normal form.
â€¢ Several strategies exist for reducing the search space of a resolution system without
compromising completeness. One of the most important issues is dealing with equality;
we showed how demodulation and paramodulation can be used.
â€¢ Efficient resolution-based theorem provers have been used to prove interesting mathematical theorems and to verify and synthesize software and hardware.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Gottlob Frege, who developed full first-order logic in 1879, based his system of inference
on a collection of valid schemas plus a single inference rule, Modus Ponens. Whitehead
and Russell (1910) expounded the so-called rules of passage (the actual term is from Herbrand (1930)) that are used to move quantifiers to the front of formulas. Skolem constants

358

RETE

Chapter

9.

Inference in First-Order Logic

and Skolem functions were introduced, appropriately enough, by Thoralf Skolem (1920).
Oddly enough, it was Skolem who introduced the Herbrand universe (Skolem, 1928).
Herbrandâ€™s theorem (Herbrand, 1930) has played a vital role in the development of
automated reasoning. Herbrand is also the inventor of unification. GoÌˆdel (1930) built on
the ideas of Skolem and Herbrand to show that first-order logic has a complete proof procedure. Alan Turing (1936) and Alonzo Church (1936) simultaneously showed, using very
different proofs, that validity in first-order logic was not decidable. The excellent text by
Enderton (1972) explains all of these results in a rigorous yet understandable fashion.
Abraham Robinson proposed that an automated reasoner could be built using propositionalization and Herbrandâ€™s theorem, and Paul Gilmore (1960) wrote the first program. Davis
and Putnam (1960) introduced the propositionalization method of Section 9.1. Prawitz (1960)
developed the key idea of letting the quest for propositional inconsistency drive the search,
and generating terms from the Herbrand universe only when they were necessary to establish propositional inconsistency. After further development by other researchers, this idea led
J. A. Robinson (no relation) to develop resolution (Robinson, 1965).
In AI, resolution was adopted for question-answering systems by Cordell Green and
Bertram Raphael (1968). Early AI implementations put a good deal of effort into data structures that would allow efficient retrieval of facts; this work is covered in AI programming
texts (Charniak et al., 1987; Norvig, 1992; Forbus and de Kleer, 1993). By the early 1970s,
forward chaining was well established in AI as an easily understandable alternative to resolution. AI applications typically involved large numbers of rules, so it was important to
develop efficient rule-matching technology, particularly for incremental updates. The technology for production systems was developed to support such applications. The production
system language O PS -5 (Forgy, 1981; Brownston et al., 1985), incorporating the efficient
rete match process (Forgy, 1982), was used for applications such as the R1 expert system for
minicomputer configuration (McDermott, 1982).
The S OAR cognitive architecture (Laird et al., 1987; Laird, 2008) was designed to handle very large rule setsâ€”up to a million rules (Doorenbos, 1994). Example applications of
S OAR include controlling simulated fighter aircraft (Jones et al., 1998), airspace management (Taylor et al., 2007), AI characters for computer games (Wintermute et al., 2007), and
training tools for soldiers (Wray and Jones, 2005).
The field of deductive databases began with a workshop in Toulouse in 1977 that
brought together experts in logical inference and database systems (Gallaire and Minker,
1978). Influential work by Chandra and Harel (1980) and Ullman (1985) led to the adoption
of Datalog as a standard language for deductive databases. The development of the magic sets
technique for rule rewriting by Bancilhon et al. (1986) allowed forward chaining to borrow
the advantage of goal-directedness from backward chaining. Current work includes the idea
of integrating multiple databases into a consistent dataspace (Halevy, 2007).
Backward chaining for logical inference appeared first in Hewittâ€™s P LANNER language (1969). Meanwhile, in 1972, Alain Colmerauer had developed and implemented Prolog for the purpose of parsing natural languageâ€”Prologâ€™s clauses were intended initially
as context-free grammar rules (Roussel, 1975; Colmerauer et al., 1973). Much of the theoretical background for logic programming was developed by Robert Kowalski, working

Bibliographical and Historical Notes

359

with Colmerauer; see Kowalski (1988) and Colmerauer and Roussel (1993) for a historical
overview. Efficient Prolog compilers are generally based on the Warren Abstract Machine
(WAM) model of computation developed by David H. D. Warren (1983). Van Roy (1990)
showed that Prolog programs can be competitive with C programs in terms of speed.
Methods for avoiding unnecessary looping in recursive logic programs were developed
independently by Smith et al. (1986) and Tamaki and Sato (1986). The latter paper also
included memoization for logic programs, a method developed extensively as tabled logic
programming by David S. Warren. Swift and Warren (1994) show how to extend the WAM
to handle tabling, enabling Datalog programs to execute an order of magnitude faster than
forward-chaining deductive database systems.
Early work on constraint logic programming was done by Jaffar and Lassez (1987).
Jaffar et al. (1992) developed the CLP(R) system for handling real-valued constraints. There
are now commercial products for solving large-scale configuration and optimization problems
with constraint programming; one of the best known is ILOG (Junker, 2003). Answer set
programming (Gelfond, 2008) extends Prolog, allowing disjunction and negation.
Texts on logic programming and Prolog, including Shoham (1994), Bratko (2001),
Clocksin (2003), and Clocksin and Mellish (2003). Prior to 2000, the Journal of Logic Programming was the journal of record; it has now been replaced by Theory and Practice of
Logic Programming. Logic programming conferences include the International Conference
on Logic Programming (ICLP) and the International Logic Programming Symposium (ILPS).
Research into mathematical theorem proving began even before the first complete
first-order systems were developed. Herbert Gelernterâ€™s Geometry Theorem Prover (Gelernter, 1959) used heuristic search methods combined with diagrams for pruning false subgoals
and was able to prove some quite intricate results in Euclidean geometry. The demodulation and paramodulation rules for equality reasoning were introduced by Wos et al. (1967)
and Wos and Robinson (1968), respectively. These rules were also developed independently
in the context of term-rewriting systems (Knuth and Bendix, 1970). The incorporation of
equality reasoning into the unification algorithm is due to Gordon Plotkin (1972). Jouannaud
and Kirchner (1991) survey equational unification from a term-rewriting perspective. An
overview of unification is given by Baader and Snyder (2001).
A number of control strategies have been proposed for resolution, beginning with the
unit preference strategy (Wos et al., 1964). The set-of-support strategy was proposed by Wos
et al. (1965) to provide a degree of goal-directedness in resolution. Linear resolution first
appeared in Loveland (1970). Genesereth and Nilsson (1987, Chapter 5) provide a short but
thorough analysis of a wide variety of control strategies.
A Computational Logic (Boyer and Moore, 1979) is the basic reference on the BoyerMoore theorem prover. Stickel (1992) covers the Prolog Technology Theorem Prover (PTTP),
which combines the advantages of Prolog compilation with the completeness of model elimination. SETHEO (Letz et al., 1992) is another widely used theorem prover based on this approach. L EAN TA P (Beckert and Posegga, 1995) is an efficient theorem prover implemented
in only 25 lines of Prolog. Weidenbach (2001) describes S PASS , one of the strongest current
theorem provers. The most successful theorem prover in recent annual competitions has been
VAMPIRE (Riazanov and Voronkov, 2002). The C OQ system (Bertot et al., 2004) and the E

360

ROBBINS ALGEBRA

Chapter

9.

Inference in First-Order Logic

equational solver (Schulz, 2004) have also proven to be valuable tools for proving correctness. Theorem provers have been used to automatically synthesize and verify software for
controlling spacecraft (Denney et al., 2006), including NASAâ€™s new Orion capsule (Lowry,
2008). The design of the FM9001 32-bit microprocessor was proved correct by the N QTHM
system (Hunt and Brock, 1992). The Conference on Automated Deduction (CADE) runs an
annual contest for automated theorem provers. From 2002 through 2008, the most successful
system has been VAMPIRE (Riazanov and Voronkov, 2002). Wiedijk (2003) compares the
strength of 15 mathematical provers. TPTP (Thousands of Problems for Theorem Provers)
is a library of theorem-proving problems, useful for comparing the performance of systems
(Sutcliffe and Suttner, 1998; Sutcliffe et al., 2006).
Theorem provers have come up with novel mathematical results that eluded human
mathematicians for decades, as detailed in the book Automated Reasoning and the Discovery of Missing Elegant Proofs (Wos and Pieper, 2003). The S AM (Semi-Automated Mathematics) program was the first, proving a lemma in lattice theory (Guard et al., 1969). The
AURA program has also answered open questions in several areas of mathematics (Wos and
Winker, 1983). The Boyerâ€“Moore theorem prover (Boyer and Moore, 1979) was used by
Natarajan Shankar to give the first fully rigorous formal proof of GoÌˆdelâ€™s Incompleteness
Theorem (Shankar, 1986). The N UPRL system proved Girardâ€™s paradox (Howe, 1987) and
Higmanâ€™s Lemma (Murthy and Russell, 1990). In 1933, Herbert Robbins proposed a simple
set of axiomsâ€”the Robbins algebraâ€”that appeared to define Boolean algebra, but no proof
could be found (despite serious work by Alfred Tarski and others). On October 10, 1996,
after eight days of computation, EQP (a version of O TTER ) found a proof (McCune, 1997).
Many early papers in mathematical logic are to be found in From Frege to GoÌˆdel:
A Source Book in Mathematical Logic (van Heijenoort, 1967). Textbooks geared toward
automated deduction include the classic Symbolic Logic and Mechanical Theorem Proving (Chang and Lee, 1973), as well as more recent works by Duffy (1991), Wos et al. (1992),
Bibel (1993), and Kaufmann et al. (2000). The principal journal for theorem proving is the
Journal of Automated Reasoning; the main conferences are the annual Conference on Automated Deduction (CADE) and the International Joint Conference on Automated Reasoning
(IJCAR). The Handbook of Automated Reasoning (Robinson and Voronkov, 2001) collects
papers in the field. MacKenzieâ€™s Mechanizing Proof (2004) covers the history and technology
of theorem proving for the popular audience.

E XERCISES
9.1 Prove that Universal Instantiation is sound and that Existential Instantiation produces
an inferentially equivalent knowledge base.
EXISTENTIAL
INTRODUCTION

9.2 From Likes(Jerry, IceCream) it seems reasonable to infer âˆƒ x Likes(x, IceCream ).
Write down a general inference rule, Existential Introduction, that sanctions this inference.
State carefully the conditions that must be satisfied by the variables and terms involved.

Exercises

361
9.3 Suppose a knowledge base contains just one sentence, âˆƒ x AsHighAs(x, Everest ).
Which of the following are legitimate results of applying Existential Instantiation?
a. AsHighAs(Everest, Everest ).
b. AsHighAs(Kilimanjaro, Everest).
c. AsHighAs(Kilimanjaro, Everest) âˆ§ AsHighAs(BenNevis, Everest)
(after two applications).
9.4
a.
b.
c.
d.
9.5

For each pair of atomic sentences, give the most general unifier if it exists:
P (A, B, B), P (x, y, z).
Q(y, G(A, B)), Q(G(x, x), y).
Older (Father (y), y), Older (Father (x), John).
Knows(Father (y), y), Knows(x, x).
Consider the subsumption lattices shown in Figure 9.2 (page 329).

a. Construct the lattice for the sentence Employs(Mother (John), Father (Richard )).
b. Construct the lattice for the sentence Employs(IBM , y) (â€œEveryone works for IBMâ€).
Remember to include every kind of query that unifies with the sentence.
c. Assume that S TORE indexes each sentence under every node in its subsumption lattice.
Explain how F ETCH should work when some of these sentences contain variables; use
as examples the sentences in (a) and (b) and the query Employs(x, Father (x)).
9.6 Write down logical representations for the following sentences, suitable for use with
Generalized Modus Ponens:
a.
b.
c.
d.
e.
f.
9.7

Horses, cows, and pigs are mammals.
An offspring of a horse is a horse.
Bluebeard is a horse.
Bluebeard is Charlieâ€™s parent.
Offspring and parent are inverse relations.
Every mammal has a parent.
These questions concern concern issues with substitution and Skolemization.

a. Given the premise âˆ€ x âˆƒ y P (x, y), it is not valid to conclude that âˆƒ q P (q, q). Give
an example of a predicate P where the first is true but the second is false.
b. Suppose that an inference engine is incorrectly written with the occurs check omitted,
so that it allows a literal like P (x, F (x)) to be unified with P (q, q). (As mentioned,
most standard implementations of Prolog actually do allow this.) Show that such an
inference engine will allow the conclusion âˆƒ y P (q, q) to be inferred from the premise
âˆ€ x âˆƒ y P (x, y).

362

Chapter

9.

Inference in First-Order Logic

c. Suppose that a procedure that converts first-order logic to clausal form incorrectly
Skolemizes âˆ€ x âˆƒ y P (x, y) to P (x, Sk0)â€”that is, it replaces y by a Skolem constant rather than by a Skolem function of x. Show that an inference engine that uses
such a procedure will likewise allow âˆƒ q P (q, q) to be inferred from the premise
âˆ€ x âˆƒ y P (x, y).
d. A common error among students is to suppose that, in unification, one is allowed to
substitute a term for a Skolem constant instead of for a variable. For instance, they will
say that the formulas P (Sk1) and P (A) can be unified under the substitution {Sk1/A}.
Give an example where this leads to an invalid inference.
9.8 Explain how to write any given 3-SAT problem of arbitrary size using a single first-order
definite clause and no more than 30 ground facts.
9.9

Suppose you are given the following axioms:
1.
2.
3.
4.
5.
6.
7.
8.

0 â‰¤ 3.
7 â‰¤ 9.
âˆ€ x x â‰¤ x.
âˆ€ x x â‰¤ x + 0.
âˆ€ x x + 0 â‰¤ x.
âˆ€ x, y x + y â‰¤ y + x.
âˆ€ w, x, y, z w â‰¤ y âˆ§ x â‰¤ z â‡’ w + x â‰¤ y + z.
âˆ€ x, y, z x â‰¤ y âˆ§ y â‰¤ z â‡’ x â‰¤ z

a. Give a backward-chaining proof of the sentence 7 â‰¤ 3 + 9. (Be sure, of course, to use
only the axioms given here, not anything else you may know about arithmetic.) Show
only the steps that leads to success, not the irrelevant steps.
b. Give a forward-chaining proof of the sentence 7 â‰¤ 3 + 9. Again, show only the steps
that lead to success.
9.10 A popular childrenâ€™s riddle is â€œBrothers and sisters have I none, but that manâ€™s father
is my fatherâ€™s son.â€ Use the rules of the family domain (Section 8.3.2 on page 301) to show
who that man is. You may apply any of the inference methods described in this chapter. Why
do you think that this riddle is difficult?
9.11 Suppose we put into a logical knowledge base a segment of the U.S. census data listing the age, city of residence, date of birth, and mother of every person, using social security numbers as identifying constants for each person. Thus, Georgeâ€™s age is given by
Age(443-65-1282, 56). Which of the following indexing schemes S1â€“S5 enable an efficient
solution for which of the queries Q1â€“Q4 (assuming normal backward chaining)?
â€¢
â€¢
â€¢
â€¢

S1:
S2:
S3:
S4:

an index for each atom in each position.
an index for each first argument.
an index for each predicate atom.
an index for each combination of predicate and first argument.

Exercises

363
â€¢ S5: an index for each combination of predicate and second argument and an index for
each first argument.
â€¢
â€¢
â€¢
â€¢

Q1:
Q2:
Q3:
Q4:

Age(443-44-4321, x)
ResidesIn(x, Houston)
Mother (x, y)
Age(x, 34) âˆ§ ResidesIn(x, TinyTownUSA)

9.12 One might suppose that we can avoid the problem of variable conflict in unification
during backward chaining by standardizing apart all of the sentences in the knowledge base
once and for all. Show that, for some sentences, this approach cannot work. (Hint: Consider
a sentence in which one part unifies with another.)
9.13 In this exercise, use the sentences you wrote in Exercise 9.6 to answer a question by
using a backward-chaining algorithm.
a. Draw the proof tree generated by an exhaustive backward-chaining algorithm for the
query âˆƒ h Horse(h), where clauses are matched in the order given.
b. What do you notice about this domain?
c. How many solutions for h actually follow from your sentences?
d. Can you think of a way to find all of them? (Hint: See Smith et al. (1986).)
9.14 Trace the execution of the backward-chaining algorithm in Figure 9.6 (page 338) when
it is applied to solve the crime problem (page 330). Show the sequence of values taken on by
the goals variable, and arrange them into a tree.
9.15 The following Prolog code defines a predicate P. (Remember that uppercase terms are
variables, not constants, in Prolog.)
P(X,[X|Y]).
P(X,[Y|Z]) :- P(X,Z).
a. Show proof trees and solutions for the queries P(A,[2,1,3]) and P(2,[1,A,3]).
b. What standard list operation does P represent?
9.16

This exercise looks at sorting in Prolog.

a. Write Prolog clauses that define the predicate sorted(L), which is true if and only if
list L is sorted in ascending order.
b. Write a Prolog definition for the predicate perm(L,M), which is true if and only if L
is a permutation of M.
c. Define sort(L,M) (M is a sorted version of L) using perm and sorted.
d. Run sort on longer and longer lists until you lose patience. What is the time complexity of your program?
e. Write a faster sorting algorithm, such as insertion sort or quicksort, in Prolog.

364

Chapter

9.

Inference in First-Order Logic

9.17 This exercise looks at the recursive application of rewrite rules, using logic programming. A rewrite rule (or demodulator in OTTER terminology) is an equation with a specified
direction. For example, the rewrite rule x + 0 â†’ x suggests replacing any expression that
matches x+0 with the expression x. Rewrite rules are a key component of equational reasoning systems. Use the predicate rewrite(X,Y) to represent rewrite rules. For example, the
earlier rewrite rule is written as rewrite(X+0,X). Some terms are primitive and cannot
be further simplified; thus, we write primitive(0) to say that 0 is a primitive term.
a. Write a definition of a predicate simplify(X,Y), that is true when Y is a simplified
version of Xâ€”that is, when no further rewrite rules apply to any subexpression of Y.
b. Write a collection of rules for the simplification of expressions involving arithmetic
operators, and apply your simplification algorithm to some sample expressions.
c. Write a collection of rewrite rules for symbolic differentiation, and use them along with
your simplification rules to differentiate and simplify expressions involving arithmetic
expressions, including exponentiation.
9.18 This exercise considers the implementation of search algorithms in Prolog. Suppose
that successor(X,Y) is true when state Y is a successor of state X; and that goal(X)
is true when X is a goal state. Write a definition for solve(X,P), which means that P is a
path (list of states) beginning with X, ending in a goal state, and consisting of a sequence of
legal steps as defined by successor. You will find that depth-first search is the easiest way
to do this. How easy would it be to add heuristic search control?
9.19

Suppose a knowledge base contains just the following first-order Horn clauses:
Ancestor(M other(x), x)
Ancestor(x, y) âˆ§ Ancestor(y, z) â‡’ Ancestor(x, z)

Consider a forward chaining algorithm that, on the jth iteration, terminates if the KB contains
a sentence that unifies with the query, else adds to the KB every atomic sentence that can be
inferred from the sentences already in the KB after iteration j âˆ’ 1.
a. For each of the following queries, say whether the algorithm will (1) give an answer (if
so, write down that answer); or (2) terminate with no answer; or (3) never terminate.
(i)
(ii)
(iii)
(iv)

Ancestor(M other(y), John)
Ancestor(M other(M other(y)), John)
Ancestor(M other(M other(M other(y))), M other(y))
Ancestor(M other(John), M other(M other(John)))

b. Can a resolution algorithm prove the sentence Â¬Ancestor(John, John) from the original knowledge base? Explain how, or why not.
c. Suppose we add the assertion that Â¬(M other(x) = x) and augment the resolution algorithm with inference rules for equality. Now what is the answer to (b)?
9.20 Let L be the first-order language with a single predicate S(p, q), meaning â€œp shaves q.â€
Assume a domain of people.

Exercises

365
a. Consider the sentence â€œThere exists a person P who shaves every one who does not
shave themselves, and only people that do not shave themselves.â€ Express this in L.
b. Convert the sentence in (a) to clausal form.
c. Construct a resolution proof to show that the clauses in (b) are inherently inconsistent.
(Note: you do not need any additional axioms.)
9.21

How can resolution be used to show that a sentence is valid? Unsatisfiable?

9.22 Construct an example of two clauses that can be resolved together in two different
ways giving two different outcomes.
9.23 From â€œHorses are animals,â€ it follows that â€œThe head of a horse is the head of an
animal.â€ Demonstrate that this inference is valid by carrying out the following steps:
a. Translate the premise and the conclusion into the language of first-order logic. Use three
predicates: HeadOf (h, x) (meaning â€œh is the head of xâ€), Horse(x), and Animal (x).
b. Negate the conclusion, and convert the premise and the negated conclusion into conjunctive normal form.
c. Use resolution to show that the conclusion follows from the premise.
9.24

Here are two sentences in the language of first-order logic:
(A) âˆ€ x âˆƒ y (x â‰¥ y)
(B) âˆƒ y âˆ€ x (x â‰¥ y)

a. Assume that the variables range over all the natural numbers 0, 1, 2, . . . , âˆž and that the
â€œâ‰¥â€ predicate means â€œis greater than or equal to.â€ Under this interpretation, translate
(A) and (B) into English.
b. Is (A) true under this interpretation?
c. Is (B) true under this interpretation?
d. Does (A) logically entail (B)?
e. Does (B) logically entail (A)?
f. Using resolution, try to prove that (A) follows from (B). Do this even if you think that
(B) does not logically entail (A); continue until the proof breaks down and you cannot
proceed (if it does break down). Show the unifying substitution for each resolution step.
If the proof fails, explain exactly where, how, and why it breaks down.
g. Now try to prove that (B) follows from (A).
9.25 Resolution can produce nonconstructive proofs for queries with variables, so we had
to introduce special mechanisms to extract definite answers. Explain why this issue does not
arise with knowledge bases containing only definite clauses.
9.26 We said in this chapter that resolution cannot be used to generate all logical consequences of a set of sentences. Can any algorithm do this?

10

CLASSICAL PLANNING

In which we see how an agent can take advantage of the structure of a problem to
construct complex plans of action.

We have defined AI as the study of rational action, which means that planningâ€”devising a
plan of action to achieve oneâ€™s goalsâ€”is a critical part of AI. We have seen two examples
of planning agents so far: the search-based problem-solving agent of Chapter 3 and the hybrid logical agent of Chapter 7. In this chapter we introduce a representation for planning
problems that scales up to problems that could not be handled by those earlier approaches.
Section 10.1 develops an expressive yet carefully constrained language for representing
planning problems. Section 10.2 shows how forward and backward search algorithms can
take advantage of this representation, primarily through accurate heuristics that can be derived
automatically from the structure of the representation. (This is analogous to the way in which
effective domain-independent heuristics were constructed for constraint satisfaction problems
in Chapter 6.) Section 10.3 shows how a data structure called the planning graph can make the
search for a plan more efficient. We then describe a few of the other approaches to planning,
and conclude by comparing the various approaches.
This chapter covers fully observable, deterministic, static environments with single
agents. Chapters 11 and 17 cover partially observable, stochastic, dynamic environments
with multiple agents.

10.1

D EFINITION OF C LASSICAL P LANNING
The problem-solving agent of Chapter 3 can find sequences of actions that result in a goal
state. But it deals with atomic representations of states and thus needs good domain-specific
heuristics to perform well. The hybrid propositional logical agent of Chapter 7 can find plans
without domain-specific heuristics because it uses domain-independent heuristics based on
the logical structure of the problem. But it relies on ground (variable-free) propositional
inference, which means that it may be swamped when there are many actions and states. For
example, in the wumpus world, the simple action of moving a step forward had to be repeated
for all four agent orientations, T time steps, and n2 current locations.
366

Section 10.1.

PDDL

SET SEMANTICS

ACTION SCHEMA

Definition of Classical Planning

367

In response to this, planning researchers have settled on a factored representationâ€”
one in which a state of the world is represented by a collection of variables. We use a language
called PDDL, the Planning Domain Definition Language, that allows us to express all 4T n2
actions with one action schema. There have been several versions of PDDL; we select a
simple version and alter its syntax to be consistent with the rest of the book.1 We now show
how PDDL describes the four things we need to define a search problem: the initial state, the
actions that are available in a state, the result of applying an action, and the goal test.
Each state is represented as a conjunction of fluents that are ground, functionless atoms.
For example, Poor âˆ§ Unknown might represent the state of a hapless agent, and a state
in a package delivery problem might be At(Truck 1 , Melbourne) âˆ§ At(Truck 2 , Sydney).
Database semantics is used: the closed-world assumption means that any fluents that are not
mentioned are false, and the unique names assumption means that Truck 1 and Truck 2 are
distinct. The following fluents are not allowed in a state: At(x, y) (because it is non-ground),
Â¬Poor (because it is a negation), and At(Father (Fred ), Sydney) (because it uses a function
symbol). The representation of states is carefully designed so that a state can be treated
either as a conjunction of fluents, which can be manipulated by logical inference, or as a set
of fluents, which can be manipulated with set operations. The set semantics is sometimes
easier to deal with.
Actions are described by a set of action schemas that implicitly define the ACTIONS (s)
and R ESULT (s, a) functions needed to do a problem-solving search. We saw in Chapter 7 that
any system for action description needs to solve the frame problemâ€”to say what changes and
what stays the same as the result of the action. Classical planning concentrates on problems
where most actions leave most things unchanged. Think of a world consisting of a bunch of
objects on a flat surface. The action of nudging an object causes that object to change its location by a vector Î”. A concise description of the action should mention only Î”; it shouldnâ€™t
have to mention all the objects that stay in place. PDDL does that by specifying the result of
an action in terms of what changes; everything that stays the same is left unmentioned.
A set of ground (variable-free) actions can be represented by a single action schema.
The schema is a lifted representationâ€”it lifts the level of reasoning from propositional logic
to a restricted subset of first-order logic. For example, here is an action schema for flying a
plane from one location to another:
Action(Fly(p, from, to),
P RECOND :At(p, from) âˆ§ Plane(p) âˆ§ Airport (from) âˆ§ Airport (to)
E FFECT:Â¬At(p, from) âˆ§ At(p, to))

PRECONDITION
EFFECT

The schema consists of the action name, a list of all the variables used in the schema, a
precondition and an effect. Although we havenâ€™t said yet how the action schema converts
into logical sentences, think of the variables as being universally quantified. We are free to
choose whatever values we want to instantiate the variables. For example, here is one ground
1

PDDL was derived from the original S TRIPS planning language(Fikes and Nilsson, 1971). which is slightly
more restricted than PDDL: S TRIPS preconditions and goals cannot contain negative literals.

368

Chapter

10.

Classical Planning

action that results from substituting values for all the variables:
Action(Fly(P1 , SFO , JFK ),
P RECOND :At(P1 , SFO) âˆ§ Plane(P1 ) âˆ§ Airport (SFO) âˆ§ Airport (JFK )
E FFECT:Â¬At(P1 , SFO ) âˆ§ At(P1 , JFK ))
The precondition and effect of an action are each conjunctions of literals (positive or negated
atomic sentences). The precondition defines the states in which the action can be executed,
and the effect defines the result of executing the action. An action a can be executed in state
s if s entails the precondition of a. Entailment can also be expressed with the set semantics:
s |= q iff every positive literal in q is in s and every negated literal in q is not. In formal
notation we say
(a âˆˆ ACTIONS (s)) â‡” s |= P RECOND (a) ,
where any variables in a are universally quantified. For example,
âˆ€ p, from, to (Fly(p, from, to) âˆˆ ACTIONS (s)) â‡”
s |= (At(p, from) âˆ§ Plane(p) âˆ§ Airport (from) âˆ§ Airport (to))
APPLICABLE

PROPOSITIONALIZE

DELETE LIST
ADD LIST

We say that action a is applicable in state s if the preconditions are satisfied by s. When
an action schema a contains variables, it may have multiple applicable instantiations. For
example, with the initial state defined in Figure 10.1, the Fly action can be instantiated as
Fly(P1 , SFO , JFK ) or as Fly(P2 , JFK , SFO), both of which are applicable in the initial
state. If an action a has v variables, then, in a domain with k unique names of objects, it takes
O(v k ) time in the worst case to find the applicable ground actions.
Sometimes we want to propositionalize a PDDL problemâ€”replace each action schema
with a set of ground actions and then use a propositional solver such as SATP LAN to find a
solution. However, this is impractical when v and k are large.
The result of executing action a in state s is defined as a state s which is represented
by the set of fluents formed by starting with s, removing the fluents that appear as negative
literals in the actionâ€™s effects (what we call the delete list or D EL (a)), and adding the fluents
that are positive literals in the actionâ€™s effects (what we call the add list or A DD (a)):
R ESULT (s, a) = (s âˆ’ D EL (a)) âˆª A DD(a) .

(10.1)

For example, with the action Fly(P1 , SFO , JFK ), we would remove At(P1 , SFO) and add
At(P1 , JFK ). It is a requirement of action schemas that any variable in the effect must also
appear in the precondition. That way, when the precondition is matched against the state s,
all the variables will be bound, and R ESULT (s, a) will therefore have only ground atoms. In
other words, ground states are closed under the R ESULT operation.
Also note that the fluents do not explicitly refer to time, as they did in Chapter 7. There
we needed superscripts for time, and successor-state axioms of the form
F t+1 â‡” ActionCausesF t âˆ¨ (F t âˆ§ Â¬ActionCausesNotF t ) .
In PDDL the times and states are implicit in the action schemas: the precondition always
refers to time t and the effect to time t + 1.
A set of action schemas serves as a definition of a planning domain. A specific problem
within the domain is defined with the addition of an initial state and a goal. The initial

Section 10.1.

Definition of Classical Planning

369

Init (At(C1 , SFO) âˆ§ At(C2 , JFK ) âˆ§ At(P1 , SFO) âˆ§ At(P2 , JFK )
âˆ§ Cargo(C1 ) âˆ§ Cargo(C2 ) âˆ§ Plane(P1 ) âˆ§ Plane(P2 )
âˆ§ Airport (JFK ) âˆ§ Airport (SFO))
Goal (At(C1 , JFK ) âˆ§ At(C2 , SFO))
Action(Load (c, p, a),
P RECOND : At(c, a) âˆ§ At(p, a) âˆ§ Cargo(c) âˆ§ Plane(p) âˆ§ Airport (a)
E FFECT: Â¬ At(c, a) âˆ§ In(c, p))
Action(Unload (c, p, a),
P RECOND : In(c, p) âˆ§ At(p, a) âˆ§ Cargo(c) âˆ§ Plane(p) âˆ§ Airport (a)
E FFECT: At(c, a) âˆ§ Â¬ In(c, p))
Action(Fly (p, from, to),
P RECOND : At(p, from) âˆ§ Plane(p) âˆ§ Airport (from) âˆ§ Airport (to)
E FFECT: Â¬ At(p, from) âˆ§ At(p, to))
Figure 10.1

INITIAL STATE
GOAL

A PDDL description of an air cargo transportation planning problem.

state is a conjunction of ground atoms. (As with all states, the closed-world assumption is
used, which means that any atoms that are not mentioned are false.) The goal is just like a
precondition: a conjunction of literals (positive or negative) that may contain variables, such
as At(p, SFO ) âˆ§ Plane(p). Any variables are treated as existentially quantified, so this goal
is to have any plane at SFO. The problem is solved when we can find a sequence of actions
that end in a state s that entails the goal. For example, the state Rich âˆ§ Famous âˆ§ Miserable
entails the goal Rich âˆ§ Famous, and the state Plane(Plane 1 ) âˆ§ At(Plane 1 , SFO ) entails
the goal At(p, SFO ) âˆ§ Plane(p).
Now we have defined planning as a search problem: we have an initial state, an ACTIONS
function, a R ESULT function, and a goal test. Weâ€™ll look at some example problems before
investigating efficient search algorithms.

10.1.1 Example: Air cargo transport
Figure 10.1 shows an air cargo transport problem involving loading and unloading cargo and
flying it from place to place. The problem can be defined with three actions: Load , Unload ,
and Fly. The actions affect two predicates: In(c, p) means that cargo c is inside plane p, and
At(x, a) means that object x (either plane or cargo) is at airport a. Note that some care must
be taken to make sure the At predicates are maintained properly. When a plane flies from
one airport to another, all the cargo inside the plane goes with it. In first-order logic it would
be easy to quantify over all objects that are inside the plane. But basic PDDL does not have
a universal quantifier, so we need a different solution. The approach we use is to say that a
piece of cargo ceases to be At anywhere when it is In a plane; the cargo only becomes At the
new airport when it is unloaded. So At really means â€œavailable for use at a given location.â€
The following plan is a solution to the problem:
[Load (C1 , P1 , SFO ), Fly(P1 , SFO , JFK ), Unload (C1 , P1 , JFK ),
Load (C2 , P2 , JFK ), Fly(P2 , JFK , SFO ), Unload (C2 , P2 , SFO)] .

370

Chapter

10.

Classical Planning

Finally, there is the problem of spurious actions such as Fly(P1 , JFK , JFK ), which should
be a no-op, but which has contradictory effects (according to the definition, the effect would
include At(P1 , JFK ) âˆ§ Â¬At(P1 , JFK )). It is common to ignore such problems, because
they seldom cause incorrect plans to be produced. The correct approach is to add inequality
preconditions saying that the from and to airports must be different; see another example of
this in Figure 10.3.

10.1.2 Example: The spare tire problem
Consider the problem of changing a flat tire (Figure 10.2). The goal is to have a good spare
tire properly mounted onto the carâ€™s axle, where the initial state has a flat tire on the axle and
a good spare tire in the trunk. To keep it simple, our version of the problem is an abstract
one, with no sticky lug nuts or other complications. There are just four actions: removing the
spare from the trunk, removing the flat tire from the axle, putting the spare on the axle, and
leaving the car unattended overnight. We assume that the car is parked in a particularly bad
neighborhood, so that the effect of leaving it overnight is that the tires disappear. A solution
to the problem is [Remove(Flat , Axle), Remove(Spare , Trunk ), PutOn(Spare , Axle)].

Init(Tire(Flat ) âˆ§ Tire(Spare) âˆ§ At(Flat , Axle) âˆ§ At(Spare, Trunk ))
Goal (At (Spare, Axle))
Action(Remove(obj , loc),
P RECOND : At(obj , loc)
E FFECT: Â¬ At(obj , loc) âˆ§ At(obj , Ground ))
Action(PutOn(t , Axle),
P RECOND : Tire(t) âˆ§ At(t , Ground ) âˆ§ Â¬ At(Flat , Axle)
E FFECT: Â¬ At(t , Ground) âˆ§ At(t , Axle))
Action(LeaveOvernight ,
P RECOND :
E FFECT: Â¬ At(Spare, Ground) âˆ§ Â¬ At(Spare, Axle) âˆ§ Â¬ At(Spare, Trunk)
âˆ§ Â¬ At(Flat , Ground ) âˆ§ Â¬ At(Flat , Axle) âˆ§ Â¬ At(Flat , Trunk))
Figure 10.2

The simple spare tire problem.

10.1.3 Example: The blocks world
BLOCKS WORLD

One of the most famous planning domains is known as the blocks world. This domain
consists of a set of cube-shaped blocks sitting on a table.2 The blocks can be stacked, but
only one block can fit directly on top of another. A robot arm can pick up a block and move
it to another position, either on the table or on top of another block. The arm can pick up
only one block at a time, so it cannot pick up a block that has another one on it. The goal will
always be to build one or more stacks of blocks, specified in terms of what blocks are on top
2

The blocks world used in planning research is much simpler than S HRDLUâ€™s version, shown on page 20.

Section 10.1.

Definition of Classical Planning

371

Init (On(A, Table) âˆ§ On(B, Table) âˆ§ On(C, A)
âˆ§ Block (A) âˆ§ Block (B) âˆ§ Block (C) âˆ§ Clear (B) âˆ§ Clear (C))
Goal (On(A, B) âˆ§ On(B, C))
Action(Move(b, x, y),
P RECOND : On(b, x) âˆ§ Clear (b) âˆ§ Clear (y) âˆ§ Block (b) âˆ§ Block (y) âˆ§
(b=x) âˆ§ (b=y) âˆ§ (x=y),
E FFECT: On(b, y) âˆ§ Clear (x) âˆ§ Â¬On(b, x) âˆ§ Â¬Clear (y))
Action(MoveToTable (b, x),
P RECOND : On(b, x) âˆ§ Clear (b) âˆ§ Block (b) âˆ§ (b=x),
E FFECT: On(b, Table) âˆ§ Clear (x) âˆ§ Â¬On(b, x))
Figure 10.3 A planning problem in the blocks world: building a three-block tower. One
solution is the sequence [MoveToTable (C, A), Move(B, Table, C), Move(A, Table, B)].

A

B

C

B

A

C

Start State

Figure 10.4

Goal State

Diagram of the blocks-world problem in Figure 10.3.

of what other blocks. For example, a goal might be to get block A on B and block B on C
(see Figure 10.4).
We use On(b, x) to indicate that block b is on x, where x is either another block or the
table. The action for moving block b from the top of x to the top of y will be Move(b, x, y).
Now, one of the preconditions on moving b is that no other block be on it. In first-order logic,
this would be Â¬âˆƒ x On(x, b) or, alternatively, âˆ€ x Â¬On(x, b). Basic PDDL does not allow
quantifiers, so instead we introduce a predicate Clear (x) that is true when nothing is on x.
(The complete problem description is in Figure 10.3.)
The action Move moves a block b from x to y if both b and y are clear. After the move
is made, b is still clear but y is not. A first attempt at the Move schema is
Action(Move(b, x, y),
P RECOND :On(b, x) âˆ§ Clear (b) âˆ§ Clear (y),
E FFECT:On(b, y) âˆ§ Clear (x) âˆ§ Â¬On(b, x) âˆ§ Â¬Clear (y)) .
Unfortunately, this does not maintain Clear properly when x or y is the table. When x is the
Table, this action has the effect Clear (Table), but the table should not become clear; and
when y = Table, it has the precondition Clear (Table), but the table does not have to be clear

372

Chapter

10.

Classical Planning

for us to move a block onto it. To fix this, we do two things. First, we introduce another
action to move a block b from x to the table:
Action(MoveToTable(b, x),
P RECOND :On(b, x) âˆ§ Clear (b),
E FFECT:On(b, Table) âˆ§ Clear (x) âˆ§ Â¬On(b, x)) .
Second, we take the interpretation of Clear (x) to be â€œthere is a clear space on x to hold a
block.â€ Under this interpretation, Clear (Table) will always be true. The only problem is that
nothing prevents the planner from using Move(b, x, Table) instead of MoveToTable(b, x).
We could live with this problemâ€”it will lead to a larger-than-necessary search space, but will
not lead to incorrect answersâ€”or we could introduce the predicate Block and add Block (b) âˆ§
Block (y) to the precondition of Move.

10.1.4 The complexity of classical planning

PLANSAT
BOUNDED PLANSAT

In this subsection we consider the theoretical complexity of planning and distinguish two
decision problems. PlanSAT is the question of whether there exists any plan that solves a
planning problem. Bounded PlanSAT asks whether there is a solution of length k or less;
this can be used to find an optimal plan.
The first result is that both decision problems are decidable for classical planning. The
proof follows from the fact that the number of states is finite. But if we add function symbols
to the language, then the number of states becomes infinite, and PlanSAT becomes only
semidecidable: an algorithm exists that will terminate with the correct answer for any solvable
problem, but may not terminate on unsolvable problems. The Bounded PlanSAT problem
remains decidable even in the presence of function symbols. For proofs of the assertions in
this section, see Ghallab et al. (2004).
Both PlanSAT and Bounded PlanSAT are in the complexity class PSPACE, a class that
is larger (and hence more difficult) than NP and refers to problems that can be solved by a
deterministic Turing machine with a polynomial amount of space. Even if we make some
rather severe restrictions, the problems remain quite difficult. For example, if we disallow
negative effects, both problems are still NP-hard. However, if we also disallow negative
preconditions, PlanSAT reduces to the class P.
These worst-case results may seem discouraging. We can take solace in the fact that
agents are usually not asked to find plans for arbitrary worst-case problem instances, but
rather are asked for plans in specific domains (such as blocks-world problems with n blocks),
which can be much easier than the theoretical worst case. For many domains (including the
blocks world and the air cargo world), Bounded PlanSAT is NP-complete while PlanSAT is
in P; in other words, optimal planning is usually hard, but sub-optimal planning is sometimes
easy. To do well on easier-than-worst-case problems, we will need good search heuristics.
Thatâ€™s the true advantage of the classical planning formalism: it has facilitated the development of very accurate domain-independent heuristics, whereas systems based on successorstate axioms in first-order logic have had less success in coming up with good heuristics.

Section 10.2.

10.2

Algorithms for Planning as State-Space Search

373

A LGORITHMS FOR P LANNING AS S TATE -S PACE S EARCH
Now we turn our attention to planning algorithms. We saw how the description of a planning
problem defines a search problem: we can search from the initial state through the space
of states, looking for a goal. One of the nice advantages of the declarative representation of
action schemas is that we can also search backward from the goal, looking for the initial state.
Figure 10.5 compares forward and backward searches.

10.2.1 Forward (progression) state-space search
Now that we have shown how a planning problem maps into a search problem, we can solve
planning problems with any of the heuristic search algorithms from Chapter 3 or a local
search algorithm from Chapter 4 (provided we keep track of the actions used to reach the
goal). From the earliest days of planning research (around 1961) until around 1998 it was
assumed that forward state-space search was too inefficient to be practical. It is not hard to
come up with reasons why.
First, forward search is prone to exploring irrelevant actions. Consider the noble task
of buying a copy of AI: A Modern Approach from an online bookseller. Suppose there is an

At(P1, B)

(a)

Fly(P1, A, B)

At(P2, A)

Fly(P2, A, B)

At(P1, A)

At(P1, A)
At(P2, A)
At(P2, B)

At(P1, A)
At(P2, B)

Fly(P1, A, B)

At(P1, B)

Fly(P2, A, B)

At(P1, B)

(b)

At(P2, B)
At(P2, A)

Figure 10.5 Two approaches to searching for a plan. (a) Forward (progression) search
through the space of states, starting in the initial state and using the problemâ€™s actions to
search forward for a member of the set of goal states. (b) Backward (regression) search
through sets of relevant states, starting at the set of states representing the goal and using the
inverse of the actions to search backward for the initial state.

374

Chapter

10.

Classical Planning

action schema Buy(isbn) with effect Own(isbn). ISBNs are 10 digits, so this action schema
represents 10 billion ground actions. An uninformed forward-search algorithm would have
to start enumerating these 10 billion actions to find one that leads to the goal.
Second, planning problems often have large state spaces. Consider an air cargo problem
with 10 airports, where each airport has 5 planes and 20 pieces of cargo. The goal is to move
all the cargo at airport A to airport B. There is a simple solution to the problem: load the 20
pieces of cargo into one of the planes at A, fly the plane to B, and unload the cargo. Finding
the solution can be difficult because the average branching factor is huge: each of the 50
planes can fly to 9 other airports, and each of the 200 packages can be either unloaded (if
it is loaded) or loaded into any plane at its airport (if it is unloaded). So in any state there
is a minimum of 450 actions (when all the packages are at airports with no planes) and a
maximum of 10,450 (when all packages and planes are at the same airport). On average, letâ€™s
say there are about 2000 possible actions per state, so the search graph up to the depth of the
obvious solution has about 200041 nodes.
Clearly, even this relatively small problem instance is hopeless without an accurate
heuristic. Although many real-world applications of planning have relied on domain-specific
heuristics, it turns out (as we see in Section 10.2.3) that strong domain-independent heuristics
can be derived automatically; that is what makes forward search feasible.

10.2.2 Backward (regression) relevant-states search
RELEVANT-STATES

In regression search we start at the goal and apply the actions backward until we find a
sequence of steps that reaches the initial state. It is called relevant-states search because we
only consider actions that are relevant to the goal (or current state). As in belief-state search
(Section 4.4), there is a set of relevant states to consider at each step, not just a single state.
We start with the goal, which is a conjunction of literals forming a description of a set of
statesâ€”for example, the goal Â¬Poor âˆ§ Famous describes those states in which Poor is false,
Famous is true, and any other fluent can have any value. If there are n ground fluents in a
domain, then there are 2n ground states (each fluent can be true or false), but 3n descriptions
of sets of goal states (each fluent can be positive, negative, or not mentioned).
In general, backward search works only when we know how to regress from a state
description to the predecessor state description. For example, it is hard to search backwards
for a solution to the n-queens problem because there is no easy way to describe the states that
are one move away from the goal. Happily, the PDDL representation was designed to make
it easy to regress actionsâ€”if a domain can be expressed in PDDL, then we can do regression
search on it. Given a ground goal description g and a ground action a, the regression from g
over a gives us a state description g defined by
g = (g âˆ’ A DD (a)) âˆª Precond (a) .
That is, the effects that were added by the action need not have been true before, and also
the preconditions must have held before, or else the action could not have been executed.
Note that D EL (a) does not appear in the formula; thatâ€™s because while we know the fluents
in D EL (a) are no longer true after the action, we donâ€™t know whether or not they were true
before, so thereâ€™s nothing to be said about them.

Section 10.2.

Algorithms for Planning as State-Space Search

375

To get the full advantage of backward search, we need to deal with partially uninstantiated actions and states, not just ground ones. For example, suppose the goal is to deliver a specific piece of cargo to SFO: At(C2 , SFO ). That suggests the action Unload (C2 , p , SFO ):
Action(Unload (C2 , p , SFO ),
P RECOND :In(C2 , p ) âˆ§ At(p , SFO ) âˆ§ Cargo(C2 ) âˆ§ Plane(p ) âˆ§ Airport(SFO )
E FFECT:At(C2 , SFO ) âˆ§ Â¬In(C2 , p ) .
(Note that we have standardized variable names (changing p to p in this case) so that there
will be no confusion between variable names if we happen to use the same action schema
twice in a plan. The same approach was used in Chapter 9 for first-order logical inference.)
This represents unloading the package from an unspecified plane at SFO; any plane will do,
but we need not say which one now. We can take advantage of the power of first-order
representations: a single description summarizes the possibility of using any of the planes by
implicitly quantifying over p . The regressed state description is
g = In(C2 , p ) âˆ§ At(p , SFO) âˆ§ Cargo(C2 ) âˆ§ Plane(p ) âˆ§ Airport (SFO ) .

RELEVANCE

The final issue is deciding which actions are candidates to regress over. In the forward direction we chose actions that were applicableâ€”those actions that could be the next step in the
plan. In backward search we want actions that are relevantâ€”those actions that could be the
last step in a plan leading up to the current goal state.
For an action to be relevant to a goal it obviously must contribute to the goal: at least
one of the actionâ€™s effects (either positive or negative) must unify with an element of the goal.
What is less obvious is that the action must not have any effect (positive or negative) that
negates an element of the goal. Now, if the goal is A âˆ§ B âˆ§ C and an action has the effect
Aâˆ§B âˆ§Â¬C then there is a colloquial sense in which that action is very relevant to the goalâ€”it
gets us two-thirds of the way there. But it is not relevant in the technical sense defined here,
because this action could not be the final step of a solutionâ€”we would always need at least
one more step to achieve C.
Given the goal At(C2 , SFO ), several instantiations of Unload are relevant: we could
chose any specific plane to unload from, or we could leave the plane unspecified by using
the action Unload (C2 , p , SFO ). We can reduce the branching factor without ruling out any
solutions by always using the action formed by substituting the most general unifier into the
(standardized) action schema.
As another example, consider the goal Own(0136042597), given an initial state with
10 billion ISBNs, and the single action schema
A = Action(Buy(i), P RECOND :ISBN (i), E FFECT:Own(i)) .
As we mentioned before, forward search without a heuristic would have to start enumerating the 10 billion ground Buy actions. But with backward search, we would unify the
goal Own(0136042597) with the (standardized) effect Own(i ), yielding the substitution
Î¸ = {i /0136042597}. Then we would regress over the action Subst(Î¸, A ) to yield the
predecessor state description ISBN (0136042597). This is part of, and thus entailed by, the
initial state, so we are done.

376

Chapter

10.

Classical Planning

We can make this more formal. Assume a goal description g which contains a goal
literal gi and an action schema A that is standardized to produce A . If A has an effect literal
ej where Unify(gi , ej ) = Î¸ and where we define a = S UBST (Î¸, A ) and if there is no effect
in a that is the negation of a literal in g, then a is a relevant action towards g.
Backward search keeps the branching factor lower than forward search, for most problem domains. However, the fact that backward search uses state sets rather than individual
states makes it harder to come up with good heuristics. That is the main reason why the
majority of current systems favor forward search.

10.2.3 Heuristics for planning

IGNORE
PRECONDITIONS
HEURISTIC

SET-COVER
PROBLEM

Neither forward nor backward search is efficient without a good heuristic function. Recall
from Chapter 3 that a heuristic function h(s) estimates the distance from a state s to the
goal and that if we can derive an admissible heuristic for this distanceâ€”one that does not
overestimateâ€”then we can use Aâˆ— search to find optimal solutions. An admissible heuristic
can be derived by defining a relaxed problem that is easier to solve. The exact cost of a
solution to this easier problem then becomes the heuristic for the original problem.
By definition, there is no way to analyze an atomic state, and thus it it requires some
ingenuity by a human analyst to define good domain-specific heuristics for search problems
with atomic states. Planning uses a factored representation for states and action schemas.
That makes it possible to define good domain-independent heuristics and for programs to
automatically apply a good domain-independent heuristic for a given problem.
Think of a search problem as a graph where the nodes are states and the edges are
actions. The problem is to find a path connecting the initial state to a goal state. There are
two ways we can relax this problem to make it easier: by adding more edges to the graph,
making it strictly easier to find a path, or by grouping multiple nodes together, forming an
abstraction of the state space that has fewer states, and thus is easier to search.
We look first at heuristics that add edges to the graph. For example, the ignore preconditions heuristic drops all preconditions from actions. Every action becomes applicable
in every state, and any single goal fluent can be achieved in one step (if there is an applicable actionâ€”if not, the problem is impossible). This almost implies that the number of steps
required to solve the relaxed problem is the number of unsatisfied goalsâ€”almost but not
quite, because (1) some action may achieve multiple goals and (2) some actions may undo
the effects of others. For many problems an accurate heuristic is obtained by considering (1)
and ignoring (2). First, we relax the actions by removing all preconditions and all effects
except those that are literals in the goal. Then, we count the minimum number of actions
required such that the union of those actionsâ€™ effects satisfies the goal. This is an instance
of the set-cover problem. There is one minor irritation: the set-cover problem is NP-hard.
Fortunately a simple greedy algorithm is guaranteed to return a set covering whose size is
within a factor of log n of the true minimum covering, where n is the number of literals in
the goal. Unfortunately, the greedy algorithm loses the guarantee of admissibility.
It is also possible to ignore only selected preconditions of actions. Consider the slidingblock puzzle (8-puzzle or 15-puzzle) from Section 3.2. We could encode this as a planning

Section 10.2.

Algorithms for Planning as State-Space Search

377

problem involving tiles with a single schema Slide:
Action(Slide(t, s1 , s2 ),
P RECOND :On(t, s1 ) âˆ§ Tile(t) âˆ§ Blank (s2 ) âˆ§ Adjacent (s1 , s2 )
E FFECT:On(t, s2 ) âˆ§ Blank (s1 ) âˆ§ Â¬On(t, s1 ) âˆ§ Â¬Blank (s2 ))

IGNORE DELETE
LISTS

STATE ABSTRACTION

As we saw in Section 3.6, if we remove the preconditions Blank (s2 ) âˆ§ Adjacent (s1 , s2 )
then any tile can move in one action to any space and we get the number-of-misplaced-tiles
heuristic. If we remove Blank (s2 ) then we get the Manhattan-distance heuristic. It is easy to
see how these heuristics could be derived automatically from the action schema description.
The ease of manipulating the schemas is the great advantage of the factored representation of
planning problems, as compared with the atomic representation of search problems.
Another possibility is the ignore delete lists heuristic. Assume for a moment that all
goals and preconditions contain only positive literals3 We want to create a relaxed version of
the original problem that will be easier to solve, and where the length of the solution will serve
as a good heuristic. We can do that by removing the delete lists from all actions (i.e., removing
all negative literals from effects). That makes it possible to make monotonic progress towards
the goalâ€”no action will ever undo progress made by another action. It turns out it is still NPhard to find the optimal solution to this relaxed problem, but an approximate solution can be
found in polynomial time by hill-climbing. Figure 10.6 diagrams part of the state space for
two planning problems using the ignore-delete-lists heuristic. The dots represent states and
the edges actions, and the height of each dot above the bottom plane represents the heuristic
value. States on the bottom plane are solutions. In both these problems, there is a wide path
to the goal. There are no dead ends, so no need for backtracking; a simple hillclimbing search
will easily find a solution to these problems (although it may not be an optimal solution).
The relaxed problems leave us with a simplifiedâ€”but still expensiveâ€”planning problem just to calculate the value of the heuristic function. Many planning problems have 10100
states or more, and relaxing the actions does nothing to reduce the number of states. Therefore, we now look at relaxations that decrease the number of states by forming a state abstractionâ€”a many-to-one mapping from states in the ground representation of the problem
to the abstract representation.
The easiest form of state abstraction is to ignore some fluents. For example, consider
an air cargo problem with 10 airports, 50 planes, and 200 pieces of cargo. Each plane can
be at one of 10 airports and each package can be either in one of the planes or unloaded at
one of the airports. So there are 5010 Ã— 20050+10 â‰ˆ 10155 states. Now consider a particular
problem in that domain in which it happens that all the packages are at just 5 of the airports,
and all packages at a given airport have the same destination. Then a useful abstraction of the
problem is to drop all the At fluents except for the ones involving one plane and one package
at each of the 5 airports. Now there are only 510 Ã— 55+10 â‰ˆ 1017 states. A solution in this
abstract state space will be shorter than a solution in the original space (and thus will be an
admissible heuristic), and the abstract solution is easy to extend to a solution to the original
problem (by adding additional Load and Unload actions).
3

Many problems are written with this convention. For problems that arenâ€™t, replace every negative literal Â¬P
in a goal or precondition with a new positive literal, P  .

378

Chapter

10.

Classical Planning

Figure 10.6 Two state spaces from planning problems with the ignore-delete-lists heuristic. The height above the bottom plane is the heuristic score of a state; states on the bottom
plane are goals. There are no local minima, so search for the goal is straightforward. From
Hoffmann (2005).

DECOMPOSITION
SUBGOAL
INDEPENDENCE

A key idea in defining heuristics is decomposition: dividing a problem into parts, solving each part independently, and then combining the parts. The subgoal independence assumption is that the cost of solving a conjunction of subgoals is approximated by the sum
of the costs of solving each subgoal independently. The subgoal independence assumption
can be optimistic or pessimistic. It is optimistic when there are negative interactions between
the subplans for each subgoalâ€”for example, when an action in one subplan deletes a goal
achieved by another subplan. It is pessimistic, and therefore inadmissible, when subplans
contain redundant actionsâ€”for instance, two actions that could be replaced by a single action
in the merged plan.
Suppose the goal is a set of fluents G, which we divide into disjoint subsets G1 , . . . , Gn .
We then find plans P1 , . . . , Pn that solve the respective subgoals. What is an estimate of the
cost of the plan for achieving all of G? We can think of each Cost (Pi ) as a heuristic estimate,
and we know that if we combine estimates by taking their maximum value, we always get an
admissible heuristic. So maxi C OST (Pi ) is admissible, and sometimes it is exactly correct:
it could be that P1 serendipitously achieves all the Gi . But in most cases, in practice the
estimate is too low. Could we sum the costs instead? For many problems that is a reasonable
estimate, but it is not admissible. The best case is when we can determine that Gi and Gj are
independent. If the effects of Pi leave all the preconditions and goals of Pj unchanged, then
the estimate C OST (Pi ) + C OST (Pj ) is admissible, and more accurate than the max estimate.
We show in Section 10.3.1 that planning graphs can help provide better heuristic estimates.
It is clear that there is great potential for cutting down the search space by forming abstractions. The trick is choosing the right abstractions and using them in a way that makes
the total costâ€”defining an abstraction, doing an abstract search, and mapping the abstraction
back to the original problemâ€”less than the cost of solving the original problem. The tech-

Section 10.3.

Planning Graphs

379

niques of pattern databases from Section 3.6.3 can be useful, because the cost of creating
the pattern database can be amortized over multiple problem instances.
An example of a system that makes use of effective heuristics is FF, or FAST F ORWARD
(Hoffmann, 2005), a forward state-space searcher that uses the ignore-delete-lists heuristic,
estimating the heuristic with the help of a planning graph (see Section 10.3). FF then uses
hill-climbing search (modified to keep track of the plan) with the heuristic to find a solution.
When it hits a plateau or local maximumâ€”when no action leads to a state with better heuristic
scoreâ€”then FF uses iterative deepening search until it finds a state that is better, or it gives
up and restarts hill-climbing.

10.3

P LANNING G RAPHS

PLANNING GRAPH

LEVEL

All of the heuristics we have suggested can suffer from inaccuracies. This section shows
how a special data structure called a planning graph can be used to give better heuristic
estimates. These heuristics can be applied to any of the search techniques we have seen so
far. Alternatively, we can search for a solution over the space formed by the planning graph,
using an algorithm called G RAPHPLAN .
A planning problem asks if we can reach a goal state from the initial state. Suppose we
are given a tree of all possible actions from the initial state to successor states, and their successors, and so on. If we indexed this tree appropriately, we could answer the planning question â€œcan we reach state G from state S0 â€ immediately, just by looking it up. Of course, the
tree is of exponential size, so this approach is impractical. A planning graph is polynomialsize approximation to this tree that can be constructed quickly. The planning graph canâ€™t
answer definitively whether G is reachable from S0 , but it can estimate how many steps it
takes to reach G. The estimate is always correct when it reports the goal is not reachable, and
it never overestimates the number of steps, so it is an admissible heuristic.
A planning graph is a directed graph organized into levels: first a level S0 for the initial
state, consisting of nodes representing each fluent that holds in S0 ; then a level A0 consisting
of nodes for each ground action that might be applicable in S0 ; then alternating levels Si
followed by Ai ; until we reach a termination condition (to be discussed later).
Roughly speaking, Si contains all the literals that could hold at time i, depending on
the actions executed at preceding time steps. If it is possible that either P or Â¬P could hold,
then both will be represented in Si . Also roughly speaking, Ai contains all the actions that
could have their preconditions satisfied at time i. We say â€œroughly speakingâ€ because the
planning graph records only a restricted subset of the possible negative interactions among
actions; therefore, a literal might show up at level Sj when actually it could not be true until
a later level, if at all. (A literal will never show up too late.) Despite the possible error, the
level j at which a literal first appears is a good estimate of how difficult it is to achieve the
literal from the initial state.
Planning graphs work only for propositional planning problemsâ€”ones with no variables. As we mentioned on page 368, it is straightforward to propositionalize a set of ac-

380

Chapter

10.

Classical Planning

Init (Have(Cake))
Goal (Have(Cake) âˆ§ Eaten(Cake))
Action(Eat (Cake)
P RECOND : Have(Cake)
E FFECT: Â¬ Have(Cake) âˆ§ Eaten(Cake))
Action(Bake(Cake)
P RECOND : Â¬ Have(Cake)
E FFECT: Have(Cake))
Figure 10.7
S0

The â€œhave cake and eat cake tooâ€ problem.
A0

S1

A1

S2

Bake(Cake)
Have(Cake)

Have(Cake)

Have(Cake)

Â¬ Have(Cake)
Eat(Cake)

Â¬

Eaten(Cake)

Â¬ Have(Cake)
Eat(Cake)

Eaten(Cake)

Eaten(Cake)

Â¬ Eaten(Cake)

Â¬ Eaten(Cake)

Figure 10.8 The planning graph for the â€œhave cake and eat cake tooâ€ problem up to level
S2 . Rectangles indicate actions (small squares indicate persistence actions), and straight
lines indicate preconditions and effects. Mutex links are shown as curved gray lines. Not all
mutex links are shown, because the graph would be too cluttered. In general, if two literals
are mutex at Si , then the persistence actions for those literals will be mutex at Ai and we
need not draw that mutex link.

PERSISTENCE
ACTION

MUTUAL EXCLUSION
MUTEX

tion schemas. Despite the resulting increase in the size of the problem description, planning
graphs have proved to be effective tools for solving hard planning problems.
Figure 10.7 shows a simple planning problem, and Figure 10.8 shows its planning
graph. Each action at level Ai is connected to its preconditions at Si and its effects at Si+1 .
So a literal appears because an action caused it, but we also want to say that a literal can
persist if no action negates it. This is represented by a persistence action (sometimes called
a no-op). For every literal C, we add to the problem a persistence action with precondition C
and effect C. Level A0 in Figure 10.8 shows one â€œrealâ€ action, Eat(Cake), along with two
persistence actions drawn as small square boxes.
Level A0 contains all the actions that could occur in state S0 , but just as important it
records conflicts between actions that would prevent them from occurring together. The gray
lines in Figure 10.8 indicate mutual exclusion (or mutex) links. For example, Eat(Cake) is
mutually exclusive with the persistence of either Have(Cake) or Â¬Eaten(Cake). We shall
see shortly how mutex links are computed.
Level S1 contains all the literals that could result from picking any subset of the actions
in A0 , as well as mutex links (gray lines) indicating literals that could not appear together,
regardless of the choice of actions. For example, Have(Cake) and Eaten(Cake) are mutex:

Section 10.3.

LEVELED OFF

Planning Graphs

381

depending on the choice of actions in A0 , either, but not both, could be the result. In other
words, S1 represents a belief state: a set of possible states. The members of this set are all
subsets of the literals such that there is no mutex link between any members of the subset.
We continue in this way, alternating between state level Si and action level Ai until we
reach a point where two consecutive levels are identical. At this point, we say that the graph
has leveled off. The graph in Figure 10.8 levels off at S2 .
What we end up with is a structure where every Ai level contains all the actions that are
applicable in Si , along with constraints saying that two actions cannot both be executed at the
same level. Every Si level contains all the literals that could result from any possible choice
of actions in Aiâˆ’1 , along with constraints saying which pairs of literals are not possible.
It is important to note that the process of constructing the planning graph does not require
choosing among actions, which would entail combinatorial search. Instead, it just records the
impossibility of certain choices using mutex links.
We now define mutex links for both actions and literals. A mutex relation holds between
two actions at a given level if any of the following three conditions holds:
â€¢ Inconsistent effects: one action negates an effect of the other. For example, Eat(Cake)
and the persistence of Have(Cake) have inconsistent effects because they disagree on
the effect Have(Cake).
â€¢ Interference: one of the effects of one action is the negation of a precondition of the
other. For example Eat(Cake) interferes with the persistence of Have(Cake) by negating its precondition.
â€¢ Competing needs: one of the preconditions of one action is mutually exclusive with a
precondition of the other. For example, Bake(Cake) and Eat(Cake) are mutex because
they compete on the value of the Have(Cake) precondition.
A mutex relation holds between two literals at the same level if one is the negation of the other
or if each possible pair of actions that could achieve the two literals is mutually exclusive.
This condition is called inconsistent support. For example, Have(Cake) and Eaten(Cake)
are mutex in S1 because the only way of achieving Have(Cake), the persistence action, is
mutex with the only way of achieving Eaten(Cake), namely Eat(Cake). In S2 the two
literals are not mutex, because there are new ways of achieving them, such as Bake(Cake)
and the persistence of Eaten(Cake), that are not mutex.
A planning graph is polynomial in the size of the planning problem. For a planning
problem with l literals and a actions, each Si has no more than l nodes and l2 mutex links,
and each Ai has no more than a + l nodes (including the no-ops), (a + l)2 mutex links, and
2(al + l) precondition and effect links. Thus, an entire graph with n levels has a size of
O(n(a + l)2 ). The time to build the graph has the same complexity.

10.3.1 Planning graphs for heuristic estimation
A planning graph, once constructed, is a rich source of information about the problem. First,
if any goal literal fails to appear in the final level of the graph, then the problem is unsolvable.
Second, we can estimate the cost of achieving any goal literal gi from state s as the level at
which gi first appears in the planning graph constructed from initial state s. We call this the

382
LEVEL COST

SERIAL PLANNING
GRAPH

MAX-LEVEL

LEVEL SUM

SET-LEVEL

Chapter

10.

Classical Planning

level cost of gi . In Figure 10.8, Have(Cake) has level cost 0 and Eaten(Cake) has level cost
1. It is easy to show (Exercise 10.10) that these estimates are admissible for the individual
goals. The estimate might not always be accurate, however, because planning graphs allow
several actions at each level, whereas the heuristic counts just the level and not the number
of actions. For this reason, it is common to use a serial planning graph for computing
heuristics. A serial graph insists that only one action can actually occur at any given time
step; this is done by adding mutex links between every pair of nonpersistence actions. Level
costs extracted from serial graphs are often quite reasonable estimates of actual costs.
To estimate the cost of a conjunction of goals, there are three simple approaches. The
max-level heuristic simply takes the maximum level cost of any of the goals; this is admissible, but not necessarily accurate.
The level sum heuristic, following the subgoal independence assumption, returns the
sum of the level costs of the goals; this can be inadmissible but works well in practice
for problems that are largely decomposable. It is much more accurate than the numberof-unsatisfied-goals heuristic from Section 10.2. For our problem, the level-sum heuristic
estimate for the conjunctive goal Have(Cake) âˆ§ Eaten(Cake) will be 0 + 1 = 1, whereas
the correct answer is 2, achieved by the plan [Eat(Cake), Bake(Cake)]. That doesnâ€™t seem
so bad. A more serious error is that if Bake(Cake) were not in the set of actions, then the
estimate would still be 1, when in fact the conjunctive goal would be impossible.
Finally, the set-level heuristic finds the level at which all the literals in the conjunctive
goal appear in the planning graph without any pair of them being mutually exclusive. This
heuristic gives the correct values of 2 for our original problem and infinity for the problem
without Bake(Cake). It is admissible, it dominates the max-level heuristic, and it works
extremely well on tasks in which there is a good deal of interaction among subplans. It is not
perfect, of course; for example, it ignores interactions among three or more literals.
As a tool for generating accurate heuristics, we can view the planning graph as a relaxed
problem that is efficiently solvable. To understand the nature of the relaxed problem, we
need to understand exactly what it means for a literal g to appear at level Si in the planning
graph. Ideally, we would like it to be a guarantee that there exists a plan with i action levels
that achieves g, and also that if g does not appear, there is no such plan. Unfortunately,
making that guarantee is as difficult as solving the original planning problem. So the planning
graph makes the second half of the guarantee (if g does not appear, there is no plan), but
if g does appear, then all the planning graph promises is that there is a plan that possibly
achieves g and has no â€œobviousâ€ flaws. An obvious flaw is defined as a flaw that can be
detected by considering two actions or two literals at a timeâ€”in other words, by looking at
the mutex relations. There could be more subtle flaws involving three, four, or more actions,
but experience has shown that it is not worth the computational effort to keep track of these
possible flaws. This is similar to a lesson learned from constraint satisfaction problemsâ€”that
it is often worthwhile to compute 2-consistency before searching for a solution, but less often
worthwhile to compute 3-consistency or higher. (See page 211.)
One example of an unsolvable problem that cannot be recognized as such by a planning
graph is the blocks-world problem where the goal is to get block A on B, B on C, and C on
A. This is an impossible goal; a tower with the bottom on top of the top. But a planning graph

Section 10.3.

Planning Graphs

383

cannot detect the impossibility, because any two of the three subgoals are achievable. There
are no mutexes between any pair of literals, only between the three as a whole. To detect that
this problem is impossible, we would have to search over the planning graph.

10.3.2 The G RAPHPLAN algorithm
This subsection shows how to extract a plan directly from the planning graph, rather than just
using the graph to provide a heuristic. The G RAPHPLAN algorithm (Figure 10.9) repeatedly
adds a level to a planning graph with E XPAND -G RAPH . Once all the goals show up as nonmutex in the graph, G RAPHPLAN calls E XTRACT-S OLUTION to search for a plan that solves
the problem. If that fails, it expands another level and tries again, terminating with failure
when there is no reason to go on.

function G RAPHPLAN( problem) returns solution or failure
graph â† I NITIAL -P LANNING -G RAPH( problem)
goals â† C ONJUNCTS(problem.G OAL)
nogoods â† an empty hash table
for tl = 0 to âˆž do
if goals all non-mutex in St of graph then
solution â† E XTRACT-S OLUTION (graph, goals, N UM L EVELS(graph), nogoods)
if solution = failure then return solution
if graph and nogoods have both leveled off then return failure
graph â† E XPAND -G RAPH(graph, problem)
Figure 10.9 The G RAPHPLAN algorithm. G RAPHPLAN calls E XPAND -G RAPH to add a
level until either a solution is found by E XTRACT-S OLUTION, or no solution is possible.

Let us now trace the operation of G RAPHPLAN on the spare tire problem from page 370.
The graph is shown in Figure 10.10. The first line of G RAPHPLAN initializes the planning
graph to a one-level (S0 ) graph representing the initial state. The positive fluents from the
problem descriptionâ€™s initial state are shown, as are the relevant negative fluents. Not shown
are the unchanging positive literals (such as Tire(Spare )) and the irrelevant negative literals.
The goal At(Spare , Axle) is not present in S0 , so we need not call E XTRACT-S OLUTION â€”
we are certain that there is no solution yet. Instead, E XPAND -G RAPH adds into A0 the three
actions whose preconditions exist at level S0 (i.e., all the actions except PutOn(Spare , Axle)),
along with persistence actions for all the literals in S0 . The effects of the actions are added at
level S1 . E XPAND -G RAPH then looks for mutex relations and adds them to the graph.
At(Spare , Axle) is still not present in S1 , so again we do not call E XTRACT-S OLUTION .
We call E XPAND -G RAPH again, adding A1 and S1 and giving us the planning graph shown
in Figure 10.10. Now that we have the full complement of actions, it is worthwhile to look at
some of the examples of mutex relations and their causes:
â€¢ Inconsistent effects: Remove(Spare , Trunk ) is mutex with LeaveOvernight because
one has the effect At(Spare , Ground ) and the other has its negation.

384

Chapter
S0

A0

S1

10.
A1

At(Spare,Trunk)

At(Spare,Trunk)

Classical Planning
S2
At(Spare,Trunk)

Remove(Spare,Trunk)
Remove(Spare,Trunk)

Â¬ At(Spare,Trunk)

At(Flat,Axle)
LeaveOvernight

Â¬ At(Spare,Trunk)
Remove(Flat,Axle)

Remove(Flat,Axle)

At(Flat,Axle)

At(Flat,Axle)

Â¬ At(Flat,Axle)

Â¬ At(Flat,Axle)
LeaveOvernight

Â¬ At(Spare,Axle)

Â¬ At(Spare,Axle)

Â¬ At(Spare,Axle)
PutOn(Spare,Axle)

Â¬ At(Flat,Ground)
Â¬ At(Spare,Ground)

Â¬ At(Flat,Ground)

At(Spare,Axle)

Â¬ At(Flat,Ground)

At(Flat,Ground)

At(Flat,Ground)

Â¬ At(Spare,Ground)

Â¬ At(Spare,Ground)

At(Spare,Ground)

At(Spare,Ground)

Figure 10.10 The planning graph for the spare tire problem after expansion to level S2 .
Mutex links are shown as gray lines. Not all links are shown, because the graph would be too
cluttered if we showed them all. The solution is indicated by bold lines and outlines.

â€¢ Interference: Remove(Flat , Axle) is mutex with LeaveOvernight because one has the
precondition At(Flat , Axle) and the other has its negation as an effect.
â€¢ Competing needs: PutOn(Spare , Axle) is mutex with Remove(Flat , Axle) because
one has At(Flat , Axle) as a precondition and the other has its negation.
â€¢ Inconsistent support: At(Spare , Axle) is mutex with At(Flat , Axle) in S2 because the
only way of achieving At(Spare , Axle) is by PutOn(Spare , Axle), and that is mutex
with the persistence action that is the only way of achieving At(Flat , Axle). Thus, the
mutex relations detect the immediate conflict that arises from trying to put two objects
in the same place at the same time.
This time, when we go back to the start of the loop, all the literals from the goal are present
in S2 , and none of them is mutex with any other. That means that a solution might exist,
and E XTRACT-S OLUTION will try to find it. We can formulate E XTRACT-S OLUTION as a
Boolean constraint satisfaction problem (CSP) where the variables are the actions at each
level, the values for each variable are in or out of the plan, and the constraints are the mutexes
and the need to satisfy each goal and precondition.
Alternatively, we can define E XTRACT-S OLUTION as a backward search problem, where
each state in the search contains a pointer to a level in the planning graph and a set of unsatisfied goals. We define this search problem as follows:
â€¢ The initial state is the last level of the planning graph, Sn , along with the set of goals
from the planning problem.
â€¢ The actions available in a state at level Si are to select any conflict-free subset of the
actions in Aiâˆ’1 whose effects cover the goals in the state. The resulting state has level
Siâˆ’1 and has as its set of goals the preconditions for the selected set of actions. By
â€œconflict free,â€ we mean a set of actions such that no two of them are mutex and no two
of their preconditions are mutex.

Section 10.3.

Planning Graphs

385

â€¢ The goal is to reach a state at level S0 such that all the goals are satisfied.
â€¢ The cost of each action is 1.
For this particular problem, we start at S2 with the goal At(Spare , Axle). The only choice we
have for achieving the goal set is PutOn(Spare , Axle). That brings us to a search state at S1
with goals At(Spare , Ground ) and Â¬At(Flat , Axle). The former can be achieved only by
Remove(Spare , Trunk ), and the latter by either Remove(Flat , Axle) or LeaveOvernight.
But LeaveOvernight is mutex with Remove(Spare , Trunk ), so the only solution is to choose
Remove(Spare , Trunk ) and Remove(Flat , Axle). That brings us to a search state at S0 with
the goals At(Spare , Trunk ) and At(Flat , Axle). Both of these are present in the state, so
we have a solution: the actions Remove(Spare , Trunk ) and Remove(Flat , Axle) in level
A0 , followed by PutOn(Spare , Axle) in A1 .
In the case where E XTRACT-S OLUTION fails to find a solution for a set of goals at
a given level, we record the (level , goals) pair as a no-good, just as we did in constraint
learning for CSPs (page 220). Whenever E XTRACT-S OLUTION is called again with the same
level and goals, we can find the recorded no-good and immediately return failure rather than
searching again. We see shortly that no-goods are also used in the termination test.
We know that planning is PSPACE-complete and that constructing the planning graph
takes polynomial time, so it must be the case that solution extraction is intractable in the worst
case. Therefore, we will need some heuristic guidance for choosing among actions during the
backward search. One approach that works well in practice is a greedy algorithm based on
the level cost of the literals. For any set of goals, we proceed in the following order:
1. Pick first the literal with the highest level cost.
2. To achieve that literal, prefer actions with easier preconditions. That is, choose an action
such that the sum (or maximum) of the level costs of its preconditions is smallest.

10.3.3 Termination of G RAPHPLAN
So far, we have skated over the question of termination. Here we show that G RAPHPLAN will
in fact terminate and return failure when there is no solution.
The first thing to understand is why we canâ€™t stop expanding the graph as soon as it has
leveled off. Consider an air cargo domain with one plane and n pieces of cargo at airport
A, all of which have airport B as their destination. In this version of the problem, only one
piece of cargo can fit in the plane at a time. The graph will level off at level 4, reflecting the
fact that for any single piece of cargo, we can load it, fly it, and unload it at the destination in
three steps. But that does not mean that a solution can be extracted from the graph at level 4;
in fact a solution will require 4n âˆ’ 1 steps: for each piece of cargo we load, fly, and unload,
and for all but the last piece we need to fly back to airport A to get the next piece.
How long do we have to keep expanding after the graph has leveled off? If the function
E XTRACT-S OLUTION fails to find a solution, then there must have been at least one set of
goals that were not achievable and were marked as a no-good. So if it is possible that there
might be fewer no-goods in the next level, then we should continue. As soon as the graph
itself and the no-goods have both leveled off, with no solution found, we can terminate with
failure because there is no possibility of a subsequent change that could add a solution.

386

Chapter

10.

Classical Planning

Now all we have to do is prove that the graph and the no-goods will always level off. The
key to this proof is that certain properties of planning graphs are monotonically increasing or
decreasing. â€œX increases monotonicallyâ€ means that the set of Xs at level i + 1 is a superset
(not necessarily proper) of the set at level i. The properties are as follows:
â€¢ Literals increase monotonically: Once a literal appears at a given level, it will appear
at all subsequent levels. This is because of the persistence actions; once a literal shows
up, persistence actions cause it to stay forever.
â€¢ Actions increase monotonically: Once an action appears at a given level, it will appear
at all subsequent levels. This is a consequence of the monotonic increase of literals; if
the preconditions of an action appear at one level, they will appear at subsequent levels,
and thus so will the action.
â€¢ Mutexes decrease monotonically: If two actions are mutex at a given level Ai , then they
will also be mutex for all previous levels at which they both appear. The same holds for
mutexes between literals. It might not always appear that way in the figures, because
the figures have a simplification: they display neither literals that cannot hold at level
Si nor actions that cannot be executed at level Ai . We can see that â€œmutexes decrease
monotonicallyâ€ is true if you consider that these invisible literals and actions are mutex
with everything.
The proof can be handled by cases: if actions A and B are mutex at level Ai , it
must be because of one of the three types of mutex. The first two, inconsistent effects
and interference, are properties of the actions themselves, so if the actions are mutex
at Ai , they will be mutex at every level. The third case, competing needs, depends on
conditions at level Si : that level must contain a precondition of A that is mutex with
a precondition of B. Now, these two preconditions can be mutex if they are negations
of each other (in which case they would be mutex in every level) or if all actions for
achieving one are mutex with all actions for achieving the other. But we already know
that the available actions are increasing monotonically, so, by induction, the mutexes
must be decreasing.
â€¢ No-goods decrease monotonically: If a set of goals is not achievable at a given level,
then they are not achievable in any previous level. The proof is by contradiction: if they
were achievable at some previous level, then we could just add persistence actions to
make them achievable at a subsequent level.
Because the actions and literals increase monotonically and because there are only a finite
number of actions and literals, there must come a level that has the same number of actions
and literals as the previous level. Because mutexes and no-goods decrease, and because there
can never be fewer than zero mutexes or no-goods, there must come a level that has the
same number of mutexes and no-goods as the previous level. Once a graph has reached this
state, then if one of the goals is missing or is mutex with another goal, then we can stop the
G RAPHPLAN algorithm and return failure. That concludes a sketch of the proof; for more
details see Ghallab et al. (2004).

Section 10.4.

Other Classical Planning Approaches

387

Year

Track

Winning Systems (approaches)

2008
2008
2006
2006
2004
2004
2002
2002
2000
2000
1998

Optimal
Satisficing
Optimal
Satisficing
Optimal
Satisficing
Automated
Hand-coded
Automated
Hand-coded
Automated

G AMER (model checking, bidirectional search)
LAMA (fast downward search with FF heuristic)
SATP LAN, M AX P LAN (Boolean satisfiability)
SGPLAN (forward search; partitions into independent subproblems)
SATP LAN (Boolean satisfiability)
FAST D IAGONALLY D OWNWARD (forward search with causal graph)
LPG (local search, planning graphs converted to CSPs)
TLPLAN (temporal action logic with control rules for forward search)
FF (forward search)
TAL P LANNER (temporal action logic with control rules for forward search)
IPP (planning graphs); HSP (forward search)

Figure 10.11 Some of the top-performing systems in the International Planning Competition. Each year there are various tracks: â€œOptimalâ€ means the planners must produce the
shortest possible plan, while â€œSatisficingâ€ means nonoptimal solutions are accepted. â€œHandcodedâ€ means domain-specific heuristics are allowed; â€œAutomatedâ€ means they are not.

10.4

OTHER C LASSICAL P LANNING A PPROACHES
Currently the most popular and effective approaches to fully automated planning are:
â€¢ Translating to a Boolean satisfiability (SAT) problem
â€¢ Forward state-space search with carefully crafted heuristics (Section 10.2)
â€¢ Search using a planning graph (Section 10.3)
These three approaches are not the only ones tried in the 40-year history of automated planning. Figure 10.11 shows some of the top systems in the International Planning Competitions,
which have been held every even year since 1998. In this section we first describe the translation to a satisfiability problem and then describe three other influential approaches: planning
as first-order logical deduction; as constraint satisfaction; and as plan refinement.

10.4.1 Classical planning as Boolean satisfiability
In Section 7.7.4 we saw how SATP LAN solves planning problems that are expressed in propositional logic. Here we show how to translate a PDDL description into a form that can be
processed by SATP LAN . The translation is a series of straightforward steps:
â€¢ Propositionalize the actions: replace each action schema with a set of ground actions
formed by substituting constants for each of the variables. These ground actions are not
part of the translation, but will be used in subsequent steps.
â€¢ Define the initial state: assert F 0 for every fluent F in the problemâ€™s initial state, and
Â¬F for every fluent not mentioned in the initial state.
â€¢ Propositionalize the goal: for every variable in the goal, replace the literals that contain
the variable with a disjunction over constants. For example, the goal of having block A

388

Chapter

10.

Classical Planning

on another block, On(A, x) âˆ§ Block (x) in a world with objects A, B and C, would be
replaced by the goal
(On(A, A) âˆ§ Block (A)) âˆ¨ (On(A, B) âˆ§ Block (B)) âˆ¨ (On(A, C) âˆ§ Block (C)) .
â€¢ Add successor-state axioms: For each fluent F , add an axiom of the form
F t+1 â‡” ActionCausesF t âˆ¨ (F t âˆ§ Â¬ActionCausesNotF t ) ,
where ActionCausesF is a disjunction of all the ground actions that have F in their
add list, and ActionCausesNotF is a disjunction of all the ground actions that have F
in their delete list.
â€¢ Add precondition axioms: For each ground action A, add the axiom At â‡’ P RE(A)t ,
that is, if an action is taken at time t, then the preconditions must have been true.
â€¢ Add action exclusion axioms: say that every action is distinct from every other action.
The resulting translation is in the form that we can hand to SATP LAN to find a solution.

10.4.2 Planning as first-order logical deduction: Situation calculus

SITUATION
CALCULUS
SITUATION

POSSIBILITY AXIOM

PDDL is a language that carefully balances the expressiveness of the language with the complexity of the algorithms that operate on it. But some problems remain difficult to express in
PDDL. For example, we canâ€™t express the goal â€œmove all the cargo from A to B regardless
of how many pieces of cargo there areâ€ in PDDL, but we can do it in first-order logic, using a
universal quantifier. Likewise, first-order logic can concisely express global constraints such
as â€œno more than four robots can be in the same place at the same time.â€ PDDL can only say
this with repetitious preconditions on every possible action that involves a move.
The propositional logic representation of planning problems also has limitations, such
as the fact that the notion of time is tied directly to fluents. For example, South 2 means
â€œthe agent is facing south at time 2.â€ With that representation, there is no way to say â€œthe
agent would be facing south at time 2 if it executed a right turn at time 1; otherwise it would
be facing east.â€ First-order logic lets us get around this limitation by replacing the notion
of linear time with a notion of branching situations, using a representation called situation
calculus that works like this:
â€¢ The initial state is called a situation. If s is a situation and a is an action, then
R ESULT (s, a) is also a situation. There are no other situations. Thus, a situation corresponds to a sequence, or history, of actions. You can also think of a situation as the
result of applying the actions, but note that two situations are the same only if their start
and actions are the same: (R ESULT (s, a) = R ESULT (s , a )) â‡” (s = s âˆ§ a = a ).
Some examples of actions and situations are shown in Figure 10.12.
â€¢ A function or relation that can vary from one situation to the next is a fluent. By convention, the situation s is always the last argument to the fluent, for example At(x, l, s) is a
relational fluent that is true when object x is at location l in situation s, and Location is a
functional fluent such that Location(x, s) = l holds in the same situations as At(x, l, s).
â€¢ Each actionâ€™s preconditions are described with a possibility axiom that says when the
action can be taken. It has the form Î¦(s) â‡’ Poss(a, s) where Î¦(s) is some formula

Section 10.4.

Other Classical Planning Approaches

389

PIT
Gold

PIT

PIT
PIT
Gold

PIT

Result(Result(S0, Forward),
Turn(Right))

PIT
PIT
Gold

Turn(Right)

PIT

Result(S0, Forward)
PIT

Forward
S0

Figure 10.12

Situations as the results of actions in the wumpus world.

involving s that describes the preconditions. An example from the wumpus world says
that it is possible to shoot if the agent is alive and has an arrow:
Alive(Agent, s) âˆ§ Have(Agent, Arrow , s) â‡’ Poss(Shoot , s)
â€¢ Each fluent is described with a successor-state axiom that says what happens to the
fluent, depending on what action is taken. This is similar to the approach we took for
propositional logic. The axiom has the form
Action is possible â‡’
(Fluent is true in result state â‡” Actionâ€™s effect made it true
âˆ¨ It was true before and action left it alone) .
For example, the axiom for the relational fluent Holding says that the agent is holding
some gold g after executing a possible action if and only if the action was a Grab of g
or if the agent was already holding g and the action was not releasing it:
Poss(a, s) â‡’
(Holding (Agent, g, Result (a, s)) â‡”
a = Grab(g) âˆ¨ (Holding (Agent, g, s) âˆ§ a = Release (g))) .
UNIQUE ACTION
AXIOMS

â€¢ We need unique action axioms so that the agent can deduce that, for example, a =
Release(g). For each distinct pair of action names Ai and Aj we have an axiom that
says the actions are different:
Ai (x, . . .) = Aj (y, . . .)

390

Chapter

10.

Classical Planning

and for each action name Ai we have an axiom that says two uses of that action name
are equal if and only if all their arguments are equal:
Ai (x1 , . . . , xn ) = Ai (y1 , . . . , yn ) â‡” x1 = y1 âˆ§ . . . âˆ§ xn = yn .
â€¢ A solution is a situation (and hence a sequence of actions) that satisfies the goal.
Work in situation calculus has done a lot to define the formal semantics of planning and to
open up new areas of investigation. But so far there have not been any practical large-scale
planning programs based on logical deduction over the situation calculus. This is in part
because of the difficulty of doing efficient inference in FOL, but is mainly because the field
has not yet developed effective heuristics for planning with situation calculus.

10.4.3 Planning as constraint satisfaction
We have seen that constraint satisfaction has a lot in common with Boolean satisfiability, and
we have seen that CSP techniques are effective for scheduling problems, so it is not surprising
that it is possible to encode a bounded planning problem (i.e., the problem of finding a plan of
length k) as a constraint satisfaction problem (CSP). The encoding is similar to the encoding
to a SAT problem (Section 10.4.1), with one important simplification: at each time step we
need only a single variable, Action t , whose domain is the set of possible actions. We no
longer need one variable for every action, and we donâ€™t need the action exclusion axioms. It
is also possible to encode a planning graph into a CSP. This is the approach taken by GP-CSP
(Do and Kambhampati, 2003).

10.4.4 Planning as refinement of partially ordered plans

FLAW

All the approaches we have seen so far construct totally ordered plans consisting of a strictly
linear sequences of actions. This representation ignores the fact that many subproblems are
independent. A solution to an air cargo problem consists of a totally ordered sequence of
actions, yet if 30 packages are being loaded onto one plane in one airport and 50 packages are
being loaded onto another at another airport, it seems pointless to come up with a strict linear
ordering of 80 load actions; the two subsets of actions should be thought of independently.
An alternative is to represent plans as partially ordered structures: a plan is a set of
actions and a set of constraints of the form Before(ai , aj ) saying that one action occurs
before another. In the bottom of Figure 10.13, we see a partially ordered plan that is a solution
to the spare tire problem. Actions are boxes and ordering constraints are arrows. Note that
Remove(Spare , Trunk ) and Remove(Flat , Axle) can be done in either order as long as they
are both completed before the PutOn(Spare , Axle) action.
Partially ordered plans are created by a search through the space of plans rather than
through the state space. We start with the empty plan consisting of just the initial state and
the goal, with no actions in between, as in the top of Figure 10.13. The search procedure then
looks for a flaw in the plan, and makes an addition to the plan to correct the flaw (or if no
correction can be made, the search backtracks and tries something else). A flaw is anything
that keeps the partial plan from being a solution. For example, one flaw in the empty plan is
that no action achieves At(Spare , Axle). One way to correct the flaw is to insert into the plan

Section 10.4.

Other Classical Planning Approaches

Start

391

At(Spare,Trunk)

At(Spare,Axle)

Finish

PutOn(Spare,Axle)

At(Spare,Axle)

Finish

PutOn(Spare,Axle)

At(Spare,Axle)

Finish

At(Flat,Axle)

(a)
At(Spare,Trunk) Remove(Spare,Trunk)

Start

At(Spare,Trunk)

At(Spare,Ground)

At(Flat,Axle)

Â¬ At(Flat,Axle)

(b)
At(Spare,Trunk) Remove(Spare,Trunk)

Start

At(Spare,Trunk)

At(Spare,Ground)

Â¬ At(Flat,Axle)

At(Flat,Axle)

At(Flat,Axle)

Remove(Flat,Axle)

(c)
Figure 10.13 (a) the tire problem expressed as an empty plan. (b) an incomplete partially
ordered plan for the tire problem. Boxes represent actions and arrows indicate that one action
must occur before another. (c) a complete partially-ordered solution.

LEAST COMMITMENT

the action PutOn(Spare , Axle). Of course that introduces some new flaws: the preconditions
of the new action are not achieved. The search keeps adding to the plan (backtracking if
necessary) until all flaws are resolved, as in the bottom of Figure 10.13. At every step, we
make the least commitment possible to fix the flaw. For example, in adding the action
Remove(Spare , Trunk ) we need to commit to having it occur before PutOn(Spare , Axle),
but we make no other commitment that places it before or after other actions. If there were a
variable in the action schema that could be left unbound, we would do so.
In the 1980s and 90s, partial-order planning was seen as the best way to handle planning problems with independent subproblemsâ€”after all, it was the only approach that explicitly represents independent branches of a plan. On the other hand, it has the disadvantage
of not having an explicit representation of states in the state-transition model. That makes
some computations cumbersome. By 2000, forward-search planners had developed excellent
heuristics that allowed them to efficiently discover the independent subproblems that partialorder planning was designed for. As a result, partial-order planners are not competitive on
fully automated classical planning problems.
However, partial-order planning remains an important part of the field. For some specific tasks, such as operations scheduling, partial-order planning with domain specific heuristics is the technology of choice. Many of these systems use libraries of high-level plans, as
described in Section 11.2. Partial-order planning is also often used in domains where it is important for humans to understand the plans. Operational plans for spacecraft and Mars rovers
are generated by partial-order planners and are then checked by human operators before being
uploaded to the vehicles for execution. The plan refinement approach makes it easier for the
humans to understand what the planning algorithms are doing and verify that they are correct.

392

10.5

SERIALIZABLE
SUBGOAL

Chapter

10.

Classical Planning

A NALYSIS OF P LANNING A PPROACHES
Planning combines the two major areas of AI we have covered so far: search and logic. A
planner can be seen either as a program that searches for a solution or as one that (constructively) proves the existence of a solution. The cross-fertilization of ideas from the two areas
has led both to improvements in performance amounting to several orders of magnitude in
the last decade and to an increased use of planners in industrial applications. Unfortunately,
we do not yet have a clear understanding of which techniques work best on which kinds of
problems. Quite possibly, new techniques will emerge that dominate existing methods.
Planning is foremost an exercise in controlling combinatorial explosion. If there are n
propositions in a domain, then there are 2n states. As we have seen, planning is PSPACEhard. Against such pessimism, the identification of independent subproblems can be a powerful weapon. In the best caseâ€”full decomposability of the problemâ€”we get an exponential
speedup. Decomposability is destroyed, however, by negative interactions between actions.
G RAPHPLAN records mutexes to point out where the difficult interactions are. SATP LAN represents a similar range of mutex relations, but does so by using the general CNF form rather
than a specific data structure. Forward search addresses the problem heuristically by trying
to find patterns (subsets of propositions) that cover the independent subproblems. Since this
approach is heuristic, it can work even when the subproblems are not completely independent.
Sometimes it is possible to solve a problem efficiently by recognizing that negative
interactions can be ruled out. We say that a problem has serializable subgoals if there exists
an order of subgoals such that the planner can achieve them in that order without having to
undo any of the previously achieved subgoals. For example, in the blocks world, if the goal
is to build a tower (e.g., A on B, which in turn is on C, which in turn is on the Table, as in
Figure 10.4 on page 371), then the subgoals are serializable bottom to top: if we first achieve
C on Table, we will never have to undo it while we are achieving the other subgoals. A
planner that uses the bottom-to-top trick can solve any problem in the blocks world without
backtracking (although it might not always find the shortest plan).
As a more complex example, for the Remote Agent planner that commanded NASAâ€™s
Deep Space One spacecraft, it was determined that the propositions involved in commanding a spacecraft are serializable. This is perhaps not too surprising, because a spacecraft is
designed by its engineers to be as easy as possible to control (subject to other constraints).
Taking advantage of the serialized ordering of goals, the Remote Agent planner was able to
eliminate most of the search. This meant that it was fast enough to control the spacecraft in
real time, something previously considered impossible.
Planners such as G RAPHPLAN , SATP LAN , and FF have moved the field of planning
forward, by raising the level of performance of planning systems, by clarifying the representational and combinatorial issues involved, and by the development of useful heuristics.
However, there is a question of how far these techniques will scale. It seems likely that further
progress on larger problems cannot rely only on factored and propositional representations,
and will require some kind of synthesis of first-order and hierarchical representations with
the efficient heuristics currently in use.

Section 10.6.

10.6

Summary

393

S UMMARY
In this chapter, we defined the problem of planning in deterministic, fully observable, static
environments. We described the PDDL representation for planning problems and several
algorithmic approaches for solving them. The points to remember:
â€¢ Planning systems are problem-solving algorithms that operate on explicit propositional
or relational representations of states and actions. These representations make possible the derivation of effective heuristics and the development of powerful and flexible
algorithms for solving problems.
â€¢ PDDL, the Planning Domain Definition Language, describes the initial and goal states
as conjunctions of literals, and actions in terms of their preconditions and effects.
â€¢ State-space search can operate in the forward direction (progression) or the backward
direction (regression). Effective heuristics can be derived by subgoal independence
assumptions and by various relaxations of the planning problem.
â€¢ A planning graph can be constructed incrementally, starting from the initial state. Each
layer contains a superset of all the literals or actions that could occur at that time step
and encodes mutual exclusion (mutex) relations among literals or actions that cannot cooccur. Planning graphs yield useful heuristics for state-space and partial-order planners
and can be used directly in the G RAPHPLAN algorithm.
â€¢ Other approaches include first-order deduction over situation calculus axioms; encoding
a planning problem as a Boolean satisfiability problem or as a constraint satisfaction
problem; and explicitly searching through the space of partially ordered plans.
â€¢ Each of the major approaches to planning has its adherents, and there is as yet no consensus on which is best. Competition and cross-fertilization among the approaches have
resulted in significant gains in efficiency for planning systems.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

AI planning arose from investigations into state-space search, theorem proving, and control
theory and from the practical needs of robotics, scheduling, and other domains. S TRIPS (Fikes
and Nilsson, 1971), the first major planning system, illustrates the interaction of these influences. S TRIPS was designed as the planning component of the software for the Shakey robot
project at SRI. Its overall control structure was modeled on that of GPS, the General Problem
Solver (Newell and Simon, 1961), a state-space search system that used meansâ€“ends analysis. Bylander (1992) shows simple S TRIPS planning to be PSPACE-complete. Fikes and
Nilsson (1993) give a historical retrospective on the S TRIPS project and its relationship to
more recent planning efforts.
The representation language used by S TRIPS has been far more influential than its algorithmic approach; what we call the â€œclassicalâ€ language is close to what S TRIPS used.

394

LINEAR PLANNING

INTERLEAVING

Chapter

10.

Classical Planning

The Action Description Language, or ADL (Pednault, 1986), relaxed some of the S TRIPS
restrictions and made it possible to encode more realistic problems. Nebel (2000) explores
schemes for compiling ADL into S TRIPS . The Problem Domain Description Language, or
PDDL (Ghallab et al., 1998), was introduced as a computer-parsable, standardized syntax for
representing planning problems and has been used as the standard language for the International Planning Competition since 1998. There have been several extensions; the most recent
version, PDDL 3.0, includes plan constraints and preferences (Gerevini and Long, 2005).
Planners in the early 1970s generally considered totally ordered action sequences. Problem decomposition was achieved by computing a subplan for each subgoal and then stringing
the subplans together in some order. This approach, called linear planning by Sacerdoti
(1975), was soon discovered to be incomplete. It cannot solve some very simple problems,
such as the Sussman anomaly (see Exercise 10.7), found by Allen Brown during experimentation with the H ACKER system (Sussman, 1975). A complete planner must allow for interleaving of actions from different subplans within a single sequence. The notion of serializable
subgoals (Korf, 1987) corresponds exactly to the set of problems for which noninterleaved
planners are complete.
One solution to the interleaving problem was goal-regression planning, a technique in
which steps in a totally ordered plan are reordered so as to avoid conflict between subgoals.
This was introduced by Waldinger (1975) and also used by Warrenâ€™s (1974) WARPLAN .
WARPLAN is also notable in that it was the first planner to be written in a logic programming language (Prolog) and is one of the best examples of the remarkable economy that can
sometimes be gained with logic programming: WARPLAN is only 100 lines of code, a small
fraction of the size of comparable planners of the time.
The ideas underlying partial-order planning include the detection of conflicts (Tate,
1975a) and the protection of achieved conditions from interference (Sussman, 1975). The
construction of partially ordered plans (then called task networks) was pioneered by the
N OAH planner (Sacerdoti, 1975, 1977) and by Tateâ€™s (1975b, 1977) N ONLIN system.
Partial-order planning dominated the next 20 years of research, yet the first clear formal exposition was T WEAK (Chapman, 1987), a planner that was simple enough to allow
proofs of completeness and intractability (NP-hardness and undecidability) of various planning problems. Chapmanâ€™s work led to a straightforward description of a complete partialorder planner (McAllester and Rosenblitt, 1991), then to the widely distributed implementations SNLP (Soderland and Weld, 1991) and UCPOP (Penberthy and Weld, 1992). Partialorder planning fell out of favor in the late 1990s as faster methods emerged. Nguyen and
Kambhampati (2001) suggest that a reconsideration is merited: with accurate heuristics derived from a planning graph, their R E POP planner scales up much better than G RAPHPLAN
in parallelizable domains and is competitive with the fastest state-space planners.
The resurgence of interest in state-space planning was pioneered by Drew McDermottâ€™s U N POP program (1996), which was the first to suggest the ignore-delete-list heuristic,
The name U N POP was a reaction to the overwhelming concentration on partial-order planning at the time; McDermott suspected that other approaches were not getting the attention
they deserved. Bonet and Geffnerâ€™s Heuristic Search Planner (HSP) and its later derivatives (Bonet and Geffner, 1999; Haslum et al., 2005; Haslum, 2006) were the first to make

Bibliographical and Historical Notes

BINARY DECISION
DIAGRAM

395

state-space search practical for large planning problems. HSP searches in the forward direction while HSP R (Bonet and Geffner, 1999) searches backward. The most successful
state-space searcher to date is FF (Hoffmann, 2001; Hoffmann and Nebel, 2001; Hoffmann,
2005), winner of the AIPS 2000 planning competition. FAST D OWNWARD (Helmert, 2006)
is a forward state-space search planner that preprocesses the action schemas into an alternative representation which makes some of the constraints more explicit. FAST D OWNWARD
(Helmert and Richter, 2004; Helmert, 2006) won the 2004 planning competition, and LAMA
(Richter and Westphal, 2008), a planner based on FAST D OWNWARD with improved heuristics, won the 2008 competition.
Bylander (1994) and Ghallab et al. (2004) discuss the computational complexity of
several variants of the planning problem. Helmert (2003) proves complexity bounds for many
of the standard benchmark problems, and Hoffmann (2005) analyzes the search space of the
ignore-delete-list heuristic. Heuristics for the set-covering problem are discussed by Caprara
et al. (1995) for scheduling operations of the Italian railway. Edelkamp (2009) and Haslum
et al. (2007) describe how to construct pattern databases for planning heuristics. As we
mentioned in Chapter 3, Felner et al. (2004) show encouraging results using pattern databases
for sliding blocks puzzles, which can be thought of as a planning domain, but Hoffmann et al.
(2006) show some limitations of abstraction for classical planning problems.
Avrim Blum and Merrick Furst (1995, 1997) revitalized the field of planning with their
G RAPHPLAN system, which was orders of magnitude faster than the partial-order planners of
the time. Other graph-planning systems, such as IPP (Koehler et al., 1997), S TAN (Fox and
Long, 1998), and SGP (Weld et al., 1998), soon followed. A data structure closely resembling
the planning graph had been developed slightly earlier by Ghallab and Laruelle (1994), whose
I X T E T partial-order planner used it to derive accurate heuristics to guide searches. Nguyen
et al. (2001) thoroughly analyze heuristics derived from planning graphs. Our discussion of
planning graphs is based partly on this work and on lecture notes and articles by Subbarao
Kambhampati (Bryce and Kambhampati, 2007). As mentioned in the chapter, a planning
graph can be used in many different ways to guide the search for a solution. The winner
of the 2002 AIPS planning competition, LPG (Gerevini and Serina, 2002, 2003), searched
planning graphs using a local search technique inspired by WALK SAT.
The situation calculus approach to planning was introduced by John McCarthy (1963).
The version we show here was proposed by Ray Reiter (1991, 2001).
Kautz et al. (1996) investigated various ways to propositionalize action schemas, finding that the most compact forms did not necessarily lead to the fastest solution times. A
systematic analysis was carried out by Ernst et al. (1997), who also developed an automatic â€œcompilerâ€ for generating propositional representations from PDDL problems. The
B LACKBOX planner, which combines ideas from G RAPHPLAN and SATP LAN , was developed by Kautz and Selman (1998). CPLAN , a planner based on constraint satisfaction, was
described by van Beek and Chen (1999).
Most recently, there has been interest in the representation of plans as binary decision
diagrams, compact data structures for Boolean expressions widely studied in the hardware
verification community (Clarke and Grumberg, 1987; McMillan, 1993). There are techniques
for proving properties of binary decision diagrams, including the property of being a solution

396

Chapter

10.

Classical Planning

to a planning problem. Cimatti et al. (1998) present a planner based on this approach. Other
representations have also been used; for example, Vossen et al. (2001) survey the use of
integer programming for planning.
The jury is still out, but there are now some interesting comparisons of the various
approaches to planning. Helmert (2001) analyzes several classes of planning problems, and
shows that constraint-based approaches such as G RAPHPLAN and SATP LAN are best for NPhard domains, while search-based approaches do better in domains where feasible solutions
can be found without backtracking. G RAPHPLAN and SATP LAN have trouble in domains
with many objects because that means they must create many actions. In some cases the
problem can be delayed or avoided by generating the propositionalized actions dynamically,
only as needed, rather than instantiating them all before the search begins.
Readings in Planning (Allen et al., 1990) is a comprehensive anthology of early work
in the field. Weld (1994, 1999) provides two excellent surveys of planning algorithms of
the 1990s. It is interesting to see the change in the five years between the two surveys:
the first concentrates on partial-order planning, and the second introduces G RAPHPLAN and
SATP LAN . Automated Planning (Ghallab et al., 2004) is an excellent textbook on all aspects
of planning. LaValleâ€™s text Planning Algorithms (2006) covers both classical and stochastic
planning, with extensive coverage of robot motion planning.
Planning research has been central to AI since its inception, and papers on planning are
a staple of mainstream AI journals and conferences. There are also specialized conferences
such as the International Conference on AI Planning Systems, the International Workshop on
Planning and Scheduling for Space, and the European Conference on Planning.

E XERCISES
10.1

Describe the differences and similarities between problem solving and planning.

10.2 Given the action schemas and initial state from Figure 10.1, what are all the applicable
concrete instances of Fly(p, from, to) in the state described by
At(P1 , JFK ) âˆ§ At(P2 , SFO) âˆ§ Plane(P1 ) âˆ§ Plane(P2 )
âˆ§ Airport (JFK ) âˆ§ Airport (SFO ) ?
10.3 The monkey-and-bananas problem is faced by a monkey in a laboratory with some
bananas hanging out of reach from the ceiling. A box is available that will enable the monkey
to reach the bananas if he climbs on it. Initially, the monkey is at A, the bananas at B, and the
box at C. The monkey and box have height Low , but if the monkey climbs onto the box he
will have height High, the same as the bananas. The actions available to the monkey include
Go from one place to another, Push an object from one place to another, ClimbUp onto or
ClimbDown from an object, and Grasp or Ungrasp an object. The result of a Grasp is that
the monkey holds the object if the monkey and object are in the same place at the same height.
a. Write down the initial state description.

Exercises

397
Switch 4

Door 4

Room 4

Switch 3

Door 3

Room 3
Shakey

Switch 2

Corridor

Door 2

Room 2

Switch 1
Box 3

Box 2
Door 1

Room 1
Box 4

Box 1

Figure 10.14 Shakeyâ€™s world. Shakey can move between landmarks within a room, can
pass through the door between rooms, can climb climbable objects and push pushable objects,
and can flip light switches.

b. Write the six action schemas.
c. Suppose the monkey wants to fool the scientists, who are off to tea, by grabbing the
bananas, but leaving the box in its original place. Write this as a general goal (i.e., not
assuming that the box is necessarily at C) in the language of situation calculus. Can this
goal be solved by a classical planning system?
d. Your schema for pushing is probably incorrect, because if the object is too heavy, its
position will remain the same when the Push schema is applied. Fix your action schema
to account for heavy objects.
10.4 The original S TRIPS planner was designed to control Shakey the robot. Figure 10.14
shows a version of Shakeyâ€™s world consisting of four rooms lined up along a corridor, where
each room has a door and a light switch. The actions in Shakeyâ€™s world include moving from
place to place, pushing movable objects (such as boxes), climbing onto and down from rigid

398

Chapter

10.

Classical Planning

objects (such as boxes), and turning light switches on and off. The robot itself could not climb
on a box or toggle a switch, but the planner was capable of finding and printing out plans that
were beyond the robotâ€™s abilities. Shakeyâ€™s six actions are the following:
â€¢ Go(x, y, r), which requires that Shakey be At x and that x and y are locations In the
same room r. By convention a door between two rooms is in both of them.
â€¢ Push a box b from location x to location y within the same room: Push(b, x, y, r). You
will need the predicate Box and constants for the boxes.
â€¢ Climb onto a box from position x: ClimbUp(x, b); climb down from a box to position
x: ClimbDown(b, x). We will need the predicate On and the constant Floor .
â€¢ Turn a light switch on or off: TurnOn(s, b); TurnOï¬€ (s, b). To turn a light on or off,
Shakey must be on top of a box at the light switchâ€™s location.
Write PDDL sentences for Shakeyâ€™s six actions and the initial state from Figure 10.14. Construct a plan for Shakey to get Box 2 into Room 2 .
10.5 A finite Turing machine has a finite one-dimensional tape of cells, each cell containing
one of a finite number of symbols. One cell has a read and write head above it. There is a
finite set of states the machine can be in, one of which is the accept state. At each time step,
depending on the symbol on the cell under the head and the machineâ€™s current state, there are
a set of actions we can choose from. Each action involves writing a symbol to the cell under
the head, transitioning the machine to a state, and optionally moving the head left or right.
The mapping that determines which actions are allowed is the Turing machineâ€™s program.
Your goal is to control the machine into the accept state.
Represent the Turing machine acceptance problem as a planning problem. If you can
do this, it demonstrates that determining whether a planning problem has a solution is at least
as hard as the Turing acceptance problem, which is PSPACE-hard.
10.6 Explain why dropping negative effects from every action schema in a planning problem results in a relaxed problem.

SUSSMAN ANOMALY

10.7 Figure 10.4 (page 371) shows a blocks-world problem that is known as the Sussman
anomaly. The problem was considered anomalous because the noninterleaved planners of
the early 1970s could not solve it. Write a definition of the problem and solve it, either by
hand or with a planning program. A noninterleaved planner is a planner that, when given two
subgoals G1 and G2 , produces either a plan for G1 concatenated with a plan for G2 , or vice
versa. Explain why a noninterleaved planner cannot solve this problem.
10.8

Prove that backward search with PDDL problems is complete.

10.9

Construct levels 0, 1, and 2 of the planning graph for the problem in Figure 10.1.

10.10

Prove the following assertions about planning graphs:

a. A literal that does not appear in the final level of the graph cannot be achieved.

Exercises

399
b. The level cost of a literal in a serial graph is no greater than the actual cost of an optimal
plan for achieving it.
10.11 The set-level heuristic (see page 382) uses a planning graph to estimate the cost of
achieving a conjunctive goal from the current state. What relaxed problem is the set-level
heuristic the solution to?
10.12

Examine the definition of bidirectional search in Chapter 3.

a. Would bidirectional state-space search be a good idea for planning?
b. What about bidirectional search in the space of partial-order plans?
c. Devise a version of partial-order planning in which an action can be added to a plan if its
preconditions can be achieved by the effects of actions already in the plan. Explain how
to deal with conflicts and ordering constraints. Is the algorithm essentially identical to
forward state-space search?
10.13 We contrasted forward and backward state-space searchers with partial-order planners, saying that the latter is a plan-space searcher. Explain how forward and backward statespace search can also be considered plan-space searchers, and say what the plan refinement
operators are.
10.14 Up to now we have assumed that the plans we create always make sure that an actionâ€™s
preconditions are satisfied. Let us now investigate what propositional successor-state axioms
such as HaveArrow t+1 â‡” (HaveArrow t âˆ§ Â¬Shoot t ) have to say about actions whose
preconditions are not satisfied.
a. Show that the axioms predict that nothing will happen when an action is executed in a
state where its preconditions are not satisfied.
b. Consider a plan p that contains the actions required to achieve a goal but also includes
illegal actions. Is it the case that
initial state âˆ§ successor-state axioms âˆ§ p |= goal ?
c. With first-order successor-state axioms in situation calculus, is it possible to prove that
a plan containing illegal actions will achieve the goal?
10.15 Consider how to translate a set of action schemas into the successor-state axioms of
situation calculus.
a. Consider the schema for Fly(p, from, to). Write a logical definition for the predicate
Poss(Fly (p, from, to), s), which is true if the preconditions for Fly(p, from, to) are
satisfied in situation s.
b. Next, assuming that Fly(p, from, to) is the only action schema available to the agent,
write down a successor-state axiom for At(p, x, s) that captures the same information
as the action schema.

400

Chapter

10.

Classical Planning

c. Now suppose there is an additional method of travel: Teleport (p, from, to). It has
the additional precondition Â¬Warped (p) and the additional effect Warped (p). Explain
how the situation calculus knowledge base must be modified.
d. Finally, develop a general and precisely specified procedure for carrying out the translation from a set of action schemas to a set of successor-state axioms.
10.16 In the SATP LAN algorithm in Figure 7.22 (page 272), each call to the satisfiability algorithm asserts a goal gT , where T ranges from 0 to Tmax . Suppose instead that the
satisfiability algorithm is called only once, with the goal g0 âˆ¨ g1 âˆ¨ Â· Â· Â· âˆ¨ gTmax .
a. Will this always return a plan if one exists with length less than or equal to Tmax ?
b. Does this approach introduce any new spurious â€œsolutionsâ€?
c. Discuss how one might modify a satisfiability algorithm such as WALK SAT so that it
finds short solutions (if they exist) when given a disjunctive goal of this form.

11

PLANNING AND ACTING
IN THE REAL WORLD

In which we see how more expressive representations and more interactive agent
architectures lead to planners that are useful in the real world.

The previous chapter introduced the most basic concepts, representations, and algorithms for
planning. Planners that are are used in the real world for planning and scheduling the operations of spacecraft, factories, and military campaigns are more complex; they extend both
the representation language and the way the planner interacts with the environment. This
chapter shows how. Section 11.1 extends the classical language for planning to talk about
actions with durations and resource constraints. Section 11.2 describes methods for constructing plans that are organized hierarchically. This allows human experts to communicate
to the planner what they know about how to solve the problem. Hierarchy also lends itself to
efficient plan construction because the planner can solve a problem at an abstract level before
delving into details. Section 11.3 presents agent architectures that can handle uncertain environments and interleave deliberation with execution, and gives some examples of real-world
systems. Section 11.4 shows how to plan when the environment contains other agents.

11.1

T IME , S CHEDULES , AND R ESOURCES
The classical planning representation talks about what to do, and in what order, but the representation cannot talk about time: how long an action takes and when it occurs. For example,
the planners of Chapter 10 could produce a schedule for an airline that says which planes are
assigned to which flights, but we really need to know departure and arrival times as well. This
is the subject matter of scheduling. The real world also imposes many resource constraints;
for example, an airline has a limited number of staffâ€”and staff who are on one flight cannot
be on another at the same time. This section covers methods for representing and solving
planning problems that include temporal and resource constraints.
The approach we take in this section is â€œplan first, schedule laterâ€: that is, we divide
the overall problem into a planning phase in which actions are selected, with some ordering
constraints, to meet the goals of the problem, and a later scheduling phase, in which temporal information is added to the plan to ensure that it meets resource and deadline constraints.
401

402

Chapter

11.

Planning and Acting in the Real World

Jobs({AddEngine1 â‰º AddWheels1 â‰º Inspect1 },
{AddEngine2 â‰º AddWheels2 â‰º Inspect2 })
Resources(EngineHoists(1), WheelStations (1), Inspectors (2), LugNuts(500))
Action(AddEngine1 , D URATION :30,
U SE :EngineHoists(1 ))
Action(AddEngine2 , D URATION :60,
U SE :EngineHoists(1 ))
Action(AddWheels1 , D URATION :30,
C ONSUME :LugNuts(20), U SE :WheelStations(1))
Action(AddWheels2 , D URATION :15,
C ONSUME :LugNuts(20), U SE :WheelStations(1))
Action(Inspect i , D URATION :10,
U SE :Inspectors (1))
Figure 11.1 A job-shop scheduling problem for assembling two cars, with resource constraints. The notation A â‰º B means that action A must precede action B.

This approach is common in real-world manufacturing and logistical settings, where the planning phase is often performed by human experts. The automated methods of Chapter 10 can
also be used for the planning phase, provided that they produce plans with just the minimal
ordering constraints required for correctness. G RAPHPLAN (Section 10.3), SATP LAN (Section 10.4.1), and partial-order planners (Section 10.4.4) can do this; search-based methods
(Section 10.2) produce totally ordered plans, but these can easily be converted to plans with
minimal ordering constraints.

11.1.1 Representing temporal and resource constraints
JOB
DURATION

CONSUMABLE
REUSABLE

MAKESPAN

A typical job-shop scheduling problem, as first introduced in Section 6.1.2, consists of a
set of jobs, each of which consists a collection of actions with ordering constraints among
them. Each action has a duration and a set of resource constraints required by the action.
Each constraint specifies a type of resource (e.g., bolts, wrenches, or pilots), the number
of that resource required, and whether that resource is consumable (e.g., the bolts are no
longer available for use) or reusable (e.g., a pilot is occupied during a flight but is available
again when the flight is over). Resources can also be produced by actions with negative consumption, including manufacturing, growing, and resupply actions. A solution to a job-shop
scheduling problem must specify the start times for each action and must satisfy all the temporal ordering constraints and resource constraints. As with search and planning problems,
solutions can be evaluated according to a cost function; this can be quite complicated, with
nonlinear resource costs, time-dependent delay costs, and so on. For simplicity, we assume
that the cost function is just the total duration of the plan, which is called the makespan.
Figure 11.1 shows a simple example: a problem involving the assembly of two cars. The
problem consists of two jobs, each of the form [AddEngine, AddWheels, Inspect ]. Then the

Section 11.1.

AGGREGATION

Time, Schedules, and Resources

403

Resources statement declares that there are four types of resources, and gives the number
of each type available at the start: 1 engine hoist, 1 wheel station, 2 inspectors, and 500 lug
nuts. The action schemas give the duration and resource needs of each action. The lug nuts
are consumed as wheels are added to the car, whereas the other resources are â€œborrowedâ€ at
the start of an action and released at the actionâ€™s end.
The representation of resources as numerical quantities, such as Inspectors (2), rather
than as named entities, such as Inspector (I1 ) and Inspector (I2 ), is an example of a very
general technique called aggregation. The central idea of aggregation is to group individual
objects into quantities when the objects are all indistinguishable with respect to the purpose
at hand. In our assembly problem, it does not matter which inspector inspects the car, so there
is no need to make the distinction. (The same idea works in the missionaries-and-cannibals
problem in Exercise 3.9.) Aggregation is essential for reducing complexity. Consider what
happens when a proposed schedule has 10 concurrent Inspect actions but only 9 inspectors
are available. With inspectors represented as quantities, a failure is detected immediately and
the algorithm backtracks to try another schedule. With inspectors represented as individuals,
the algorithm backtracks to try all 10! ways of assigning inspectors to actions.

11.1.2 Solving scheduling problems

CRITICAL PATH
METHOD

CRITICAL PATH

SLACK

SCHEDULE

We begin by considering just the temporal scheduling problem, ignoring resource constraints.
To minimize makespan (plan duration), we must find the earliest start times for all the actions
consistent with the ordering constraints supplied with the problem. It is helpful to view these
ordering constraints as a directed graph relating the actions, as shown in Figure 11.2. We can
apply the critical path method (CPM) to this graph to determine the possible start and end
times of each action. A path through a graph representing a partial-order plan is a linearly
ordered sequence of actions beginning with Start and ending with Finish. (For example,
there are two paths in the partial-order plan in Figure 11.2.)
The critical path is that path whose total duration is longest; the path is â€œcriticalâ€
because it determines the duration of the entire planâ€”shortening other paths doesnâ€™t shorten
the plan as a whole, but delaying the start of any action on the critical path slows down the
whole plan. Actions that are off the critical path have a window of time in which they can be
executed. The window is specified in terms of an earliest possible start time, ES , and a latest
possible start time, LS . The quantity LS â€“ ES is known as the slack of an action. We can
see in Figure 11.2 that the whole plan will take 85 minutes, that each action in the top job
has 15 minutes of slack, and that each action on the critical path has no slack (by definition).
Together the ES and LS times for all the actions constitute a schedule for the problem.
The following formulas serve as a definition for ES and LS and also as the outline of a
dynamic-programming algorithm to compute them. A and B are actions, and A â‰º B means
that A comes before B:
ES (Start ) = 0
ES (B) = maxA â‰º B ES (A) + Duration(A)
LS (Finish) = ES (Finish)
LS (A) = minB 	 A LS (B) âˆ’ Duration(A) .

404

Chapter

11.

Planning and Acting in the Real World
[60,75]

[0,15]

[30,45]

AddEngine1

AddWheels1

Inspect1

30

30

10

[0,0]

[85,85]

Start

Finish

[0,0]

[60,60]

AddEngine2

AddWheels2

[75,75]
Inspect2

60

15

10

AddWheels1
AddEngine1

Inspect1

AddEngine2

Inspect2
AddWheels2

0

10

20

30

40

50

60

70

80

90

Figure 11.2 Top: a representation of the temporal constraints for the job-shop scheduling
problem of Figure 11.1. The duration of each action is given at the bottom of each rectangle.
In solving the problem, we compute the earliest and latest start times as the pair [ES , LS ],
displayed in the upper left. The difference between these two numbers is the slack of an
action; actions with zero slack are on the critical path, shown with bold arrows. Bottom: the
same solution shown as a timeline. Grey rectangles represent time intervals during which an
action may be executed, provided that the ordering constraints are respected. The unoccupied
portion of a gray rectangle indicates the slack.

The idea is that we start by assigning ES (Start ) to be 0. Then, as soon as we get an action
B such that all the actions that come immediately before B have ES values assigned, we
set ES (B) to be the maximum of the earliest finish times of those immediately preceding
actions, where the earliest finish time of an action is defined as the earliest start time plus the
duration. This process repeats until every action has been assigned an ES value. The LS
values are computed in a similar manner, working backward from the Finish action.
The complexity of the critical path algorithm is just O(N b), where N is the number of
actions and b is the maximum branching factor into or out of an action. (To see this, note that
the LS and ES computations are done once for each action, and each computation iterates
over at most b other actions.) Therefore, finding a minimum-duration schedule, given a partial
ordering on the actions and no resource constraints, is quite easy.
Mathematically speaking, critical-path problems are easy to solve because they are defined as a conjunction of linear inequalities on the start and end times. When we introduce
resource constraints, the resulting constraints on start and end times become more complicated. For example, the AddEngine actions, which begin at the same time in Figure 11.2,

Section 11.1.

Time, Schedules, and Resources

405

AddEngine1

EngineHoists(1)

AddEngine2
AddWheels1

WheelStations(1)

AddWheels2
Inspect1

Inspectors(2)
Inspect2

0

10

20

30

40

50

60

70

80

90

100

110

120

Figure 11.3 A solution to the job-shop scheduling problem from Figure 11.1, taking into
account resource constraints. The left-hand margin lists the three reusable resources, and
actions are shown aligned horizontally with the resources they use. There are two possible schedules, depending on which assembly uses the engine hoist first; weâ€™ve shown the
shortest-duration solution, which takes 115 minutes.

MINIMUM SLACK

require the same EngineHoist and so cannot overlap. The â€œcannot overlapâ€ constraint is a
disjunction of two linear inequalities, one for each possible ordering. The introduction of
disjunctions turns out to make scheduling with resource constraints NP-hard.
Figure 11.3 shows the solution with the fastest completion time, 115 minutes. This is
30 minutes longer than the 85 minutes required for a schedule without resource constraints.
Notice that there is no time at which both inspectors are required, so we can immediately
move one of our two inspectors to a more productive position.
The complexity of scheduling with resource constraints is often seen in practice as
well as in theory. A challenge problem posed in 1963â€”to find the optimal schedule for a
problem involving just 10 machines and 10 jobs of 100 actions eachâ€”went unsolved for
23 years (Lawler et al., 1993). Many approaches have been tried, including branch-andbound, simulated annealing, tabu search, constraint satisfaction, and other techniques from
Chapters 3 and 4. One simple but popular heuristic is the minimum slack algorithm: on
each iteration, schedule for the earliest possible start whichever unscheduled action has all
its predecessors scheduled and has the least slack; then update the ES and LS times for each
affected action and repeat. The heuristic resembles the minimum-remaining-values (MRV)
heuristic in constraint satisfaction. It often works well in practice, but for our assembly
problem it yields a 130â€“minute solution, not the 115â€“minute solution of Figure 11.3.
Up to this point, we have assumed that the set of actions and ordering constraints is
fixed. Under these assumptions, every scheduling problem can be solved by a nonoverlapping
sequence that avoids all resource conflicts, provided that each action is feasible by itself. If
a scheduling problem is proving very difficult, however, it may not be a good idea to solve
it this wayâ€”it may be better to reconsider the actions and constraints, in case that leads to a
much easier scheduling problem. Thus, it makes sense to integrate planning and scheduling
by taking into account durations and overlaps during the construction of a partial-order plan.
Several of the planning algorithms in Chapter 10 can be augmented to handle this information.
For example, partial-order planners can detect resource constraint violations in much the
same way they detect conflicts with causal links. Heuristics can be devised to estimate the
total completion time of a plan. This is currently an active area of research.

406

11.2

Chapter

11.

Planning and Acting in the Real World

H IERARCHICAL P LANNING

HIERARCHICAL
DECOMPOSITION

The problem-solving and planning methods of the preceding chapters all operate with a fixed
set of atomic actions. Actions can be strung together into sequences or branching networks;
state-of-the-art algorithms can generate solutions containing thousands of actions.
For plans executed by the human brain, atomic actions are muscle activations. In very
round numbers, we have about 103 muscles to activate (639, by some counts, but many of
them have multiple subunits); we can modulate their activation perhaps 10 times per second;
and we are alive and awake for about 109 seconds in all. Thus, a human life contains about
1013 actions, give or take one or two orders of magnitude. Even if we restrict ourselves to
planning over much shorter time horizonsâ€”for example, a two-week vacation in Hawaiiâ€”a
detailed motor plan would contain around 1010 actions. This is a lot more than 1000.
To bridge this gap, AI systems will probably have to do what humans appear to do: plan
at higher levels of abstraction. A reasonable plan for the Hawaii vacation might be â€œGo to
San Francisco airport; take Hawaiian Airlines flight 11 to Honolulu; do vacation stuff for two
weeks; take Hawaiian Airlines flight 12 back to San Francisco; go home.â€ Given such a plan,
the action â€œGo to San Francisco airportâ€ can be viewed as a planning task in itself, with a
solution such as â€œDrive to the long-term parking lot; park; take the shuttle to the terminal.â€
Each of these actions, in turn, can be decomposed further, until we reach the level of actions
that can be executed without deliberation to generate the required motor control sequences.
In this example, we see that planning can occur both before and during the execution
of the plan; for example, one would probably defer the problem of planning a route from a
parking spot in long-term parking to the shuttle bus stop until a particular parking spot has
been found during execution. Thus, that particular action will remain at an abstract level
prior to the execution phase. We defer discussion of this topic until Section 11.3. Here, we
concentrate on the aspect of hierarchical decomposition, an idea that pervades almost all
attempts to manage complexity. For example, complex software is created from a hierarchy
of subroutines or object classes; armies operate as a hierarchy of units; governments and corporations have hierarchies of departments, subsidiaries, and branch offices. The key benefit
of hierarchical structure is that, at each level of the hierarchy, a computational task, military
mission, or administrative function is reduced to a small number of activities at the next lower
level, so the computational cost of finding the correct way to arrange those activities for the
current problem is small. Nonhierarchical methods, on the other hand, reduce a task to a
large number of individual actions; for large-scale problems, this is completely impractical.

11.2.1 High-level actions
HIERARCHICAL TASK
NETWORK

PRIMITIVE ACTION
HIGH-LEVEL ACTION

The basic formalism we adopt to understand hierarchical decomposition comes from the area
of hierarchical task networks or HTN planning. As in classical planning (Chapter 10), we
assume full observability and determinism and the availability of a set of actions, now called
primitive actions, with standard preconditionâ€“effect schemas. The key additional concept is
the high-level action or HLAâ€”for example, the action â€œGo to San Francisco airportâ€ in the

Section 11.2.

Hierarchical Planning

407

Reï¬nement(Go(Home, SFO),
S TEPS: [Drive(Home, SFOLongTermParking),
Shuttle(SFOLongTermParking, SFO)] )
Reï¬nement(Go(Home, SFO),
S TEPS: [Taxi (Home, SFO)] )
Reï¬nement(Navigate([a, b], [x, y]),
P RECOND : a = x âˆ§ b = y
S TEPS: [ ] )
Reï¬nement(Navigate([a, b], [x, y]),
P RECOND :Connected ([a, b], [a âˆ’ 1, b])
S TEPS: [Left , Navigate([a âˆ’ 1, b], [x, y])] )
Reï¬nement(Navigate([a, b], [x, y]),
P RECOND :Connected ([a, b], [a + 1, b])
S TEPS: [Right , Navigate([a + 1, b], [x, y])] )
...
Figure 11.4 Definitions of possible refinements for two high-level actions: going to San
Francisco airport and navigating in the vacuum world. In the latter case, note the recursive
nature of the refinements and the use of preconditions.

REFINEMENT

IMPLEMENTATION

example given earlier. Each HLA has one or more possible refinements, into a sequence1
of actions, each of which may be an HLA or a primitive action (which has no refinements
by definition). For example, the action â€œGo to San Francisco airport,â€ represented formally
as Go(Home, SFO ), might have two possible refinements, as shown in Figure 11.4. The
same figure shows a recursive refinement for navigation in the vacuum world: to get to a
destination, take a step, and then go to the destination.
These examples show that high-level actions and their refinements embody knowledge
about how to do things. For instance, the refinements for Go(Home, SFO ) say that to get to
the airport you can drive or take a taxi; buying milk, sitting down, and moving the knight to
e4 are not to be considered.
An HLA refinement that contains only primitive actions is called an implementation
of the HLA. For example, in the vacuum world, the sequences [Right, Right, Down] and
[Down, Right, Right ] both implement the HLA Navigate([1, 3], [3, 2]). An implementation
of a high-level plan (a sequence of HLAs) is the concatenation of implementations of each
HLA in the sequence. Given the preconditionâ€“effect definitions of each primitive action, it is
straightforward to determine whether any given implementation of a high-level plan achieves
the goal. We can say, then, that a high-level plan achieves the goal from a given state if at
least one of its implementations achieves the goal from that state. The â€œat least oneâ€ in this
definition is crucialâ€”not all implementations need to achieve the goal, because the agent gets
1 HTN planners often allow refinement into partially ordered plans, and they allow the refinements of two
different HLAs in a plan to share actions. We omit these important complications in the interest of understanding
the basic concepts of hierarchical planning.

408

Chapter

11.

Planning and Acting in the Real World

to decide which implementation it will execute. Thus, the set of possible implementations in
HTN planningâ€”each of which may have a different outcomeâ€”is not the same as the set of
possible outcomes in nondeterministic planning. There, we required that a plan work for all
outcomes because the agent doesnâ€™t get to choose the outcome; nature does.
The simplest case is an HLA that has exactly one implementation. In that case, we
can compute the preconditions and effects of the HLA from those of the implementation
(see Exercise 11.3) and then treat the HLA exactly as if it were a primitive action itself. It
can be shown that the right collection of HLAs can result in the time complexity of blind
search dropping from exponential in the solution depth to linear in the solution depth, although devising such a collection of HLAs may be a nontrivial task in itself. When HLAs
have multiple possible implementations, there are two options: one is to search among the
implementations for one that works, as in Section 11.2.2; the other is to reason directly about
the HLAsâ€”despite the multiplicity of implementationsâ€”as explained in Section 11.2.3. The
latter method enables the derivation of provably correct abstract plans, without the need to
consider their implementations.

11.2.2 Searching for primitive solutions
HTN planning is often formulated with a single â€œtop levelâ€ action called Act, where the aim is
to find an implementation of Act that achieves the goal. This approach is entirely general. For
example, classical planning problems can be defined as follows: for each primitive action ai ,
provide one refinement of Act with steps [ai , Act]. That creates a recursive definition of Act
that lets us add actions. But we need some way to stop the recursion; we do that by providing
one more refinement for Act, one with an empty list of steps and with a precondition equal
to the goal of the problem. This says that if the goal is already achieved, then the right
implementation is to do nothing.
The approach leads to a simple algorithm: repeatedly choose an HLA in the current
plan and replace it with one of its refinements, until the plan achieves the goal. One possible
implementation based on breadth-first tree search is shown in Figure 11.5. Plans are considered in order of depth of nesting of the refinements, rather than number of primitive steps. It
is straightforward to design a graph-search version of the algorithm as well as depth-first and
iterative deepening versions.
In essence, this form of hierarchical search explores the space of sequences that conform
to the knowledge contained in the HLA library about how things are to be done. A great deal
of knowledge can be encoded, not just in the action sequences specified in each refinement but
also in the preconditions for the refinements. For some domains, HTN planners have been
able to generate huge plans with very little search. For example, O-P LAN (Bell and Tate,
1985), which combines HTN planning with scheduling, has been used to develop production
plans for Hitachi. A typical problem involves a product line of 350 different products, 35
assembly machines, and over 2000 different operations. The planner generates a 30-day
schedule with three 8-hour shifts a day, involving tens of millions of steps. Another important
aspect of HTN plans is that they are, by definition, hierarchically structured; usually this
makes them easy for humans to understand.

Section 11.2.

Hierarchical Planning

409

function H IERARCHICAL -S EARCH ( problem, hierarchy ) returns a solution, or failure
frontier â† a FIFO queue with [Act] as the only element
loop do
if E MPTY ?( frontier ) then return failure
plan â† P OP ( frontier ) /* chooses the shallowest plan in frontier */
hla â† the first HLA in plan, or null if none
preï¬x ,suï¬ƒx â† the action subsequences before and after hla in plan
outcome â† R ESULT(problem.I NITIAL -S TATE, preï¬x )
if hla is null then /* so plan is primitive and outcome is its result */
if outcome satisfies problem.G OAL then return plan
else for each sequence in R EFINEMENTS(hla, outcome, hierarchy ) do
frontier â† I NSERT(A PPEND( preï¬x , sequence, suï¬ƒx ), frontier )
Figure 11.5 A breadth-first implementation of hierarchical forward planning search. The
initial plan supplied to the algorithm is [Act]. The R EFINEMENTS function returns a set of
action sequences, one for each refinement of the HLA whose preconditions are satisfied by
the specified state, outcome.

The computational benefits of hierarchical search can be seen by examining an idealized case. Suppose that a planning problem has a solution with d primitive actions. For
a nonhierarchical, forward state-space planner with b allowable actions at each state, the
cost is O(bd ), as explained in Chapter 3. For an HTN planner, let us suppose a very regular refinement structure: each nonprimitive action has r possible refinements, each into
k actions at the next lower level. We want to know how many different refinement trees
there are with this structure. Now, if there are d actions at the primitive level, then the
number of levels below the root is logk d, so the number of internal refinement nodes is
1 + k + k2 + Â· Â· Â· + klogk dâˆ’1 = (d âˆ’ 1)/(k âˆ’ 1). Each internal node has r possible refinements, so r (dâˆ’1)/(kâˆ’1) possible regular decomposition trees could be constructed. Examining
this formula, we see that keeping r small and k large can result in huge savings: essentially
we are taking the kth root of the nonhierarchical cost, if b and r are comparable. Small r and
large k means a library of HLAs with a small number of refinements each yielding a long
action sequence (that nonetheless allows us to solve any problem). This is not always possible: long action sequences that are usable across a wide range of problems are extremely
precious.
The key to HTN planning, then, is the construction of a plan library containing known
methods for implementing complex, high-level actions. One method of constructing the library is to learn the methods from problem-solving experience. After the excruciating experience of constructing a plan from scratch, the agent can save the plan in the library as a
method for implementing the high-level action defined by the task. In this way, the agent can
become more and more competent over time as new methods are built on top of old methods.
One important aspect of this learning process is the ability to generalize the methods that
are constructed, eliminating detail that is specific to the problem instance (e.g., the name of

410

Chapter

11.

Planning and Acting in the Real World

the builder or the address of the plot of land) and keeping just the key elements of the plan.
Methods for achieving this kind of generalization are described in Chapter 19. It seems to us
inconceivable that humans could be as competent as they are without some such mechanism.

11.2.3 Searching for abstract solutions
The hierarchical search algorithm in the preceding section refines HLAs all the way to primitive action sequences to determine if a plan is workable. This contradicts common sense: one
should be able to determine that the two-HLA high-level plan
[Drive(Home, SFOLongTermParking ), Shuttle(SFOLongTermParking , SFO )]

DOWNWARD
REFINEMENT
PROPERTY

DEMONIC
NONDETERMINISM

gets one to the airport without having to determine a precise route, choice of parking spot,
and so on. The solution seems obvious: write preconditionâ€“effect descriptions of the HLAs,
just as we write down what the primitive actions do. From the descriptions, it ought to be
easy to prove that the high-level plan achieves the goal. This is the holy grail, so to speak, of
hierarchical planning because if we derive a high-level plan that provably achieves the goal,
working in a small search space of high-level actions, then we can commit to that plan and
work on the problem of refining each step of the plan. This gives us the exponential reduction
we seek. For this to work, it has to be the case that every high-level plan that â€œclaimsâ€ to
achieve the goal (by virtue of the descriptions of its steps) does in fact achieve the goal in
the sense defined earlier: it must have at least one implementation that does achieve the goal.
This property has been called the downward refinement property for HLA descriptions.
Writing HLA descriptions that satisfy the downward refinement property is, in principle, easy: as long as the descriptions are true, then any high-level plan that claims to achieve
the goal must in fact do soâ€”otherwise, the descriptions are making some false claim about
what the HLAs do. We have already seen how to write true descriptions for HLAs that have
exactly one implementation (Exercise 11.3); a problem arises when the HLA has multiple
implementations. How can we describe the effects of an action that can be implemented in
many different ways?
One safe answer (at least for problems where all preconditions and goals are positive) is
to include only the positive effects that are achieved by every implementation of the HLA and
the negative effects of any implementation. Then the downward refinement property would
be satisfied. Unfortunately, this semantics for HLAs is much too conservative. Consider again
the HLA Go(Home, SFO ), which has two refinements, and suppose, for the sake of argument, a simple world in which one can always drive to the airport and park, but taking a taxi
requires Cash as a precondition. In that case, Go(Home, SFO ) doesnâ€™t always get you to
the airport. In particular, it fails if Cash is false, and so we cannot assert At(Agent, SFO ) as
an effect of the HLA. This makes no sense, however; if the agent didnâ€™t have Cash, it would
drive itself. Requiring that an effect hold for every implementation is equivalent to assuming
that someone elseâ€”an adversaryâ€”will choose the implementation. It treats the HLAâ€™s multiple outcomes exactly as if the HLA were a nondeterministic action, as in Section 4.3. For
our case, the agent itself will choose the implementation.
The programming languages community has coined the term demonic nondeterminism for the case where an adversary makes the choices, contrasting this with angelic nonde-

Section 11.2.

Hierarchical Planning

(a)

411

(b)

Figure 11.6 Schematic examples of reachable sets. The set of goal states is shaded. Black
and gray arrows indicate possible implementations of h1 and h2 , respectively. (a) The reachable set of an HLA h1 in a state s. (b) The reachable set for the sequence [h1 , h2 ]. Because
this intersects the goal set, the sequence achieves the goal.
ANGELIC
NONDETERMINISM
ANGELIC SEMANTICS
REACHABLE SET

terminism, where the agent itself makes the choices. We borrow this term to define angelic
semantics for HLA descriptions. The basic concept required for understanding angelic semantics is the reachable set of an HLA: given a state s, the reachable set for an HLA h,
written as R EACH (s, h), is the set of states reachable by any of the HLAâ€™s implementations.
The key idea is that the agent can choose which element of the reachable set it ends up in
when it executes the HLA; thus, an HLA with multiple refinements is more â€œpowerfulâ€ than
the same HLA with fewer refinements. We can also define the reachable set of a sequences of
HLAs. For example, the reachable set of a sequence [h1 , h2 ] is the union of all the reachable
sets obtained by applying h2 in each state in the reachable set of h1 :

R EACH (s , h2 ) .
R EACH (s, [h1 , h2 ]) =
s âˆˆR EACH (s, h1 )
Given these definitions, a high-level planâ€”a sequence of HLAsâ€”achieves the goal if its
reachable set intersects the set of goal states. (Compare this to the much stronger condition
for demonic semantics, where every member of the reachable set has to be a goal state.)
Conversely, if the reachable set doesnâ€™t intersect the goal, then the plan definitely doesnâ€™t
work. Figure 11.6 illustrates these ideas.
The notion of reachable sets yields a straightforward algorithm: search among highlevel plans, looking for one whose reachable set intersects the goal; once that happens, the
algorithm can commit to that abstract plan, knowing that it works, and focus on refining
the plan further. We will come back to the algorithmic issues later; first, we consider the
question of how the effects of an HLAâ€”the reachable set for each possible initial stateâ€”are
represented. As with the classical action schemas of Chapter 10, we represent the changes

412

OPTIMISTIC
DESCRIPTION
PESSIMISTIC
DESCRIPTION

Chapter

11.

Planning and Acting in the Real World

made to each fluent. Think of a fluent as a state variable. A primitive action can add or delete
a variable or leave it unchanged. (With conditional effects (see Section 11.3.1) there is a
fourth possibility: flipping a variable to its opposite.)
An HLA under angelic semantics can do more: it can control the value of a variable,
setting it to true or false depending on which implementation is chosen. In fact, an HLA can
have nine different effects on a variable: if the variable starts out true, it can always keep
it true, always make it false, or have a choice; if the variable starts out false, it can always
keep it false, always make it true, or have a choice; and the three choices for each case can
be combined arbitrarily, making nine. Notationally, this is a bit challenging. Weâ€™ll use the 
 means â€œpossibly add
symbol to mean â€œpossibly, if the agent so chooses.â€ Thus, an effect +A

A,â€ that is, either leave A unchanged or make it true. Similarly, âˆ’A means â€œpossibly delete
 means â€œpossibly add or delete A.â€ For example, the HLA Go(Home, SFO ),
Aâ€ and Â±A
with the two refinements shown in Figure 11.4, possibly deletes Cash (if the agent decides to

take a taxi), so it should have the effect âˆ’Cash.
Thus, we see that the descriptions of HLAs
are derivable, in principle, from the descriptions of their refinementsâ€”in fact, this is required
if we want true HLA descriptions, such that the downward refinement property holds. Now,
suppose we have the following schemas for the HLAs h1 and h2 :

,
Action(h1 , P RECOND :Â¬A, E FFECT:A âˆ§ âˆ’B)
 âˆ§ Â±C)

Action(h2 , P RECOND :Â¬B, E FFECT: +A
.
That is, h1 adds A and possible deletes B, while h2 possibly adds A and has full control over
C. Now, if only B is true in the initial state and the goal is A âˆ§ C then the sequence [h1 , h2 ]
achieves the goal: we choose an implementation of h1 that makes B false, then choose an
implementation of h2 that leaves A true and makes C true.
The preceding discussion assumes that the effects of an HLAâ€”the reachable set for
any given initial stateâ€”can be described exactly by describing the effect on each variable. It
would be nice if this were always true, but in many cases we can only approximate the effects because an HLA may have infinitely many implementations and may produce arbitrarily
wiggly reachable setsâ€”rather like the wiggly-belief-state problem illustrated in Figure 7.21
on page 271. For example, we said that Go(Home, SFO ) possibly deletes Cash; it also
possibly adds At(Car , SFOLongTermParking); but it cannot do bothâ€”in fact, it must do
exactly one. As with belief states, we may need to write approximate descriptions. We will
use two kinds of approximation: an optimistic description R EACH + (s, h) of an HLA h may
overstate the reachable set, while a pessimistic description R EACH âˆ’ (s, h) may understate
the reachable set. Thus, we have
R EACH âˆ’ (s, h) âŠ† R EACH (s, h) âŠ† R EACH + (s, h) .
For example, an optimistic description of Go(Home, SFO ) says that it possible deletes Cash
and possibly adds At(Car , SFOLongTermParking). Another good example arises in the
8-puzzle, half of whose states are unreachable from any given state (see Exercise 3.4 on
page 113): the optimistic description of Act might well include the whole state space, since
the exact reachable set is quite wiggly.
With approximate descriptions, the test for whether a plan achieves the goal needs to
be modified slightly. If the optimistic reachable set for the plan doesnâ€™t intersect the goal,

Section 11.2.

Hierarchical Planning

(a)

413

(b)

Figure 11.7 Goal achievement for high-level plans with approximate descriptions. The
set of goal states is shaded. For each plan, the pessimistic (solid lines) and optimistic (dashed
lines) reachable sets are shown. (a) The plan indicated by the black arrow definitely achieves
the goal, while the plan indicated by the gray arrow definitely doesnâ€™t. (b) A plan that would
need to be refined further to determine if it really does achieve the goal.

then the plan doesnâ€™t work; if the pessimistic reachable set intersects the goal, then the plan
does work (Figure 11.7(a)). With exact descriptions, a plan either works or it doesnâ€™t, but
with approximate descriptions, there is a middle ground: if the optimistic set intersects the
goal but the pessimistic set doesnâ€™t, then we cannot tell if the plan works (Figure 11.7(b)).
When this circumstance arises, the uncertainty can be resolved by refining the plan. This is
a very common situation in human reasoning. For example, in planning the aforementioned
two-week Hawaii vacation, one might propose to spend two days on each of seven islands.
Prudence would indicate that this ambitious plan needs to be refined by adding details of
inter-island transportation.
An algorithm for hierarchical planning with approximate angelic descriptions is shown
in Figure 11.8. For simplicity, we have kept to the same overall scheme used previously in
Figure 11.5, that is, a breadth-first search in the space of refinements. As just explained, the
algorithm can detect plans that will and wonâ€™t work by checking the intersections of the optimistic and pessimistic reachable sets with the goal. (The details of how to compute the reachable sets of a plan, given approximate descriptions of each step, are covered in Exercise 11.5.)
When a workable abstract plan is found, the algorithm decomposes the original problem into
subproblems, one for each step of the plan. The initial state and goal for each subproblem
are obtained by regressing a guaranteed-reachable goal state through the action schemas for
each step of the plan. (See Section 10.2.2 for a discussion of how regression works.) Figure 11.6(b) illustrates the basic idea: the right-hand circled state is the guaranteed-reachable
goal state, and the left-hand circled state is the intermediate goal obtained by regressing the

414

Chapter

11.

Planning and Acting in the Real World

function A NGELIC -S EARCH( problem, hierarchy , initialPlan ) returns solution or fail
frontier â† a FIFO queue with initialPlan as the only element
loop do
if E MPTY ?( frontier ) then return fail
plan â† P OP ( frontier ) /* chooses the shallowest node in frontier */
if R EACH + (problem.I NITIAL -S TATE, plan) intersects problem.G OAL then
if plan is primitive then return plan /* R EACH + is exact for primitive plans */
guaranteed â† R EACH âˆ’ (problem.I NITIAL -S TATE, plan) âˆ© problem.G OAL
if guaranteed={ } and M AKING-P ROGRESS(plan, initialPlan) then
ï¬nalState â† any element of guaranteed
return D ECOMPOSE(hierarchy , problem.I NITIAL -S TATE, plan, ï¬nalState)
hla â† some HLA in plan
preï¬x ,suï¬ƒx â† the action subsequences before and after hla in plan
for each sequence in R EFINEMENTS(hla, outcome, hierarchy ) do
frontier â† I NSERT(A PPEND( preï¬x , sequence, suï¬ƒx ), frontier )
function D ECOMPOSE(hierarchy , s0 , plan, sf ) returns a solution
solution â† an empty plan
while plan is not empty do
action â† R EMOVE -L AST(plan)
si â† a state in R EACH âˆ’ (s0 , plan) such that sf âˆˆR EACH âˆ’ (si , action )
problem â† a problem with I NITIAL -S TATE = si and G OAL = sf
solution â† A PPEND(A NGELIC -S EARCH (problem, hierarchy , action ), solution)
sf â† si
return solution
Figure 11.8 A hierarchical planning algorithm that uses angelic semantics to identify and
commit to high-level plans that work while avoiding high-level plans that donâ€™t. The predicate M AKING-P ROGRESS checks to make sure that we arenâ€™t stuck in an infinite regression
of refinements. At top level, call A NGELIC -S EARCH with [Act ] as the initialPlan .

goal through the final action.
The ability to commit to or reject high-level plans can give A NGELIC -S EARCH a significant computational advantage over H IERARCHICAL-S EARCH , which in turn may have
a large advantage over plain old B READTH -F IRST-S EARCH . Consider, for example, cleaning up a large vacuum world consisting of rectangular rooms connected by narrow corridors. It makes sense to have an HLA for Navigate (as shown in Figure 11.4) and one for
CleanWholeRoom. (Cleaning the room could be implemented with the repeated application
of another HLA to clean each row.) Since there are five actions in this domain, the cost
for B READTH -F IRST-S EARCH grows as 5d , where d is the length of the shortest solution
(roughly twice the total number of squares); the algorithm cannot manage even two 2 Ã— 2
rooms. H IERARCHICAL-S EARCH is more efficient, but still suffers from exponential growth
because it tries all ways of cleaning that are consistent with the hierarchy. A NGELIC -S EARCH
scales approximately linearly in the number of squaresâ€”it commits to a good high-level se-

Section 11.3.

HIERARCHICAL
LOOKAHEAD

11.3

Planning and Acting in Nondeterministic Domains

415

quence and prunes away the other options. Notice that cleaning a set of rooms by cleaning
each room in turn is hardly rocket science: it is easy for humans precisely because of the
hierarchical structure of the task. When we consider how difficult humans find it to solve
small puzzles such as the 8-puzzle, it seems likely that the human capacity for solving complex problems derives to a great extent from their skill in abstracting and decomposing the
problem to eliminate combinatorics.
The angelic approach can be extended to find least-cost solutions by generalizing the
notion of reachable set. Instead of a state being reachable or not, it has a cost for the most
efficient way to get there. (The cost is âˆž for unreachable states.) The optimistic and pessimistic descriptions bound these costs. In this way, angelic search can find provably optimal
abstract plans without considering their implementations. The same approach can be used to
obtain effective hierarchical lookahead algorithms for online search, in the style of LRTAâˆ—
(page 152). In some ways, such algorithms mirror aspects of human deliberation in tasks such
as planning a vacation to Hawaiiâ€”consideration of alternatives is done initially at an abstract
level over long time scales; some parts of the plan are left quite abstract until execution time,
such as how to spend two lazy days on Molokai, while others parts are planned in detail, such
as the flights to be taken and lodging to be reservedâ€”without these refinements, there is no
guarantee that the plan would be feasible.

P LANNING AND ACTING IN N ONDETERMINISTIC D OMAINS
In this section we extend planning to handle partially observable, nondeterministic, and unknown environments. Chapter 4 extended search in similar ways, and the methods here are
also similar: sensorless planning (also known as conformant planning) for environments
with no observations; contingency planning for partially observable and nondeterministic
environments; and online planning and replanning for unknown environments.
While the basic concepts are the same as in Chapter 4, there are also significant differences. These arise because planners deal with factored representations rather than atomic
representations. This affects the way we represent the agentâ€™s capability for action and observation and the way we represent belief statesâ€”the sets of possible physical states the agent
might be inâ€”for unobservable and partially observable environments. We can also take advantage of many of the domain-independent methods given in Chapter 10 for calculating
search heuristics.
Consider this problem: given a chair and a table, the goal is to have them matchâ€”have
the same color. In the initial state we have two cans of paint, but the colors of the paint and
the furniture are unknown. Only the table is initially in the agentâ€™s field of view:
Init(Object(Table) âˆ§ Object(Chair ) âˆ§ Can(C1 ) âˆ§ Can(C2 ) âˆ§ InView (Table))
Goal (Color (Chair , c) âˆ§ Color (Table, c))
There are two actions: removing the lid from a paint can and painting an object using the
paint from an open can. The action schemas are straightforward, with one exception: we now
allow preconditions and effects to contain variables that are not part of the actionâ€™s variable

416

Chapter

11.

Planning and Acting in the Real World

list. That is, Paint(x, can) does not mention the variable c, representing the color of the
paint in the can. In the fully observable case, this is not allowedâ€”we would have to name
the action Paint(x, can, c). But in the partially observable case, we might or might not
know what color is in the can. (The variable c is universally quantified, just like all the other
variables in an action schema.)
Action(RemoveLid (can),
P RECOND :Can(can)
E FFECT:Open(can))
Action(Paint(x , can),
P RECOND :Object(x) âˆ§ Can(can) âˆ§ Color (can, c) âˆ§ Open(can)
E FFECT:Color (x , c))

PERCEPT SCHEMA

To solve a partially observable problem, the agent will have to reason about the percepts it will
obtain when it is executing the plan. The percept will be supplied by the agentâ€™s sensors when
it is actually acting, but when it is planning it will need a model of its sensors. In Chapter 4,
this model was given by a function, P ERCEPT (s). For planning, we augment PDDL with a
new type of schema, the percept schema:
Percept (Color (x, c),
P RECOND :Object(x) âˆ§ InView(x)
Percept (Color (can, c),
P RECOND :Can(can) âˆ§ InView (can) âˆ§ Open(can)
The first schema says that whenever an object is in view, the agent will perceive the color
of the object (that is, for the object x, the agent will learn the truth value of Color (x, c) for
all c). The second schema says that if an open can is in view, then the agent perceives the
color of the paint in the can. Because there are no exogenous events in this world, the color
of an object will remain the same, even if it is not being perceived, until the agent performs
an action to change the objectâ€™s color. Of course, the agent will need an action that causes
objects (one at a time) to come into view:
Action(LookAt(x),
P RECOND :InView(y) âˆ§ (x = y)
E FFECT:InView(x) âˆ§ Â¬InView(y))
For a fully observable environment, we would have a Percept axiom with no preconditions
for each fluent. A sensorless agent, on the other hand, has no Percept axioms at all. Note
that even a sensorless agent can solve the painting problem. One solution is to open any can
of paint and apply it to both chair and table, thus coercing them to be the same color (even
though the agent doesnâ€™t know what the color is).
A contingent planning agent with sensors can generate a better plan. First, look at the
table and chair to obtain their colors; if they are already the same then the plan is done. If
not, look at the paint cans; if the paint in a can is the same color as one piece of furniture,
then apply that paint to the other piece. Otherwise, paint both pieces with any color.
Finally, an online planning agent might generate a contingent plan with fewer branches
at firstâ€”perhaps ignoring the possibility that no cans match any of the furnitureâ€”and deal

Section 11.3.

Planning and Acting in Nondeterministic Domains

417

with problems when they arise by replanning. It could also deal with incorrectness of its
action schemas. Whereas a contingent planner simply assumes that the effects of an action
always succeedâ€”that painting the chair does the jobâ€”a replanning agent would check the
result and make an additional plan to fix any unexpected failure, such as an unpainted area or
the original color showing through.
In the real world, agents use a combination of approaches. Car manufacturers sell spare
tires and air bags, which are physical embodiments of contingent plan branches designed
to handle punctures or crashes. On the other hand, most car drivers never consider these
possibilities; when a problem arises they respond as replanning agents. In general, agents
plan only for contingencies that have important consequences and a nonnegligible chance
of happening. Thus, a car driver contemplating a trip across the Sahara desert should make
explicit contingency plans for breakdowns, whereas a trip to the supermarket requires less
advance planning. We next look at each of the three approaches in more detail.

11.3.1 Sensorless planning
Section 4.4.1 (page 138) introduced the basic idea of searching in belief-state space to find
a solution for sensorless problems. Conversion of a sensorless planning problem to a beliefstate planning problem works much the same way as it did in Section 4.4.1; the main differences are that the underlying physical transition model is represented by a collection of action
schemas and the belief state can be represented by a logical formula instead of an explicitly
enumerated set of states. For simplicity, we assume that the underlying planning problem is
deterministic.
The initial belief state for the sensorless painting problem can ignore InView fluents
because the agent has no sensors. Furthermore, we take as given the unchanging facts
Object(Table) âˆ§ Object(Chair ) âˆ§ Can(C1 ) âˆ§ Can(C2 ) because these hold in every belief state. The agent doesnâ€™t know the colors of the cans or the objects, or whether the cans
are open or closed, but it does know that objects and cans have colors: âˆ€ x âˆƒ c Color (x, c).
After Skolemizing, (see Section 9.5), we obtain the initial belief state:
b0 = Color (x, C(x)) .
In classical planning, where the closed-world assumption is made, we would assume that
any fluent not mentioned in a state is false, but in sensorless (and partially observable) planning we have to switch to an open-world assumption in which states contain both positive
and negative fluents, and if a fluent does not appear, its value is unknown. Thus, the belief
state corresponds exactly to the set of possible worlds that satisfy the formula. Given this
initial belief state, the following action sequence is a solution:
[RemoveLid (Can 1 ), Paint(Chair , Can 1 ), Paint (Table, Can 1 )] .
We now show how to progress the belief state through the action sequence to show that the
final belief state satisfies the goal.
First, note that in a given belief state b, the agent can consider any action whose preconditions are satisfied by b. (The other actions cannot be used because the transition model
doesnâ€™t define the effects of actions whose preconditions might be unsatisfied.) According

418

Chapter

11.

Planning and Acting in the Real World

to Equation (4.4) (page 139), the general formula for updating the belief state b given an
applicable action a in a deterministic world is as follows:
b = R ESULT (b, a) = {s : s = R ESULT P (s, a) and s âˆˆ b}
where R ESULT P defines the physical transition model. For the time being, we assume that the
initial belief state is always a conjunction of literals, that is, a 1-CNF formula. To construct
the new belief state b , we must consider what happens to each literal  in each physical state
s in b when action a is applied. For literals whose truth value is already known in b, the truth
value in b is computed from the current value and the add list and delete list of the action.
(For example, if  is in the delete list of the action, then Â¬ is added to b .) What about a
literal whose truth value is unknown in b? There are three cases:
1. If the action adds , then  will be true in b regardless of its initial value.
2. If the action deletes , then  will be false in b regardless of its initial value.
3. If the action does not affect , then  will retain its initial value (which is unknown) and
will not appear in b .
Hence, we see that the calculation of b is almost identical to the observable case, which was
specified by Equation (10.1) on page 368:
b = R ESULT (b, a) = (b âˆ’ D EL (a)) âˆª A DD (a) .
We cannot quite use the set semantics because (1) we must make sure that b does not contain both  and Â¬, and (2) atoms may contain unbound variables. But it is still the case
that R ESULT (b, a) is computed by starting with b, setting any atom that appears in D EL (a)
to false, and setting any atom that appears in A DD (a) to true. For example, if we apply
RemoveLid(Can 1 ) to the initial belief state b0 , we get
b1 = Color (x, C(x)) âˆ§ Open(Can 1 ) .
When we apply the action Paint(Chair , Can 1 ), the precondition Color (Can 1 , c) is satisfied
by the known literal Color (x, C(x)) with binding {x/Can 1 , c/C(Can 1 )} and the new belief
state is
b2 = Color (x, C(x)) âˆ§ Open(Can 1 ) âˆ§ Color (Chair , C(Can 1 )) .
Finally, we apply the action Paint(Table, Can 1 ) to obtain
b3 = Color (x, C(x)) âˆ§ Open(Can 1 ) âˆ§ Color (Chair , C(Can 1 ))
âˆ§ Color (Table, C(Can 1 )) .
The final belief state satisfies the goal, Color (Table, c) âˆ§ Color (Chair , c), with the variable
c bound to C(Can 1 ).
The preceding analysis of the update rule has shown a very important fact: the family
of belief states defined as conjunctions of literals is closed under updates defined by PDDL
action schemas. That is, if the belief state starts as a conjunction of literals, then any update
will yield a conjunction of literals. That means that in a world with n fluents, any belief
state can be represented by a conjunction of size O(n). This is a very comforting result,
considering that there are 2n states in the world. It says we can compactly represent all the
subsets of those 2n states that we will ever need. Moreover, the process of checking for belief

Section 11.3.

CONDITIONAL
EFFECT

Planning and Acting in Nondeterministic Domains

419

states that are subsets or supersets of previously visited belief states is also easy, at least in
the propositional case.
The fly in the ointment of this pleasant picture is that it only works for action schemas
that have the same effects for all states in which their preconditions are satisfied. It is this
property that enables the preservation of the 1-CNF belief-state representation. As soon as the
effect can depend on the state, dependencies are introduced between fluents and the 1-CNF
property is lost. Consider, for example, the simple vacuum world defined in Section 3.2.1.
Let the fluents be AtL and AtR for the location of the robot and CleanL and CleanR for
the state of the squares. According to the definition of the problem, the Suck action has no
preconditionâ€”it can always be done. The difficulty is that its effect depends on the robotâ€™s location: when the robot is AtL, the result is CleanL, but when it is AtR, the result is CleanR.
For such actions, our action schemas will need something new: a conditional effect. These
have the syntax â€œwhen condition: eï¬€ect,â€ where condition is a logical formula to be compared against the current state, and effect is a formula describing the resulting state. For the
vacuum world, we have
Action(Suck ,
E FFECT:when AtL: CleanL âˆ§ when AtR: CleanR) .
When applied to the initial belief state True, the resulting belief state is (AtL âˆ§ CleanL) âˆ¨
(AtR âˆ§ CleanR), which is no longer in 1-CNF. (This transition can be seen in Figure 4.14
on page 141.) In general, conditional effects can induce arbitrary dependencies among the
fluents in a belief state, leading to belief states of exponential size in the worst case.
It is important to understand the difference between preconditions and conditional effects. All conditional effects whose conditions are satisfied have their effects applied to generate the resulting state; if none are satisfied, then the resulting state is unchanged. On the other
hand, if a precondition is unsatisfied, then the action is inapplicable and the resulting state
is undefined. From the point of view of sensorless planning, it is better to have conditional
effects than an inapplicable action. For example, we could split Suck into two actions with
unconditional effects as follows:
Action(SuckL,
P RECOND :AtL; E FFECT:CleanL)
Action(SuckR,
P RECOND :AtR; E FFECT:CleanR) .
Now we have only unconditional schemas, so the belief states all remain in 1-CNF; unfortunately, we cannot determine the applicability of SuckL and SuckR in the initial belief state.
It seems inevitable, then, that nontrivial problems will involve wiggly belief states, just
like those encountered when we considered the problem of state estimation for the wumpus
world (see Figure 7.21 on page 271). The solution suggested then was to use a conservative
approximation to the exact belief state; for example, the belief state can remain in 1-CNF
if it contains all literals whose truth values can be determined and treats all other literals as
unknown. While this approach is sound, in that it never generates an incorrect plan, it is
incomplete because it may be unable to find solutions to problems that necessarily involve
interactions among literals. To give a trivial example, if the goal is for the robot to be on

420

Chapter

11.

Planning and Acting in the Real World

a clean square, then [Suck ] is a solution but a sensorless agent that insists on 1-CNF belief
states will not find it.
Perhaps a better solution is to look for action sequences that keep the belief state
as simple as possible. For example, in the sensorless vacuum world, the action sequence
[Right, Suck , Left, Suck ] generates the following sequence of belief states:
b0 = True
b1 = AtR
b2 = AtR âˆ§ CleanR
b3 = AtL âˆ§ CleanR
b4 = AtL âˆ§ CleanR âˆ§ CleanL
That is, the agent can solve the problem while retaining a 1-CNF belief state, even though
some sequences (e.g., those beginning with Suck) go outside 1-CNF. The general lesson is
not lost on humans: we are always performing little actions (checking the time, patting our
pockets to make sure we have the car keys, reading street signs as we navigate through a city)
to eliminate uncertainty and keep our belief state manageable.
There is another, quite different approach to the problem of unmanageably wiggly belief states: donâ€™t bother computing them at all. Suppose the initial belief state is b0 and we
would like to know the belief state resulting from the action sequence [a1 , . . . , am ]. Instead
of computing it explicitly, just represent it as â€œb0 then [a1 , . . . , am ].â€ This is a lazy but unambiguous representation of the belief state, and itâ€™s quite conciseâ€”O(n + m) where n is
the size of the initial belief state (assumed to be in 1-CNF) and m is the maximum length
of an action sequence. As a belief-state representation, it suffers from one drawback, however: determining whether the goal is satisfied, or an action is applicable, may require a lot
of computation.
The computation can be implemented as an entailment test: if Am represents the collection of successor-state axioms required to define occurrences of the actions a1 , . . . , am â€”as
explained for SATP LAN in Section 10.4.1â€”and Gm asserts that the goal is true after m steps,
then the plan achieves the goal if b0 âˆ§ Am |= Gm , that is, if b0 âˆ§ Am âˆ§ Â¬Gm is unsatisfiable.
Given a modern SAT solver, it may be possible to do this much more quickly than computing
the full belief state. For example, if none of the actions in the sequence has a particular goal
fluent in its add list, the solver will detect this immediately. It also helps if partial results
about the belief stateâ€”for example, fluents known to be true or falseâ€”are cached to simplify
subsequent computations.
The final piece of the sensorless planning puzzle is a heuristic function to guide the
search. The meaning of the heuristic function is the same as for classical planning: an estimate (perhaps admissible) of the cost of achieving the goal from the given belief state. With
belief states, we have one additional fact: solving any subset of a belief state is necessarily
easier than solving the belief state:
if b1 âŠ† b2 then hâˆ— (b1 ) â‰¤ hâˆ— (b2 ) .
Hence, any admissible heuristic computed for a subset is admissible for the belief state itself.
The most obvious candidates are the singleton subsets, that is, individual physical states. We

Section 11.3.

Planning and Acting in Nondeterministic Domains

421

can take any random collection of states s1 , . . . , sN that are in the belief state b, apply any
admissible heuristic h from Chapter 10, and return
H(b) = max{h(s1 ), . . . , h(sN )}
as the heuristic estimate for solving b. We could also use a planning graph directly on b itself:
if it is a conjunction of literals (1-CNF), simply set those literals to be the initial state layer
of the graph. If b is not in 1-CNF, it may be possible to find sets of literals that together entail
b. For example, if b is in disjunctive normal form (DNF), each term of the DNF formula is
a conjunction of literals that entails b and can form the initial layer of a planning graph. As
before, we can take the maximum of the heuristics obtained from each set of literals. We can
also use inadmissible heuristics such as the ignore-delete-lists heuristic (page 377), which
seems to work quite well in practice.

11.3.2 Contingent planning
We saw in Chapter 4 that contingent planningâ€”the generation of plans with conditional
branching based on perceptsâ€”is appropriate for environments with partial observability, nondeterminism, or both. For the partially observable painting problem with the percept axioms
given earlier, one possible contingent solution is as follows:
[LookAt (Table), LookAt (Chair ),
if Color (Table, c) âˆ§ Color (Chair , c) then NoOp
else [RemoveLid (Can 1 ), LookAt (Can 1 ), RemoveLid (Can 2 ), LookAt (Can 2 ),
if Color (Table, c) âˆ§ Color (can, c) then Paint(Chair , can)
else if Color (Chair , c) âˆ§ Color (can, c) then Paint(Table, can)
else [Paint(Chair , Can 1 ), Paint (Table, Can 1 )]]]
Variables in this plan should be considered existentially quantified; the second line says
that if there exists some color c that is the color of the table and the chair, then the agent
need not do anything to achieve the goal. When executing this plan, a contingent-planning
agent can maintain its belief state as a logical formula and evaluate each branch condition
by determining if the belief state entails the condition formula or its negation. (It is up to
the contingent-planning algorithm to make sure that the agent will never end up in a belief state where the condition formulaâ€™s truth value is unknown.) Note that with first-order
conditions, the formula may be satisfied in more than one way; for example, the condition
Color (Table, c) âˆ§ Color (can, c) might be satisfied by {can/Can 1 } and by {can/Can 2 } if
both cans are the same color as the table. In that case, the agent can choose any satisfying
substitution to apply to the rest of the plan.
As shown in Section 4.4.2, calculating the new belief state after an action and subsequent percept is done in two stages. The first stage calculates the belief state after the action,
just as for the sensorless agent:
bÌ‚ = (b âˆ’ D EL (a)) âˆª A DD(a)
where, as before, we have assumed a belief state represented as a conjunction of literals. The
second stage is a little trickier. Suppose that percept literals p1 , . . . , pk are received. One
might think that we simply need to add these into the belief state; in fact, we can also infer

422

Chapter

11.

Planning and Acting in the Real World

that the preconditions for sensing are satisfied. Now, if a percept p has exactly one percept
axiom, Percept(p, P RECOND :c), where c is a conjunction of literals, then those literals can
be thrown into the belief state along with p. On the other hand, if p has more than one percept
axiom whose preconditions might hold according to the predicted belief state bÌ‚, then we have
to add in the disjunction of the preconditions. Obviously, this takes the belief state outside
1-CNF and brings up the same complications as conditional effects, with much the same
classes of solutions.
Given a mechanism for computing exact or approximate belief states, we can generate
contingent plans with an extension of the AND â€“ OR forward search over belief states used
in Section 4.4. Actions with nondeterministic effectsâ€”which are defined simply by using a
disjunction in the E FFECT of the action schemaâ€”can be accommodated with minor changes
to the belief-state update calculation and no change to the search algorithm.2 For the heuristic
function, many of the methods suggested for sensorless planning are also applicable in the
partially observable, nondeterministic case.

11.3.3 Online replanning

EXECUTION
MONITORING

Imagine watching a spot-welding robot in a car plant. The robotâ€™s fast, accurate motions are
repeated over and over again as each car passes down the line. Although technically impressive, the robot probably does not seem at all intelligent because the motion is a fixed,
preprogrammed sequence; the robot obviously doesnâ€™t â€œknow what itâ€™s doingâ€ in any meaningful sense. Now suppose that a poorly attached door falls off the car just as the robot is
about to apply a spot-weld. The robot quickly replaces its welding actuator with a gripper,
picks up the door, checks it for scratches, reattaches it to the car, sends an email to the floor
supervisor, switches back to the welding actuator, and resumes its work. All of a sudden,
the robotâ€™s behavior seems purposive rather than rote; we assume it results not from a vast,
precomputed contingent plan but from an online replanning processâ€”which means that the
robot does need to know what itâ€™s trying to do.
Replanning presupposes some form of execution monitoring to determine the need for
a new plan. One such need arises when a contingent planning agent gets tired of planning
for every little contingency, such as whether the sky might fall on its head.3 Some branches
of a partially constructed contingent plan can simply say Replan; if such a branch is reached
during execution, the agent reverts to planning mode. As we mentioned earlier, the decision
as to how much of the problem to solve in advance and how much to leave to replanning
is one that involves tradeoffs among possible events with different costs and probabilities of
occurring. Nobody wants to have their car break down in the middle of the Sahara desert and
only then think about having enough water.
2 If cyclic solutions are required for a nondeterministic problem, AND â€“ OR search must be generalized to a loopy
version such as LAOâˆ— (Hansen and Zilberstein, 2001).
3 In 1954, a Mrs. Hodges of Alabama was hit by meteorite that crashed through her roof. In 1992, a piece of
the Mbale meteorite hit a small boy on the head; fortunately, its descent was slowed by banana leaves (Jenniskens
et al., 1994). And in 2009, a German boy claimed to have been hit in the hand by a pea-sized meteorite. No serious
injuries resulted from any of these incidents, suggesting that the need for preplanning against such contingencies
is sometimes overstated.

Section 11.3.

Planning and Acting in Nondeterministic Domains

423

whole plan
plan

S

P

E

G
continuation

repair

O
Figure 11.9 Before execution, the planner comes up with a plan, here called whole plan,
to get from S to G. The agent executes steps of the plan until it expects to be in state E, but
observes it is actually in O. The agent then replans for the minimal repair plus continuation
to reach G.

MISSING
PRECONDITION

MISSING EFFECT
MISSING STATE
VARIABLE

EXOGENOUS EVENT

ACTION MONITORING

PLAN MONITORING

GOAL MONITORING

Replanning may also be needed if the agentâ€™s model of the world is incorrect. The model
for an action may have a missing preconditionâ€”for example, the agent may not know that
removing the lid of a paint can often requires a screwdriver; the model may have a missing
effectâ€”for example, painting an object may get paint on the floor as well; or the model may
have a missing state variableâ€”for example, the model given earlier has no notion of the
amount of paint in a can, of how its actions affect this amount, or of the need for the amount
to be nonzero. The model may also lack provision for exogenous events such as someone
knocking over the paint can. Exogenous events can also include changes in the goal, such
as the addition of the requirement that the table and chair not be painted black. Without the
ability to monitor and replan, an agentâ€™s behavior is likely to be extremely fragile if it relies
on absolute correctness of its model.
The online agent has a choice of how carefully to monitor the environment. We distinguish three levels:
â€¢ Action monitoring: before executing an action, the agent verifies that all the preconditions still hold.
â€¢ Plan monitoring: before executing an action, the agent verifies that the remaining plan
will still succeed.
â€¢ Goal monitoring: before executing an action, the agent checks to see if there is a better
set of goals it could be trying to achieve.
In Figure 11.9 we see a schematic of action monitoring. The agent keeps track of both its
original plan, wholeplan , and the part of the plan that has not been executed yet, which is
denoted by plan. After executing the first few steps of the plan, the agent expects to be in
state E. But the agent observes it is actually in state O. It then needs to repair the plan by
finding some point P on the original plan that it can get back to. (It may be that P is the goal
state, G.) The agent tries to minimize the total cost of the plan: the repair part (from O to P )
plus the continuation (from P to G).

424

Chapter

11.

Planning and Acting in the Real World

Now letâ€™s return to the example problem of achieving a chair and table of matching
color. Suppose the agent comes up with this plan:
[LookAt (Table), LookAt (Chair ),
if Color (Table, c) âˆ§ Color (Chair , c) then NoOp
else [RemoveLid (Can 1 ), LookAt (Can 1 ),
if Color (Table, c) âˆ§ Color (Can 1 , c) then Paint(Chair , Can 1 )
else R EPLAN ]] .
Now the agent is ready to execute the plan. Suppose the agent observes that the table and
can of paint are white and the chair is black. It then executes Paint(Chair , Can 1 ). At this
point a classical planner would declare victory; the plan has been executed. But an online
execution monitoring agent needs to check the preconditions of the remaining empty planâ€”
that the table and chair are the same color. Suppose the agent perceives that they do not
have the same colorâ€”in fact, the chair is now a mottled gray because the black paint is
showing through. The agent then needs to figure out a position in whole plan to aim for
and a repair action sequence to get there. The agent notices that the current state is identical
to the precondition before the Paint(Chair , Can 1 ) action, so the agent chooses the empty
sequence for repair and makes its plan be the same [Paint] sequence that it just attempted.
With this new plan in place, execution monitoring resumes, and the Paint action is retried.
This behavior will loop until the chair is perceived to be completely painted. But notice that
the loop is created by a process of planâ€“executeâ€“replan, rather than by an explicit loop in a
plan. Note also that the original plan need not cover every contingency. If the agent reaches
the step marked R EPLAN , it can then generate a new plan (perhaps involving Can 2 ).
Action monitoring is a simple method of execution monitoring, but it can sometimes
lead to less than intelligent behavior. For example, suppose there is no black or white paint,
and the agent constructs a plan to solve the painting problem by painting both the chair and
table red. Suppose that there is only enough red paint for the chair. With action monitoring,
the agent would go ahead and paint the chair red, then notice that it is out of paint and cannot
paint the table, at which point it would replan a repairâ€”perhaps painting both chair and table
green. A plan-monitoring agent can detect failure whenever the current state is such that the
remaining plan no longer works. Thus, it would not waste time painting the chair red. Plan
monitoring achieves this by checking the preconditions for success of the entire remaining
planâ€”that is, the preconditions of each step in the plan, except those preconditions that are
achieved by another step in the remaining plan. Plan monitoring cuts off execution of a
doomed plan as soon as possible, rather than continuing until the failure actually occurs.4
Plan monitoring also allows for serendipityâ€”accidental success. If someone comes along
and paints the table red at the same time that the agent is painting the chair red, then the final
plan preconditions are satisfied (the goal has been achieved), and the agent can go home early.
It is straightforward to modify a planning algorithm so that each action in the plan
is annotated with the actionâ€™s preconditions, thus enabling action monitoring. It is slightly
4 Plan monitoring means that finally, after 424 pages, we have an agent that is smarter than a dung beetle (see
page 39). A plan-monitoring agent would notice that the dung ball was missing from its grasp and would replan
to get another ball and plug its hole.

Section 11.4.

Multiagent Planning

425

more complex to enable plan monitoring. Partial-order and planning-graph planners have
the advantage that they have already built up structures that contain the relations necessary
for plan monitoring. Augmenting state-space planners with the necessary annotations can be
done by careful bookkeeping as the goal fluents are regressed through the plan.
Now that we have described a method for monitoring and replanning, we need to ask,
â€œDoes it work?â€ This is a surprisingly tricky question. If we mean, â€œCan we guarantee that
the agent will always achieve the goal?â€ then the answer is no, because the agent could
inadvertently arrive at a dead end from which there is no repair. For example, the vacuum
agent might have a faulty model of itself and not know that its batteries can run out. Once
they do, it cannot repair any plans. If we rule out dead endsâ€”assume that there exists a plan
to reach the goal from any state in the environmentâ€”and assume that the environment is
really nondeterministic, in the sense that such a plan always has some chance of success on
any given execution attempt, then the agent will eventually reach the goal.
Trouble occurs when an action is actually not nondeterministic, but rather depends on
some precondition that the agent does not know about. For example, sometimes a paint
can may be empty, so painting from that can has no effect. No amount of retrying is going to
change this.5 One solution is to choose randomly from among the set of possible repair plans,
rather than to try the same one each time. In this case, the repair plan of opening another can
might work. A better approach is to learn a better model. Every prediction failure is an
opportunity for learning; an agent should be able to modify its model of the world to accord
with its percepts. From then on, the replanner will be able to come up with a repair that gets
at the root problem, rather than relying on luck to choose a good repair. This kind of learning
is described in Chapters 18 and 19.

11.4

M ULTIAGENT P LANNING

MULTIAGENT
PLANNING PROBLEM

MULTIEFFECTOR
PLANNING

MULTIBODY
PLANNING

So far, we have assumed that only one agent is doing the sensing, planning, and acting.
When there are multiple agents in the environment, each agent faces a multiagent planning
problem in which it tries to achieve its own goals with the help or hindrance of others.
Between the purely single-agent and truly multiagent cases is a wide spectrum of problems that exhibit various degrees of decomposition of the monolithic agent. An agent with
multiple effectors that can operate concurrentlyâ€”for example, a human who can type and
speak at the same timeâ€”needs to do multieffector planning to manage each effector while
handling positive and negative interactions among the effectors. When the effectors are
physically decoupled into detached unitsâ€”as in a fleet of delivery robots in a factoryâ€”
multieffector planning becomes multibody planning. A multibody problem is still a â€œstandardâ€ single-agent problem as long as the relevant sensor information collected by each body
can be pooledâ€”either centrally or within each bodyâ€”to form a common estimate of the
world state that then informs the execution of the overall plan; in this case, the multiple bodies act as a single body. When communication constraints make this impossible, we have
5

Futile repetition of a plan repair is exactly the behavior exhibited by the sphex wasp (page 39).

426
DECENTRALIZED
PLANNING

COORDINATION

INCENTIVE

Chapter

11.

Planning and Acting in the Real World

what is sometimes called a decentralized planning problem; this is perhaps a misnomer, because the planning phase is centralized but the execution phase is at least partially decoupled.
In this case, the subplan constructed for each body may need to include explicit communicative actions with other bodies. For example, multiple reconnaissance robots covering a wide
area may often be out of radio contact with each other and should share their findings during
times when communication is feasible.
When a single entity is doing the planning, there is really only one goal, which all the
bodies necessarily share. When the bodies are distinct agents that do their own planning, they
may still share identical goals; for example, two human tennis players who form a doubles
team share the goal of winning the match. Even with shared goals, however, the multibody
and multiagent cases are quite different. In a multibody robotic doubles team, a single plan
dictates which body will go where on the court and which body will hit the ball. In a multiagent doubles team, on the other hand, each agent decides what to do; without some method
for coordination, both agents may decide to cover the same part of the court and each may
leave the ball for the other to hit.
The clearest case of a multiagent problem, of course, is when the agents have different
goals. In tennis, the goals of two opposing teams are in direct conflict, leading to the zerosum situation of Chapter 5. Spectators could be viewed as agents if their support or disdain
is a significant factor and can be influenced by the playersâ€™ conduct; otherwise, they can be
treated as an aspect of natureâ€”just like the weatherâ€”that is assumed to be indifferent to the
playersâ€™ intentions.6
Finally, some systems are a mixture of centralized and multiagent planning. For example, a delivery company may do centralized, offline planning for the routes of its trucks
and planes each day, but leave some aspects open for autonomous decisions by drivers and
pilots who can respond individually to traffic and weather situations. Also, the goals of the
company and its employees are brought into alignment, to some extent, by the payment of
incentives (salaries and bonuses)â€”a sure sign that this is a true multiagent system.
The issues involved in multiagent planning can be divided roughly into two sets. The
first, covered in Section 11.4.1, involves issues of representing and planning for multiple
simultaneous actions; these issues occur in all settings from multieffector to multiagent planning. The second, covered in Section 11.4.2, involves issues of cooperation, coordination,
and competition arising in true multiagent settings.

11.4.1 Planning with multiple simultaneous actions
MULTIACTOR
ACTOR

For the time being, we will treat the multieffector, multibody, and multiagent settings in the
same way, labeling them generically as multiactor settings, using the generic term actor to
cover effectors, bodies, and agents. The goal of this section is to work out how to define
transition models, correct plans, and efficient planning algorithms for the multiactor setting.
A correct plan is one that, if executed by the actors, achieves the goal. (In the true multiagent
setting, of course, the agents may not agree to execute any particular plan, but at least they
6

We apologize to residents of the United Kingdom, where the mere act of contemplating a game of tennis
guarantees rain.

Section 11.4.

Multiagent Planning

427

Actors(A, B)
Init (At(A, LeftBaseline) âˆ§ At(B, RightNet) âˆ§
Approaching (Ball , RightBaseline)) âˆ§ Partner (A, B) âˆ§ Partner (B, A)
Goal (Returned(Ball ) âˆ§ (At(a, RightNet) âˆ¨ At(a, LeftNet))
Action(Hit (actor , Ball ),
P RECOND :Approaching (Ball , loc) âˆ§ At(actor , loc)
E FFECT:Returned(Ball ))
Action(Go(actor , to),
P RECOND :At(actor , loc) âˆ§ to = loc,
E FFECT:At(actor , to) âˆ§ Â¬ At(actor , loc))
Figure 11.10 The doubles tennis problem. Two actors A and B are playing together and
can be in one of four locations: LeftBaseline, RightBaseline, LeftNet, and RightNet . The
ball can be returned only if a player is in the right place. Note that each action must include
the actor as an argument.

SYNCHRONIZATION

JOINT ACTION

LOOSELY COUPLED

will know what plans would work if they did agree to execute them.) For simplicity, we
assume perfect synchronization: each action takes the same amount of time and actions at
each point in the joint plan are simultaneous.
We begin with the transition model; for the deterministic case, this is the function
R ESULT (s, a). In the single-agent setting, there might be b different choices for the action;
b can be quite large, especially for first-order representations with many objects to act on,
but action schemas provide a concise representation nonetheless. In the multiactor setting
with n actors, the single action a is replaced by a joint action a1 , . . . , an , where ai is the
action taken by the ith actor. Immediately, we see two problems: first, we have to describe
the transition model for bn different joint actions; second, we have a joint planning problem
with a branching factor of bn .
Having put the actors together into a multiactor system with a huge branching factor,
the principal focus of research on multiactor planning has been to decouple the actors to
the extent possible, so that the complexity of the problem grows linearly with n rather than
exponentially. If the actors have no interaction with one anotherâ€”for example, n actors each
playing a game of solitaireâ€”then we can simply solve n separate problems. If the actors are
loosely coupled, can we attain something close to this exponential improvement? This is, of
course, a central question in many areas of AI. We have seen it explicitly in the context of
CSPs, where â€œtree likeâ€ constraint graphs yielded efficient solution methods (see page 225),
as well as in the context of disjoint pattern databases (page 106) and additive heuristics for
planning (page 378).
The standard approach to loosely coupled problems is to pretend the problems are completely decoupled and then fix up the interactions. For the transition model, this means writing
action schemas as if the actors acted independently. Letâ€™s see how this works for the doubles
tennis problem. Letâ€™s suppose that at one point in the game, the team has the goal of returning
the ball that has been hit to them and ensuring that at least one of them is covering the net.

428

JOINT PLAN

CONCURRENT
ACTION LIST

Chapter

11.

Planning and Acting in the Real World

A first pass at a multiactor definition might look like Figure 11.10. With this definition, it is
easy to see that the following joint plan plan works:
P LAN 1:
A : [Go(A, RightBaseline), Hit(A, Ball )]
B : [NoOp(B), NoOp(B)] .
Problems arise, however, when a plan has both agents hitting the ball at the same time. In the
real world, this wonâ€™t work, but the action schema for Hit says that the ball will be returned
successfully. Technically, the difficulty is that preconditions constrain the state in which an
action can be executed successfully, but do not constrain other actions that might mess it up.
We solve this by augmenting action schemas with one new feature: a concurrent action list
stating which actions must or must not be executed concurrently. For example, the Hit action
could be described as follows:
Action(Hit(a, Ball ),
C ONCURRENT:b = a â‡’ Â¬Hit(b, Ball )
P RECOND :Approaching (Ball , loc) âˆ§ At(a, loc)
E FFECT:Returned (Ball )) .
In other words, the Hit action has its stated effect only if no other Hit action by another
agent occurs at the same time. (In the SATP LAN approach, this would be handled by a
partial action exclusion axiom.) For some actions, the desired effect is achieved only when
another action occurs concurrently. For example, two agents are needed to carry a cooler full
of beverages to the tennis court:
Action(Carry(a, cooler , here, there),
C ONCURRENT:b = a âˆ§ Carry (b, cooler , here, there)
P RECOND :At(a, here ) âˆ§ At(cooler , here) âˆ§ Cooler (cooler )
E FFECT:At(a, there) âˆ§ At(cooler , there) âˆ§ Â¬At(a, here) âˆ§ Â¬At(cooler , here)).
With these kinds of action schemas, any of the planning algorithms described in Chapter 10
can be adapted with only minor modifications to generate multiactor plans. To the extent that
the coupling among subplans is looseâ€”meaning that concurrency constraints come into play
only rarely during plan searchâ€”one would expect the various heuristics derived for singleagent planning to also be effective in the multiactor context. We could extend this approach
with the refinements of the last two chaptersâ€”HTNs, partial observability, conditionals, execution monitoring, and replanningâ€”but that is beyond the scope of this book.

11.4.2 Planning with multiple agents: Cooperation and coordination
Now let us consider the true multiagent setting in which each agent makes its own plan. To
start with, let us assume that the goals and knowledge base are shared. One might think
that this reduces to the multibody caseâ€”each agent simply computes the joint solution and
executes its own part of that solution. Alas, the â€œtheâ€ in â€œthe joint solutionâ€ is misleading.
For our doubles team, more than one joint solution exists:
P LAN 2:
A : [Go(A, LeftNet ), NoOp(A)]
B : [Go(B, RightBaseline), Hit(B, Ball )] .

Section 11.4.

CONVENTION

SOCIAL LAWS

PLAN RECOGNITION

BOID

Multiagent Planning

429

If both agents can agree on either plan 1 or plan 2, the goal will be achieved. But if A chooses
plan 2 and B chooses plan 1, then nobody will return the ball. Conversely, if A chooses 1 and
B chooses 2, then they will both try to hit the ball. The agents may realize this, but how can
they coordinate to make sure they agree on the plan?
One option is to adopt a convention before engaging in joint activity. A convention is
any constraint on the selection of joint plans. For example, the convention â€œstick to your side
of the courtâ€ would rule out plan 1, causing the doubles partners to select plan 2. Drivers on
a road face the problem of not colliding with each other; this is (partially) solved by adopting
the convention â€œstay on the right side of the roadâ€ in most countries; the alternative, â€œstay
on the left side,â€ works equally well as long as all agents in an environment agree. Similar
considerations apply to the development of human language, where the important thing is not
which language each individual should speak, but the fact that a community all speaks the
same language. When conventions are widespread, they are called social laws.
In the absence of a convention, agents can use communication to achieve common
knowledge of a feasible joint plan. For example, a tennis player could shout â€œMine!â€ or
â€œYours!â€ to indicate a preferred joint plan. We cover mechanisms for communication in more
depth in Chapter 22, where we observe that communication does not necessarily involve a
verbal exchange. For example, one player can communicate a preferred joint plan to the other
simply by executing the first part of it. If agent A heads for the net, then agent B is obliged
to go back to the baseline to hit the ball, because plan 2 is the only joint plan that begins with
Aâ€™s heading for the net. This approach to coordination, sometimes called plan recognition,
works when a single action (or short sequence of actions) is enough to determine a joint plan
unambiguously. Note that communication can work as well with competitive agents as with
cooperative ones.
Conventions can also arise through evolutionary processes. For example, seed-eating
harvester ants are social creatures that evolved from the less social wasps. Colonies of ants
execute very elaborate joint plans without any centralized controlâ€”the queenâ€™s job is to reproduce, not to do centralized planningâ€”and with very limited computation, communication, and memory capabilities in each ant (Gordon, 2000, 2007). The colony has many roles,
including interior workers, patrollers, and foragers. Each ant chooses to perform a role according to the local conditions it observes. For example, foragers travel away from the nest,
search for a seed, and when they find one, bring it back immediately. Thus, the rate at which
foragers return to the nest is an approximation of the availability of food today. When the
rate is high, other ants abandon their current role and take on the role of scavenger. The ants
appear to have a convention on the importance of rolesâ€”foraging is the most importantâ€”and
ants will easily switch into the more important roles, but not into the less important. There is
some learning mechanism: a colony learns to make more successful and prudent actions over
the course of its decades-long life, even though individual ants live only about a year.
One final example of cooperative multiagent behavior appears in the flocking behavior
of birds. We can obtain a reasonable simulation of a flock if each bird agent (sometimes
called a boid) observes the positions of its nearest neighbors and then chooses the heading
and acceleration that maximizes the weighted sum of these three components:

430

Chapter

(a)

11.

(b)

Planning and Acting in the Real World

(c)

Figure 11.11 (a) A simulated flock of birds, using Reynoldâ€™s boids model. Image courtesy
Giuseppe Randazzo, novastructura.net. (b) An actual flock of starlings. Image by Eduardo
(pastaboy sleeps on flickr). (c) Two competitive teams of agents attempting to capture the
towers in the N ERO game. Image courtesy Risto Miikkulainen.

1. Cohesion: a positive score for getting closer to the average position of the neighbors
2. Separation: a negative score for getting too close to any one neighbor
3. Alignment: a positive score for getting closer to the average heading of the neighbors
EMERGENT
BEHAVIOR

11.5

If all the boids execute this policy, the flock exhibits the emergent behavior of flying as a
pseudorigid body with roughly constant density that does not disperse over time, and that
occasionally makes sudden swooping motions. You can see a still images in Figure 11.11(a)
and compare it to an actual flock in (b). As with ants, there is no need for each agent to
possess a joint plan that models the actions of other agents.
The most difficult multiagent problems involve both cooperation with members of oneâ€™s
own team and competition against members of opposing teams, all without centralized control. We see this in games such as robotic soccer or the N ERO game shown in Figure 11.11(c),
in which two teams of software agents compete to capture the control towers. As yet, methods for efficient planning in these kinds of environmentsâ€”for example, taking advantage of
loose couplingâ€”are in their infancy.

S UMMARY
This chapter has addressed some of the complications of planning and acting in the real world.
The main points:
â€¢ Many actions consume resources, such as money, gas, or raw materials. It is convenient
to treat these resources as numeric measures in a pool rather than try to reason about,
say, each individual coin and bill in the world. Actions can generate and consume
resources, and it is usually cheap and effective to check partial plans for satisfaction of
resource constraints before attempting further refinements.
â€¢ Time is one of the most important resources. It can be handled by specialized scheduling algorithms, or scheduling can be integrated with planning.

Bibliographical and Historical Notes

431

â€¢ Hierarchical task network (HTN) planning allows the agent to take advice from the
domain designer in the form of high-level actions (HLAs) that can be implemented in
various ways by lower-level action sequences. The effects of HLAs can be defined with
angelic semantics, allowing provably correct high-level plans to be derived without
consideration of lower-level implementations. HTN methods can create the very large
plans required by many real-world applications.
â€¢ Standard planning algorithms assume complete and correct information and deterministic, fully observable environments. Many domains violate this assumption.
â€¢ Contingent plans allow the agent to sense the world during execution to decide what
branch of the plan to follow. In some cases, sensorless or conformant planning can be
used to construct a plan that works without the need for perception. Both conformant
and contingent plans can be constructed by search in the space of belief states. Efficient
representation or computation of belief states is a key problem.
â€¢ An online planning agent uses execution monitoring and splices in repairs as needed
to recover from unexpected situations, which can be due to nondeterministic actions,
exogenous events, or incorrect models of the environment.
â€¢ Multiagent planning is necessary when there are other agents in the environment with
which to cooperate or compete. Joint plans can be constructed, but must be augmented
with some form of coordination if two agents are to agree on which joint plan to execute.
â€¢ This chapter extends classic planning to cover nondeterministic environments (where
outcomes of actions are uncertain), but it is not the last word on planning. Chapter 17
describes techniques for stochastic environments (in which outcomes of actions have
probabilities associated with them): Markov decision processes, partially observable
Markov decision processes, and game theory. In Chapter 21 we show that reinforcement
learning allows an agent to learn how to behave from past successes and failures.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Planning with time constraints was first dealt with by D EVISER (Vere, 1983). The representation of time in plans was addressed by Allen (1984) and by Dean et al. (1990) in the
F ORBIN system. N ONLIN + (Tate and Whiter, 1984) and S IPE (Wilkins, 1988, 1990) could
reason about the allocation of limited resources to various plan steps. O-P LAN (Bell and
Tate, 1985), an HTN planner, had a uniform, general representation for constraints on time
and resources. In addition to the Hitachi application mentioned in the text, O-P LAN has
been applied to software procurement planning at Price Waterhouse and back-axle assembly
planning at Jaguar Cars.
The two planners S APA (Do and Kambhampati, 2001) and T4 (Haslum and Geffner,
2001) both used forward state-space search with sophisticated heuristics to handle actions
with durations and resources. An alternative is to use very expressive action languages, but
guide them by human-written domain-specific heuristics, as is done by ASPEN (Fukunaga
et al., 1997), HSTS (Jonsson et al., 2000), and IxTeT (Ghallab and Laruelle, 1994).

432

Chapter

11.

Planning and Acting in the Real World

A number of hybrid planning-and-scheduling systems have been deployed: I SIS (Fox
et al., 1982; Fox, 1990) has been used for job shop scheduling at Westinghouse, G ARI (Descotte and Latombe, 1985) planned the machining and construction of mechanical parts,
F ORBIN was used for factory control, and N ONLIN + was used for naval logistics planning.
We chose to present planning and scheduling as two separate problems; (Cushing et al., 2007)
show that this can lead to incompleteness on certain problems. There is a long history of
scheduling in aerospace. T-S CHED (Drabble, 1990) was used to schedule mission-command
sequences for the U OSAT-II satellite. O PTIMUM -AIV (Aarup et al., 1994) and P LAN -ERS1
(Fuchs et al., 1990), both based on O-P LAN , were used for spacecraft assembly and observation planning, respectively, at the European Space Agency. S PIKE (Johnston and Adorf,
1992) was used for observation planning at NASA for the Hubble Space Telescope, while
the Space Shuttle Ground Processing Scheduling System (Deale et al., 1994) does job-shop
scheduling of up to 16,000 worker-shifts. Remote Agent (Muscettola et al., 1998) became
the first autonomous plannerâ€“scheduler to control a spacecraft when it flew onboard the Deep
Space One probe in 1999. Space applications have driven the development of algorithms for
resource allocations; see Laborie (2003) and Muscettola (2002). The literature on scheduling
is presented in a classic survey article (Lawler et al., 1993), a recent book (Pinedo, 2008),
and an edited handbook (Blazewicz et al., 2007).
MACROPS

ABSTRACTION
HIERARCHY

CASE-BASED
PLANNING

The facility in the S TRIPS program for learning macropsâ€”â€œmacro-operatorsâ€ consisting of a sequence of primitive stepsâ€”could be considered the first mechanism for hierarchical planning (Fikes et al., 1972). Hierarchy was also used in the L AWALY system (Siklossy
and Dreussi, 1973). The A BSTRIPS system (Sacerdoti, 1974) introduced the idea of an abstraction hierarchy, whereby planning at higher levels was permitted to ignore lower-level
preconditions of actions in order to derive the general structure of a working plan. Austin
Tateâ€™s Ph.D. thesis (1975b) and work by Earl Sacerdoti (1977) developed the basic ideas of
HTN planning in its modern form. Many practical planners, including O-P LAN and S IPE ,
are HTN planners. Yang (1990) discusses properties of actions that make HTN planning efficient. Erol, Hendler, and Nau (1994, 1996) present a complete hierarchical decomposition
planner as well as a range of complexity results for pure HTN planners. Our presentation of
HLAs and angelic semantics is due to Marthi et al. (2007, 2008). Kambhampati et al. (1998)
have proposed an approach in which decompositions are just another form of plan refinement,
similar to the refinements for non-hierarchical partial-order planning.
Beginning with the work on macro-operators in S TRIPS , one of the goals of hierarchical
planning has been the reuse of previous planning experience in the form of generalized plans.
The technique of explanation-based learning, described in depth in Chapter 19, has been
applied in several systems as a means of generalizing previously computed plans, including
S OAR (Laird et al., 1986) and P RODIGY (Carbonell et al., 1989). An alternative approach is
to store previously computed plans in their original form and then reuse them to solve new,
similar problems by analogy to the original problem. This is the approach taken by the field
called case-based planning (Carbonell, 1983; Alterman, 1988; Hammond, 1989). Kambhampati (1994) argues that case-based planning should be analyzed as a form of refinement
planning and provides a formal foundation for case-based partial-order planning.

Bibliographical and Historical Notes

433

Early planners lacked conditionals and loops, but some could use coercion to form
conformant plans. Sacerdotiâ€™s N OAH solved the â€œkeys and boxesâ€ problem, a planning challenge problem in which the planner knows little about the initial state, using coercion. Mason (1993) argued that sensing often can and should be dispensed with in robotic planning,
and described a sensorless plan that can move a tool into a specific position on a table by a
sequence of tilting actions, regardless of the initial position.
Goldman and Boddy (1996) introduced the term conformant planning, noting that sensorless plans are often effective even if the agent has sensors. The first moderately efficient
conformant planner was Smith and Weldâ€™s (1998) Conformant Graphplan or CGP. Ferraris
and Giunchiglia (2000) and Rintanen (1999) independently developed SATP LAN -based conformant planners. Bonet and Geffner (2000) describe a conformant planner based on heuristic
search in the space of belief states, drawing on ideas first developed in the 1960s for partially
observable Markov decision processes, or POMDPs (see Chapter 17).
Currently, there are three main approaches to conformant planning. The first two use
heuristic search in belief-state space: HSCP (Bertoli et al., 2001a) uses binary decision
diagrams (BDDs) to represent belief states, whereas Hoffmann and Brafman (2006) adopt
the lazy approach of computing precondition and goal tests on demand using a SAT solver.
The third approach, championed primarily by Jussi Rintanen (2007), formulates the entire
sensorless planning problem as a quantified Boolean formula (QBF) and solves it using a
general-purpose QBF solver. Current conformant planners are five orders of magnitude faster
than CGP. The winner of the 2006 conformant-planning track at the International Planning
Competition was T0 (Palacios and Geffner, 2007), which uses heuristic search in belief-state
space while keeping the belief-state representation simple by defining derived literals that
cover conditional effects. Bryce and Kambhampati (2007) discuss how a planning graph can
be generalized to generate good heuristics for conformant and contingent planning.
There has been some confusion in the literature between the terms â€œconditionalâ€ and
â€œcontingentâ€ planning. Following Majercik and Littman (2003), we use â€œconditionalâ€ to
mean a plan (or action) that has different effects depending on the actual state of the world,
and â€œcontingentâ€ to mean a plan in which the agent can choose different actions depending
on the results of sensing. The problem of contingent planning received more attention after
the publication of Drew McDermottâ€™s (1978a) influential article, Planning and Acting.
The contingent-planning approach described in the chapter is based on Hoffmann and
Brafman (2005), and was influenced by the efficient search algorithms for cyclic AND â€“ OR
graphs developed by Jimenez and Torras (2000) and Hansen and Zilberstein (2001). Bertoli
et al. (2001b) describe MBP (Model-Based Planner), which uses binary decision diagrams
to do conformant and contingent planning.
In retrospect, it is now possible to see how the major classical planning algorithms led
to extended versions for uncertain domains. Fast-forward heuristic search through state space
led to forward search in belief space (Bonet and Geffner, 2000; Hoffmann and Brafman,
2005); SATP LAN led to stochastic SATP LAN (Majercik and Littman, 2003) and to planning
with quantified Boolean logic (Rintanen, 2007); partial order planning led to UWL (Etzioni
et al., 1992) and CNLP (Peot and Smith, 1992); G RAPHPLAN led to Sensory Graphplan or
SGP (Weld et al., 1998).

434

REACTIVE PLANNING

POLICY

Chapter

11.

Planning and Acting in the Real World

The first online planner with execution monitoring was P LANEX (Fikes et al., 1972),
which worked with the S TRIPS planner to control the robot Shakey. The N ASL planner
(McDermott, 1978a) treated a planning problem simply as a specification for carrying out a
complex action, so that execution and planning were completely unified. S IPE (System for
Interactive Planning and Execution monitoring) (Wilkins, 1988, 1990) was the first planner
to deal systematically with the problem of replanning. It has been used in demonstration
projects in several domains, including planning operations on the flight deck of an aircraft
carrier, job-shop scheduling for an Australian beer factory, and planning the construction of
multistory buildings (Kartam and Levitt, 1990).
In the mid-1980s, pessimism about the slow run times of planning systems led to the
proposal of reflex agents called reactive planning systems (Brooks, 1986; Agre and Chapman, 1987). P ENGI (Agre and Chapman, 1987) could play a (fully observable) video game
by using Boolean circuits combined with a â€œvisualâ€ representation of current goals and the
agentâ€™s internal state. â€œUniversal plansâ€ (Schoppers, 1987, 1989) were developed as a lookuptable method for reactive planning, but turned out to be a rediscovery of the idea of policies
that had long been used in Markov decision processes (see Chapter 17). A universal plan (or
a policy) contains a mapping from any state to the action that should be taken in that state.
Koenig (2001) surveys online planning techniques, under the name Agent-Centered Search.
Multiagent planning has leaped in popularity in recent years, although it does have
a long history. Konolige (1982) formalizes multiagent planning in first-order logic, while
Pednault (1986) gives a S TRIPS -style description. The notion of joint intention, which is essential if agents are to execute a joint plan, comes from work on communicative acts (Cohen
and Levesque, 1990; Cohen et al., 1990). Boutilier and Brafman (2001) show how to adapt
partial-order planning to a multiactor setting. Brafman and Domshlak (2008) devise a multiactor planning algorithm whose complexity grows only linearly with the number of actors,
provided that the degree of coupling (measured partly by the tree width of the graph of interactions among agents) is bounded. Petrik and Zilberstein (2009) show that an approach based
on bilinear programming outperforms the cover-set approach we outlined in the chapter.
We have barely skimmed the surface of work on negotiation in multiagent planning.
Durfee and Lesser (1989) discuss how tasks can be shared out among agents by negotiation.
Kraus et al. (1991) describe a system for playing Diplomacy, a board game requiring negotiation, coalition formation, and dishonesty. Stone (2000) shows how agents can cooperate as
teammates in the competitive, dynamic, partially observable environment of robotic soccer. In
a later article, Stone (2003) analyzes two competitive multiagent environmentsâ€”RoboCup,
a robotic soccer competition, and TAC, the auction-based Trading Agents Competitionâ€”
and finds that the computational intractability of our current theoretically well-founded approaches has led to many multiagent systems being designed by ad hoc methods.
In his highly influential Society of Mind theory, Marvin Minsky (1986, 2007) proposes
that human minds are constructed from an ensemble of agents. Livnat and Pippenger (2006)
prove that, for the problem of optimal path-finding, and given a limitation on the total amount
of computing resources, the best architecture for an agent is an ensemble of subagents, each
of which tries to optimize its own objective, and all of which are in conflict with one another.

Exercises

435
The boid model on page 429 is due to Reynolds (1987), who won an Academy Award
for its application to swarms of penguins in Batman Returns. The N ERO game and the methods for learning strategies are described by Bryant and Miikkulainen (2007).
Recent book on multiagent systems include those by Weiss (2000a), Young (2004),
Vlassis (2008), and Shoham and Leyton-Brown (2009). There is an annual conference on
autonomous agents and multiagent systems (AAMAS).

E XERCISES
11.1 The goals we have considered so far all ask the planner to make the world satisfy the
goal at just one time step. Not all goals can be expressed this way: you do not achieve the
goal of suspending a chandelier above the ground by throwing it in the air. More seriously,
you wouldnâ€™t want your spacecraft life-support system to supply oxygen one day but not
the next. A maintenance goal is achieved when the agentâ€™s plan causes a condition to hold
continuously from a given state onward. Describe how to extend the formalism of this chapter
to support maintenance goals.
11.2 You have a number of trucks with which to deliver a set of packages. Each package
starts at some location on a grid map, and has a destination somewhere else. Each truck is directly controlled by moving forward and turning. Construct a hierarchy of high-level actions
for this problem. What knowledge about the solution does your hierarchy encode?
11.3 Suppose that a high-level action has exactly one implementation as a sequence of
primitive actions. Give an algorithm for computing its preconditions and effects, given the
complete refinement hierarchy and schemas for the primitive actions.
11.4 Suppose that the optimistic reachable set of a high-level plan is a superset of the goal
set; can anything be concluded about whether the plan achieves the goal? What if the pessimistic reachable set doesnâ€™t intersect the goal set? Explain.
11.5 Write an algorithm that takes an initial state (specified by a set of propositional literals)
and a sequence of HLAs (each defined by preconditions and angelic specifications of optimistic and pessimistic reachable sets) and computes optimistic and pessimistic descriptions
of the reachable set of the sequence.
11.6 In Figure 11.2 we showed how to describe actions in a scheduling problem by using
separate fields for D URATION , U SE , and C ONSUME . Now suppose we wanted to combine
scheduling with nondeterministic planning, which requires nondeterministic and conditional
effects. Consider each of the three fields and explain if they should remain separate fields, or
if they should become effects of the action. Give an example for each of the three.
11.7 Some of the operations in standard programming languages can be modeled as actions
that change the state of the world. For example, the assignment operation changes the contents of a memory location, and the print operation changes the state of the output stream. A
program consisting of these operations can also be considered as a plan, whose goal is given

436

Chapter

11.

Planning and Acting in the Real World

by the specification of the program. Therefore, planning algorithms can be used to construct
programs that achieve a given specification.
a. Write an action schema for the assignment operator (assigning the value of one variable
to another). Remember that the original value will be overwritten!
b. Show how object creation can be used by a planner to produce a plan for exchanging
the values of two variables by using a temporary variable.
11.8 Suppose the Flip action always changes the truth value of variable L. Show how
to define its effects by using an action schema with conditional effects. Show that, despite
the use of conditional effects, a 1-CNF belief state representation remains in 1-CNF after a
Flip.
11.9 In the blocks world we were forced to introduce two action schemas, Move and
MoveToTable, in order to maintain the Clear predicate properly. Show how conditional
effects can be used to represent both of these cases with a single action.
11.10 Conditional effects were illustrated for the Suck action in the vacuum worldâ€”which
square becomes clean depends on which square the robot is in. Can you think of a new set of
propositional variables to define states of the vacuum world, such that Suck has an unconditional description? Write out the descriptions of Suck , Left, and Right, using your propositions, and demonstrate that they suffice to describe all possible states of the world.
11.11 Find a suitably dirty carpet, free of obstacles, and vacuum it. Draw the path taken
by the vacuum cleaner as accurately as you can. Explain it, with reference to the forms of
planning discussed in this chapter.
11.12 To the medication problem in the previous exercise, add a Test action that has the
conditional effect CultureGrowth when Disease is true and in any case has the perceptual
effect Known(CultureGrowth ). Diagram a conditional plan that solves the problem and
minimizes the use of the Medicate action.

12

KNOWLEDGE
REPRESENTATION

In which we show how to use first-order logic to represent the most important
aspects of the real world, such as action, space, time, thoughts, and shopping.

The previous chapters described the technology for knowledge-based agents: the syntax,
semantics, and proof theory of propositional and first-order logic, and the implementation of
agents that use these logics. In this chapter we address the question of what content to put
into such an agentâ€™s knowledge baseâ€”how to represent facts about the world.
Section 12.1 introduces the idea of a general ontology, which organizes everything in
the world into a hierarchy of categories. Section 12.2 covers the basic categories of objects,
substances, and measures; Section 12.3 covers events, and Section 12.4 discusses knowledge
about beliefs. We then return to consider the technology for reasoning with this content:
Section 12.5 discusses reasoning systems designed for efficient inference with categories,
and Section 12.6 discusses reasoning with default information. Section 12.7 brings all the
knowledge together in the context of an Internet shopping environment.

12.1

ONTOLOGICAL
ENGINEERING

O NTOLOGICAL E NGINEERING
In â€œtoyâ€ domains, the choice of representation is not that important; many choices will work.
Complex domains such as shopping on the Internet or driving a car in traffic require more
general and flexible representations. This chapter shows how to create these representations,
concentrating on general conceptsâ€”such as Events, Time, Physical Objects, and Beliefsâ€”
that occur in many different domains. Representing these abstract concepts is sometimes
called ontological engineering.
The prospect of representing everything in the world is daunting. Of course, we wonâ€™t
actually write a complete description of everythingâ€”that would be far too much for even a
1000-page textbookâ€”but we will leave placeholders where new knowledge for any domain
can fit in. For example, we will define what it means to be a physical object, and the details of
different types of objectsâ€”robots, televisions, books, or whateverâ€”can be filled in later. This
is analogous to the way that designers of an object-oriented programming framework (such as
the Java Swing graphical framework) define general concepts like Window, expecting users to
437

438

Chapter

12.

Knowledge Representation

Anything

GeneralizedEvents

AbstractObjects
Sets Numbers RepresentationalObjects
Categories

Sentences Measurements

Times

Weights

Interval
Moments

Places

PhysicalObjects Processes

Things

Animals Agents

Stuff

Solid Liquid Gas

Humans
Figure 12.1 The upper ontology of the world, showing the topics to be covered later in
the chapter. Each link indicates that the lower concept is a specialization of the upper one.
Specializations are not necessarily disjoint; a human is both an animal and an agent, for
example. We will see in Section 12.3.3 why physical objects come under generalized events.

UPPER ONTOLOGY

use these to define more specific concepts like SpreadsheetWindow. The general framework
of concepts is called an upper ontology because of the convention of drawing graphs with
the general concepts at the top and the more specific concepts below them, as in Figure 12.1.
Before considering the ontology further, we should state one important caveat. We
have elected to use first-order logic to discuss the content and organization of knowledge,
although certain aspects of the real world are hard to capture in FOL. The principal difficulty
is that most generalizations have exceptions or hold only to a degree. For example, although
â€œtomatoes are redâ€ is a useful rule, some tomatoes are green, yellow, or orange. Similar
exceptions can be found to almost all the rules in this chapter. The ability to handle exceptions
and uncertainty is extremely important, but is orthogonal to the task of understanding the
general ontology. For this reason, we delay the discussion of exceptions until Section 12.5 of
this chapter, and the more general topic of reasoning with uncertainty until Chapter 13.
Of what use is an upper ontology? Consider the ontology for circuits in Section 8.4.2.
It makes many simplifying assumptions: time is omitted completely; signals are fixed and do
not propagate; the structure of the circuit remains constant. A more general ontology would
consider signals at particular times, and would include the wire lengths and propagation delays. This would allow us to simulate the timing properties of the circuit, and indeed such
simulations are often carried out by circuit designers. We could also introduce more interesting classes of gates, for example, by describing the technology (TTL, CMOS, and so on)
as well as the inputâ€“output specification. If we wanted to discuss reliability or diagnosis, we
would include the possibility that the structure of the circuit or the properties of the gates
might change spontaneously. To account for stray capacitances, we would need to represent
where the wires are on the board.

Section 12.1.

Ontological Engineering

439

If we look at the wumpus world, similar considerations apply. Although we do represent
time, it has a simple structure: Nothing happens except when the agent acts, and all changes
are instantaneous. A more general ontology, better suited for the real world, would allow for
simultaneous changes extended over time. We also used a Pit predicate to say which squares
have pits. We could have allowed for different kinds of pits by having several individuals
belonging to the class of pits, each having different properties. Similarly, we might want to
allow for other animals besides wumpuses. It might not be possible to pin down the exact
species from the available percepts, so we would need to build up a biological taxonomy to
help the agent predict the behavior of cave-dwellers from scanty clues.
For any special-purpose ontology, it is possible to make changes like these to move
toward greater generality. An obvious question then arises: do all these ontologies converge
on a general-purpose ontology? After centuries of philosophical and computational investigation, the answer is â€œMaybe.â€ In this section, we present one general-purpose ontology
that synthesizes ideas from those centuries. Two major characteristics of general-purpose
ontologies distinguish them from collections of special-purpose ontologies:
â€¢ A general-purpose ontology should be applicable in more or less any special-purpose
domain (with the addition of domain-specific axioms). This means that no representational issue can be finessed or brushed under the carpet.
â€¢ In any sufficiently demanding domain, different areas of knowledge must be unified,
because reasoning and problem solving could involve several areas simultaneously. A
robot circuit-repair system, for instance, needs to reason about circuits in terms of electrical connectivity and physical layout, and about time, both for circuit timing analysis
and estimating labor costs. The sentences describing time therefore must be capable
of being combined with those describing spatial layout and must work equally well for
nanoseconds and minutes and for angstroms and meters.
We should say up front that the enterprise of general ontological engineering has so far had
only limited success. None of the top AI applications (as listed in Chapter 1) make use
of a shared ontologyâ€”they all use special-purpose knowledge engineering. Social/political
considerations can make it difficult for competing parties to agree on an ontology. As Tom
Gruber (2004) says, â€œEvery ontology is a treatyâ€”a social agreementâ€”among people with
some common motive in sharing.â€ When competing concerns outweigh the motivation for
sharing, there can be no common ontology. Those ontologies that do exist have been created
along four routes:
1. By a team of trained ontologist/logicians, who architect the ontology and write axioms.
The CYC system was mostly built this way (Lenat and Guha, 1990).
2. By importing categories, attributes, and values from an existing database or databases.
DB PEDIA was built by importing structured facts from Wikipedia (Bizer et al., 2007).
3. By parsing text documents and extracting information from them. T EXT RUNNER was
built by reading a large corpus of Web pages (Banko and Etzioni, 2008).
4. By enticing unskilled amateurs to enter commonsense knowledge. The O PEN M IND
system was built by volunteers who proposed facts in English (Singh et al., 2002;
Chklovski and Gil, 2005).

440

12.2
CATEGORY

REIFICATION

SUBCATEGORY
INHERITANCE

TAXONOMY

Chapter

12.

Knowledge Representation

C ATEGORIES AND O BJECTS
The organization of objects into categories is a vital part of knowledge representation. Although interaction with the world takes place at the level of individual objects, much reasoning takes place at the level of categories. For example, a shopper would normally have the
goal of buying a basketball, rather than a particular basketball such as BB 9 . Categories also
serve to make predictions about objects once they are classified. One infers the presence of
certain objects from perceptual input, infers category membership from the perceived properties of the objects, and then uses category information to make predictions about the objects.
For example, from its green and yellow mottled skin, one-foot diameter, ovoid shape, red
flesh, black seeds, and presence in the fruit aisle, one can infer that an object is a watermelon;
from this, one infers that it would be useful for fruit salad.
There are two choices for representing categories in first-order logic: predicates and
objects. That is, we can use the predicate Basketball (b), or we can reify1 the category as
an object, Basketballs. We could then say Member (b, Basketballs ), which we will abbreviate as b âˆˆ Basketballs, to say that b is a member of the category of basketballs. We say
Subset(Basketballs, Balls), abbreviated as Basketballs âŠ‚ Balls, to say that Basketballs is
a subcategory of Balls. We will use subcategory, subclass, and subset interchangeably.
Categories serve to organize and simplify the knowledge base through inheritance. If
we say that all instances of the category Food are edible, and if we assert that Fruit is a
subclass of Food and Apples is a subclass of Fruit, then we can infer that every apple is
edible. We say that the individual apples inherit the property of edibility, in this case from
their membership in the Food category.
Subclass relations organize categories into a taxonomy, or taxonomic hierarchy. Taxonomies have been used explicitly for centuries in technical fields. The largest such taxonomy
organizes about 10 million living and extinct species, many of them beetles,2 into a single hierarchy; library science has developed a taxonomy of all fields of knowledge, encoded as the
Dewey Decimal system; and tax authorities and other government departments have developed extensive taxonomies of occupations and commercial products. Taxonomies are also an
important aspect of general commonsense knowledge.
First-order logic makes it easy to state facts about categories, either by relating objects to categories or by quantifying over their members. Here are some types of facts, with
examples of each:
â€¢ An object is a member of a category.
BB 9 âˆˆ Basketballs
â€¢ A category is a subclass of another category.
Basketballs âŠ‚ Balls
â€¢ All members of a category have some properties.
(x âˆˆ Basketballs) â‡’ Spherical (x)
1 Turning a proposition into an object is called reification, from the Latin word res, or thing. John McCarthy
proposed the term â€œthingification,â€ but it never caught on.
2 The famous biologist J. B. S. Haldane deduced â€œAn inordinate fondness for beetlesâ€ on the part of the Creator.

Section 12.2.

Categories and Objects

441

â€¢ Members of a category can be recognized by some properties.
Orange(x) âˆ§ Round (x) âˆ§ Diameter (x) = 9.5 âˆ§ x âˆˆ Balls â‡’ x âˆˆ Basketballs
â€¢ A category as a whole has some properties.
Dogs âˆˆ DomesticatedSpecies

DISJOINT

EXHAUSTIVE
DECOMPOSITION
PARTITION

Notice that because Dogs is a category and is a member of DomesticatedSpecies , the latter
must be a category of categories. Of course there are exceptions to many of the above rules
(punctured basketballs are not spherical); we deal with these exceptions later.
Although subclass and member relations are the most important ones for categories,
we also want to be able to state relations between categories that are not subclasses of each
other. For example, if we just say that Males and Females are subclasses of Animals, then
we have not said that a male cannot be a female. We say that two or more categories are
disjoint if they have no members in common. And even if we know that males and females
are disjoint, we will not know that an animal that is not a male must be a female, unless
we say that males and females constitute an exhaustive decomposition of the animals. A
disjoint exhaustive decomposition is known as a partition. The following examples illustrate
these three concepts:
Disjoint({Animals, Vegetables})
ExhaustiveDecomposition ({Americans, Canadians , Mexicans},
NorthAmericans)
Partition({Males, Females}, Animals) .
(Note that the ExhaustiveDecomposition of NorthAmericans is not a Partition, because
some people have dual citizenship.) The three predicates are defined as follows:
Disjoint(s) â‡” (âˆ€ c1 , c2 c1 âˆˆ s âˆ§ c2 âˆˆ s âˆ§ c1 = c2 â‡’ Intersection(c1 , c2 ) = { })
ExhaustiveDecomposition (s, c) â‡” (âˆ€ i i âˆˆ c â‡” âˆƒ c2 c2 âˆˆ s âˆ§ i âˆˆ c2 )
Partition(s, c) â‡” Disjoint(s) âˆ§ ExhaustiveDecomposition (s, c) .
Categories can also be defined by providing necessary and sufficient conditions for
membership. For example, a bachelor is an unmarried adult male:
x âˆˆ Bachelors â‡” Unmarried (x) âˆ§ x âˆˆ Adults âˆ§ x âˆˆ Males .
As we discuss in the sidebar on natural kinds on page 443, strict logical definitions for categories are neither always possible nor always necessary.

12.2.1 Physical composition
The idea that one object can be part of another is a familiar one. Oneâ€™s nose is part of oneâ€™s
head, Romania is part of Europe, and this chapter is part of this book. We use the general
PartOf relation to say that one thing is part of another. Objects can be grouped into PartOf
hierarchies, reminiscent of the Subset hierarchy:
PartOf (Bucharest , Romania )
PartOf (Romania, EasternEurope )
PartOf (EasternEurope , Europe)
PartOf (Europe, Earth) .

442

Chapter

12.

Knowledge Representation

The PartOf relation is transitive and reflexive; that is,
PartOf (x, y) âˆ§ PartOf (y, z) â‡’ PartOf (x, z) .
PartOf (x, x) .
COMPOSITE OBJECT

Therefore, we can conclude PartOf (Bucharest , Earth).
Categories of composite objects are often characterized by structural relations among
parts. For example, a biped has two legs attached to a body:
Biped (a)

â‡’

âˆƒ l1 , l2 , b Leg (l1 ) âˆ§ Leg(l2 ) âˆ§ Body(b) âˆ§
PartOf (l1 , a) âˆ§ PartOf (l2 , a) âˆ§ PartOf (b, a) âˆ§
Attached (l1 , b) âˆ§ Attached (l2 , b) âˆ§
l1 = l2 âˆ§ [âˆ€ l3 Leg(l3 ) âˆ§ PartOf (l3 , a) â‡’ (l3 = l1 âˆ¨ l3 = l2 )] .

BUNCH

The notation for â€œexactly twoâ€ is a little awkward; we are forced to say that there are two
legs, that they are not the same, and that if anyone proposes a third leg, it must be the same
as one of the other two. In Section 12.5.2, we describe a formalism called description logic
makes it easier to represent constraints like â€œexactly two.â€
We can define a PartPartition relation analogous to the Partition relation for categories. (See Exercise 12.8.) An object is composed of the parts in its PartPartition and can
be viewed as deriving some properties from those parts. For example, the mass of a composite object is the sum of the masses of the parts. Notice that this is not the case with categories,
which have no mass, even though their elements might.
It is also useful to define composite objects with definite parts but no particular structure. For example, we might want to say â€œThe apples in this bag weigh two pounds.â€ The
temptation would be to ascribe this weight to the set of apples in the bag, but this would be
a mistake because the set is an abstract mathematical concept that has elements but does not
have weight. Instead, we need a new concept, which we will call a bunch. For example, if
the apples are Apple 1 , Apple 2 , and Apple 3 , then
BunchOf ({Apple 1 , Apple 2 , Apple 3 })
denotes the composite object with the three apples as parts (not elements). We can then use the
bunch as a normal, albeit unstructured, object. Notice that BunchOf ({x}) = x. Furthermore,
BunchOf (Apples) is the composite object consisting of all applesâ€”not to be confused with
Apples, the category or set of all apples.
We can define BunchOf in terms of the PartOf relation. Obviously, each element of
s is part of BunchOf (s):
âˆ€ x x âˆˆ s â‡’ PartOf (x, BunchOf (s)) .
Furthermore, BunchOf (s) is the smallest object satisfying this condition. In other words,
BunchOf (s) must be part of any object that has all the elements of s as parts:
âˆ€ y [âˆ€ x x âˆˆ s â‡’ PartOf (x, y)] â‡’ PartOf (BunchOf (s), y) .

LOGICAL
MINIMIZATION

These axioms are an example of a general technique called logical minimization, which
means defining an object as the smallest one satisfying certain conditions.

Section 12.2.

Categories and Objects

NATURAL K INDS
Some categories have strict definitions: an object is a triangle if and only if it is
a polygon with three sides. On the other hand, most categories in the real world
have no clear-cut definition; these are called natural kind categories. For example,
tomatoes tend to be a dull scarlet; roughly spherical; with an indentation at the top
where the stem was; about two to four inches in diameter; with a thin but tough
skin; and with flesh, seeds, and juice inside. There is, however, variation: some
tomatoes are yellow or orange, unripe tomatoes are green, some are smaller or
larger than average, and cherry tomatoes are uniformly small. Rather than having
a complete definition of tomatoes, we have a set of features that serves to identify
objects that are clearly typical tomatoes, but might not be able to decide for other
objects. (Could there be a tomato that is fuzzy like a peach?)
This poses a problem for a logical agent. The agent cannot be sure that an
object it has perceived is a tomato, and even if it were sure, it could not be certain which of the properties of typical tomatoes this one has. This problem is an
inevitable consequence of operating in partially observable environments.
One useful approach is to separate what is true of all instances of a category from what is true only of typical instances. So in addition to the category
Tomatoes , we will also have the category Typical (Tomatoes ). Here, the Typical
function maps a category to the subclass that contains only typical instances:
Typical (c) âŠ† c .
Most knowledge about natural kinds will actually be about their typical instances:
x âˆˆ Typical (Tomatoes ) â‡’ Red (x) âˆ§ Round (x) .
Thus, we can write down useful facts about categories without exact definitions. The difficulty of providing exact definitions for most natural categories was
explained in depth by Wittgenstein (1953). He used the example of games to show
that members of a category shared â€œfamily resemblancesâ€ rather than necessary
and sufficient characteristics: what strict definition encompasses chess, tag, solitaire, and dodgeball?
The utility of the notion of strict definition was also challenged by
Quine (1953). He pointed out that even the definition of â€œbachelorâ€ as an unmarried adult male is suspect; one might, for example, question a statement such
as â€œthe Pope is a bachelor.â€ While not strictly false, this usage is certainly infelicitous because it induces unintended inferences on the part of the listener. The
tension could perhaps be resolved by distinguishing between logical definitions
suitable for internal knowledge representation and the more nuanced criteria for
felicitous linguistic usage. The latter may be achieved by â€œfilteringâ€ the assertions
derived from the former. It is also possible that failures of linguistic usage serve as
feedback for modifying internal definitions, so that filtering becomes unnecessary.

443

444

Chapter

12.

Knowledge Representation

12.2.2 Measurements
MEASURE

UNITS FUNCTION

In both scientific and commonsense theories of the world, objects have height, mass, cost,
and so on. The values that we assign for these properties are called measures. Ordinary quantitative measures are quite easy to represent. We imagine that the universe includes abstract â€œmeasure objects,â€ such as the length that is the length of this line seg. We can call this length 1.5 inches or 3.81 centimeters. Thus,
ment:
the same length has different names in our language.We represent the length with a units
function that takes a number as argument. (An alternative scheme is explored in Exercise 12.9.) If the line segment is called L1 , we can write
Length(L1 ) = Inches(1.5) = Centimeters(3.81) .
Conversion between units is done by equating multiples of one unit to another:
Centimeters(2.54 Ã— d) = Inches(d) .
Similar axioms can be written for pounds and kilograms, seconds and days, and dollars and
cents. Measures can be used to describe objects as follows:
Diameter (Basketball 12 ) = Inches(9.5) .
ListPrice(Basketball 12 ) = $(19) .
d âˆˆ Days â‡’ Duration(d) = Hours(24) .
Note that $(1) is not a dollar bill! One can have two dollar bills, but there is only one object
named $(1). Note also that, while Inches(0) and Centimeters(0) refer to the same zero
length, they are not identical to other zero measures, such as Seconds(0).
Simple, quantitative measures are easy to represent. Other measures present more of a
problem, because they have no agreed scale of values. Exercises have difficulty, desserts have
deliciousness, and poems have beauty, yet numbers cannot be assigned to these qualities. One
might, in a moment of pure accountancy, dismiss such properties as useless for the purpose of
logical reasoning; or, still worse, attempt to impose a numerical scale on beauty. This would
be a grave mistake, because it is unnecessary. The most important aspect of measures is not
the particular numerical values, but the fact that measures can be ordered.
Although measures are not numbers, we can still compare them, using an ordering
symbol such as >. For example, we might well believe that Norvigâ€™s exercises are tougher
than Russellâ€™s, and that one scores less on tougher exercises:
e1 âˆˆ Exercises âˆ§ e2 âˆˆ Exercises âˆ§ Wrote(Norvig, e1 ) âˆ§ Wrote(Russell , e2 ) â‡’
Diï¬ƒculty(e1 ) > Diï¬ƒculty(e2 ) .
e1 âˆˆ Exercises âˆ§ e2 âˆˆ Exercises âˆ§ Diï¬ƒculty(e1 ) > Diï¬ƒculty(e2 ) â‡’
ExpectedScore (e1 ) < ExpectedScore (e2 ) .
This is enough to allow one to decide which exercises to do, even though no numerical values
for difficulty were ever used. (One does, however, have to discover who wrote which exercises.) These sorts of monotonic relationships among measures form the basis for the field of
qualitative physics, a subfield of AI that investigates how to reason about physical systems
without plunging into detailed equations and numerical simulations. Qualitative physics is
discussed in the historical notes section.

Section 12.2.

Categories and Objects

445

12.2.3 Objects: Things and stuff

INDIVIDUATION
STUFF

COUNT NOUNS
MASS NOUN

The real world can be seen as consisting of primitive objects (e.g., atomic particles) and
composite objects built from them. By reasoning at the level of large objects such as apples
and cars, we can overcome the complexity involved in dealing with vast numbers of primitive
objects individually. There is, however, a significant portion of reality that seems to defy any
obvious individuationâ€”division into distinct objects. We give this portion the generic name
stuff. For example, suppose I have some butter and an aardvark in front of me. I can say
there is one aardvark, but there is no obvious number of â€œbutter-objects,â€ because any part of
a butter-object is also a butter-object, at least until we get to very small parts indeed. This is
the major distinction between stuff and things. If we cut an aardvark in half, we do not get
two aardvarks (unfortunately).
The English language distinguishes clearly between stuff and things. We say â€œan aardvark,â€ but, except in pretentious California restaurants, one cannot say â€œa butter.â€ Linguists
distinguish between count nouns, such as aardvarks, holes, and theorems, and mass nouns,
such as butter, water, and energy. Several competing ontologies claim to handle this distinction. Here we describe just one; the others are covered in the historical notes section.
To represent stuff properly, we begin with the obvious. We need to have as objects in
our ontology at least the gross â€œlumpsâ€ of stuff we interact with. For example, we might
recognize a lump of butter as the one left on the table the night before; we might pick it up,
weigh it, sell it, or whatever. In these senses, it is an object just like the aardvark. Let us
call it Butter 3 . We also define the category Butter . Informally, its elements will be all those
things of which one might say â€œItâ€™s butter,â€ including Butter 3 . With some caveats about very
small parts that we w omit for now, any part of a butter-object is also a butter-object:
b âˆˆ Butter âˆ§ PartOf (p, b) â‡’ p âˆˆ Butter .
We can now say that butter melts at around 30 degrees centigrade:
b âˆˆ Butter â‡’ MeltingPoint(b, Centigrade (30)) .

INTRINSIC

EXTRINSIC

We could go on to say that butter is yellow, is less dense than water, is soft at room temperature, has a high fat content, and so on. On the other hand, butter has no particular size, shape,
or weight. We can define more specialized categories of butter such as UnsaltedButter ,
which is also a kind of stuff. Note that the category PoundOfButter , which includes as
members all butter-objects weighing one pound, is not a kind of stuff. If we cut a pound of
butter in half, we do not, alas, get two pounds of butter.
What is actually going on is this: some properties are intrinsic: they belong to the very
substance of the object, rather than to the object as a whole. When you cut an instance of
stuff in half, the two pieces retain the intrinsic propertiesâ€”things like density, boiling point,
flavor, color, ownership, and so on. On the other hand, their extrinsic propertiesâ€”weight,
length, shape, and so onâ€”are not retained under subdivision. A category of objects that
includes in its definition only intrinsic properties is then a substance, or mass noun; a class
that includes any extrinsic properties in its definition is a count noun. The category Stuï¬€ is
the most general substance category, specifying no intrinsic properties. The category Thing
is the most general discrete object category, specifying no extrinsic properties.

446

12.3

Chapter

12.

Knowledge Representation

E VENTS

EVENT CALCULUS

In Section 10.4.2, we showed how situation calculus represents actions and their effects.
Situation calculus is limited in its applicability: it was designed to describe a world in which
actions are discrete, instantaneous, and happen one at a time. Consider a continuous action,
such as filling a bathtub. Situation calculus can say that the tub is empty before the action and
full when the action is done, but it canâ€™t talk about what happens during the action. It also
canâ€™t describe two actions happening at the same timeâ€”such as brushing oneâ€™s teeth while
waiting for the tub to fill. To handle such cases we introduce an alternative formalism known
as event calculus, which is based on points of time rather than on situations.3
Event calculus reifies fluents and events. The fluent At(Shankar , Berkeley) is an object that refers to the fact of Shankar being in Berkeley, but does not by itself say anything
about whether it is true. To assert that a fluent is actually true at some point in time we use
the predicate T , as in T (At(Shankar , Berkeley), t).
Events are described as instances of event categories. 4 The event E1 of Shankar flying
from San Francisco to Washington, D.C. is described as
E1 âˆˆ F lyings âˆ§ Flyer (E1 , Shankar ) âˆ§ Origin(E1 , SF ) âˆ§ Destination(E1 , DC ) .
If this is too verbose, we can define an alternative three-argument version of the category of
flying events and say
E1 âˆˆ F lyings(Shankar , SF , DC ) .
We then use Happens(E1 , i) to say that the event E1 took place over the time interval i, and
we say the same thing in functional form with Extent(E1 ) = i. We represent time intervals
by a (start, end) pair of times; that is, i = (t1 , t2 ) is the time interval that starts at t1 and ends
at t2 . The complete set of predicates for one version of the event calculus is
T (f, t)
Happens(e, i)
Initiates(e, f, t)
Terminates(e, f, t)
Clipped (f, i)
Restored (f, i)

Fluent f is true at time t
Event e happens over the time interval i
Event e causes fluent f to start to hold at time t
Event e causes fluent f to cease to hold at time t
Fluent f ceases to be true at some point during time interval i
Fluent f becomes true sometime during time interval i

We assume a distinguished event, Start , that describes the initial state by saying which fluents
are initiated or terminated at the start time. We define T by saying that a fluent holds at a point
in time if the fluent was initiated by an event at some time in the past and was not made false
(clipped) by an intervening event. A fluent does not hold if it was terminated by an event and
3 The terms â€œeventâ€ and â€œactionâ€ may be used interchangeably. Informally, â€œactionâ€ connotes an agent while
â€œeventâ€ connotes the possibility of agentless actions.
4 Some versions of event calculus do not distinguish event categories from instances of the categories.

Section 12.3.

Events

447

not made true (restored) by another event. Formally, the axioms are:
Happens(e, (t1 , t2 )) âˆ§ Initiates(e, f, t1 ) âˆ§ Â¬Clipped (f, (t1 , t)) âˆ§ t1 < t â‡’
T (f, t)
Happens(e, (t1 , t2 )) âˆ§ Terminates(e, f, t1 ) âˆ§ Â¬Restored (f, (t1 , t)) âˆ§ t1 < t â‡’
Â¬T (f, t)
where Clipped and Restored are defined by
Clipped (f, (t1 , t2 )) â‡”
âˆƒ e, t, t3 Happens(e, (t, t3 )) âˆ§ t1 â‰¤ t < t2 âˆ§ Terminates(e, f, t)
Restored (f, (t1 , t2 )) â‡”
âˆƒ e, t, t3 Happens(e, (t, t3 )) âˆ§ t1 â‰¤ t < t2 âˆ§ Initiates(e, f, t)
It is convenient to extend T to work over intervals as well as time points; a fluent holds over
an interval if it holds on every point within the interval:
T (f, (t1 , t2 )) â‡” [âˆ€ t (t1 â‰¤ t < t2 ) â‡’ T (f, t)]
Fluents and actions are defined with domain-specific axioms that are similar to successorstate axioms. For example, we can say that the only way a wumpus-world agent gets an
arrow is at the start, and the only way to use up an arrow is to shoot it:
Initiates(e, HaveArrow (a), t) â‡” e = Start
Terminates(e, HaveArrow (a), t) â‡” e âˆˆ Shootings(a)
By reifying events we make it possible to add any amount of arbitrary information about
them. For example, we can say that Shankarâ€™s flight was bumpy with Bumpy(E1 ). In an
ontology where events are n-ary predicates, there would be no way to add extra information
like this; moving to an n + 1-ary predicate isnâ€™t a scalable solution.
We can extend event calculus to make it possible to represent simultaneous events (such
as two people being necessary to ride a seesaw), exogenous events (such as the wind blowing
and changing the location of an object), continuous events (such as the level of water in the
bathtub continuously rising) and other complications.

12.3.1 Processes
DISCRETE EVENTS

PROCESS
LIQUID EVENT

The events we have seen so far are what we call discrete eventsâ€”they have a definite structure. Shankarâ€™s trip has a beginning, middle, and end. If interrupted halfway, the event would
be something differentâ€”it would not be a trip from San Francisco to Washington, but instead
a trip from San Francisco to somewhere over Kansas. On the other hand, the category of
events denoted by Flyings has a different quality. If we take a small interval of Shankarâ€™s
flight, say, the third 20-minute segment (while he waits anxiously for a bag of peanuts), that
event is still a member of Flyings. In fact, this is true for any subinterval.
Categories of events with this property are called process categories or liquid event
categories. Any process e that happens over an interval also happens over any subinterval:
(e âˆˆ Processes) âˆ§ Happens(e, (t1 , t4 )) âˆ§ (t1 < t2 < t3 < t4 ) â‡’ Happens(e, (t2 , t3 )) .

TEMPORAL
SUBSTANCE
SPATIAL SUBSTANCE

The distinction between liquid and nonliquid events is exactly analogous to the difference
between substances, or stuff, and individual objects, or things. In fact, some have called
liquid events temporal substances, whereas substances like butter are spatial substances.

448

Chapter

12.

Knowledge Representation

12.3.2 Time intervals
Event calculus opens us up to the possibility of talking about time, and time intervals. We
will consider two kinds of time intervals: moments and extended intervals. The distinction is
that only moments have zero duration:
Partition({Moments, ExtendedIntervals }, Intervals )
i âˆˆ Moments â‡” Duration(i) = Seconds(0) .
Next we invent a time scale and associate points on that scale with moments, giving us absolute times. The time scale is arbitrary; we measure it in seconds and say that the moment
at midnight (GMT) on January 1, 1900, has time 0. The functions Begin and End pick out
the earliest and latest moments in an interval, and the function Time delivers the point on the
time scale for a moment. The function Duration gives the difference between the end time
and the start time.
Interval (i) â‡’ Duration(i) = (Time(End (i)) âˆ’ Time(Begin(i))) .
Time(Begin(AD 1900)) = Seconds(0) .
Time(Begin(AD 2001)) = Seconds(3187324800) .
Time(End (AD2001)) = Seconds(3218860800) .
Duration(AD 2001) = Seconds(31536000) .
To make these numbers easier to read, we also introduce a function Date, which takes six
arguments (hours, minutes, seconds, day, month, and year) and returns a time point:
Time(Begin(AD 2001)) = Date(0, 0, 0, 1, Jan , 2001)
Date(0, 20, 21, 24, 1, 1995) = Seconds(3000000000) .
Two intervals Meet if the end time of the first equals the start time of the second. The complete set of interval relations, as proposed by Allen (1983), is shown graphically in Figure 12.2
and logically below:
Meet(i, j)
Before(i, j)
After (j, i)
During(i, j)
Overlap(i, j)
Begins(i, j)
Finishes(i, j)
Equals(i, j)

â‡”
â‡”
â‡”
â‡”
â‡”
â‡”
â‡”
â‡”

End (i) = Begin(j)
End (i) < Begin(j)
Before(i, j)
Begin(j) < Begin(i) < End (i) < End(j)
Begin(i) < Begin(j) < End (i) < End(j)
Begin(i) = Begin(j)
End (i) = End (j)
Begin(i) = Begin(j) âˆ§ End(i) = End(j)

These all have their intuitive meaning, with the exception of Overlap: we tend to think of
overlap as symmetric (if i overlaps j then j overlaps i), but in this definition, Overlap(i, j)
only holds if i begins before j. To say that the reign of Elizabeth II immediately followed that
of George VI, and the reign of Elvis overlapped with the 1950s, we can write the following:
Meets(ReignOf (GeorgeVI ), ReignOf (ElizabethII )) .
Overlap(Fifties, ReignOf (Elvis)) .
Begin(Fifties) = Begin(AD1950) .
End (Fifties) = End (AD1959) .

Section 12.3.

Events

449

Figure 12.2

Predicates on time intervals.

Ada

Wa

shi

ngt

Jef
ms

1797
on

fers

on

1801
time

1789
Figure 12.3
existence.

A schematic view of the object President (USA) for the first 15 years of its

12.3.3 Fluents and objects
Physical objects can be viewed as generalized events, in the sense that a physical object is
a chunk of spaceâ€“time. For example, USA can be thought of as an event that began in,
say, 1776 as a union of 13 states and is still in progress today as a union of 50. We can
describe the changing properties of USA using state fluents, such as Population(USA). A
property of the USA that changes every four or eight years, barring mishaps, is its president.
One might propose that President(USA) is a logical term that denotes a different object
at different times. Unfortunately, this is not possible, because a term denotes exactly one
object in a given model structure. (The term President(USA, t) can denote different objects,
depending on the value of t, but our ontology keeps time indices separate from fluents.) The

450

Chapter

12.

Knowledge Representation

only possibility is that President (USA) denotes a single object that consists of different
people at different times. It is the object that is George Washington from 1789 to 1797, John
Adams from 1797 to 1801, and so on, as in Figure 12.3. To say that George Washington was
president throughout 1790, we can write
T (Equals(President (USA), GeorgeWashington ), AD1790) .
We use the function symbol Equals rather than the standard logical predicate =, because
we cannot have a predicate as an argument to T , and because the interpretation is not that
GeorgeWashington and President(USA) are logically identical in 1790; logical identity is
not something that can change over time. The identity is between the subevents of each object
that are defined by the period 1790.

12.4

PROPOSITIONAL
ATTITUDE

M ENTAL E VENTS AND M ENTAL O BJECTS
The agents we have constructed so far have beliefs and can deduce new beliefs. Yet none
of them has any knowledge about beliefs or about deduction. Knowledge about oneâ€™s own
knowledge and reasoning processes is useful for controlling inference. For example, suppose
Alice asks â€œwhat is the square root of 1764â€ and Bob replies â€œI donâ€™t know.â€ If Alice insists
â€œthink harder,â€ Bob should realize that with some more thought, this question can in fact
be answered. On the other hand, if the question were â€œIs your mother sitting down right
now?â€ then Bob should realize that thinking harder is unlikely to help. Knowledge about
the knowledge of other agents is also important; Bob should realize that his mother knows
whether she is sitting or not, and that asking her would be a way to find out.
What we need is a model of the mental objects that are in someoneâ€™s head (or somethingâ€™s knowledge base) and of the mental processes that manipulate those mental objects.
The model does not have to be detailed. We do not have to be able to predict how many
milliseconds it will take for a particular agent to make a deduction. We will be happy just to
be able to conclude that mother knows whether or not she is sitting.
We begin with the propositional attitudes that an agent can have toward mental objects: attitudes such as Believes, Knows, Wants, Intends, and Informs. The difficulty is
that these attitudes do not behave like â€œnormalâ€ predicates. For example, suppose we try to
assert that Lois knows that Superman can fly:
Knows(Lois, CanFly (Superman)) .
One minor issue with this is that we normally think of CanFly (Superman) as a sentence, but
here it appears as a term. That issue can be patched up just be reifying CanFly(Superman);
making it a fluent. A more serious problem is that, if it is true that Superman is Clark Kent,
then we must conclude that Lois knows that Clark can fly:
(Superman = Clark ) âˆ§ Knows(Lois, CanFly (Superman))
|= Knows(Lois, CanFly (Clark )) .
This is a consequence of the fact that equality reasoning is built into logic. Normally that is
a good thing; if our agent knows that 2 + 2 = 4 and 4 < 5, then we want our agent to know

Section 12.4.
REFERENTIAL
TRANSPARENCY

MODAL LOGIC

POSSIBLE WORLD
ACCESSIBILITY
RELATIONS

Mental Events and Mental Objects

451

that 2 + 2 < 5. This property is called referential transparencyâ€”it doesnâ€™t matter what
term a logic uses to refer to an object, what matters is the object that the term names. But for
propositional attitudes like believes and knows, we would like to have referential opacityâ€”the
terms used do matter, because not all agents know which terms are co-referential.
Modal logic is designed to address this problem. Regular logic is concerned with a single modality, the modality of truth, allowing us to express â€œP is true.â€ Modal logic includes
special modal operators that take sentences (rather than terms) as arguments. For example,
â€œA knows Pâ€ is represented with the notation KA P , where K is the modal operator for knowledge. It takes two arguments, an agent (written as the subscript) and a sentence. The syntax
of modal logic is the same as first-order logic, except that sentences can also be formed with
modal operators.
The semantics of modal logic is more complicated. In first-order logic a model contains a set of objects and an interpretation that maps each name to the appropriate object,
relation, or function. In modal logic we want to be able to consider both the possibility that
Supermanâ€™s secret identity is Clark and that it isnâ€™t. Therefore, we will need a more complicated model, one that consists of a collection of possible worlds rather than just one true
world. The worlds are connected in a graph by accessibility relations, one relation for each
modal operator. We say that world w1 is accessible from world w0 with respect to the modal
operator KA if everything in w1 is consistent with what A knows in w0 , and we write this
as Acc(KA , w0 , w1 ). In diagrams such as Figure 12.4 we show accessibility as an arrow between possible worlds. As an example, in the real world, Bucharest is the capital of Romania,
but for an agent that did not know that, other possible worlds are accessible, including ones
where the capital of Romania is Sibiu or Sofia. Presumably a world where 2 + 2 = 5 would
not be accessible to any agent.
In general, a knowledge atom KA P is true in world w if and only if P is true in every
world accessible from w. The truth of more complex sentences is derived by recursive application of this rule and the normal rules of first-order logic. That means that modal logic can
be used to reason about nested knowledge sentences: what one agent knows about another
agentâ€™s knowledge. For example, we can say that, even though Lois doesnâ€™t know whether
Supermanâ€™s secret identity is Clark Kent, she does know that Clark knows:
KLois [KClark Identity(Superman, Clark ) âˆ¨ KClark Â¬Identity(Superman, Clark )]
Figure 12.4 shows some possible worlds for this domain, with accessibility relations for Lois
and Superman.
In the TOP - LEFT diagram, it is common knowledge that Superman knows his own identity, and neither he nor Lois has seen the weather report. So in w0 the worlds w0 and w2 are
accessible to Superman; maybe rain is predicted, maybe not. For Lois all four worlds are accessible from each other; she doesnâ€™t know anything about the report or if Clark is Superman.
But she does know that Superman knows whether he is Clark, because in every world that is
accessible to Lois, either Superman knows I, or he knows Â¬I. Lois does not know which is
the case, but either way she knows Superman knows.
In the TOP - RIGHT diagram it is common knowledge that Lois has seen the weather
report. So in w4 she knows rain is predicted and in w6 she knows rain is not predicted.

452

Chapter

12.

Knowledge Representation

w0: I,R

w1: Â¬I,R

w4: I,R

w5: Â¬I,R

w2: I,Â¬R

w3: Â¬I,Â¬R

w6: I,Â¬R

w7: Â¬I,Â¬R

(a)

(b)

w5: Â¬I,R

w4: I,R
w0: I,R

w1: Â¬I,R

w2: I,Â¬R

w3: Â¬I,Â¬R
w7: Â¬I,Â¬R

w6: I,Â¬R
(c)

Figure 12.4 Possible worlds with accessibility relations KSuperman (solid arrows) and
KLois (dotted arrows). The proposition R means â€œthe weather report for tomorrow is rainâ€
and I means â€œSupermanâ€™s secret identity is Clark Kent.â€ All worlds are accessible to themselves; the arrows from a world to itself are not shown.

Superman does not know the report, but he knows that Lois knows, because in every world
that is accessible to him, either she knows R or she knows Â¬R.
In the BOTTOM diagram we represent the scenario where it is common knowledge that
Superman knows his identity, and Lois might or might not have seen the weather report. We
represent this by combining the two top scenarios, and adding arrows to show that Superman
does not know which scenario actually holds. Lois does know, so we donâ€™t need to add any
arrows for her. In w0 Superman still knows I but not R, and now he does not know whether
Lois knows R. From what Superman knows, he might be in w0 or w2 , in which case Lois
does not know whether R is true, or he could be in w4 , in which case she knows R, or w6 , in
which case she knows Â¬R.
There are an infinite number of possible worlds, so the trick is to introduce just the ones
you need to represent what you are trying to model. A new possible world is needed to talk
about different possible facts (e.g., rain is predicted or not), or to talk about different states
of knowledge (e.g., does Lois know that rain is predicted). That means two possible worlds,
such as w4 and w0 in Figure 12.4, might have the same base facts about the world, but differ
in their accessibility relations, and therefore in facts about knowledge.
Modal logic solves some tricky issues with the interplay of quantifiers and knowledge.
The English sentence â€œBond knows that someone is a spyâ€ is ambiguous. The first reading is

Section 12.5.

Reasoning Systems for Categories

453

that there is a particular someone who Bond knows is a spy; we can write this as
âˆƒ x KBond Spy(x) ,
which in modal logic means that there is an x that, in all accessible worlds, Bond knows to
be a spy. The second reading is that Bond just knows that there is at least one spy:
KBond âˆƒ x Spy(x) .
The modal logic interpretation is that in each accessible world there is an x that is a spy, but
it need not be the same x in each world.
Now that we have a modal operator for knowledge, we can write axioms for it. First,
we can say that agents are able to draw deductions; if an agent knows P and knows that P
implies Q, then the agent knows Q:
(Ka P âˆ§ Ka (P â‡’ Q)) â‡’ Ka Q .
From this (and a few other rules about logical identities) we can establish that KA (P âˆ¨ Â¬P )
is a tautology; every agent knows every proposition P is either true or false. On the other
hand, (KA P ) âˆ¨ (KA Â¬P ) is not a tautology; in general, there will be lots of propositions that
an agent does not know to be true and does not know to be false.
It is said (going back to Plato) that knowledge is justified true belief. That is, if it is
true, if you believe it, and if you have an unassailably good reason, then you know it. That
means that if you know something, it must be true, and we have the axiom:
Ka P â‡’ P .
Furthermore, logical agents should be able to introspect on their own knowledge. If they
know something, then they know that they know it:
Ka P â‡’ Ka (Ka P ) .
LOGICAL
OMNISCIENCE

12.5

We can define similar axioms for belief (often denoted by B) and other modalities. However,
one problem with the modal logic approach is that it assumes logical omniscience on the
part of agents. That is, if an agent knows a set of axioms, then it knows all consequences of
those axioms. This is on shaky ground even for the somewhat abstract notion of knowledge,
but it seems even worse for belief, because belief has more connotation of referring to things
that are physically represented in the agent, not just potentially derivable. There have been
attempts to define a form of limited rationality for agents; to say that agents believe those
assertions that can be derived with the application of no more than k reasoning steps, or no
more than s seconds of computation. These attempts have been generally unsatisfactory.

R EASONING S YSTEMS FOR C ATEGORIES
Categories are the primary building blocks of large-scale knowledge representation schemes.
This section describes systems specially designed for organizing and reasoning with categories. There are two closely related families of systems: semantic networks provide graphical aids for visualizing a knowledge base and efficient algorithms for inferring properties

454

Chapter

12.

Knowledge Representation

of an object on the basis of its category membership; and description logics provide a formal language for constructing and combining category definitions and efficient algorithms
for deciding subset and superset relationships between categories.

12.5.1 Semantic networks
EXISTENTIAL
GRAPHS

In 1909, Charles S. Peirce proposed a graphical notation of nodes and edges called existential
graphs that he called â€œthe logic of the future.â€ Thus began a long-running debate between
advocates of â€œlogicâ€ and advocates of â€œsemantic networks.â€ Unfortunately, the debate obscured the fact that semantics networksâ€”at least those with well-defined semanticsâ€”are a
form of logic. The notation that semantic networks provide for certain kinds of sentences
is often more convenient, but if we strip away the â€œhuman interfaceâ€ issues, the underlying
conceptsâ€”objects, relations, quantification, and so onâ€”are the same.
There are many variants of semantic networks, but all are capable of representing individual objects, categories of objects, and relations among objects. A typical graphical notation displays object or category names in ovals or boxes, and connects them with labeled
links. For example, Figure 12.5 has a MemberOf link between Mary and FemalePersons,
corresponding to the logical assertion Mary âˆˆ FemalePersons ; similarly, the SisterOf link
between Mary and John corresponds to the assertion SisterOf (Mary, John). We can connect categories using SubsetOf links, and so on. It is such fun drawing bubbles and arrows
that one can get carried away. For example, we know that persons have female persons as
mothers, so can we draw a HasMother link from Persons to FemalePersons? The answer
is no, because HasMother is a relation between a person and his or her mother, and categories
do not have mothers.5
For this reason, we have used a special notationâ€”the double-boxed linkâ€”in Figure 12.5.
This link asserts that
âˆ€ x x âˆˆ Persons â‡’ [âˆ€ y HasMother (x, y) â‡’ y âˆˆ FemalePersons] .
We might also want to assert that persons have two legsâ€”that is,
âˆ€ x x âˆˆ Persons â‡’ Legs(x, 2) .
As before, we need to be careful not to assert that a category has legs; the single-boxed link
in Figure 12.5 is used to assert properties of every member of a category.
The semantic network notation makes it convenient to perform inheritance reasoning
of the kind introduced in Section 12.2. For example, by virtue of being a person, Mary inherits
the property of having two legs. Thus, to find out how many legs Mary has, the inheritance
algorithm follows the MemberOf link from Mary to the category she belongs to, and then
follows SubsetOf links up the hierarchy until it finds a category for which there is a boxed
Legs linkâ€”in this case, the Persons category. The simplicity and efficiency of this inference
5

Several early systems failed to distinguish between properties of members of a category and properties of the
category as a whole. This can lead directly to inconsistencies, as pointed out by Drew McDermott (1976) in his
article â€œArtificial Intelligence Meets Natural Stupidity.â€ Another common problem was the use of IsA links for
both subset and membership relations, in correspondence with English usage: â€œa cat is a mammalâ€ and â€œFifi is a
cat.â€ See Exercise 12.22 for more on these issues.

Section 12.5.

Reasoning Systems for Categories

455

Mammals
SubsetOf
Legs

HasMother

2

Persons
SubsetOf

Female
Persons

SubsetOf

MemberOf

Male
Persons

MemberOf
SisterOf

Mary

John

Legs

1

Figure 12.5 A semantic network with four objects (John, Mary, 1, and 2) and four categories. Relations are denoted by labeled links.

FlyEvents
MemberOf

Fly17
Agent
Origin

Shankar

NewYork

During
Destination

NewDelhi

Yesterday

Figure 12.6 A fragment of a semantic network showing the representation of the logical
assertion Fly(Shankar , NewYork , NewDelhi , Yesterday).

MULTIPLE
INHERITANCE

mechanism, compared with logical theorem proving, has been one of the main attractions of
semantic networks.
Inheritance becomes complicated when an object can belong to more than one category
or when a category can be a subset of more than one other category; this is called multiple inheritance. In such cases, the inheritance algorithm might find two or more conflicting values
answering the query. For this reason, multiple inheritance is banned in some object-oriented
programming (OOP) languages, such as Java, that use inheritance in a class hierarchy. It is
usually allowed in semantic networks, but we defer discussion of that until Section 12.6.
The reader might have noticed an obvious drawback of semantic network notation, compared to first-order logic: the fact that links between bubbles represent only binary relations.
For example, the sentence Fly(Shankar , NewYork , NewDelhi , Yesterday ) cannot be asserted directly in a semantic network. Nonetheless, we can obtain the effect of n-ary assertions by reifying the proposition itself as an event belonging to an appropriate event category.
Figure 12.6 shows the semantic network structure for this particular event. Notice that the
restriction to binary relations forces the creation of a rich ontology of reified concepts.
Reification of propositions makes it possible to represent every ground, function-free
atomic sentence of first-order logic in the semantic network notation. Certain kinds of univer-

456

DEFAULT VALUE

OVERRIDING

Chapter

12.

Knowledge Representation

sally quantified sentences can be asserted using inverse links and the singly boxed and doubly
boxed arrows applied to categories, but that still leaves us a long way short of full first-order
logic. Negation, disjunction, nested function symbols, and existential quantification are all
missing. Now it is possible to extend the notation to make it equivalent to first-order logicâ€”as
in Peirceâ€™s existential graphsâ€”but doing so negates one of the main advantages of semantic
networks, which is the simplicity and transparency of the inference processes. Designers can
build a large network and still have a good idea about what queries will be efficient, because
(a) it is easy to visualize the steps that the inference procedure will go through and (b) in some
cases the query language is so simple that difficult queries cannot be posed. In cases where
the expressive power proves to be too limiting, many semantic network systems provide for
procedural attachment to fill in the gaps. Procedural attachment is a technique whereby
a query about (or sometimes an assertion of) a certain relation results in a call to a special
procedure designed for that relation rather than a general inference algorithm.
One of the most important aspects of semantic networks is their ability to represent
default values for categories. Examining Figure 12.5 carefully, one notices that John has one
leg, despite the fact that he is a person and all persons have two legs. In a strictly logical KB,
this would be a contradiction, but in a semantic network, the assertion that all persons have
two legs has only default status; that is, a person is assumed to have two legs unless this is
contradicted by more specific information. The default semantics is enforced naturally by the
inheritance algorithm, because it follows links upwards from the object itself (John in this
case) and stops as soon as it finds a value. We say that the default is overridden by the more
specific value. Notice that we could also override the default number of legs by creating a
category of OneLeggedPersons, a subset of Persons of which John is a member.
We can retain a strictly logical semantics for the network if we say that the Legs assertion for Persons includes an exception for John:
âˆ€ x x âˆˆ Persons âˆ§ x = John â‡’ Legs(x, 2) .
For a fixed network, this is semantically adequate but will be much less concise than the
network notation itself if there are lots of exceptions. For a network that will be updated with
more assertions, however, such an approach failsâ€”we really want to say that any persons as
yet unknown with one leg are exceptions too. Section 12.6 goes into more depth on this issue
and on default reasoning in general.

12.5.2 Description logics
DESCRIPTION LOGIC

SUBSUMPTION
CLASSIFICATION

The syntax of first-order logic is designed to make it easy to say things about objects. Description logics are notations that are designed to make it easier to describe definitions and
properties of categories. Description logic systems evolved from semantic networks in response to pressure to formalize what the networks mean while retaining the emphasis on
taxonomic structure as an organizing principle.
The principal inference tasks for description logics are subsumption (checking if one
category is a subset of another by comparing their definitions) and classification (checking
whether an object belongs to a category).. Some systems also include consistency of a category definitionâ€”whether the membership criteria are logically satisfiable.

Section 12.5.

Reasoning Systems for Categories

457

Concept â†’ Thing | ConceptName
|

And(Concept , . . .)

|

All(RoleName, Concept )

|

AtLeast(Integer , RoleName )

|

AtMost(Integer , RoleName )

|

Fills(RoleName , IndividualName, . . .)

|

SameAs(Path, Path)

|

OneOf(IndividualName, . . .)

Path â†’ [RoleName, . . .]
Figure 12.7

The syntax of descriptions in a subset of the C LASSIC language.

The C LASSIC language (Borgida et al., 1989) is a typical description logic. The syntax
of C LASSIC descriptions is shown in Figure 12.7.6 For example, to say that bachelors are
unmarried adult males we would write
Bachelor = And(Unmarried , Adult , Male) .
The equivalent in first-order logic would be
Bachelor (x) â‡” Unmarried (x) âˆ§ Adult(x) âˆ§ Male(x) .
Notice that the description logic has an an algebra of operations on predicates, which of
course we canâ€™t do in first-order logic. Any description in C LASSIC can be translated into an
equivalent first-order sentence, but some descriptions are more straightforward in C LASSIC .
For example, to describe the set of men with at least three sons who are all unemployed
and married to doctors, and at most two daughters who are all professors in physics or math
departments, we would use
And(Man, AtLeast (3, Son), AtMost(2, Daughter ),
All(Son, And(Unemployed , Married , All(Spouse, Doctor ))),
All(Daughter , And(Professor , Fills(Department , Physics, Math)))) .
We leave it as an exercise to translate this into first-order logic.
Perhaps the most important aspect of description logics is their emphasis on tractability
of inference. A problem instance is solved by describing it and then asking if it is subsumed
by one of several possible solution categories. In standard first-order logic systems, predicting
the solution time is often impossible. It is frequently left to the user to engineer the representation to detour around sets of sentences that seem to be causing the system to take several
weeks to solve a problem. The thrust in description logics, on the other hand, is to ensure that
subsumption-testing can be solved in time polynomial in the size of the descriptions.7
6 Notice that the language does not allow one to simply state that one concept, or category, is a subset of
another. This is a deliberate policy: subsumption between categories must be derivable from some aspects of the
descriptions of the categories. If not, then something is missing from the descriptions.
7 C LASSIC provides efficient subsumption testing in practice, but the worst-case run time is exponential.

458

Chapter

12.

Knowledge Representation

This sounds wonderful in principle, until one realizes that it can only have one of two
consequences: either hard problems cannot be stated at all, or they require exponentially
large descriptions! However, the tractability results do shed light on what sorts of constructs
cause problems and thus help the user to understand how different representations behave.
For example, description logics usually lack negation and disjunction. Each forces firstorder logical systems to go through a potentially exponential case analysis in order to ensure
completeness. C LASSIC allows only a limited form of disjunction in the Fills and OneOf
constructs, which permit disjunction over explicitly enumerated individuals but not over descriptions. With disjunctive descriptions, nested definitions can lead easily to an exponential
number of alternative routes by which one category can subsume another.

12.6

R EASONING WITH D EFAULT I NFORMATION
In the preceding section, we saw a simple example of an assertion with default status: people
have two legs. This default can be overridden by more specific information, such as that
Long John Silver has one leg. We saw that the inheritance mechanism in semantic networks
implements the overriding of defaults in a simple and natural way. In this section, we study
defaults more generally, with a view toward understanding the semantics of defaults rather
than just providing a procedural mechanism.

12.6.1 Circumscription and default logic

NONMONOTONICITY
NONMONOTONIC
LOGIC

We have seen two examples of reasoning processes that violate the monotonicity property of
logic that was proved in Chapter 7.8 In this chapter we saw that a property inherited by all
members of a category in a semantic network could be overridden by more specific information for a subcategory. In Section 9.4.5, we saw that under the closed-world assumption, if a
proposition Î± is not mentioned in KB then KB |= Â¬Î±, but KB âˆ§ Î± |= Î±.
Simple introspection suggests that these failures of monotonicity are widespread in
commonsense reasoning. It seems that humans often â€œjump to conclusions.â€ For example,
when one sees a car parked on the street, one is normally willing to believe that it has four
wheels even though only three are visible. Now, probability theory can certainly provide a
conclusion that the fourth wheel exists with high probability, yet, for most people, the possibility of the carâ€™s not having four wheels does not arise unless some new evidence presents
itself. Thus, it seems that the four-wheel conclusion is reached by default, in the absence of
any reason to doubt it. If new evidence arrivesâ€”for example, if one sees the owner carrying
a wheel and notices that the car is jacked upâ€”then the conclusion can be retracted. This kind
of reasoning is said to exhibit nonmonotonicity, because the set of beliefs does not grow
monotonically over time as new evidence arrives. Nonmonotonic logics have been devised
with modified notions of truth and entailment in order to capture such behavior. We will look
at two such logics that have been studied extensively: circumscription and default logic.
8

Recall that monotonicity requires all entailed sentences to remain entailed after new sentences are added to the
KB. That is, if KB |= Î± then KB âˆ§ Î² |= Î±.

Section 12.6.
CIRCUMSCRIPTION

Reasoning with Default Information

459

Circumscription can be seen as a more powerful and precise version of the closedworld assumption. The idea is to specify particular predicates that are assumed to be â€œas false
as possibleâ€â€”that is, false for every object except those for which they are known to be true.
For example, suppose we want to assert the default rule that birds fly. We would introduce a
predicate, say Abnormal 1 (x), and write
Bird (x) âˆ§ Â¬Abnormal 1 (x) â‡’ Flies(x) .

MODEL
PREFERENCE

If we say that Abnormal 1 is to be circumscribed, a circumscriptive reasoner is entitled to
assume Â¬Abnormal 1 (x) unless Abnormal 1 (x) is known to be true. This allows the conclusion Flies(Tweety) to be drawn from the premise Bird (Tweety ), but the conclusion no
longer holds if Abnormal 1 (Tweety) is asserted.
Circumscription can be viewed as an example of a model preference logic. In such
logics, a sentence is entailed (with default status) if it is true in all preferred models of the KB,
as opposed to the requirement of truth in all models in classical logic. For circumscription,
one model is preferred to another if it has fewer abnormal objects.9 Let us see how this idea
works in the context of multiple inheritance in semantic networks. The standard example for
which multiple inheritance is problematic is called the â€œNixon diamond.â€ It arises from the
observation that Richard Nixon was both a Quaker (and hence by default a pacifist) and a
Republican (and hence by default not a pacifist). We can write this as follows:
Republican(Nixon) âˆ§ Quaker(Nixon) .
Republican(x) âˆ§ Â¬Abnormal 2 (x) â‡’ Â¬Paciï¬st(x) .
Quaker (x) âˆ§ Â¬Abnormal 3 (x) â‡’ Paciï¬st(x) .

PRIORITIZED
CIRCUMSCRIPTION

DEFAULT LOGIC
DEFAULT RULES

If we circumscribe Abnormal 2 and Abnormal 3 , there are two preferred models: one in
which Abnormal 2 (Nixon) and Paciï¬st(Nixon) hold and one in which Abnormal 3 (Nixon)
and Â¬Paciï¬st(Nixon) hold. Thus, the circumscriptive reasoner remains properly agnostic as
to whether Nixon was a pacifist. If we wish, in addition, to assert that religious beliefs take
precedence over political beliefs, we can use a formalism called prioritized circumscription
to give preference to models where Abnormal 3 is minimized.
Default logic is a formalism in which default rules can be written to generate contingent, nonmonotonic conclusions. A default rule looks like this:
Bird (x) : Flies(x)/Flies(x) .
This rule means that if Bird (x) is true, and if Flies(x) is consistent with the knowledge base,
then Flies(x) may be concluded by default. In general, a default rule has the form
P : J1 , . . . , Jn /C
where P is called the prerequisite, C is the conclusion, and Ji are the justificationsâ€”if any
one of them can be proven false, then the conclusion cannot be drawn. Any variable that
9 For the closed-world assumption, one model is preferred to another if it has fewer true atomsâ€”that is, preferred
models are minimal models. There is a natural connection between the closed-world assumption and definiteclause KBs, because the fixed point reached by forward chaining on definite-clause KBs is the unique minimal
model. See page 258 for more on this point.

460

Chapter

12.

Knowledge Representation

appears in Ji or C must also appear in P . The Nixon-diamond example can be represented
in default logic with one fact and two default rules:
Republican(Nixon) âˆ§ Quaker(Nixon) .
Republican(x) : Â¬Paciï¬st(x)/Â¬Paciï¬st(x) .
Quaker (x) : Paciï¬st(x)/Paciï¬st (x) .
EXTENSION

To interpret what the default rules mean, we define the notion of an extension of a default
theory to be a maximal set of consequences of the theory. That is, an extension S consists
of the original known facts and a set of conclusions from the default rules, such that no
additional conclusions can be drawn from S and the justifications of every default conclusion
in S are consistent with S. As in the case of the preferred models in circumscription, we have
two possible extensions for the Nixon diamond: one wherein he is a pacifist and one wherein
he is not. Prioritized schemes exist in which some default rules can be given precedence over
others, allowing some ambiguities to be resolved.
Since 1980, when nonmonotonic logics were first proposed, a great deal of progress
has been made in understanding their mathematical properties. There are still unresolved
questions, however. For example, if â€œCars have four wheelsâ€ is false, what does it mean
to have it in oneâ€™s knowledge base? What is a good set of default rules to have? If we
cannot decide, for each rule separately, whether it belongs in our knowledge base, then we
have a serious problem of nonmodularity. Finally, how can beliefs that have default status be
used to make decisions? This is probably the hardest issue for default reasoning. Decisions
often involve tradeoffs, and one therefore needs to compare the strengths of belief in the
outcomes of different actions, and the costs of making a wrong decision. In cases where the
same kinds of decisions are being made repeatedly, it is possible to interpret default rules
as â€œthreshold probabilityâ€ statements. For example, the default rule â€œMy brakes are always
OKâ€ really means â€œThe probability that my brakes are OK, given no other information, is
sufficiently high that the optimal decision is for me to drive without checking them.â€ When
the decision context changesâ€”for example, when one is driving a heavily laden truck down a
steep mountain roadâ€”the default rule suddenly becomes inappropriate, even though there is
no new evidence of faulty brakes. These considerations have led some researchers to consider
how to embed default reasoning within probability theory or utility theory.

12.6.2 Truth maintenance systems

BELIEF REVISION

We have seen that many of the inferences drawn by a knowledge representation system will
have only default status, rather than being absolutely certain. Inevitably, some of these inferred facts will turn out to be wrong and will have to be retracted in the face of new information. This process is called belief revision.10 Suppose that a knowledge base KB contains
a sentence P â€”perhaps a default conclusion recorded by a forward-chaining algorithm, or
perhaps just an incorrect assertionâ€”and we want to execute T ELL (KB, Â¬P ). To avoid creating a contradiction, we must first execute R ETRACT (KB, P ). This sounds easy enough.
10 Belief revision is often contrasted with belief update, which occurs when a knowledge base is revised to reflect
a change in the world rather than new information about a fixed world. Belief update combines belief revision
with reasoning about time and change; it is also related to the process of filtering described in Chapter 15.

Section 12.6.

TRUTH
MAINTENANCE
SYSTEM

JTMS
JUSTIFICATION

Reasoning with Default Information

461

Problems arise, however, if any additional sentences were inferred from P and asserted in
the KB. For example, the implication P â‡’ Q might have been used to add Q. The obvious
â€œsolutionâ€â€”retracting all sentences inferred from P â€”fails because such sentences may have
other justifications besides P . For example, if R and R â‡’ Q are also in the KB, then Q
does not have to be removed after all. Truth maintenance systems, or TMSs, are designed
to handle exactly these kinds of complications.
One simple approach to truth maintenance is to keep track of the order in which sentences are told to the knowledge base by numbering them from P1 to Pn . When the call
R ETRACT (KB, Pi ) is made, the system reverts to the state just before Pi was added, thereby
removing both Pi and any inferences that were derived from Pi . The sentences Pi+1 through
Pn can then be added again. This is simple, and it guarantees that the knowledge base will
be consistent, but retracting Pi requires retracting and reasserting n âˆ’ i sentences as well as
undoing and redoing all the inferences drawn from those sentences. For systems to which
many facts are being addedâ€”such as large commercial databasesâ€”this is impractical.
A more efficient approach is the justification-based truth maintenance system, or JTMS.
In a JTMS, each sentence in the knowledge base is annotated with a justification consisting
of the set of sentences from which it was inferred. For example, if the knowledge base
already contains P â‡’ Q, then T ELL (P ) will cause Q to be added with the justification
{P, P â‡’ Q}. In general, a sentence can have any number of justifications. Justifications make retraction efficient. Given the call R ETRACT (P ), the JTMS will delete exactly
those sentences for which P is a member of every justification. So, if a sentence Q had
the single justification {P, P â‡’ Q}, it would be removed; if it had the additional justification {P, P âˆ¨ R â‡’ Q}, it would still be removed; but if it also had the justification
{R, P âˆ¨ R â‡’ Q}, then it would be spared. In this way, the time required for retraction of P
depends only on the number of sentences derived from P rather than on the number of other
sentences added since P entered the knowledge base.
The JTMS assumes that sentences that are considered once will probably be considered
again, so rather than deleting a sentence from the knowledge base entirely when it loses
all justifications, we merely mark the sentence as being out of the knowledge base. If a
subsequent assertion restores one of the justifications, then we mark the sentence as being
back in. In this way, the JTMS retains all the inference chains that it uses and need not
rederive sentences when a justification becomes valid again.
In addition to handling the retraction of incorrect information, TMSs can be used to
speed up the analysis of multiple hypothetical situations. Suppose, for example, that the
Romanian Olympic Committee is choosing sites for the swimming, athletics, and equestrian events at the 2048 Games to be held in Romania. For example, let the first hypothesis be Site(Swimming , Pitesti ), Site(Athletics, Bucharest ), and Site(Equestrian, Arad ).
A great deal of reasoning must then be done to work out the logistical consequences and
hence the desirability of this selection. If we want to consider Site(Athletics, Sibiu) instead, the TMS avoids the need to start again from scratch. Instead, we simply retract
Site(Athletics, Bucharest ) and assert Site(Athletics, Sibiu) and the TMS takes care of the
necessary revisions. Inference chains generated from the choice of Bucharest can be reused
with Sibiu, provided that the conclusions are the same.

462
ATMS

EXPLANATION

ASSUMPTION

12.7

Chapter

12.

Knowledge Representation

An assumption-based truth maintenance system, or ATMS, makes this type of contextswitching between hypothetical worlds particularly efficient. In a JTMS, the maintenance of
justifications allows you to move quickly from one state to another by making a few retractions and assertions, but at any time only one state is represented. An ATMS represents all the
states that have ever been considered at the same time. Whereas a JTMS simply labels each
sentence as being in or out, an ATMS keeps track, for each sentence, of which assumptions
would cause the sentence to be true. In other words, each sentence has a label that consists of
a set of assumption sets. The sentence holds just in those cases in which all the assumptions
in one of the assumption sets hold.
Truth maintenance systems also provide a mechanism for generating explanations.
Technically, an explanation of a sentence P is a set of sentences E such that E entails P .
If the sentences in E are already known to be true, then E simply provides a sufficient basis for proving that P must be the case. But explanations can also include assumptionsâ€”
sentences that are not known to be true, but would suffice to prove P if they were true. For
example, one might not have enough information to prove that oneâ€™s car wonâ€™t start, but a
reasonable explanation might include the assumption that the battery is dead. This, combined
with knowledge of how cars operate, explains the observed nonbehavior. In most cases, we
will prefer an explanation E that is minimal, meaning that there is no proper subset of E that
is also an explanation. An ATMS can generate explanations for the â€œcar wonâ€™t startâ€ problem
by making assumptions (such as â€œgas in carâ€ or â€œbattery deadâ€) in any order we like, even if
some assumptions are contradictory. Then we look at the label for the sentence â€œcar wonâ€™t
startâ€ to read off the sets of assumptions that would justify the sentence.
The exact algorithms used to implement truth maintenance systems are a little complicated, and we do not cover them here. The computational complexity of the truth maintenance
problem is at least as great as that of propositional inferenceâ€”that is, NP-hard. Therefore,
you should not expect truth maintenance to be a panacea. When used carefully, however, a
TMS can provide a substantial increase in the ability of a logical system to handle complex
environments and hypotheses.

T HE I NTERNET S HOPPING W ORLD
In this final section we put together all we have learned to encode knowledge for a shopping
research agent that helps a buyer find product offers on the Internet. The shopping agent is
given a product description by the buyer and has the task of producing a list of Web pages
that offer such a product for sale, and ranking which offers are best. In some cases the
buyerâ€™s product description will be precise, as in Canon Rebel XTi digital camera, and the
task is then to find the store(s) with the best offer. In other cases the description will be only
partially specified, as in digital camera for under $300, and the agent will have to compare
different products.
The shopping agentâ€™s environment is the entire World Wide Web in its full complexityâ€”
not a toy simulated environment. The agentâ€™s percepts are Web pages, but whereas a human

Section 12.7.

The Internet Shopping World

463

Example Online Store
Select from our fine line of products:
â€¢ Computers
â€¢ Cameras
â€¢ Books
â€¢ Videos
â€¢ Music
<h1>Example Online Store</h1>
<i>Select</i> from our fine line of products:
<ul>
<li> <a href="http://example.com/compu">Computers</a>
<li> <a href="http://example.com/camer">Cameras</a>
<li> <a href="http://example.com/books">Books</a>
<li> <a href="http://example.com/video">Videos</a>
<li> <a href="http://example.com/music">Music</a>
</ul>
Figure 12.8 A Web page from a generic online store in the form perceived by the human
user of a browser (top), and the corresponding HTML string as perceived by the browser or
the shopping agent (bottom). In HTML, characters between < and > are markup directives
that specify how the page is displayed. For example, the string <i>Select</i> means
to switch to italic font, display the word Select, and then end the use of italic font. A page
identifier such as http://example.com/books is called a uniform resource locator
(URL). The markup <a href="url">Books</a> means to create a hypertext link to url
with the anchor text Books.

Web user would see pages displayed as an array of pixels on a screen, the shopping agent
will perceive a page as a character string consisting of ordinary words interspersed with formatting commands in the HTML markup language. Figure 12.8 shows a Web page and a
corresponding HTML character string. The perception problem for the shopping agent involves extracting useful information from percepts of this kind.
Clearly, perception on Web pages is easier than, say, perception while driving a taxi in
Cairo. Nonetheless, there are complications to the Internet perception task. The Web page in
Figure 12.8 is simple compared to real shopping sites, which may include CSS, cookies, Java,
Javascript, Flash, robot exclusion protocols, malformed HTML, sound files, movies, and text
that appears only as part of a JPEG image. An agent that can deal with all of the Internet is
almost as complex as a robot that can move in the real world. We concentrate on a simple
agent that ignores most of these complications.
The agentâ€™s first task is to collect product offers that are relevant to a query. If the query
is â€œlaptops,â€ then a Web page with a review of the latest high-end laptop would be relevant,
but if it doesnâ€™t provide a way to buy, it isnâ€™t an offer. For now, we can say a page is an offer
if it contains the words â€œbuyâ€ or â€œpriceâ€ or â€œadd to cartâ€ within an HTML link or form on the

464

Chapter

12.

Knowledge Representation

page. For example, if the page contains a string of the form â€œ<a . . . add to cart . . . </aâ€
then it is an offer. This could be represented in first-order logic, but it is more straightforward
to encode it into program code. We show how to do more sophisticated information extraction
in Section 22.4.

12.7.1 Following links
The strategy is to start at the home page of an online store and consider all pages that can be
reached by following relevant links.11 The agent will have knowledge of a number of stores,
for example:
Amazon âˆˆ OnlineStores âˆ§ Homepage(Amazon, â€œamazon.comâ€) .
Ebay âˆˆ OnlineStores âˆ§ Homepage(Ebay, â€œebay.comâ€) .
ExampleStore âˆˆ OnlineStores âˆ§ Homepage(ExampleStore , â€œexample.comâ€) .
These stores classify their goods into product categories, and provide links to the major categories from their home page. Minor categories can be reached through a chain of relevant
links, and eventually we will reach offers. In other words, a page is relevant to the query if it
can be reached by a chain of zero or more relevant category links from a storeâ€™s home page,
and then from one more link to the product offer. We can define relevance:
Relevant (page, query) â‡”
âˆƒ store, home store âˆˆ OnlineStores âˆ§ Homepage(store, home)
âˆ§ âˆƒ url , url 2 RelevantChain(home, url 2 , query) âˆ§ Link (url 2 , url )
âˆ§ page = Contents(url ) .
Here the predicate Link (from, to) means that there is a hyperlink from the from URL to
the to URL. To define what counts as a RelevantChain, we need to follow not just any old
hyperlinks, but only those links whose associated anchor text indicates that the link is relevant
to the product query. For this, we use LinkText(from, to, text) to mean that there is a link
between from and to with text as the anchor text. A chain of links between two URLs, start
and end, is relevant to a description d if the anchor text of each link is a relevant category
name for d. The existence of the chain itself is determined by a recursive definition, with the
empty chain (start = end ) as the base case:
RelevantChain(start , end , query) â‡” (start = end )
âˆ¨ (âˆƒ u, text LinkText(start , u, text ) âˆ§ RelevantCategoryName (query, text )
âˆ§ RelevantChain(u, end , query)) .
Now we must define what it means for text to be a RelevantCategoryName for query.
First, we need to relate strings to the categories they name. This is done using the predicate
Name(s, c), which says that string s is a name for category câ€”for example, we might assert
that Name(â€œlaptopsâ€, LaptopComputers ). Some more examples of the Name predicate
appear in Figure 12.9(b). Next, we define relevance. Suppose that query is â€œlaptops.â€ Then
RelevantCategoryName (query, text ) is true when one of the following holds:
â€¢ The text and query name the same categoryâ€”e.g., â€œnotebooksâ€ and â€œlaptops.â€
11

An alternative to the link-following strategy is to use an Internet search engine; the technology behind Internet
search, information retrieval, will be covered in Section 22.3.

Section 12.7.

The Internet Shopping World

Books âŠ‚ Products
MusicRecordings âŠ‚ Products
MusicCDs âŠ‚ MusicRecordings
Electronics âŠ‚ Products
DigitalCameras âŠ‚ Electronics
StereoEquipment âŠ‚ Electronics
Computers âŠ‚ Electronics
DesktopComputers âŠ‚ Computers
LaptopComputers âŠ‚ Computers
...

(a)
Figure 12.9

465
Name(â€œbooksâ€, Books)
Name(â€œmusicâ€, MusicRecordings)
Name(â€œCDsâ€, MusicCDs)
Name(â€œelectronicsâ€, Electronics)
Name(â€œdigital camerasâ€, DigitalCameras )
Name(â€œstereosâ€, StereoEquipment )
Name(â€œcomputersâ€, Computers)
Name(â€œdesktopsâ€, DesktopComputers)
Name(â€œlaptopsâ€, LaptopComputers )
Name(â€œnotebooksâ€, LaptopComputers)
...

(b)

(a) Taxonomy of product categories. (b) Names for those categories.

â€¢ The text names a supercategory such as â€œcomputers.â€
â€¢ The text names a subcategory such as â€œultralight notebooks.â€
The logical definition of RelevantCategoryName is as follows:
RelevantCategoryName (query, text ) â‡”
âˆƒ c1 , c2 Name(query, c1 ) âˆ§ Name(text, c2 ) âˆ§ (c1 âŠ† c2 âˆ¨ c2 âŠ† c1 ) .

(12.1)

Otherwise, the anchor text is irrelevant because it names a category outside this line, such as
â€œclothesâ€ or â€œlawn & garden.â€
To follow relevant links, then, it is essential to have a rich hierarchy of product categories. The top part of this hierarchy might look like Figure 12.9(a). It will not be feasible to
list all possible shopping categories, because a buyer could always come up with some new
desire and manufacturers will always come out with new products to satisfy them (electric
kneecap warmers?). Nonetheless, an ontology of about a thousand categories will serve as a
very useful tool for most buyers.
In addition to the product hierarchy itself, we also need to have a rich vocabulary of
names for categories. Life would be much easier if there were a one-to-one correspondence between categories and the character strings that name them. We have already seen
the problem of synonymyâ€”two names for the same category, such as â€œlaptop computersâ€
and â€œlaptops.â€ There is also the problem of ambiguityâ€”one name for two or more different
categories. For example, if we add the sentence
Name(â€œCDsâ€, Certiï¬catesOfDeposit )
to the knowledge base in Figure 12.9(b), then â€œCDsâ€ will name two different categories.
Synonymy and ambiguity can cause a significant increase in the number of paths that
the agent has to follow, and can sometimes make it difficult to determine whether a given
page is indeed relevant. A much more serious problem is the very broad range of descriptions
that a user can type and category names that a store can use. For example, the link might say
â€œlaptopâ€ when the knowledge base has only â€œlaptopsâ€ or the user might ask for â€œa computer

466

PROCEDURAL
ATTACHMENT

Chapter

12.

Knowledge Representation

I can fit on the tray table of an economy-class airline seat.â€ It is impossible to enumerate in
advance all the ways a category can be named, so the agent will have to be able to do additional reasoning in some cases to determine if the Name relation holds. In the worst case, this
requires full natural language understanding, a topic that we will defer to Chapter 22. In practice, a few simple rulesâ€”such as allowing â€œlaptopâ€ to match a category named â€œlaptopsâ€â€”go
a long way. Exercise 12.10 asks you to develop a set of such rules after doing some research
into online stores.
Given the logical definitions from the preceding paragraphs and suitable knowledge
bases of product categories and naming conventions, are we ready to apply an inference
algorithm to obtain a set of relevant offers for our query? Not quite! The missing element
is the Contents(url ) function, which refers to the HTML page at a given URL. The agent
doesnâ€™t have the page contents of every URL in its knowledge base; nor does it have explicit
rules for deducing what those contents might be. Instead, we can arrange for the right HTTP
procedure to be executed whenever a subgoal involves the Contents function. In this way, it
appears to the inference engine as if the entire Web is inside the knowledge base. This is an
example of a general technique called procedural attachment, whereby particular predicates
and functions can be handled by special-purpose methods.

12.7.2 Comparing offers

WRAPPER

Let us assume that the reasoning processes of the preceding section have produced a set of
offer pages for our â€œlaptopsâ€ query. To compare those offers, the agent must extract the relevant informationâ€”price, speed, disk size, weight, and so onâ€”from the offer pages. This can
be a difficult task with real Web pages, for all the reasons mentioned previously. A common
way of dealing with this problem is to use programs called wrappers to extract information
from a page. The technology of information extraction is discussed in Section 22.4. For
now we assume that wrappers exist, and when given a page and a knowledge base, they add
assertions to the knowledge base. Typically, a hierarchy of wrappers would be applied to a
page: a very general one to extract dates and prices, a more specific one to extract attributes
for computer-related products, and if necessary a site-specific one that knows the format of a
particular store. Given a page on the example.com site with the text
IBM ThinkBook 970.

Our price:

$399.00

followed by various technical specifications, we would like a wrapper to extract information
such as the following:
âˆƒ c, oï¬€er c âˆˆ LaptopComputers âˆ§ oï¬€er âˆˆ ProductOï¬€ers âˆ§
Manufacturer(c, IBM ) âˆ§ Model (c, ThinkBook970 ) âˆ§
ScreenSize(c, Inches(14)) âˆ§ ScreenType(c, ColorLCD ) âˆ§
MemorySize(c, Gigabytes(2)) âˆ§ CPUSpeed (c, GHz (1.2)) âˆ§
Oï¬€eredProduct (oï¬€er , c) âˆ§ Store(oï¬€er , GenStore) âˆ§
URL(oï¬€er , â€œexample.com/computers/34356.htmlâ€) âˆ§
Price(oï¬€er , $(399)) âˆ§ Date(oï¬€er , Today ) .
This example illustrates several issues that arise when we take seriously the task of knowledge
engineering for commercial transactions. For example, notice that the price is an attribute of

Section 12.8.

Summary

467

the offer, not the product itself. This is important because the offer at a given store may
change from day to day even for the same individual laptop; for some categoriesâ€”such as
houses and paintingsâ€”the same individual object may even be offered simultaneously by
different intermediaries at different prices. There are still more complications that we have
not handled, such as the possibility that the price depends on the method of payment and on
the buyerâ€™s qualifications for certain discounts. The final task is to compare the offers that
have been extracted. For example, consider these three offers:
A : 1.4 GHz CPU, 2GB RAM, 250 GB disk, $299 .
B : 1.2 GHz CPU, 4GB RAM, 350 GB disk, $500 .
C : 1.2 GHz CPU, 2GB RAM, 250 GB disk, $399 .
C is dominated by A; that is, A is cheaper and faster, and they are otherwise the same. In
general, X dominates Y if X has a better value on at least one attribute, and is not worse on
any attribute. But neither A nor B dominates the other. To decide which is better we need
to know how the buyer weighs CPU speed and price against memory and disk space. The
general topic of preferences among multiple attributes is addressed in Section 16.4; for now,
our shopping agent will simply return a list of all undominated offers that meet the buyerâ€™s
description. In this example, both A and B are undominated. Notice that this outcome relies
on the assumption that everyone prefers cheaper prices, faster processors, and more storage.
Some attributes, such as screen size on a notebook, depend on the userâ€™s particular preference
(portability versus visibility); for these, the shopping agent will just have to ask the user.
The shopping agent we have described here is a simple one; many refinements are
possible. Still, it has enough capability that with the right domain-specific knowledge it can
actually be of use to a shopper. Because of its declarative construction, it extends easily to
more complex applications. The main point of this section is to show that some knowledge
representationâ€”in particular, the product hierarchyâ€”is necessary for such an agent, and that
once we have some knowledge in this form, the rest follows naturally.

12.8

S UMMARY
By delving into the details of how one represents a variety of knowledge, we hope we have
given the reader a sense of how real knowledge bases are constructed and a feeling for the
interesting philosophical issues that arise. The major points are as follows:
â€¢ Large-scale knowledge representation requires a general-purpose ontology to organize
and tie together the various specific domains of knowledge.
â€¢ A general-purpose ontology needs to cover a wide variety of knowledge and should be
capable, in principle, of handling any domain.
â€¢ Building a large, general-purpose ontology is a significant challenge that has yet to be
fully realized, although current frameworks seem to be quite robust.
â€¢ We presented an upper ontology based on categories and the event calculus. We
covered categories, subcategories, parts, structured objects, measurements, substances,
events, time and space, change, and beliefs.

468

Chapter

12.

Knowledge Representation

â€¢ Natural kinds cannot be defined completely in logic, but properties of natural kinds can
be represented.
â€¢ Actions, events, and time can be represented either in situation calculus or in more
expressive representations such as event calculus. Such representations enable an agent
to construct plans by logical inference.
â€¢ We presented a detailed analysis of the Internet shopping domain, exercising the general
ontology and showing how the domain knowledge can be used by a shopping agent.
â€¢ Special-purpose representation systems, such as semantic networks and description
logics, have been devised to help in organizing a hierarchy of categories. Inheritance
is an important form of inference, allowing the properties of objects to be deduced from
their membership in categories.
â€¢ The closed-world assumption, as implemented in logic programs, provides a simple
way to avoid having to specify lots of negative information. It is best interpreted as a
default that can be overridden by additional information.
â€¢ Nonmonotonic logics, such as circumscription and default logic, are intended to capture default reasoning in general.
â€¢ Truth maintenance systems handle knowledge updates and revisions efficiently.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Briggs (1985) claims that formal knowledge representation research began with classical Indian theorizing about the grammar of Shastric Sanskrit, which dates back to the first millennium B . C . In the West, the use of definitions of terms in ancient Greek mathematics can be
regarded as the earliest instance: Aristotleâ€™s Metaphysics (literally, what comes after the book
on physics) is a near-synonym for Ontology. Indeed, the development of technical terminology in any field can be regarded as a form of knowledge representation.
Early discussions of representation in AI tended to focus on â€œproblem representationâ€
rather than â€œknowledge representation.â€ (See, for example, Amarelâ€™s (1968) discussion of the
Missionaries and Cannibals problem.) In the 1970s, AI emphasized the development of â€œexpert systemsâ€ (also called â€œknowledge-based systemsâ€) that could, if given the appropriate
domain knowledge, match or exceed the performance of human experts on narrowly defined
tasks. For example, the first expert system, D ENDRAL (Feigenbaum et al., 1971; Lindsay
et al., 1980), interpreted the output of a mass spectrometer (a type of instrument used to analyze the structure of organic chemical compounds) as accurately as expert chemists. Although
the success of D ENDRAL was instrumental in convincing the AI research community of the
importance of knowledge representation, the representational formalisms used in D ENDRAL
are highly specific to the domain of chemistry. Over time, researchers became interested in
standardized knowledge representation formalisms and ontologies that could streamline the
process of creating new expert systems. In so doing, they ventured into territory previously
explored by philosophers of science and of language. The discipline imposed in AI by the
need for oneâ€™s theories to â€œworkâ€ has led to more rapid and deeper progress than was the case

Bibliographical and Historical Notes

469

when these problems were the exclusive domain of philosophy (although it has at times also
led to the repeated reinvention of the wheel).
The creation of comprehensive taxonomies or classifications dates back to ancient times.
Aristotle (384â€“322 B . C .) strongly emphasized classification and categorization schemes. His
Organon, a collection of works on logic assembled by his students after his death, included a
treatise called Categories in which he attempted to construct what we would now call an upper
ontology. He also introduced the notions of genus and species for lower-level classification.
Our present system of biological classification, including the use of â€œbinomial nomenclatureâ€
(classification via genus and species in the technical sense), was invented by the Swedish
biologist Carolus Linnaeus, or Carl von Linne (1707â€“1778). The problems associated with
natural kinds and inexact category boundaries have been addressed by Wittgenstein (1953),
Quine (1953), Lakoff (1987), and Schwartz (1977), among others.
Interest in larger-scale ontologies is increasing, as documented by the Handbook on
Ontologies (Staab, 2004). The O PEN CYC project (Lenat and Guha, 1990; Matuszek et al.,
2006) has released a 150,000-concept ontology, with an upper ontology similar to the one in
Figure 12.1 as well as specific concepts like â€œOLED Displayâ€ and â€œiPhone,â€ which is a type
of â€œcellular phone,â€ which in turn is a type of â€œconsumer electronics,â€ â€œphone,â€ â€œwireless
communication device,â€ and other concepts. The DB PEDIA project extracts structured data
from Wikipedia; specifically from Infoboxes: the boxes of attribute/value pairs that accompany many Wikipedia articles (Wu and Weld, 2008; Bizer et al., 2007). As of mid-2009,
DB PEDIA contains 2.6 million concepts, with about 100 facts per concept. The IEEE working group P1600.1 created the Suggested Upper Merged Ontology (SUMO) (Niles and Pease,
2001; Pease and Niles, 2002), which contains about 1000 terms in the upper ontology and
links to over 20,000 domain-specific terms. Stoffel et al. (1997) describe algorithms for efficiently managing a very large ontology. A survey of techniques for extracting knowledge
from Web pages is given by Etzioni et al. (2008).
On the Web, representation languages are emerging. RDF (Brickley and Guha, 2004)
allows for assertions to be made in the form of relational triples, and provides some means
for evolving the meaning of names over time. OWL (Smith et al., 2004) is a description logic
that supports inferences over these triples. So far, usage seems to be inversely proportional to
representational complexity: the traditional HTML and CSS formats account for over 99% of
Web content, followed by the simplest representation schemes, such as microformats (Khare,
2006) and RDFa (Adida and Birbeck, 2008), which use HTML and XHTML markup to
add attributes to literal text. Usage of sophisticated RDF and OWL ontologies is not yet
widespread, and the full vision of the Semantic Web (Berners-Lee et al., 2001) has not yet
been realized. The conferences on Formal Ontology in Information Systems (FOIS) contain
many interesting papers on both general and domain-specific ontologies.
The taxonomy used in this chapter was developed by the authors and is based in part
on their experience in the CYC project and in part on work by Hwang and Schubert (1993)
and Davis (1990, 2005). An inspirational discussion of the general project of commonsense
knowledge representation appears in Hayesâ€™s (1978, 1985b) â€œNaive Physics Manifesto.â€
Successful deep ontologies within a specific field include the Gene Ontology project
(Consortium, 2008) and CML, the Chemical Markup Language (Murray-Rust et al., 2003).

470

MEREOLOGY

SYNTACTIC THEORY

Chapter

12.

Knowledge Representation

Doubts about the feasibility of a single ontology for all knowledge are expressed by
Doctorow (2001), Gruber (2004), Halevy et al. (2009), and Smith (2004), who states, â€œthe
initial project of building one single ontology . . . has . . . largely been abandoned.â€
The event calculus was introduced by Kowalski and Sergot (1986) to handle continuous
time, and there have been several variations (Sadri and Kowalski, 1995; Shanahan, 1997) and
overviews (Shanahan, 1999; Mueller, 2006). van Lambalgen and Hamm (2005) show how
the logic of events maps onto the language we use to talk about events. An alternative to the
event and situation calculi is the fluent calculus (Thielscher, 1999). James Allen introduced
time intervals for the same reason (Allen, 1984), arguing that intervals were much more natural than situations for reasoning about extended and concurrent events. Peter Ladkin (1986a,
1986b) introduced â€œconcaveâ€ time intervals (intervals with gaps; essentially, unions of ordinary â€œconvexâ€ time intervals) and applied the techniques of mathematical abstract algebra to
time representation. Allen (1991) systematically investigates the wide variety of techniques
available for time representation; van Beek and Manchak (1996) analyze algorithms for temporal reasoning. There are significant commonalities between the event-based ontology given
in this chapter and an analysis of events due to the philosopher Donald Davidson (1980).
The histories in Pat Hayesâ€™s (1985a) ontology of liquids and the chronicles in McDermottâ€™s
(1985) theory of plans were also important influences on the field and this chapter.
The question of the ontological status of substances has a long history. Plato proposed
that substances were abstract entities entirely distinct from physical objects; he would say
MadeOf (Butter 3 , Butter ) rather than Butter 3 âˆˆ Butter . This leads to a substance hierarchy in which, for example, UnsaltedButter is a more specific substance than Butter . The position adopted in this chapter, in which substances are categories of objects, was championed
by Richard Montague (1973). It has also been adopted in the CYC project. Copeland (1993)
mounts a serious, but not invincible, attack. The alternative approach mentioned in the chapter, in which butter is one object consisting of all buttery objects in the universe, was proposed
originally by the Polish logician LesÌniewski (1916). His mereology (the name is derived from
the Greek word for â€œpartâ€) used the partâ€“whole relation as a substitute for mathematical set
theory, with the aim of eliminating abstract entities such as sets. A more readable exposition
of these ideas is given by Leonard and Goodman (1940), and Goodmanâ€™s The Structure of
Appearance (1977) applies the ideas to various problems in knowledge representation. While
some aspects of the mereological approach are awkwardâ€”for example, the need for a separate inheritance mechanism based on partâ€“whole relationsâ€”the approach gained the support
of Quine (1960). Harry Bunt (1985) has provided an extensive analysis of its use in knowledge representation. Casati and Varzi (1999) cover parts, wholes, and the spatial locations.
Mental objects have been the subject of intensive study in philosophy and AI. There
are three main approaches. The one taken in this chapter, based on modal logic and possible
worlds, is the classical approach from philosophy (Hintikka, 1962; Kripke, 1963; Hughes
and Cresswell, 1996). The book Reasoning about Knowledge (Fagin et al., 1995) provides a
thorough introduction. The second approach is a first-order theory in which mental objects
are fluents. Davis (2005) and Davis and Morgenstern (2005) describe this approach. It relies
on the possible-worlds formalism, and builds on work by Robert Moore (1980, 1985). The
third approach is a syntactic theory, in which mental objects are represented by character

Bibliographical and Historical Notes

471

strings. A string is just a complex term denoting a list of symbols, so CanFly (Clark ) can
be represented by the list of symbols [C, a, n, F, l, y, (, C, l, a, r, k, )]. The syntactic theory
of mental objects was first studied in depth by Kaplan and Montague (1960), who showed
that it led to paradoxes if not handled carefully. Ernie Davis (1990) provides an excellent
comparison of the syntactic and modal theories of knowledge.
The Greek philosopher Porphyry (c. 234â€“305 A . D .), commenting on Aristotleâ€™s Categories, drew what might qualify as the first semantic network. Charles S. Peirce (1909)
developed existential graphs as the first semantic network formalism using modern logic.
Ross Quillian (1961), driven by an interest in human memory and language processing, initiated work on semantic networks within AI. An influential paper by Marvin Minsky (1975)
presented a version of semantic networks called frames; a frame was a representation of
an object or category, with attributes and relations to other objects or categories. The question of semantics arose quite acutely with respect to Quillianâ€™s semantic networks (and those
of others who followed his approach), with their ubiquitous and very vague â€œIS-A linksâ€
Woodsâ€™s (1975) famous article â€œWhatâ€™s In a Link?â€ drew the attention of AI researchers to the
need for precise semantics in knowledge representation formalisms. Brachman (1979) elaborated on this point and proposed solutions. Patrick Hayesâ€™s (1979) â€œThe Logic of Framesâ€
cut even deeper, claiming that â€œMost of â€˜framesâ€™ is just a new syntax for parts of first-order
logic.â€ Drew McDermottâ€™s (1978b) â€œTarskian Semantics, or, No Notation without Denotation!â€ argued that the model-theoretic approach to semantics used in first-order logic should
be applied to all knowledge representation formalisms. This remains a controversial idea;
notably, McDermott himself has reversed his position in â€œA Critique of Pure Reasonâ€ (McDermott, 1987). Selman and Levesque (1993) discuss the complexity of inheritance with
exceptions, showing that in most formulations it is NP-complete.
The development of description logics is the most recent stage in a long line of research aimed at finding useful subsets of first-order logic for which inference is computationally tractable. Hector Levesque and Ron Brachman (1987) showed that certain logical
constructsâ€”notably, certain uses of disjunction and negationâ€”were primarily responsible
for the intractability of logical inference. Building on the KL-ONE system (Schmolze and
Lipkis, 1983), several researchers developed systems that incorporate theoretical complexity analysis, most notably K RYPTON (Brachman et al., 1983) and Classic (Borgida et al.,
1989). The result has been a marked increase in the speed of inference and a much better
understanding of the interaction between complexity and expressiveness in reasoning systems. Calvanese et al. (1999) summarize the state of the art, and Baader et al. (2007) present
a comprehensive handbook of description logic. Against this trend, Doyle and Patil (1991)
have argued that restricting the expressiveness of a language either makes it impossible to
solve certain problems or encourages the user to circumvent the language restrictions through
nonlogical means.
The three main formalisms for dealing with nonmonotonic inferenceâ€”circumscription
(McCarthy, 1980), default logic (Reiter, 1980), and modal nonmonotonic logic (McDermott
and Doyle, 1980)â€”were all introduced in one special issue of the AI Journal. Delgrande and
Schaub (2003) discuss the merits of the variants, given 25 years of hindsight. Answer set
programming can be seen as an extension of negation as failure or as a refinement of circum-

472

Chapter

12.

Knowledge Representation

scription; the underlying theory of stable model semantics was introduced by Gelfond and
Lifschitz (1988), and the leading answer set programming systems are DLV (Eiter et al., 1998)
and S MODELS (NiemelaÌˆ et al., 2000). The disk drive example comes from the S MODELS user
manual (SyrjaÌˆnen, 2000). Lifschitz (2001) discusses the use of answer set programming for
planning. Brewka et al. (1997) give a good overview of the various approaches to nonmonotonic logic. Clark (1978) covers the negation-as-failure approach to logic programming and
Clark completion. Van Emden and Kowalski (1976) show that every Prolog program without
negation has a unique minimal model. Recent years have seen renewed interest in applications of nonmonotonic logics to large-scale knowledge representation systems. The B EN I NQ
systems for handling insurance-benefit inquiries was perhaps the first commercially successful application of a nonmonotonic inheritance system (Morgenstern, 1998). Lifschitz (2001)
discusses the application of answer set programming to planning. A variety of nonmonotonic
reasoning systems based on logic programming are documented in the proceedings of the
conferences on Logic Programming and Nonmonotonic Reasoning (LPNMR).
The study of truth maintenance systems began with the TMS (Doyle, 1979) and RUP
(McAllester, 1980) systems, both of which were essentially JTMSs. Forbus and de Kleer
(1993) explain in depth how TMSs can be used in AI applications. Nayak and Williams
(1997) show how an efficient incremental TMS called an ITMS makes it feasible to plan the
operations of a NASA spacecraft in real time.
This chapter could not cover every area of knowledge representation in depth. The three
principal topics omitted are the following:
QUALITATIVE
PHYSICS

Qualitative physics: Qualitative physics is a subfield of knowledge representation concerned
specifically with constructing a logical, nonnumeric theory of physical objects and processes.
The term was coined by Johan de Kleer (1975), although the enterprise could be said to
have started in Fahlmanâ€™s (1974) B UILD , a sophisticated planner for constructing complex
towers of blocks. Fahlman discovered in the process of designing it that most of the effort
(80%, by his estimate) went into modeling the physics of the blocks world to calculate the
stability of various subassemblies of blocks, rather than into planning per se. He sketches a
hypothetical naive-physics-like process to explain why young children can solve B UILD -like
problems without access to the high-speed floating-point arithmetic used in B UILD â€™s physical
modeling. Hayes (1985a) uses â€œhistoriesâ€â€”four-dimensional slices of space-time similar to
Davidsonâ€™s eventsâ€”to construct a fairly complex naive physics of liquids. Hayes was the
first to prove that a bath with the plug in will eventually overflow if the tap keeps running and
that a person who falls into a lake will get wet all over. Davis (2008) gives an update to the
ontology of liquids that describes the pouring of liquids into containers.
De Kleer and Brown (1985), Ken Forbus (1985), and Benjamin Kuipers (1985) independently and almost simultaneously developed systems that can reason about a physical
system based on qualitative abstractions of the underlying equations. Qualitative physics
soon developed to the point where it became possible to analyze an impressive variety of
complex physical systems (Yip, 1991). Qualitative techniques have been used to construct
novel designs for clocks, windshield wipers, and six-legged walkers (Subramanian and Wang,
1994). The collection Readings in Qualitative Reasoning about Physical Systems (Weld and

Exercises

473
de Kleer, 1990) an encyclopedia article by Kuipers (2001), and a handbook article by Davis
(2007) introduce to the field.

SPATIAL REASONING

Spatial reasoning: The reasoning necessary to navigate in the wumpus world and shopping
world is trivial in comparison to the rich spatial structure of the real world. The earliest
serious attempt to capture commonsense reasoning about space appears in the work of Ernest
Davis (1986, 1990). The region connection calculus of Cohn et al. (1997) supports a form of
qualitative spatial reasoning and has led to new kinds of geographical information systems;
see also (Davis, 2006). As with qualitative physics, an agent can go a long way, so to speak,
without resorting to a full metric representation. When such a representation is necessary,
techniques developed in robotics (Chapter 25) can be used.

PSYCHOLOGICAL
REASONING

Psychological reasoning: Psychological reasoning involves the development of a working
psychology for artificial agents to use in reasoning about themselves and other agents. This
is often based on so-called folk psychology, the theory that humans in general are believed
to use in reasoning about themselves and other humans. When AI researchers provide their
artificial agents with psychological theories for reasoning about other agents, the theories are
frequently based on the researchersâ€™ description of the logical agentsâ€™ own design. Psychological reasoning is currently most useful within the context of natural language understanding,
where divining the speakerâ€™s intentions is of paramount importance.
Minker (2001) collects papers by leading researchers in knowledge representation, summarizing 40 years of work in the field. The proceedings of the international conferences on
Principles of Knowledge Representation and Reasoning provide the most up-to-date sources
for work in this area. Readings in Knowledge Representation (Brachman and Levesque,
1985) and Formal Theories of the Commonsense World (Hobbs and Moore, 1985) are excellent anthologies on knowledge representation; the former focuses more on historically
important papers in representation languages and formalisms, the latter on the accumulation
of the knowledge itself. Davis (1990), Stefik (1995), and Sowa (1999) provide textbook introductions to knowledge representation, van Harmelen et al. (2007) contributes a handbook,
and a special issue of AI Journal covers recent progress (Davis and Morgenstern, 2004). The
biennial conference on Theoretical Aspects of Reasoning About Knowledge (TARK) covers
applications of the theory of knowledge in AI, economics, and distributed systems.

E XERCISES
12.1 Define an ontology in first-order logic for tic-tac-toe. The ontology should contain
situations, actions, squares, players, marks (X, O, or blank), and the notion of winning, losing,
or drawing a game. Also define the notion of a forced win (or draw): a position from which
a player can force a win (or draw) with the right sequence of actions. Write axioms for the
domain. (Note: The axioms that enumerate the different squares and that characterize the
winning positions are rather long. You need not write these out in full, but indicate clearly
what they look like.)

474

Chapter

12.

Knowledge Representation

12.2 Figure 12.1 shows the top levels of a hierarchy for everything. Extend it to include
as many real categories as possible. A good way to do this is to cover all the things in your
everyday life. This includes objects and events. Start with waking up, and proceed in an
orderly fashion noting everything that you see, touch, do, and think about. For example,
a random sampling produces music, news, milk, walking, driving, gas, Soda Hall, carpet,
talking, Professor Fateman, chicken curry, tongue, $7, sun, the daily newspaper, and so on.
You should produce both a single hierarchy chart (on a large sheet of paper) and a
listing of objects and categories with the relations satisfied by members of each category.
Every object should be in a category, and every category should be in the hierarchy.
12.3 Develop a representational system for reasoning about windows in a window-based
computer interface. In particular, your representation should be able to describe:
â€¢
â€¢
â€¢
â€¢
â€¢

The state of a window: minimized, displayed, or nonexistent.
Which window (if any) is the active window.
The position of every window at a given time.
The order (front to back) of overlapping windows.
The actions of creating, destroying, resizing, and moving windows; changing the state
of a window; and bringing a window to the front. Treat these actions as atomic; that is,
do not deal with the issue of relating them to mouse actions. Give axioms describing
the effects of actions on fluents. You may use either event or situation calculus.

Assume an ontology containing situations, actions, integers (for x and y coordinates) and
windows. Define a language over this ontology; that is, a list of constants, function symbols,
and predicates with an English description of each. If you need to add more categories to the
ontology (e.g., pixels), you may do so, but be sure to specify these in your write-up. You may
(and should) use symbols defined in the text, but be sure to list these explicitly.
12.4

State the following in the language you developed for the previous exercise:

a. In situation S0 , window W1 is behind W2 but sticks out on the left and right. Do not
state exact coordinates for these; describe the general situation.
b. If a window is displayed, then its top edge is higher than its bottom edge.
c. After you create a window w, it is displayed.
d. A window can be minimized if it is displayed.
12.5 (Adapted from an example by Doug Lenat.) Your mission is to capture, in logical
form, enough knowledge to answer a series of questions about the following simple scenario:
Yesterday John went to the North Berkeley Safeway supermarket and bought two
pounds of tomatoes and a pound of ground beef.
Start by trying to represent the content of the sentence as a series of assertions. You should
write sentences that have straightforward logical structure (e.g., statements that objects have
certain properties, that objects are related in certain ways, that all objects satisfying one property satisfy another). The following might help you get started:

Exercises

475
â€¢ Which classes, objects, and relations would you need? What are their parents, siblings
and so on? (You will need events and temporal ordering, among other things.)
â€¢ Where would they fit in a more general hierarchy?
â€¢ What are the constraints and interrelationships among them?
â€¢ How detailed must you be about each of the various concepts?
To answer the questions below, your knowledge base must include background knowledge.
Youâ€™ll have to deal with what kind of things are at a supermarket, what is involved with
purchasing the things one selects, what the purchases will be used for, and so on. Try to make
your representation as general as possible. To give a trivial example: donâ€™t say â€œPeople buy
food from Safeway,â€ because that wonâ€™t help you with those who shop at another supermarket.
Also, donâ€™t turn the questions into answers; for example, question (c) asks â€œDid John buy any
meat?â€â€”not â€œDid John buy a pound of ground beef?â€
Sketch the chains of reasoning that would answer the questions. If possible, use a
logical reasoning system to demonstrate the sufficiency of your knowledge base. Many of the
things you write might be only approximately correct in reality, but donâ€™t worry too much;
the idea is to extract the common sense that lets you answer these questions at all. A truly
complete answer to this question is extremely difficult, probably beyond the state of the art of
current knowledge representation. But you should be able to put together a consistent set of
axioms for the limited questions posed here.
a.
b.
c.
d.
e.
f.
g.
h.
i.

Is John a child or an adult? [Adult]
Does John now have at least two tomatoes? [Yes]
Did John buy any meat? [Yes]
If Mary was buying tomatoes at the same time as John, did he see her? [Yes]
Are the tomatoes made in the supermarket? [No]
What is John going to do with the tomatoes? [Eat them]
Does Safeway sell deodorant? [Yes]
Did John bring some money or a credit card to the supermarket? [Yes]
Does John have less money after going to the supermarket? [Yes]

12.6 Make the necessary additions or changes to your knowledge base from the previous
exercise so that the questions that follow can be answered. Include in your report a discussion
of your changes, explaining why they were needed, whether they were minor or major, and
what kinds of questions would necessitate further changes.
a.
b.
c.
d.
e.
f.

Are there other people in Safeway while John is there? [Yesâ€”staff!]
Is John a vegetarian? [No]
Who owns the deodorant in Safeway? [Safeway Corporation]
Did John have an ounce of ground beef? [Yes]
Does the Shell station next door have any gas? [Yes]
Do the tomatoes fit in Johnâ€™s car trunk? [Yes]

476

Chapter

12.

Knowledge Representation

12.7 Represent the following seven sentences using and extending the representations developed in the chapter:
a.
b.
c.
d.
e.
f.
g.
12.8

Water is a liquid between 0 and 100 degrees.
Water boils at 100 degrees.
The water in Johnâ€™s water bottle is frozen.
Perrier is a kind of water.
John has Perrier in his water bottle.
All liquids have a freezing point.
A liter of water weighs more than a liter of alcohol.
Write definitions for the following:

a. ExhaustivePartDecomposition
b. PartPartition
c. PartwiseDisjoint
These should be analogous to the definitions for ExhaustiveDecomposition , Partition, and
Disjoint. Is it the case that PartPartition(s, BunchOf (s))? If so, prove it; if not, give a
counterexample and define sufficient conditions under which it does hold.
12.9 An alternative scheme for representing measures involves applying the units function
to an abstract length object. In such a scheme, one would write Inches(Length(L1 )) =
1.5. How does this scheme compare with the one in the chapter? Issues include conversion
axioms, names for abstract quantities (such as â€œ50 dollarsâ€), and comparisons of abstract
measures in different units (50 inches is more than 50 centimeters).
12.10 Add sentences to extend the definition of the predicate Name(s, c) so that a string
such as â€œlaptop computerâ€ matches the appropriate category names from a variety of stores.
Try to make your definition general. Test it by looking at ten online stores, and at the category
names they give for three different categories. For example, for the category of laptops, we
found the names â€œNotebooks,â€ â€œLaptops,â€ â€œNotebook Computers,â€ â€œNotebook,â€ â€œLaptops
and Notebooks,â€ and â€œNotebook PCs.â€ Some of these can be covered by explicit Name facts,
while others could be covered by sentences for handling plurals, conjunctions, etc.
12.11

Write event calculus axioms to describe the actions in the wumpus world.

12.12 State the interval-algebra relation that holds between every pair of the following realworld events:
LK: The life of President Kennedy.
IK: The infancy of President Kennedy.
P K: The presidency of President Kennedy.
LJ: The life of President Johnson.
P J: The presidency of President Johnson.
LO: The life of President Obama.

Exercises

477
12.13 Investigate ways to extend the event calculus to handle simultaneous events. Is it
possible to avoid a combinatorial explosion of axioms?
12.14 Construct a representation for exchange rates between currencies that allows for daily
fluctuations.
12.15 Define the predicate Fixed , where Fixed (Location (x)) means that the location of
object x is fixed over time.
12.16 Describe the event of trading something for something else. Describe buying as a
kind of trading in which one of the objects traded is a sum of money.
12.17 The two preceding exercises assume a fairly primitive notion of ownership. For example, the buyer starts by owning the dollar bills. This picture begins to break down when,
for example, oneâ€™s money is in the bank, because there is no longer any specific collection
of dollar bills that one owns. The picture is complicated still further by borrowing, leasing,
renting, and bailment. Investigate the various commonsense and legal concepts of ownership,
and propose a scheme by which they can be represented formally.
12.18 (Adapted from Fagin et al. (1995).) Consider a game played with a deck of just 8
cards, 4 aces and 4 kings. The three players, Alice, Bob, and Carlos, are dealt two cards each.
Without looking at them, they place the cards on their foreheads so that the other players can
see them. Then the players take turns either announcing that they know what cards are on
their own forehead, thereby winning the game, or saying â€œI donâ€™t know.â€ Everyone knows
the players are truthful and are perfect at reasoning about beliefs.
a. Game 1. Alice and Bob have both said â€œI donâ€™t know.â€ Carlos sees that Alice has two
aces (A-A) and Bob has two kings (K-K). What should Carlos say? (Hint: consider all
three possible cases for Carlos: A-A, K-K, A-K.)
b. Describe each step of Game 1 using the notation of modal logic.
c. Game 2. Carlos, Alice, and Bob all said â€œI donâ€™t knowâ€ on their first turn. Alice holds
K-K and Bob holds A-K. What should Carlos say on his second turn?
d. Game 3. Alice, Carlos, and Bob all say â€œI donâ€™t knowâ€ on their first turn, as does Alice
on her second turn. Alice and Bob both hold A-K. What should Carlos say?
e. Prove that there will always be a winner to this game.
12.19 The assumption of logical omniscience, discussed on page 453, is of course not true
of any actual reasoners. Rather, it is an idealization of the reasoning process that may be
more or less acceptable depending on the applications. Discuss the reasonableness of the
assumption for each of the following applications of reasoning about knowledge:
a. Partial knowledge adversary games, such as card games. Here one player wants to
reason about what his opponent knows about the state of the game.
b. Chess with a clock. Here the player may wish to reason about the limits of his opponentâ€™s or his own ability to find the best move in the time available. For instance, if
player A has much more time left than player B, then A will sometimes make a move
that greatly complicates the situation, in the hopes of gaining an advantage because he
has more time to work out the proper strategy.

478

Chapter

12.

Knowledge Representation

c. A shopping agent in an environment in which there are costs of gathering information.
d. Reasoning about public key cryptography, which rests on the intractability of certain
computational problems.
12.20 Translate the following description logic expression (from page 457) into first-order
logic, and comment on the result:
And(Man, AtLeast (3, Son), AtMost(2, Daughter ),
All(Son, And(Unemployed , Married , All(Spouse, Doctor ))),
All(Daughter , And(Professor , Fills(Department , Physics, Math)))) .
12.21 Recall that inheritance information in semantic networks can be captured logically
by suitable implication sentences. This exercise investigates the efficiency of using such
sentences for inheritance.
a. Consider the information in a used-car catalog such as Kellyâ€™s Blue Bookâ€”for example, that 1973 Dodge vans are (or perhaps were once) worth $575. Suppose all this
information (for 11,000 models) is encoded as logical sentences, as suggested in the
chapter. Write down three such sentences, including that for 1973 Dodge vans. How
would you use the sentences to find the value of a particular car, given a backwardchaining theorem prover such as Prolog?
b. Compare the time efficiency of the backward-chaining method for solving this problem
with the inheritance method used in semantic nets.
c. Explain how forward chaining allows a logic-based system to solve the same problem
efficiently, assuming that the KB contains only the 11,000 sentences about prices.
d. Describe a situation in which neither forward nor backward chaining on the sentences
will allow the price query for an individual car to be handled efficiently.
e. Can you suggest a solution enabling this type of query to be solved efficiently in all
cases in logic systems? (Hint: Remember that two cars of the same year and model
have the same price.)
12.22 One might suppose that the syntactic distinction between unboxed links and singly
boxed links in semantic networks is unnecessary, because singly boxed links are always attached to categories; an inheritance algorithm could simply assume that an unboxed link
attached to a category is intended to apply to all members of that category. Show that this
argument is fallacious, giving examples of errors that would arise.
12.23 One part of the shopping process that was not covered in this chapter is checking
for compatibility between items. For example, if a digital camera is ordered, what accessory
batteries, memory cards, and cases are compatible with the camera? Write a knowledge base
that can determine the compatibility of a set of items and suggest replacements or additional
items if the shopper makes a choice that is not compatible. The knowledge base should works
with at least one line of products and extend easily to other lines.
12.24 A complete solution to the problem of inexact matches to the buyerâ€™s description
in shopping is very difficult and requires a full array of natural language processing and

Exercises

479
information retrieval techniques. (See Chapters 22 and 23.) One small step is to allow the
user to specify minimum and maximum values for various attributes. The buyer must use the
following grammar for product descriptions:
Description
Connector
Modiï¬er
Op

â†’
â†’
â†’
â†’

Category [Connector Modiï¬er ]âˆ—
â€œwithâ€ | â€œandâ€ | â€œ,â€
Attribute | Attribute Op Value
â€œ=â€ | â€œ>â€ | â€œ<â€

Here, Category names a product category, Attribute is some feature such as â€œCPUâ€ or
â€œprice,â€ and Value is the target value for the attribute. So the query â€œcomputer with at least a
2.5 GHz CPU for under $500â€ must be re-expressed as â€œcomputer with CPU > 2.5 GHz and
price < $500.â€ Implement a shopping agent that accepts descriptions in this language.
12.25 Our description of Internet shopping omitted the all-important step of actually buying
the product. Provide a formal logical description of buying, using event calculus. That is,
define the sequence of events that occurs when a buyer submits a credit-card purchase and
then eventually gets billed and receives the product.

13

QUANTIFYING
UNCERTAINTY

In which we see how an agent can tame uncertainty with degrees of belief.

13.1
UNCERTAINTY

ACTING UNDER U NCERTAINTY
Agents may need to handle uncertainty, whether due to partial observability, nondeterminism, or a combination of the two. An agent may never know for certain what state itâ€™s in or
where it will end up after a sequence of actions.
We have seen problem-solving agents (Chapter 4) and logical agents (Chapters 7 and 11)
designed to handle uncertainty by keeping track of a belief stateâ€”a representation of the set
of all possible world states that it might be inâ€”and generating a contingency plan that handles every possible eventuality that its sensors may report during execution. Despite its many
virtues, however, this approach has significant drawbacks when taken literally as a recipe for
creating agent programs:
â€¢ When interpreting partial sensor information, a logical agent must consider every logically possible explanation for the observations, no matter how unlikely. This leads to
impossible large and complex belief-state representations.
â€¢ A correct contingent plan that handles every eventuality can grow arbitrarily large and
must consider arbitrarily unlikely contingencies.
â€¢ Sometimes there is no plan that is guaranteed to achieve the goalâ€”yet the agent must
act. It must have some way to compare the merits of plans that are not guaranteed.
Suppose, for example, that an automated taxi!automated has the goal of delivering a passenger to the airport on time. The agent forms a plan, A90 , that involves leaving home 90
minutes before the flight departs and driving at a reasonable speed. Even though the airport
is only about 5 miles away, a logical taxi agent will not be able to conclude with certainty
that â€œPlan A90 will get us to the airport in time.â€ Instead, it reaches the weaker conclusion
â€œPlan A90 will get us to the airport in time, as long as the car doesnâ€™t break down or run out
of gas, and I donâ€™t get into an accident, and there are no accidents on the bridge, and the plane
doesnâ€™t leave early, and no meteorite hits the car, and . . . .â€ None of these conditions can be
480

Section 13.1.

Acting under Uncertainty

481

deduced for sure, so the planâ€™s success cannot be inferred. This is the qualification problem
(page 268), for which we so far have seen no real solution.
Nonetheless, in some sense A90 is in fact the right thing to do. What do we mean by
this? As we discussed in Chapter 2, we mean that out of all the plans that could be executed,
A90 is expected to maximize the agentâ€™s performance measure (where the expectation is relative to the agentâ€™s knowledge about the environment). The performance measure includes
getting to the airport in time for the flight, avoiding a long, unproductive wait at the airport,
and avoiding speeding tickets along the way. The agentâ€™s knowledge cannot guarantee any of
these outcomes for A90 , but it can provide some degree of belief that they will be achieved.
Other plans, such as A180 , might increase the agentâ€™s belief that it will get to the airport on
time, but also increase the likelihood of a long wait. The right thing to doâ€”the rational
decisionâ€”therefore depends on both the relative importance of various goals and the likelihood that, and degree to which, they will be achieved. The remainder of this section hones
these ideas, in preparation for the development of the general theories of uncertain reasoning
and rational decisions that we present in this and subsequent chapters.

13.1.1 Summarizing uncertainty
Letâ€™s consider an example of uncertain reasoning: diagnosing a dental patientâ€™s toothache.
Diagnosisâ€”whether for medicine, automobile repair, or whateverâ€”almost always involves
uncertainty. Let us try to write rules for dental diagnosis using propositional logic, so that we
can see how the logical approach breaks down. Consider the following simple rule:
Toothache â‡’ Cavity .
The problem is that this rule is wrong. Not all patients with toothaches have cavities; some
of them have gum disease, an abscess, or one of several other problems:
Toothache â‡’ Cavity âˆ¨ GumProblem âˆ¨ Abscess . . .
Unfortunately, in order to make the rule true, we have to add an almost unlimited list of
possible problems. We could try turning the rule into a causal rule:
Cavity â‡’ Toothache .
But this rule is not right either; not all cavities cause pain. The only way to fix the rule
is to make it logically exhaustive: to augment the left-hand side with all the qualifications
required for a cavity to cause a toothache. Trying to use logic to cope with a domain like
medical diagnosis thus fails for three main reasons:
LAZINESS

THEORETICAL
IGNORANCE
PRACTICAL
IGNORANCE

â€¢ Laziness: It is too much work to list the complete set of antecedents or consequents
needed to ensure an exceptionless rule and too hard to use such rules.
â€¢ Theoretical ignorance: Medical science has no complete theory for the domain.
â€¢ Practical ignorance: Even if we know all the rules, we might be uncertain about a
particular patient because not all the necessary tests have been or can be run.
The connection between toothaches and cavities is just not a logical consequence in either
direction. This is typical of the medical domain, as well as most other judgmental domains:
law, business, design, automobile repair, gardening, dating, and so on. The agentâ€™s knowledge

482
DEGREE OF BELIEF
PROBABILITY
THEORY

Chapter

13.

Quantifying Uncertainty

can at best provide only a degree of belief in the relevant sentences. Our main tool for
dealing with degrees of belief is probability theory. In the terminology of Section 8.1, the
ontological commitments of logic and probability theory are the sameâ€”that the world is
composed of facts that do or do not hold in any particular caseâ€”but the epistemological
commitments are different: a logical agent believes each sentence to be true or false or has
no opinion, whereas a probabilistic agent may have a numerical degree of belief between 0
(for sentences that are certainly false) and 1 (certainly true).
Probability provides a way of summarizing the uncertainty that comes from our laziness and ignorance, thereby solving the qualification problem. We might not know for sure
what afflicts a particular patient, but we believe that there is, say, an 80% chanceâ€”that is,
a probability of 0.8â€”that the patient who has a toothache has a cavity. That is, we expect
that out of all the situations that are indistinguishable from the current situation as far as our
knowledge goes, the patient will have a cavity in 80% of them. This belief could be derived
from statistical dataâ€”80% of the toothache patients seen so far have had cavitiesâ€”or from
some general dental knowledge, or from a combination of evidence sources.
One confusing point is that at the time of our diagnosis, there is no uncertainty in the
actual world: the patient either has a cavity or doesnâ€™t. So what does it mean to say the
probability of a cavity is 0.8? Shouldnâ€™t it be either 0 or 1? The answer is that probability
statements are made with respect to a knowledge state, not with respect to the real world. We
say â€œThe probability that the patient has a cavity, given that she has a toothache, is 0.8.â€ If we
later learn that the patient has a history of gum disease, we can make a different statement:
â€œThe probability that the patient has a cavity, given that she has a toothache and a history of
gum disease, is 0.4.â€ If we gather further conclusive evidence against a cavity, we can say
â€œThe probability that the patient has a cavity, given all we now know, is almost 0.â€ Note that
these statements do not contradict each other; each is a separate assertion about a different
knowledge state.

13.1.2 Uncertainty and rational decisions

PREFERENCE
OUTCOME

UTILITY THEORY

Consider again the A90 plan for getting to the airport. Suppose it gives us a 97% chance
of catching our flight. Does this mean it is a rational choice? Not necessarily: there might
be other plans, such as A180 , with higher probabilities. If it is vital not to miss the flight,
then it is worth risking the longer wait at the airport. What about A1440 , a plan that involves
leaving home 24 hours in advance? In most circumstances, this is not a good choice, because
although it almost guarantees getting there on time, it involves an intolerable waitâ€”not to
mention a possibly unpleasant diet of airport food.
To make such choices, an agent must first have preferences between the different possible outcomes of the various plans. An outcome is a completely specified state, including
such factors as whether the agent arrives on time and the length of the wait at the airport. We
use utility theory to represent and reason with preferences. (The term utility is used here in
the sense of â€œthe quality of being useful,â€ not in the sense of the electric company or water
works.) Utility theory says that every state has a degree of usefulness, or utility, to an agent
and that the agent will prefer states with higher utility.

Section 13.2.

DECISION THEORY

Basic Probability Notation

483

The utility of a state is relative to an agent. For example, the utility of a state in which
White has checkmated Black in a game of chess is obviously high for the agent playing White,
but low for the agent playing Black. But we canâ€™t go strictly by the scores of 1, 1/2, and 0 that
are dictated by the rules of tournament chessâ€”some players (including the authors) might be
thrilled with a draw against the world champion, whereas other players (including the former
world champion) might not. There is no accounting for taste or preferences: you might think
that an agent who prefers jalapenÌƒo bubble-gum ice cream to chocolate chocolate chip is odd
or even misguided, but you could not say the agent is irrational. A utility function can account
for any set of preferencesâ€”quirky or typical, noble or perverse. Note that utilities can account
for altruism, simply by including the welfare of others as one of the factors.
Preferences, as expressed by utilities, are combined with probabilities in the general
theory of rational decisions called decision theory:
Decision theory = probability theory + utility theory .

MAXIMUM EXPECTED
UTILITY

13.2

The fundamental idea of decision theory is that an agent is rational if and only if it chooses
the action that yields the highest expected utility, averaged over all the possible outcomes
of the action. This is called the principle of maximum expected utility (MEU). Note that
â€œexpectedâ€ might seem like a vague, hypothetical term, but as it is used here it has a precise
meaning: it means the â€œaverage,â€ or â€œstatistical meanâ€ of the outcomes, weighted by the
probability of the outcome. We saw this principle in action in Chapter 5 when we touched
briefly on optimal decisions in backgammon; it is in fact a completely general principle.
Figure 13.1 sketches the structure of an agent that uses decision theory to select actions.
The agent is identical, at an abstract level, to the agents described in Chapters 4 and 7 that
maintain a belief state reflecting the history of percepts to date. The primary difference is
that the decision-theoretic agentâ€™s belief state represents not just the possibilities for world
states but also their probabilities. Given the belief state, the agent can make probabilistic
predictions of action outcomes and hence select the action with highest expected utility. This
chapter and the next concentrate on the task of representing and computing with probabilistic
information in general. Chapter 15 deals with methods for the specific tasks of representing
and updating the belief state over time and predicting the environment. Chapter 16 covers
utility theory in more depth, and Chapter 17 develops algorithms for planning sequences of
actions in uncertain environments.

BASIC P ROBABILITY N OTATION
For our agent to represent and use probabilistic information, we need a formal language.
The language of probability theory has traditionally been informal, written by human mathematicians to other human mathematicians. Appendix A includes a standard introduction to
elementary probability theory; here, we take an approach more suited to the needs of AI and
more consistent with the concepts of formal logic.

484

Chapter

13.

Quantifying Uncertainty

function DT-AGENT( percept ) returns an action
persistent: belief state, probabilistic beliefs about the current state of the world
action, the agentâ€™s action
update belief state based on action and percept
calculate outcome probabilities for actions,
given action descriptions and current belief state
select action with highest expected utility
given probabilities of outcomes and utility information
return action
Figure 13.1

A decision-theoretic agent that selects rational actions.

13.2.1 What probabilities are about

SAMPLE SPACE

PROBABILITY MODEL

Like logical assertions, probabilistic assertions are about possible worlds. Whereas logical
assertions say which possible worlds are strictly ruled out (all those in which the assertion is
false), probabilistic assertions talk about how probable the various worlds are. In probability
theory, the set of all possible worlds is called the sample space. The possible worlds are
mutually exclusive and exhaustiveâ€”two possible worlds cannot both be the case, and one
possible world must be the case. For example, if we are about to roll two (distinguishable)
dice, there are 36 possible worlds to consider: (1,1), (1,2), . . ., (6,6). The Greek letter Î©
(uppercase omega) is used to refer to the sample space, and Ï‰ (lowercase omega) refers to
elements of the space, that is, particular possible worlds.
A fully specified probability model associates a numerical probability P (Ï‰) with each
possible world.1 The basic axioms of probability theory say that every possible world has a
probability between 0 and 1 and that the total probability of the set of possible worlds is 1:

P (Ï‰) = 1 .
(13.1)
0 â‰¤ P (Ï‰) â‰¤ 1 for every Ï‰ and
Ï‰âˆˆÎ©

EVENT

For example, if we assume that each die is fair and the rolls donâ€™t interfere with each other,
then each of the possible worlds (1,1), (1,2), . . ., (6,6) has probability 1/36. On the other
hand, if the dice conspire to produce the same number, then the worlds (1,1), (2,2), (3,3), etc.,
might have higher probabilities, leaving the others with lower probabilities.
Probabilistic assertions and queries are not usually about particular possible worlds, but
about sets of them. For example, we might be interested in the cases where the two dice add
up to 11, the cases where doubles are rolled, and so on. In probability theory, these sets are
called eventsâ€”a term already used extensively in Chapter 12 for a different concept. In AI,
the sets are always described by propositions in a formal language. (One such language is
described in Section 13.2.2.) For each proposition, the corresponding set contains just those
possible worlds in which the proposition holds. The probability associated with a proposition
1

For now, we assume a discrete, countable set of worlds. The proper treatment of the continuous case brings in
certain complications that are less relevant for most purposes in AI.

Section 13.2.

Basic Probability Notation

485

is defined to be the sum of the probabilities of the worlds in which it holds:

P (Ï‰) .
For any proposition Ï†, P (Ï†) =

(13.2)

Ï‰âˆˆÏ†

UNCONDITIONAL
PROBABILITY
PRIOR PROBABILITY

EVIDENCE

CONDITIONAL
PROBABILITY
POSTERIOR
PROBABILITY

For example, when rolling fair dice, we have P (Total = 11) = P ((5, 6)) + P ((6, 5)) =
1/36 + 1/36 = 1/18. Note that probability theory does not require complete knowledge
of the probabilities of each possible world. For example, if we believe the dice conspire to
produce the same number, we might assert that P (doubles) = 1/4 without knowing whether
the dice prefer double 6 to double 2. Just as with logical assertions, this assertion constrains
the underlying probability model without fully determining it.
Probabilities such as P (Total = 11) and P (doubles) are called unconditional or prior
probabilities (and sometimes just â€œpriorsâ€ for short); they refer to degrees of belief in propositions in the absence of any other information. Most of the time, however, we have some
information, usually called evidence, that has already been revealed. For example, the first
die may already be showing a 5 and we are waiting with bated breath for the other one to
stop spinning. In that case, we are interested not in the unconditional probability of rolling
doubles, but the conditional or posterior probability (or just â€œposteriorâ€ for short) of rolling
doubles given that the first die is a 5. This probability is written P (doubles | Die 1 = 5), where
the â€œ | â€ is pronounced â€œgiven.â€ Similarly, if I am going to the dentist for a regular checkup,
the probability P (cavity) = 0.2 might be of interest; but if I go to the dentist because I have
a toothache, itâ€™s P (cavity | toothache ) = 0.6 that matters. Note that the precedence of â€œ | â€ is
such that any expression of the form P (. . . | . . .) always means P ((. . .)|(. . .)).
It is important to understand that P (cavity) = 0.2 is still valid after toothache is observed; it just isnâ€™t especially useful. When making decisions, an agent needs to condition
on all the evidence it has observed. It is also important to understand the difference between conditioning and logical implication. The assertion that P (cavity | toothache ) = 0.6
does not mean â€œWhenever toothache is true, conclude that cavity is true with probability 0.6â€ rather it means â€œWhenever toothache is true and we have no further information,
conclude that cavity is true with probability 0.6.â€ The extra condition is important; for example, if we had the further information that the dentist found no cavities, we definitely
would not want to conclude that cavity is true with probability 0.6; instead we need to use
P (cavity|toothache âˆ§ Â¬cavity) = 0.
Mathematically speaking, conditional probabilities are defined in terms of unconditional probabilities as follows: for any propositions a and b, we have
P (a | b) =

P (a âˆ§ b)
,
P (b)

(13.3)

which holds whenever P (b) > 0. For example,
P (doubles | Die 1 = 5) =

P (doubles âˆ§ Die 1 = 5)
.
P (Die 1 = 5)

The definition makes sense if you remember that observing b rules out all those possible
worlds where b is false, leaving a set whose total probability is just P (b). Within that set, the
a-worlds satisfy a âˆ§ b and constitute a fraction P (a âˆ§ b)/P (b).

486

PRODUCT RULE

Chapter

13.

Quantifying Uncertainty

The definition of conditional probability, Equation (13.3), can be written in a different
form called the product rule:
P (a âˆ§ b) = P (a | b)P (b) ,
The product rule is perhaps easier to remember: it comes from the fact that, for a and b to be
true, we need b to be true, and we also need a to be true given b.

13.2.2 The language of propositions in probability assertions

RANDOM VARIABLE

DOMAIN

In this chapter and the next, propositions describing sets of possible worlds are written in a
notation that combines elements of propositional logic and constraint satisfaction notation. In
the terminology of Section 2.4.7, it is a factored representation, in which a possible world
is represented by a set of variable/value pairs.
Variables in probability theory are called random variables and their names begin with
an uppercase letter. Thus, in the dice example, Total and Die 1 are random variables. Every
random variable has a domainâ€”the set of possible values it can take on. The domain of
Total for two dice is the set {2, . . . , 12} and the domain of Die 1 is {1, . . . , 6}. A Boolean
random variable has the domain {true, false} (notice that values are always lowercase); for
example, the proposition that doubles are rolled can be written as Doubles = true. By convention, propositions of the form A = true are abbreviated simply as a, while A = false is
abbreviated as Â¬a. (The uses of doubles, cavity, and toothache in the preceding section are
abbreviations of this kind.) As in CSPs, domains can be sets of arbitrary tokens; we might
choose the domain of Age to be {juvenile, teen, adult } and the domain of Weather might
be {sunny, rain, cloudy, snow }. When no ambiguity is possible, it is common to use a value
by itself to stand for the proposition that a particular variable has that value; thus, sunny can
stand for Weather = sunny.
The preceding examples all have finite domains. Variables can have infinite domains,
tooâ€”either discrete (like the integers) or continuous (like the reals). For any variable with an
ordered domain, inequalities are also allowed, such as NumberOfAtomsInUniverse â‰¥ 1070 .
Finally, we can combine these sorts of elementary propositions (including the abbreviated forms for Boolean variables) by using the connectives of propositional logic. For
example, we can express â€œThe probability that the patient has a cavity, given that she is a
teenager with no toothache, is 0.1â€ as follows:
P (cavity | Â¬toothache âˆ§ teen) = 0.1 .
Sometimes we will want to talk about the probabilities of all the possible values of a random
variable. We could write:
P (Weather = sunny) = 0.6
P (Weather = rain) = 0.1
P (Weather = cloudy) = 0.29
P (Weather = snow ) = 0.01 ,
but as an abbreviation we will allow
P(Weather ) = 0.6, 0.1, 0.29, 0.01 ,

Section 13.2.

PROBABILITY
DISTRIBUTION

Basic Probability Notation

487

where the bold P indicates that the result is a vector of numbers, and where we assume a predefined ordering sunny, rain, cloudy , snow  on the domain of Weather . We say that the
P statement defines a probability distribution for the random variable Weather . The P notation is also used for conditional distributions: P(X | Y ) gives the values of P (X = xi | Y = yj )
for each possible i, j pair.
For continuous variables, it is not possible to write out the entire distribution as a vector,
because there are infinitely many values. Instead, we can define the probability that a random
variable takes on some value x as a parameterized function of x. For example, the sentence
P (NoonTemp = x) = Uniform [18C,26C] (x)

PROBABILITY
DENSITY FUNCTION

expresses the belief that the temperature at noon is distributed uniformly between 18 and 26
degrees Celsius. We call this a probability density function.
Probability density functions (sometimes called pdfs) differ in meaning from discrete
distributions. Saying that the probability density is uniform from 18C to 26C means that
there is a 100% chance that the temperature will fall somewhere in that 8C-wide region
and a 50% chance that it will fall in any 4C-wide region, and so on. We write the probability
density for a continuous random variable X at value x as P (X = x) or just P (x); the intuitive
definition of P (x) is the probability that X falls within an arbitrarily small region beginning
at x, divided by the width of the region:
P (x) = lim P (x â‰¤ X â‰¤ x + dx)/dx .
dxâ†’0

For NoonTemp we have

P (NoonTemp = x) = Uniform [18C,26C] (x) =

JOINT PROBABILITY
DISTRIBUTION

if 18C â‰¤ x â‰¤ 26C
,
0 otherwise
1
8C

1
, note
where C stands for centigrade (not for a constant). In P (NoonTemp = 20.18C) = 8C
1
that 8C is not a probability, it is a probability density. The probability that NoonTemp is
exactly 20.18C is zero, because 20.18C is a region of width 0. Some authors use different
symbols for discrete distributions and density functions; we use P in both cases, since confusion seldom arises and the equations are usually identical. Note that probabilities are unitless
numbers, whereas density functions are measured with a unit, in this case reciprocal degrees.
In addition to distributions on single variables, we need notation for distributions on
multiple variables. Commas are used for this. For example, P(Weather , Cavity) denotes
the probabilities of all combinations of the values of Weather and Cavity. This is a 4 Ã— 2
table of probabilities called the joint probability distribution of Weather and Cavity. We
can also mix variables with and without values; P(sunny, Cavity) would be a two-element
vector giving the probabilities of a sunny day with a cavity and a sunny day with no cavity.
The P notation makes certain expressions much more concise than they might otherwise be.
For example, the product rules for all possible values of Weather and Cavity can be written
as a single equation:

P(Weather , Cavity) = P(Weather | Cavity)P(Cavity) ,

488

Chapter

13.

Quantifying Uncertainty

instead of as these 4 Ã— 2 = 8 equations (using abbreviations W and C):
P (W
P (W
P (W
P (W
P (W
P (W
P (W
P (W

FULL JOINT
PROBABILITY
DISTRIBUTION

= sunny âˆ§ C = true) = P (W = sunny|C = true) P (C = true)
= rain âˆ§ C = true) = P (W = rain|C = true) P (C = true)
= cloudy âˆ§ C = true) = P (W = cloudy|C = true) P (C = true)
= snow âˆ§ C = true) = P (W = snow |C = true) P (C = true)
= sunny âˆ§ C = false) = P (W = sunny|C = false) P (C = false)
= rain âˆ§ C = false) = P (W = rain|C = false) P (C = false)
= cloudy âˆ§ C = false) = P (W = cloudy|C = false) P (C = false)
= snow âˆ§ C = false) = P (W = snow |C = false) P (C = false) .

As a degenerate case, P(sunny, cavity) has no variables and thus is a one-element vector that is the probability of a sunny day with a cavity, which could also be written as
P (sunny, cavity) or P (sunny âˆ§ cavity). We will sometimes use P notation to derive results
about individual P values, and when we say â€œP(sunny) = 0.6â€ it is really an abbreviation for
â€œP(sunny) is the one-element vector 0.6, which means that P (sunny) = 0.6.â€
Now we have defined a syntax for propositions and probability assertions and we have
given part of the semantics: Equation (13.2) defines the probability of a proposition as the sum
of the probabilities of worlds in which it holds. To complete the semantics, we need to say
what the worlds are and how to determine whether a proposition holds in a world. We borrow
this part directly from the semantics of propositional logic, as follows. A possible world is
defined to be an assignment of values to all of the random variables under consideration. It is
easy to see that this definition satisfies the basic requirement that possible worlds be mutually
exclusive and exhaustive (Exercise 13.5). For example, if the random variables are Cavity,
Toothache , and Weather , then there are 2 Ã— 2 Ã— 4 = 16 possible worlds. Furthermore, the
truth of any given proposition, no matter how complex, can be determined easily in such
worlds using the same recursive definition of truth as for formulas in propositional logic.
From the preceding definition of possible worlds, it follows that a probability model is
completely determined by the joint distribution for all of the random variablesâ€”the so-called
full joint probability distribution. For example, if the variables are Cavity, Toothache ,
and Weather , then the full joint distribution is given by P(Cavity, Toothache , Weather ).
This joint distribution can be represented as a 2 Ã— 2 Ã— 4 table with 16 entries. Because every
propositionâ€™s probability is a sum over possible worlds, a full joint distribution suffices, in
principle, for calculating the probability of any proposition.

13.2.3 Probability axioms and their reasonableness
The basic axioms of probability (Equations (13.1) and (13.2)) imply certain relationships
among the degrees of belief that can be accorded to logically related propositions. For example, we can derive the familiar relationship between the probability of a proposition and the
probability of its negation:

P (Â¬a) = Ï‰âˆˆÂ¬a P (Ï‰) 
by Equation (13.2)

= Ï‰âˆˆÂ¬a P (Ï‰) + Ï‰âˆˆa P (Ï‰) âˆ’ Ï‰âˆˆa P (Ï‰)
grouping the first two terms
=
Ï‰âˆˆÎ© P (Ï‰) âˆ’
Ï‰âˆˆa P (Ï‰)
= 1 âˆ’ P (a)
by (13.1) and (13.2).

Section 13.2.

INCLUSIONâ€“
EXCLUSION
PRINCIPLE

Basic Probability Notation

489

We can also derive the well-known formula for the probability of a disjunction, sometimes
called the inclusionâ€“exclusion principle:
P (a âˆ¨ b) = P (a) + P (b) âˆ’ P (a âˆ§ b) .

KOLMOGOROVâ€™S
AXIOMS

(13.4)

This rule is easily remembered by noting that the cases where a holds, together with the cases
where b holds, certainly cover all the cases where a âˆ¨ b holds; but summing the two sets of
cases counts their intersection twice, so we need to subtract P (a âˆ§ b). The proof is left as an
exercise (Exercise 13.6).
Equations (13.1) and (13.4) are often called Kolmogorovâ€™s axioms in honor of the Russian mathematician Andrei Kolmogorov, who showed how to build up the rest of probability
theory from this simple foundation and how to handle the difficulties caused by continuous
variables.2 While Equation (13.2) has a definitional flavor, Equation (13.4) reveals that the
axioms really do constrain the degrees of belief an agent can have concerning logically related propositions. This is analogous to the fact that a logical agent cannot simultaneously
believe A, B, and Â¬(A âˆ§ B), because there is no possible world in which all three are true.
With probabilities, however, statements refer not to the world directly, but to the agentâ€™s own
state of knowledge. Why, then, can an agent not hold the following set of beliefs (even though
they violate Kolmogorovâ€™s axioms)?
P (a) = 0.4
P (b) = 0.3

P (a âˆ§ b) = 0.0
P (a âˆ¨ b) = 0.8 .

(13.5)

This kind of question has been the subject of decades of intense debate between those who
advocate the use of probabilities as the only legitimate form for degrees of belief and those
who advocate alternative approaches.
One argument for the axioms of probability, first stated in 1931 by Bruno de Finetti
(and translated into English in de Finetti (1993)), is as follows: If an agent has some degree of
belief in a proposition a, then the agent should be able to state odds at which it is indifferent
to a bet for or against a.3 Think of it as a game between two agents: Agent 1 states, â€œmy
degree of belief in event a is 0.4.â€ Agent 2 is then free to choose whether to wager for or
against a at stakes that are consistent with the stated degree of belief. That is, Agent 2 could
choose to accept Agent 1â€™s bet that a will occur, offering $6 against Agent 1â€™s $4. Or Agent
2 could accept Agent 1â€™s bet that Â¬a will occur, offering $4 against Agent 1â€™s $6. Then we
observe the outcome of a, and whoever is right collects the money. If an agentâ€™s degrees of
belief do not accurately reflect the world, then you would expect that it would tend to lose
money over the long run to an opposing agent whose beliefs more accurately reflect the state
of the world.
But de Finetti proved something much stronger: If Agent 1 expresses a set of degrees
of belief that violate the axioms of probability theory then there is a combination of bets by
Agent 2 that guarantees that Agent 1 will lose money every time. For example, suppose that
Agent 1 has the set of degrees of belief from Equation (13.5). Figure 13.2 shows that if Agent
The difficulties include the Vitali set, a well-defined subset of the interval [0, 1] with no well-defined size.
One might argue that the agentâ€™s preferences for different bank balances are such that the possibility of losing
$1 is not counterbalanced by an equal possibility of winning $1. One possible response is to make the bet amounts
small enough to avoid this problem. Savageâ€™s analysis (1954) circumvents the issue altogether.
2
3

490

Chapter

13.

Quantifying Uncertainty

2 chooses to bet $4 on a, $3 on b, and $2 on Â¬(a âˆ¨ b), then Agent 1 always loses money,
regardless of the outcomes for a and b. De Finettiâ€™s theorem implies that no rational agent
can have beliefs that violate the axioms of probability.
Agent 1
Proposition Belief
a
b
aâˆ¨b

0.4
0.3
0.8

Bet

Agent 2
Stakes

a
b
Â¬(a âˆ¨ b)

4 to 6
3 to 7
2 to 8

Outcomes and payoffs to Agent 1
a, b a, Â¬b Â¬a, b
Â¬a, Â¬b
â€“6
â€“7
2

â€“6
3
2

4
â€“7
2

4
3
â€“8

â€“11

â€“1

â€“1

â€“1

Figure 13.2 Because Agent 1 has inconsistent beliefs, Agent 2 is able to devise a set of
bets that guarantees a loss for Agent 1, no matter what the outcome of a and b.

One common objection to de Finettiâ€™s theorem is that this betting game is rather contrived. For example, what if one refuses to bet? Does that end the argument? The answer is
that the betting game is an abstract model for the decision-making situation in which every
agent is unavoidably involved at every moment. Every action (including inaction) is a kind
of bet, and every outcome can be seen as a payoff of the bet. Refusing to bet is like refusing
to allow time to pass.
Other strong philosophical arguments have been put forward for the use of probabilities,
most notably those of Cox (1946), Carnap (1950), and Jaynes (2003). They each construct a
set of axioms for reasoning with degrees of beliefs: no contradictions, correspondence with
ordinary logic (for example, if belief in A goes up, then belief in Â¬A must go down), and so
on. The only controversial axiom is that degrees of belief must be numbers, or at least act
like numbers in that they must be transitive (if belief in A is greater than belief in B, which is
greater than belief in C, then belief in A must be greater than C) and comparable (the belief
in A must be one of equal to, greater than, or less than belief in B). It can then be proved that
probability is the only approach that satisfies these axioms.
The world being the way it is, however, practical demonstrations sometimes speak
louder than proofs. The success of reasoning systems based on probability theory has been
much more effective in making converts. We now look at how the axioms can be deployed to
make inferences.

13.3
PROBABILISTIC
INFERENCE

I NFERENCE U SING F ULL J OINT D ISTRIBUTIONS
In this section we describe a simple method for probabilistic inferenceâ€”that is, the computation of posterior probabilities for query propositions given observed evidence. We use the
full joint distribution as the â€œknowledge baseâ€ from which answers to all questions may be derived. Along the way we also introduce several useful techniques for manipulating equations
involving probabilities.

Section 13.3.

Inference Using Full Joint Distributions

W HERE D O P ROBABILITIES C OME F ROM ?
There has been endless debate over the source and status of probability numbers.
The frequentist position is that the numbers can come only from experiments: if
we test 100 people and find that 10 of them have a cavity, then we can say that
the probability of a cavity is approximately 0.1. In this view, the assertion â€œthe
probability of a cavity is 0.1â€ means that 0.1 is the fraction that would be observed
in the limit of infinitely many samples. From any finite sample, we can estimate
the true fraction and also calculate how accurate our estimate is likely to be.
The objectivist view is that probabilities are real aspects of the universeâ€”
propensities of objects to behave in certain waysâ€”rather than being just descriptions of an observerâ€™s degree of belief. For example, the fact that a fair coin comes
up heads with probability 0.5 is a propensity of the coin itself. In this view, frequentist measurements are attempts to observe these propensities. Most physicists
agree that quantum phenomena are objectively probabilistic, but uncertainty at the
macroscopic scaleâ€”e.g., in coin tossingâ€”usually arises from ignorance of initial
conditions and does not seem consistent with the propensity view.
The subjectivist view describes probabilities as a way of characterizing an
agentâ€™s beliefs, rather than as having any external physical significance. The subjective Bayesian view allows any self-consistent ascription of prior probabilities to
propositions, but then insists on proper Bayesian updating as evidence arrives.
In the end, even a strict frequentist position involves subjective analysis because of the reference class problem: in trying to determine the outcome probability of a particular experiment, the frequentist has to place it in a reference class of
â€œsimilarâ€ experiments with known outcome frequencies. I. J. Good (1983, p. 27)
wrote, â€œevery event in life is unique, and every real-life probability that we estimate in practice is that of an event that has never occurred before.â€ For example,
given a particular patient, a frequentist who wants to estimate the probability of a
cavity will consider a reference class of other patients who are similar in important
waysâ€”age, symptoms, dietâ€”and see what proportion of them had a cavity. If the
dentist considers everything that is known about the patientâ€”weight to the nearest
gram, hair color, motherâ€™s maiden nameâ€”then the reference class becomes empty.
This has been a vexing problem in the philosophy of science.
The principle of indifference attributed to Laplace (1816) states that propositions that are syntactically â€œsymmetricâ€ with respect to the evidence should be
accorded equal probability. Various refinements have been proposed, culminating
in the attempt by Carnap and others to develop a rigorous inductive logic, capable of computing the correct probability for any proposition from any collection of
observations. Currently, it is believed that no unique inductive logic exists; rather,
any such logic rests on a subjective prior probability distribution whose effect is
diminished as more observations are collected.

491

492

Chapter

13.

Â¬toothache

toothache
cavity
Â¬cavity

Quantifying Uncertainty

catch

Â¬catch

catch

Â¬catch

0.108
0.016

0.012
0.064

0.072
0.144

0.008
0.576

A full joint distribution for the Toothache, Cavity , Catch world.

Figure 13.3

We begin with a simple example: a domain consisting of just the three Boolean variables
Toothache , Cavity, and Catch (the dentistâ€™s nasty steel probe catches in my tooth). The full
joint distribution is a 2 Ã— 2 Ã— 2 table as shown in Figure 13.3.
Notice that the probabilities in the joint distribution sum to 1, as required by the axioms
of probability. Notice also that Equation (13.2) gives us a direct way to calculate the probability of any proposition, simple or complex: simply identify those possible worlds in which the
proposition is true and add up their probabilities. For example, there are six possible worlds
in which cavity âˆ¨ toothache holds:
P (cavity âˆ¨ toothache ) = 0.108 + 0.012 + 0.072 + 0.008 + 0.016 + 0.064 = 0.28 .

MARGINAL
PROBABILITY

One particularly common task is to extract the distribution over some subset of variables or
a single variable. For example, adding the entries in the first row gives the unconditional or
marginal probability4 of cavity:
P (cavity) = 0.108 + 0.012 + 0.072 + 0.008 = 0.2 .

MARGINALIZATION

This process is called marginalization, or summing outâ€”because we sum up the probabilities for each possible value of the other variables, thereby taking them out of the equation.
We can write the following general marginalization rule for any sets of variables Y and Z:

P(Y, z) ,
(13.6)
P(Y) =
zâˆˆZ

where zâˆˆZ means to sum over all the
 possible combinations of values of the set of variables
Z. We sometimes abbreviate this as z , leaving Z implicit. We just used the rule as

P(Cavity, z) .
(13.7)
P(Cavity) =
zâˆˆ{Catch,Toothache}

A variant of this rule involves conditional probabilities instead of joint probabilities, using
the product rule:

P(Y | z)P (z) .
(13.8)
P(Y) =
z
CONDITIONING

This rule is called conditioning. Marginalization and conditioning turn out to be useful rules
for all kinds of derivations involving probability expressions.
In most cases, we are interested in computing conditional probabilities of some variables, given evidence about others. Conditional probabilities can be found by first using
4

So called because of a common practice among actuaries of writing the sums of observed frequencies in the
margins of insurance tables.

Section 13.3.

NORMALIZATION

Inference Using Full Joint Distributions

493

Equation (13.3) to obtain an expression in terms of unconditional probabilities and then evaluating the expression from the full joint distribution. For example, we can compute the
probability of a cavity, given evidence of a toothache, as follows:
P (cavity âˆ§ toothache )
P (cavity | toothache ) =
P (toothache )
0.108 + 0.012
= 0.6 .
=
0.108 + 0.012 + 0.016 + 0.064
Just to check, we can also compute the probability that there is no cavity, given a toothache:
P (Â¬cavity âˆ§ toothache )
P (Â¬cavity | toothache ) =
P (toothache )
0.016 + 0.064
= 0.4 .
=
0.108 + 0.012 + 0.016 + 0.064
The two values sum to 1.0, as they should. Notice that in these two calculations the term
1/P (toothache ) remains constant, no matter which value of Cavity we calculate. In fact,
it can be viewed as a normalization constant for the distribution P(Cavity | toothache ),
ensuring that it adds up to 1. Throughout the chapters dealing with probability, we use Î± to
denote such constants. With this notation, we can write the two preceding equations in one:
P(Cavity | toothache ) = Î± P(Cavity, toothache )
= Î± [P(Cavity, toothache , catch) + P(Cavity, toothache , Â¬catch)]
= Î± [0.108, 0.016 + 0.012, 0.064] = Î± 0.12, 0.08 = 0.6, 0.4 .
In other words, we can calculate P(Cavity | toothache ) even if we donâ€™t know the value of
P (toothache )! We temporarily forget about the factor 1/P (toothache ) and add up the values
for cavity and Â¬cavity, getting 0.12 and 0.08. Those are the correct relative proportions, but
they donâ€™t sum to 1, so we normalize them by dividing each one by 0.12 + 0.08, getting
the true probabilities of 0.6 and 0.4. Normalization turns out to be a useful shortcut in many
probability calculations, both to make the computation easier and to allow us to proceed when
some probability assessment (such as P (toothache )) is not available.
From the example, we can extract a general inference procedure. We begin with the
case in which the query involves a single variable, X (Cavity in the example). Let E be the
list of evidence variables (just Toothache in the example), let e be the list of observed values
for them, and let Y be the remaining unobserved variables (just Catch in the example). The
query is P(X | e) and can be evaluated as

P(X, e, y) ,
(13.9)
P(X | e) = Î± P(X, e) = Î±
y

where the summation is over all possible ys (i.e., all possible combinations of values of the
unobserved variables Y). Notice that together the variables X, E, and Y constitute the complete set of variables for the domain, so P(X, e, y) is simply a subset of probabilities from the
full joint distribution.
Given the full joint distribution to work with, Equation (13.9) can answer probabilistic
queries for discrete variables. It does not scale well, however: for a domain described by n
Boolean variables, it requires an input table of size O(2n ) and takes O(2n ) time to process the

494

Chapter

13.

Quantifying Uncertainty

table. In a realistic problem we could easily have n > 100, making O(2n ) impractical. The
full joint distribution in tabular form is just not a practical tool for building reasoning systems.
Instead, it should be viewed as the theoretical foundation on which more effective approaches
may be built, just as truth tables formed a theoretical foundation for more practical algorithms
like DPLL. The remainder of this chapter introduces some of the basic ideas required in
preparation for the development of realistic systems in Chapter 14.

13.4

I NDEPENDENCE
Let us expand the full joint distribution in Figure 13.3 by adding a fourth variable, Weather .
The full joint distribution then becomes P(Toothache , Catch, Cavity, Weather ), which has
2 Ã— 2 Ã— 2 Ã— 4 = 32 entries. It contains four â€œeditionsâ€ of the table shown in Figure 13.3,
one for each kind of weather. What relationship do these editions have to each other and to
the original three-variable table? For example, how are P (toothache , catch, cavity, cloudy )
and P (toothache , catch, cavity ) related? We can use the product rule:
P (toothache , catch, cavity, cloudy)
= P (cloudy | toothache , catch, cavity)P (toothache , catch, cavity) .
Now, unless one is in the deity business, one should not imagine that oneâ€™s dental problems
influence the weather. And for indoor dentistry, at least, it seems safe to say that the weather
does not influence the dental variables. Therefore, the following assertion seems reasonable:
P (cloudy | toothache , catch, cavity) = P (cloudy) .

(13.10)

From this, we can deduce
P (toothache , catch, cavity, cloudy) = P (cloudy)P (toothache , catch, cavity) .
A similar equation exists for every entry in P(Toothache , Catch, Cavity, Weather ). In fact,
we can write the general equation
P(Toothache, Catch, Cavity, Weather ) = P(Toothache , Catch, Cavity )P(Weather ) .

INDEPENDENCE

Thus, the 32-element table for four variables can be constructed from one 8-element table
and one 4-element table. This decomposition is illustrated schematically in Figure 13.4(a).
The property we used in Equation (13.10) is called independence (also marginal independence and absolute independence). In particular, the weather is independent of oneâ€™s
dental problems. Independence between propositions a and b can be written as
P (a | b) = P (a) or

P (b | a) = P (b)

or P (a âˆ§ b) = P (a)P (b) .

(13.11)

All these forms are equivalent (Exercise 13.12). Independence between variables X and Y
can be written as follows (again, these are all equivalent):
P(X | Y ) = P(X)

or

P(Y | X) = P(Y ) or

P(X, Y ) = P(X)P(Y ) .

Independence assertions are usually based on knowledge of the domain. As the toothacheâ€“
weather example illustrates, they can dramatically reduce the amount of information necessary to specify the full joint distribution. If the complete set of variables can be divided

Section 13.5.

Bayesâ€™ Rule and Its Use

495

Cavity
Toothache
Weather

Coin1

decomposes
into

decomposes
into

Cavity
Toothache Catch

Coinn

Catch

Weather

(a)

Coin1

Coinn
(b)

Figure 13.4 Two examples of factoring a large joint distribution into smaller distributions,
using absolute independence. (a) Weather and dental problems are independent. (b) Coin
flips are independent.

into independent subsets, then the full joint distribution can be factored into separate joint
distributions on those subsets. For example, the full joint distribution on the outcome of n
independent coin flips, P(C1 , . . . , Cn ), has 2n entries, but it can be represented as the product of n single-variable distributions P(Ci ). In a more practical vein, the independence of
dentistry and meteorology is a good thing, because otherwise the practice of dentistry might
require intimate knowledge of meteorology, and vice versa.
When they are available, then, independence assertions can help in reducing the size of
the domain representation and the complexity of the inference problem. Unfortunately, clean
separation of entire sets of variables by independence is quite rare. Whenever a connection,
however indirect, exists between two variables, independence will fail to hold. Moreover,
even independent subsets can be quite largeâ€”for example, dentistry might involve dozens of
diseases and hundreds of symptoms, all of which are interrelated. To handle such problems,
we need more subtle methods than the straightforward concept of independence.

13.5

BAYES â€™ RULE AND I TS U SE
On page 486, we defined the product rule. It can actually be written in two forms:
P (a âˆ§ b) = P (a | b)P (b)

BAYESâ€™ RULE

and

P (a âˆ§ b) = P (b | a)P (a) .

Equating the two right-hand sides and dividing by P (a), we get
P (a | b)P (b)
.
(13.12)
P (b | a) =
P (a)
This equation is known as Bayesâ€™ rule (also Bayesâ€™ law or Bayesâ€™ theorem). This simple
equation underlies most modern AI systems for probabilistic inference.

496

Chapter

13.

Quantifying Uncertainty

The more general case of Bayesâ€™ rule for multivalued variables can be written in the P
notation as follows:
P(X | Y )P(Y )
,
P(Y | X) =
P(X)
As before, this is to be taken as representing a set of equations, each dealing with specific values of the variables. We will also have occasion to use a more general version conditionalized
on some background evidence e:
P(Y | X, e) =

P(X | Y, e)P(Y | e)
.
P(X | e)

(13.13)

13.5.1 Applying Bayesâ€™ rule: The simple case
On the surface, Bayesâ€™ rule does not seem very useful. It allows us to compute the single
term P (b | a) in terms of three terms: P (a | b), P (b), and P (a). That seems like two steps
backwards, but Bayesâ€™ rule is useful in practice because there are many cases where we do
have good probability estimates for these three numbers and need to compute the fourth.
Often, we perceive as evidence the effect of some unknown cause and we would like to
determine that cause. In that case, Bayesâ€™ rule becomes
P (cause | effect) =
CAUSAL
DIAGNOSTIC

P (effect | cause)P (cause)
.
P (effect)

The conditional probability P (effect | cause) quantifies the relationship in the causal direction, whereas P (cause | effect) describes the diagnostic direction. In a task such as medical
diagnosis, we often have conditional probabilities on causal relationships (that is, the doctor
knows P (symptoms | disease)) and want to derive a diagnosis, P (disease | symptoms). For
example, a doctor knows that the disease meningitis causes the patient to have a stiff neck,
say, 70% of the time. The doctor also knows some unconditional facts: the prior probability that a patient has meningitis is 1/50,000, and the prior probability that any patient has a
stiff neck is 1%. Letting s be the proposition that the patient has a stiff neck and m be the
proposition that the patient has meningitis, we have
P (s | m) = 0.7
P (m) = 1/50000
P (s) = 0.01
0.7 Ã— 1/50000
P (s | m)P (m)
=
= 0.0014 .
P (m | s) =
P (s)
0.01

(13.14)

That is, we expect less than 1 in 700 patients with a stiff neck to have meningitis. Notice that
even though a stiff neck is quite strongly indicated by meningitis (with probability 0.7), the
probability of meningitis in the patient remains small. This is because the prior probability of
stiff necks is much higher than that of meningitis.
Section 13.3 illustrated a process by which one can avoid assessing the prior probability
of the evidence (here, P (s)) by instead computing a posterior probability for each value of

Section 13.5.

Bayesâ€™ Rule and Its Use

497

the query variable (here, m and Â¬m) and then normalizing the results. The same process can
be applied when using Bayesâ€™ rule. We have
P(M | s) = Î± P (s | m)P (m), P (s | Â¬m)P (Â¬m) .
Thus, to use this approach we need to estimate P (s | Â¬m) instead of P (s). There is no free
lunchâ€”sometimes this is easier, sometimes it is harder. The general form of Bayesâ€™ rule with
normalization is
P(Y | X) = Î± P(X | Y )P(Y ) ,

(13.15)

where Î± is the normalization constant needed to make the entries in P(Y | X) sum to 1.
One obvious question to ask about Bayesâ€™ rule is why one might have available the
conditional probability in one direction, but not the other. In the meningitis domain, perhaps
the doctor knows that a stiff neck implies meningitis in 1 out of 5000 cases; that is, the doctor
has quantitative information in the diagnostic direction from symptoms to causes. Such a
doctor has no need to use Bayesâ€™ rule. Unfortunately, diagnostic knowledge is often more
fragile than causal knowledge. If there is a sudden epidemic of meningitis, the unconditional
probability of meningitis, P (m), will go up. The doctor who derived the diagnostic probability P (m | s) directly from statistical observation of patients before the epidemic will have
no idea how to update the value, but the doctor who computes P (m | s) from the other three
values will see that P (m | s) should go up proportionately with P (m). Most important, the
causal information P (s | m) is unaffected by the epidemic, because it simply reflects the way
meningitis works. The use of this kind of direct causal or model-based knowledge provides
the crucial robustness needed to make probabilistic systems feasible in the real world.

13.5.2 Using Bayesâ€™ rule: Combining evidence
We have seen that Bayesâ€™ rule can be useful for answering probabilistic queries conditioned
on one piece of evidenceâ€”for example, the stiff neck. In particular, we have argued that
probabilistic information is often available in the form P (effect | cause). What happens when
we have two or more pieces of evidence? For example, what can a dentist conclude if her
nasty steel probe catches in the aching tooth of a patient? If we know the full joint distribution
(Figure 13.3), we can read off the answer:
P(Cavity | toothache âˆ§ catch) = Î± 0.108, 0.016 â‰ˆ 0.871, 0.129 .
We know, however, that such an approach does not scale up to larger numbers of variables.
We can try using Bayesâ€™ rule to reformulate the problem:
P(Cavity | toothache âˆ§ catch)
= Î± P(toothache âˆ§ catch | Cavity) P(Cavity) .

(13.16)

For this reformulation to work, we need to know the conditional probabilities of the conjunction toothache âˆ§ catch for each value of Cavity. That might be feasible for just two evidence
variables, but again it does not scale up. If there are n possible evidence variables (X rays,
diet, oral hygiene, etc.), then there are 2n possible combinations of observed values for which
we would need to know conditional probabilities. We might as well go back to using the
full joint distribution. This is what first led researchers away from probability theory toward

498

Chapter

13.

Quantifying Uncertainty

approximate methods for evidence combination that, while giving incorrect answers, require
fewer numbers to give any answer at all.
Rather than taking this route, we need to find some additional assertions about the
domain that will enable us to simplify the expressions. The notion of independence in Section 13.4 provides a clue, but needs refining. It would be nice if Toothache and Catch were
independent, but they are not: if the probe catches in the tooth, then it is likely that the tooth
has a cavity and that the cavity causes a toothache. These variables are independent, however, given the presence or the absence of a cavity. Each is directly caused by the cavity, but
neither has a direct effect on the other: toothache depends on the state of the nerves in the
tooth, whereas the probeâ€™s accuracy depends on the dentistâ€™s skill, to which the toothache is
irrelevant.5 Mathematically, this property is written as
P(toothache âˆ§ catch | Cavity) = P(toothache | Cavity)P(catch | Cavity) .
CONDITIONAL
INDEPENDENCE

(13.17)

This equation expresses the conditional independence of toothache and catch given Cavity.
We can plug it into Equation (13.16) to obtain the probability of a cavity:
P(Cavity | toothache âˆ§ catch)
= Î± P(toothache | Cavity) P(catch | Cavity) P(Cavity) .

(13.18)

Now the information requirements are the same as for inference, using each piece of evidence separately: the prior probability P(Cavity) for the query variable and the conditional
probability of each effect, given its cause.
The general definition of conditional independence of two variables X and Y , given a
third variable Z, is
P(X, Y | Z) = P(X | Z)P(Y | Z) .
In the dentist domain, for example, it seems reasonable to assert conditional independence of
the variables Toothache and Catch, given Cavity:
P(Toothache , Catch | Cavity) = P(Toothache | Cavity)P(Catch | Cavity) . (13.19)
Notice that this assertion is somewhat stronger than Equation (13.17), which asserts independence only for specific values of Toothache and Catch. As with absolute independence in
Equation (13.11), the equivalent forms
P(X | Y, Z) = P(X | Z) and

P(Y | X, Z) = P(Y | Z)

can also be used (see Exercise 13.17). Section 13.4 showed that absolute independence assertions allow a decomposition of the full joint distribution into much smaller pieces. It turns
out that the same is true for conditional independence assertions. For example, given the
assertion in Equation (13.19), we can derive a decomposition as follows:
P(Toothache , Catch, Cavity)
= P(Toothache , Catch | Cavity)P(Cavity)

(product rule)

= P(Toothache | Cavity)P(Catch | Cavity)P(Cavity)

(using 13.19).

(The reader can easily check that this equation does in fact hold in Figure 13.3.) In this way,
the original large table is decomposed into three smaller tables. The original table has seven
5

We assume that the patient and dentist are distinct individuals.

Section 13.6.

SEPARATION

The Wumpus World Revisited

499

independent numbers (23 = 8 entries in the table, but they must sum to 1, so 7 are independent). The smaller tables contain five independent numbers (for a conditional probability
distributions such as P(T |C there are two rows of two numbers, and each row sums to 1, so
thatâ€™s two independent numbers; for a prior distribution like P(C) there is only one independent number). Going from seven to five might not seem like a major triumph, but the point
is that, for n symptoms that are all conditionally independent given Cavity, the size of the
representation grows as O(n) instead of O(2n ). That means that conditional independence
assertions can allow probabilistic systems to scale up; moreover, they are much more commonly available than absolute independence assertions. Conceptually, Cavity separates
Toothache and Catch because it is a direct cause of both of them. The decomposition of
large probabilistic domains into weakly connected subsets through conditional independence
is one of the most important developments in the recent history of AI.
The dentistry example illustrates a commonly occurring pattern in which a single cause
directly influences a number of effects, all of which are conditionally independent, given the
cause. The full joint distribution can be written as

P(Effecti | Cause) .
P(Cause, Effect1 , . . . , Effectn ) = P(Cause)
i

NAIVE BAYES

13.6

Such a probability distribution is called a naive Bayes modelâ€”â€œnaiveâ€ because it is often
used (as a simplifying assumption) in cases where the â€œeffectâ€ variables are not actually
conditionally independent given the cause variable. (The naive Bayes model is sometimes
called a Bayesian classifier, a somewhat careless usage that has prompted true Bayesians
to call it the idiot Bayes model.) In practice, naive Bayes systems can work surprisingly
well, even when the conditional independence assumption is not true. Chapter 20 describes
methods for learning naive Bayes distributions from observations.

T HE W UMPUS W ORLD R EVISITED
We can combine of the ideas in this chapter to solve probabilistic reasoning problems in the
wumpus world. (See Chapter 7 for a complete description of the wumpus world.) Uncertainty
arises in the wumpus world because the agentâ€™s sensors give only partial information about
the world. For example, Figure 13.5 shows a situation in which each of the three reachable
squaresâ€”[1,3], [2,2], and [3,1]â€”might contain a pit. Pure logical inference can conclude
nothing about which square is most likely to be safe, so a logical agent might have to choose
randomly. We will see that a probabilistic agent can do much better than the logical agent.
Our aim is to calculate the probability that each of the three squares contains a pit. (For
this example we ignore the wumpus and the gold.) The relevant properties of the wumpus
world are that (1) a pit causes breezes in all neighboring squares, and (2) each square other
than [1,1] contains a pit with probability 0.2. The first step is to identify the set of random
variables we need:
â€¢ As in the propositional logic case, we want one Boolean variable Pij for each square,
which is true iff square [i, j] actually contains a pit.

500

Chapter

13.

Quantifying Uncertainty

1,4

2,4

3,4

4,4

1,4

2,4

3,4

4,4

1,3

2,3

3,3

4,3

1,3

2,3

3,3

4,3

2,2

3,2

4,2

OTHER

QUERY

1,2

2,2

3,2

4,2

1,2

2,1

3,1

4,1

1,1

B
OK
1,1

B
OK

KNOWN

2,1

FRONTIER

3,1

4,1

OK

(a)

(b)

Figure 13.5 (a) After finding a breeze in both [1,2] and [2,1], the agent is stuckâ€”there is
no safe place to explore. (b) Division of the squares into Known, Frontier , and Other , for
a query about [1,3].

â€¢ We also have Boolean variables Bij that are true iff square [i, j] is breezy; we include
these variables only for the observed squaresâ€”in this case, [1,1], [1,2], and [2,1].
The next step is to specify the full joint distribution, P(P1,1 , . . . , P4,4 , B1,1 , B1,2 , B2,1 ). Applying the product rule, we have
P(P1,1 , . . . , P4,4 , B1,1 , B1,2 , B2,1 ) =
P(B1,1 , B1,2 , B2,1 | P1,1 , . . . , P4,4 )P(P1,1 , . . . , P4,4 ) .
This decomposition makes it easy to see what the joint probability values should be. The
first term is the conditional probability distribution of a breeze configuration, given a pit
configuration; its values are 1 if the breezes are adjacent to the pits and 0 otherwise. The
second term is the prior probability of a pit configuration. Each square contains a pit with
probability 0.2, independently of the other squares; hence,
P(P1,1 , . . . , P4,4 ) =

4,4


P(Pi,j ) .

(13.20)

i,j = 1,1

For a particular configuration with exactly n pits, P (P1,1 , . . . , P4,4 ) = 0.2n Ã— 0.816âˆ’n .
In the situation in Figure 13.5(a), the evidence consists of the observed breeze (or its
absence) in each square that is visited, combined with the fact that each such square contains
no pit. We abbreviate these facts as b = Â¬b1,1 âˆ§ b1,2 âˆ§ b2,1 and known = Â¬p1,1 âˆ§ Â¬p1,2 âˆ§ Â¬p2,1 .
We are interested in answering queries such as P(P1,3 | known, b): how likely is it that [1,3]
contains a pit, given the observations so far?
To answer this query, we can follow the standard approach of Equation (13.9), namely,
summing over entries from the full joint distribution. Let Unknown be the set of Pi,j vari-

Section 13.6.

The Wumpus World Revisited

501

ables for squares other than the Known squares and the query square [1,3]. Then, by Equation (13.9), we have

P(P1,3 , unknown, known, b) .
P(P1,3 | known, b) = Î±
unknown

The full joint probabilities have already been specified, so we are doneâ€”that is, unless we
care about computation. There are 12 unknown squares; hence the summation contains
212 = 4096 terms. In general, the summation grows exponentially with the number of squares.
Surely, one might ask, arenâ€™t the other squares irrelevant? How could [4,4] affect
whether [1,3] has a pit? Indeed, this intuition is correct. Let Frontier be the pit variables
(other than the query variable) that are adjacent to visited squares, in this case just [2,2] and
[3,1]. Also, let Other be the pit variables for the other unknown squares; in this case, there are
10 other squares, as shown in Figure 13.5(b). The key insight is that the observed breezes are
conditionally independent of the other variables, given the known, frontier, and query variables. To use the insight, we manipulate the query formula into a form in which the breezes
are conditioned on all the other variables, and then we apply conditional independence:
P(P1,3 | known, b)

P(P1,3 , known, b, unknown)
(by Equation (13.9))
= Î±
unknown



= Î±

P(b | P1,3 , known, unknown)P(P1,3 , known, unknown)

unknown

 

= Î±

(by the product rule)
P(b | known, P1,3 , frontier , other )P(P1,3 , known, frontier , other )

frontier other

 

= Î±

P(b | known, P1,3 , frontier )P(P1,3 , known, frontier , other ) ,

frontier other

where the final step uses conditional independence: b is independent of other given known,
P1,3 , and frontier . Now, the first term in this expression does not depend on the Other
variables, so we can move the summation inward:
P(P1,3 | known, b)


P(b | known, P1,3 , frontier )
P(P1,3 , known, frontier , other ) .
= Î±
frontier

other

By independence, as in Equation (13.20), the prior term can be factored, and then the terms
can be reordered:
P(P1,3 | known, b)


P(b | known, P1,3 , frontier )
P(P1,3 )P (known)P (frontier )P (other )
= Î±
frontier

= Î± P (known)P(P1,3 )


= Î± P(P1,3 )


frontier



other

P(b | known, P1,3 , frontier )P (frontier )

frontier

P(b | known, P1,3 , frontier )P (frontier ) ,


other

P (other )

502

Chapter

13.

Quantifying Uncertainty

1,3

1,3

1,3

1,3

1,3

2,2
B
OK
1,1
2,1

1,2

2,2
B
OK
1,1
2,1

1,2

1,2

2,2
B
OK
1,1
2,1

1,2

1,2

OK

B
OK

3,1

0.2 x 0.2 = 0.04

OK

B
OK

3,1

0.2 x 0.8 = 0.16
(a)

2,2
B
OK
1,1
2,1
OK

B
OK

3,1

0.8 x 0.2 = 0.16

OK

B
OK

2,2
B
OK
1,1
2,1

3,1

OK

0.2 x 0.2 = 0.04

B
OK

3,1

0.2 x 0.8 = 0.16
(b)

Figure 13.6 Consistent models for the frontier variables P2,2 and P3,1 , showing
P (frontier ) for each model: (a) three models with P1,3 = true showing two or three pits,
and (b) two models with P1,3 = false showing one or two pits.

where the last step folds P (known) into the normalizing constant and uses the fact that

other P (other ) equals 1.
Now, there are just four terms in the summation over the frontier variables P2,2 and
P3,1 . The use of independence and conditional independence has completely eliminated the
other squares from consideration.
Notice that the expression P(b | known, P1,3 , frontier ) is 1 when the frontier is consistent with the breeze observations, and 0 otherwise. Thus, for each value of P1,3 , we sum over
the logical models for the frontier variables that are consistent with the known facts. (Compare with the enumeration over models in Figure 7.5 on page 241.) The models and their
associated prior probabilitiesâ€”P (frontier )â€”are shown in Figure 13.6. We have
P(P1,3 | known, b) = Î± 0.2(0.04 + 0.16 + 0.16), 0.8(0.04 + 0.16) â‰ˆ 0.31, 0.69 .
That is, [1,3] (and [3,1] by symmetry) contains a pit with roughly 31% probability. A similar
calculation, which the reader might wish to perform, shows that [2,2] contains a pit with
roughly 86% probability. The wumpus agent should definitely avoid [2,2]! Note that our
logical agent from Chapter 7 did not know that [2,2] was worse than the other squares. Logic
can tell us that it is unknown whether there is a pit in [2, 2], but we need probability to tell us
how likely it is.
What this section has shown is that even seemingly complicated problems can be formulated precisely in probability theory and solved with simple algorithms. To get efficient
solutions, independence and conditional independence relationships can be used to simplify
the summations required. These relationships often correspond to our natural understanding
of how the problem should be decomposed. In the next chapter, we develop formal representations for such relationships as well as algorithms that operate on those representations to
perform probabilistic inference efficiently.

Section 13.7.

13.7

Summary

503

S UMMARY
This chapter has suggested probability theory as a suitable foundation for uncertain reasoning
and provided a gentle introduction to its use.
â€¢ Uncertainty arises because of both laziness and ignorance. It is inescapable in complex,
nondeterministic, or partially observable environments.
â€¢ Probabilities express the agentâ€™s inability to reach a definite decision regarding the truth
of a sentence. Probabilities summarize the agentâ€™s beliefs relative to the evidence.
â€¢ Decision theory combines the agentâ€™s beliefs and desires, defining the best action as the
one that maximizes expected utility.
â€¢ Basic probability statements include prior probabilities and conditional probabilities
over simple and complex propositions.
â€¢ The axioms of probability constrain the possible assignments of probabilities to propositions. An agent that violates the axioms must behave irrationally in some cases.
â€¢ The full joint probability distribution specifies the probability of each complete assignment of values to random variables. It is usually too large to create or use in its
explicit form, but when it is available it can be used to answer queries simply by adding
up entries for the possible worlds corresponding to the query propositions.
â€¢ Absolute independence between subsets of random variables allows the full joint distribution to be factored into smaller joint distributions, greatly reducing its complexity.
Absolute independence seldom occurs in practice.
â€¢ Bayesâ€™ rule allows unknown probabilities to be computed from known conditional
probabilities, usually in the causal direction. Applying Bayesâ€™ rule with many pieces of
evidence runs into the same scaling problems as does the full joint distribution.
â€¢ Conditional independence brought about by direct causal relationships in the domain
might allow the full joint distribution to be factored into smaller, conditional distributions. The naive Bayes model assumes the conditional independence of all effect
variables, given a single cause variable, and grows linearly with the number of effects.
â€¢ A wumpus-world agent can calculate probabilities for unobserved aspects of the world,
thereby improving on the decisions of a purely logical agent. Conditional independence
makes these calculations tractable.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Probability theory was invented as a way of analyzing games of chance. In about 850 A . D .
the Indian mathematician Mahaviracarya described how to arrange a set of bets that canâ€™t lose
(what we now call a Dutch book). In Europe, the first significant systematic analyses were
produced by Girolamo Cardano around 1565, although publication was posthumous (1663).
By that time, probability had been established as a mathematical discipline due to a series of

504

PRINCIPLE OF
INDIFFERENCE
PRINCIPLE OF
INSUFFICIENT
REASON

Chapter

13.

Quantifying Uncertainty

results established in a famous correspondence between Blaise Pascal and Pierre de Fermat
in 1654. As with probability itself, the results were initially motivated by gambling problems
(see Exercise 13.9). The first published textbook on probability was De Ratiociniis in Ludo
Aleae (Huygens, 1657). The â€œlaziness and ignoranceâ€ view of uncertainty was described
by John Arbuthnot in the preface of his translation of Huygens (Arbuthnot, 1692): â€œIt is
impossible for a Die, with such determinâ€™d force and direction, not to fall on such determinâ€™d
side, only I donâ€™t know the force and direction which makes it fall on such determinâ€™d side,
and therefore I call it Chance, which is nothing but the want of art...â€
Laplace (1816) gave an exceptionally accurate and modern overview of probability; he
was the first to use the example â€œtake two urns, A and B, the first containing four white and
two black balls, . . . â€ The Rev. Thomas Bayes (1702â€“1761) introduced the rule for reasoning
about conditional probabilities that was named after him (Bayes, 1763). Bayes only considered the case of uniform priors; it was Laplace who independently developed the general
case. Kolmogorov (1950, first published in German in 1933) presented probability theory in
a rigorously axiomatic framework for the first time. ReÌnyi (1970) later gave an axiomatic
presentation that took conditional probability, rather than absolute probability, as primitive.
Pascal used probability in ways that required both the objective interpretation, as a property of the world based on symmetry or relative frequency, and the subjective interpretation,
based on degree of beliefâ€”the former in his analyses of probabilities in games of chance, the
latter in the famous â€œPascalâ€™s wagerâ€ argument about the possible existence of God. However, Pascal did not clearly realize the distinction between these two interpretations. The
distinction was first drawn clearly by James Bernoulli (1654â€“1705).
Leibniz introduced the â€œclassicalâ€ notion of probability as a proportion of enumerated,
equally probable cases, which was also used by Bernoulli, although it was brought to prominence by Laplace (1749â€“1827). This notion is ambiguous between the frequency interpretation and the subjective interpretation. The cases can be thought to be equally probable either
because of a natural, physical symmetry between them, or simply because we do not have
any knowledge that would lead us to consider one more probable than another. The use of
this latter, subjective consideration to justify assigning equal probabilities is known as the
principle of indifference. The principle is often attributed to Laplace, but he never isolated
the principle explicitly. George Boole and John Venn both referred to it as the principle of
insufficient reason; the modern name is due to Keynes (1921).
The debate between objectivists and subjectivists became sharper in the 20th century.
Kolmogorov (1963), R. A. Fisher (1922), and Richard von Mises (1928) were advocates of
the relative frequency interpretation. Karl Popperâ€™s (1959, first published in German in 1934)
â€œpropensityâ€ interpretation traces relative frequencies to an underlying physical symmetry.
Frank Ramsey (1931), Bruno de Finetti (1937), R. T. Cox (1946), Leonard Savage (1954),
Richard Jeffrey (1983), and E. T. Jaynes (2003) interpreted probabilities as the degrees of
belief of specific individuals. Their analyses of degree of belief were closely tied to utilities and to behaviorâ€”specifically, to the willingness to place bets. Rudolf Carnap, following
Leibniz and Laplace, offered a different kind of subjective interpretation of probabilityâ€”
not as any actual individualâ€™s degree of belief, but as the degree of belief that an idealized
individual should have in a particular proposition a, given a particular body of evidence e.

Bibliographical and Historical Notes

CONFIRMATION
INDUCTIVE LOGIC

505

Carnap attempted to go further than Leibniz or Laplace by making this notion of degree of
confirmation mathematically precise, as a logical relation between a and e. The study of this
relation was intended to constitute a mathematical discipline called inductive logic, analogous to ordinary deductive logic (Carnap, 1948, 1950). Carnap was not able to extend his
inductive logic much beyond the propositional case, and Putnam (1963) showed by adversarial arguments that some fundamental difficulties would prevent a strict extension to languages
capable of expressing arithmetic.
Coxâ€™s theorem (1946) shows that any system for uncertain reasoning that meets his set
of assumptions is equivalent to probability theory. This gave renewed confidence to those
who already favored probability, but others were not convinced, pointing to the assumptions
(primarily that belief must be represented by a single number, and thus the belief in Â¬p must
be a function of the belief in p). Halpern (1999) describes the assumptions and shows some
gaps in Coxâ€™s original formulation. Horn (2003) shows how to patch up the difficulties.
Jaynes (2003) has a similar argument that is easier to read.
The question of reference classes is closely tied to the attempt to find an inductive logic.
The approach of choosing the â€œmost specificâ€ reference class of sufficient size was formally
proposed by Reichenbach (1949). Various attempts have been made, notably by Henry Kyburg (1977, 1983), to formulate more sophisticated policies in order to avoid some obvious
fallacies that arise with Reichenbachâ€™s rule, but such approaches remain somewhat ad hoc.
More recent work by Bacchus, Grove, Halpern, and Koller (1992) extends Carnapâ€™s methods
to first-order theories, thereby avoiding many of the difficulties associated with the straightforward reference-class method. Kyburg and Teng (2006) contrast probabilistic inference
with nonmonotonic logic.
Bayesian probabilistic reasoning has been used in AI since the 1960s, especially in
medical diagnosis. It was used not only to make a diagnosis from available evidence, but also
to select further questions and tests by using the theory of information value (Section 16.6)
when available evidence was inconclusive (Gorry, 1968; Gorry et al., 1973). One system
outperformed human experts in the diagnosis of acute abdominal illnesses (de Dombal et al.,
1974). Lucas et al. (2004) gives an overview. These early Bayesian systems suffered from a
number of problems, however. Because they lacked any theoretical model of the conditions
they were diagnosing, they were vulnerable to unrepresentative data occurring in situations
for which only a small sample was available (de Dombal et al., 1981). Even more fundamentally, because they lacked a concise formalism (such as the one to be described in Chapter 14)
for representing and using conditional independence information, they depended on the acquisition, storage, and processing of enormous tables of probabilistic data. Because of these
difficulties, probabilistic methods for coping with uncertainty fell out of favor in AI from the
1970s to the mid-1980s. Developments since the late 1980s are described in the next chapter.
The naive Bayes model for joint distributions has been studied extensively in the pattern recognition literature since the 1950s (Duda and Hart, 1973). It has also been used, often
unwittingly, in information retrieval, beginning with the work of Maron (1961). The probabilistic foundations of this technique, described further in Exercise 13.22, were elucidated by
Robertson and Sparck Jones (1976). Domingos and Pazzani (1997) provide an explanation

506

Chapter

13.

Quantifying Uncertainty

for the surprising success of naive Bayesian reasoning even in domains where the independence assumptions are clearly violated.
There are many good introductory textbooks on probability theory, including those by
Bertsekas and Tsitsiklis (2008) and Grinstead and Snell (1997). DeGroot and Schervish
(2001) offer a combined introduction to probability and statistics from a Bayesian standpoint. Richard Hammingâ€™s (1991) textbook gives a mathematically sophisticated introduction to probability theory from the standpoint of a propensity interpretation based on physical
symmetry. Hacking (1975) and Hald (1990) cover the early history of the concept of probability. Bernstein (1996) gives an entertaining popular account of the story of risk.

E XERCISES
13.1

Show from first principles that P (a | b âˆ§ a) = 1.

13.2 Using the axioms of probability, prove that any probability distribution on a discrete
random variable must sum to 1.
13.3

For each of the following statements, either prove it is true or give a counterexample.

a. If P (a | b, c) = P (b | a, c), then P (a | c) = P (b | c)
b. If P (a | b, c) = P (a), then P (b | c) = P (b)
c. If P (a | b) = P (a), then P (a | b, c) = P (a | c)
13.4 Would it be rational for an agent to hold the three beliefs P (A) = 0.4, P (B) = 0.3, and
P (A âˆ¨ B) = 0.5? If so, what range of probabilities would be rational for the agent to hold for
A âˆ§ B? Make up a table like the one in Figure 13.2, and show how it supports your argument
about rationality. Then draw another version of the table where P (A âˆ¨ B) = 0.7. Explain
why it is rational to have this probability, even though the table shows one case that is a loss
and three that just break even. (Hint: what is Agent 1 committed to about the probability of
each of the four cases, especially the case that is a loss?)

ATOMIC EVENT

13.5 This question deals with the properties of possible worlds, defined on page 488 as
assignments to all random variables. We will work with propositions that correspond to
exactly one possible world because they pin down the assignments of all the variables. In
probability theory, such propositions are called atomic events. For example, with Boolean
variables X1 , X2 , X3 , the proposition x1 âˆ§ Â¬x2 âˆ§ Â¬x3 fixes the assignment of the variables;
in the language of propositional logic, we would say it has exactly one model.
a. Prove, for the case of n Boolean variables, that any two distinct atomic events are
mutually exclusive; that is, their conjunction is equivalent to false.
b. Prove that the disjunction of all possible atomic events is logically equivalent to true.
c. Prove that any proposition is logically equivalent to the disjunction of the atomic events
that entail its truth.

Exercises

507
13.6

Prove Equation (13.4) from Equations (13.1) and (13.2).

13.7 Consider the set of all possible five-card poker hands dealt fairly from a standard deck
of fifty-two cards.
a. How many atomic events are there in the joint probability distribution (i.e., how many
five-card hands are there)?
b. What is the probability of each atomic event?
c. What is the probability of being dealt a royal straight flush? Four of a kind?
13.8
a.
b.
c.
d.

Given the full joint distribution shown in Figure 13.3, calculate the following:
P(toothache ) .
P(Cavity) .
P(Toothache | cavity) .
P(Cavity | toothache âˆ¨ catch) .

13.9 In his letter of August 24, 1654, Pascal was trying to show how a pot of money should
be allocated when a gambling game must end prematurely. Imagine a game where each turn
consists of the roll of a die, player E gets a point when the die is even, and player O gets a
point when the die is odd. The first player to get 7 points wins the pot. Suppose the game is
interrupted with E leading 4â€“2. How should the money be fairly split in this case? What is
the general formula? (Fermat and Pascal made several errors before solving the problem, but
you should be able to get it right the first time.)
13.10 Deciding to put probability theory to good use, we encounter a slot machine with
three independent wheels, each producing one of the four symbols BAR, BELL, LEMON, or
CHERRY with equal probability. The slot machine has the following payout scheme for a bet
of 1 coin (where â€œ?â€ denotes that we donâ€™t care what comes up for that wheel):
BAR/BAR/BAR

pays 20 coins
BELL/BELL/BELL pays 15 coins
LEMON/LEMON/LEMON pays 5 coins
CHERRY/CHERRY/CHERRY pays 3 coins
CHERRY/CHERRY/? pays 2 coins
CHERRY/?/? pays 1 coin
a. Compute the expected â€œpaybackâ€ percentage of the machine. In other words, for each
coin played, what is the expected coin return?
b. Compute the probability that playing the slot machine once will result in a win.
c. Estimate the mean and median number of plays you can expect to make until you go
broke, if you start with 10 coins. You can run a simulation to estimate this, rather than
trying to compute an exact answer.
13.11 We wish to transmit an n-bit message to a receiving agent. The bits in the message are
independently corrupted (flipped) during transmission with  probability each. With an extra
parity bit sent along with the original information, a message can be corrected by the receiver

508

Chapter

13.

Quantifying Uncertainty

if at most one bit in the entire message (including the parity bit) has been corrupted. Suppose
we want to ensure that the correct message is received with probability at least 1 âˆ’ Î´. What is
the maximum feasible value of n? Calculate this value for the case  = 0.001, Î´ = 0.01.
13.12

Show that the three forms of independence in Equation (13.11) are equivalent.

13.13 Consider two medical tests, A and B, for a virus. Test A is 95% effective at recognizing the virus when it is present, but has a 10% false positive rate (indicating that the virus
is present, when it is not). Test B is 90% effective at recognizing the virus, but has a 5% false
positive rate. The two tests use independent methods of identifying the virus. The virus is
carried by 1% of all people. Say that a person is tested for the virus using only one of the tests,
and that test comes back positive for carrying the virus. Which test returning positive is more
indicative of someone really carrying the virus? Justify your answer mathematically.
13.14 Suppose you are given a coin that lands heads with probability x and tails with
probability 1 âˆ’ x. Are the outcomes of successive flips of the coin independent of each
other given that you know the value of x? Are the outcomes of successive flips of the coin
independent of each other if you do not know the value of x? Justify your answer.
13.15 After your yearly checkup, the doctor has bad news and good news. The bad news
is that you tested positive for a serious disease and that the test is 99% accurate (i.e., the
probability of testing positive when you do have the disease is 0.99, as is the probability of
testing negative when you donâ€™t have the disease). The good news is that this is a rare disease,
striking only 1 in 10,000 people of your age. Why is it good news that the disease is rare?
What are the chances that you actually have the disease?
13.16 It is quite often useful to consider the effect of some specific propositions in the
context of some general background evidence that remains fixed, rather than in the complete
absence of information. The following questions ask you to prove more general versions of
the product rule and Bayesâ€™ rule, with respect to some background evidence e:
a. Prove the conditionalized version of the general product rule:
P(X, Y | e) = P(X | Y, e)P(Y | e) .
b. Prove the conditionalized version of Bayesâ€™ rule in Equation (13.13).
13.17

Show that the statement of conditional independence
P(X, Y | Z) = P(X | Z)P(Y | Z)

is equivalent to each of the statements
P(X | Y, Z) = P(X | Z) and

P(B | X, Z) = P(Y | Z) .

13.18 Suppose you are given a bag containing n unbiased coins. You are told that n âˆ’ 1 of
these coins are normal, with heads on one side and tails on the other, whereas one coin is a
fake, with heads on both sides.
a. Suppose you reach into the bag, pick out a coin at random, flip it, and get a head. What
is the (conditional) probability that the coin you chose is the fake coin?

Exercises

509
b. Suppose you continue flipping the coin for a total of k times after picking it and see k
heads. Now what is the conditional probability that you picked the fake coin?
c. Suppose you wanted to decide whether the chosen coin was fake by flipping it k times.
The decision procedure returns fake if all k flips come up heads; otherwise it returns
normal . What is the (unconditional) probability that this procedure makes an error?
13.19 In this exercise, you will complete the normalization calculation for the meningitis
example. First, make up a suitable value for P (s | Â¬m), and use it to calculate unnormalized
values for P (m | s) and P (Â¬m | s) (i.e., ignoring the P (s) term in the Bayesâ€™ rule expression,
Equation (13.14)). Now normalize these values so that they add to 1.
13.20 Let X, Y , Z be Boolean random variables. Label the eight entries in the joint distribution P(X, Y, Z) as a through h. Express the statement that X and Y are conditionally
independent given Z, as a set of equations relating a through h. How many nonredundant
equations are there?
13.21 (Adapted from Pearl (1988).) Suppose you are a witness to a nighttime hit-and-run
accident involving a taxi in Athens. All taxis in Athens are blue or green. You swear, under
oath, that the taxi was blue. Extensive testing shows that, under the dim lighting conditions,
discrimination between blue and green is 75% reliable.
a. Is it possible to calculate the most likely color for the taxi? (Hint: distinguish carefully
between the proposition that the taxi is blue and the proposition that it appears blue.)
b. What if you know that 9 out of 10 Athenian taxis are green?
13.22 Text categorization is the task of assigning a given document to one of a fixed set of
categories on the basis of the text it contains. Naive Bayes models are often used for this
task. In these models, the query variable is the document category, and the â€œeffectâ€ variables
are the presence or absence of each word in the language; the assumption is that words occur
independently in documents, with frequencies determined by the document category.
a. Explain precisely how such a model can be constructed, given as â€œtraining dataâ€ a set
of documents that have been assigned to categories.
b. Explain precisely how to categorize a new document.
c. Is the conditional independence assumption reasonable? Discuss.
13.23 In our analysis of the wumpus world, we used the fact that each square contains a
pit with probability 0.2, independently of the contents of the other squares. Suppose instead
that exactly N/5 pits are scattered at random among the N squares other than [1,1]. Are
the variables Pi,j and Pk,l still independent? What is the joint distribution P(P1,1 , . . . , P4,4 )
now? Redo the calculation for the probabilities of pits in [1,3] and [2,2].
13.24 Redo the probability calculation for pits in [1,3] and [2,2], assuming that each square
contains a pit with probability 0.01, independent of the other squares. What can you say
about the relative performance of a logical versus a probabilistic agent in this case?
13.25 Implement a hybrid probabilistic agent for the wumpus world, based on the hybrid
agent in Figure 7.20 and the probabilistic inference procedure outlined in this chapter.

14

PROBABILISTIC
REASONING

In which we explain how to build network models to reason under uncertainty
according to the laws of probability theory.

Chapter 13 introduced the basic elements of probability theory and noted the importance of
independence and conditional independence relationships in simplifying probabilistic representations of the world. This chapter introduces a systematic way to represent such relationships explicitly in the form of Bayesian networks. We define the syntax and semantics of
these networks and show how they can be used to capture uncertain knowledge in a natural and efficient way. We then show how probabilistic inference, although computationally
intractable in the worst case, can be done efficiently in many practical situations. We also
describe a variety of approximate inference algorithms that are often applicable when exact
inference is infeasible. We explore ways in which probability theory can be applied to worlds
with objects and relationsâ€”that is, to first-order, as opposed to propositional, representations.
Finally, we survey alternative approaches to uncertain reasoning.

14.1

R EPRESENTING K NOWLEDGE IN AN U NCERTAIN D OMAIN

BAYESIAN NETWORK

In Chapter 13, we saw that the full joint probability distribution can answer any question about
the domain, but can become intractably large as the number of variables grows. Furthermore,
specifying probabilities for possible worlds one by one is unnatural and tedious.
We also saw that independence and conditional independence relationships among variables can greatly reduce the number of probabilities that need to be specified in order to define
the full joint distribution. This section introduces a data structure called a Bayesian network1
to represent the dependencies among variables. Bayesian networks can represent essentially
any full joint probability distribution and in many cases can do so very concisely.
1 This is the most common name, but there are many synonyms, including belief network, probabilistic network, causal network, and knowledge map. In statistics, the term graphical model refers to a somewhat
broader class that includes Bayesian networks. An extension of Bayesian networks called a decision network or
influence diagram is covered in Chapter 16.

510

Section 14.1.

Representing Knowledge in an Uncertain Domain

511

A Bayesian network is a directed graph in which each node is annotated with quantitative probability information. The full specification is as follows:
1. Each node corresponds to a random variable, which may be discrete or continuous.
2. A set of directed links or arrows connects pairs of nodes. If there is an arrow from node
X to node Y , X is said to be a parent of Y. The graph has no directed cycles (and hence
is a directed acyclic graph, or DAG.
3. Each node Xi has a conditional probability distribution P(Xi | P arents(Xi )) that quantifies the effect of the parents on the node.
The topology of the networkâ€”the set of nodes and linksâ€”specifies the conditional independence relationships that hold in the domain, in a way that will be made precise shortly. The
intuitive meaning of an arrow is typically that X has a direct influence on Y, which suggests
that causes should be parents of effects. It is usually easy for a domain expert to decide what
direct influences exist in the domainâ€”much easier, in fact, than actually specifying the probabilities themselves. Once the topology of the Bayesian network is laid out, we need only
specify a conditional probability distribution for each variable, given its parents. We will
see that the combination of the topology and the conditional distributions suffices to specify
(implicitly) the full joint distribution for all the variables.
Recall the simple world described in Chapter 13, consisting of the variables Toothache ,
Cavity, Catch, and Weather . We argued that Weather is independent of the other variables; furthermore, we argued that Toothache and Catch are conditionally independent,
given Cavity. These relationships are represented by the Bayesian network structure shown
in Figure 14.1. Formally, the conditional independence of Toothache and Catch, given
Cavity, is indicated by the absence of a link between Toothache and Catch. Intuitively, the
network represents the fact that Cavity is a direct cause of Toothache and Catch, whereas
no direct causal relationship exists between Toothache and Catch.
Now consider the following example, which is just a little more complex. You have
a new burglar alarm installed at home. It is fairly reliable at detecting a burglary, but also
responds on occasion to minor earthquakes. (This example is due to Judea Pearl, a resident
of Los Angelesâ€”hence the acute interest in earthquakes.) You also have two neighbors, John
and Mary, who have promised to call you at work when they hear the alarm. John nearly
always calls when he hears the alarm, but sometimes confuses the telephone ringing with

Cavity

Weather

Toothache

Catch

Figure 14.1 A simple Bayesian network in which Weather is independent of the other
three variables and Toothache and Catch are conditionally independent, given Cavity .

512

Chapter 14.

Burglary

P(B)
.001

Earthquake

Alarm

B
t
t
f
f

E
t
f
t
f

t
f

.90
.05

P(E)
.002

P(A)
.95
.94
.29
.001

A P(J)

JohnCalls

Probabilistic Reasoning

A P(M)

MaryCalls

t
f

.70
.01

Figure 14.2 A typical Bayesian network, showing both the topology and the conditional
probability tables (CPTs). In the CPTs, the letters B, E, A, J, and M stand for Burglary,
Earthquake, Alarm, JohnCalls, and MaryCalls , respectively.

CONDITIONAL
PROBABILITY TABLE

CONDITIONING CASE

the alarm and calls then, too. Mary, on the other hand, likes rather loud music and often
misses the alarm altogether. Given the evidence of who has or has not called, we would like
to estimate the probability of a burglary.
A Bayesian network for this domain appears in Figure 14.2. The network structure
shows that burglary and earthquakes directly affect the probability of the alarmâ€™s going off,
but whether John and Mary call depends only on the alarm. The network thus represents
our assumptions that they do not perceive burglaries directly, they do not notice minor earthquakes, and they do not confer before calling.
The conditional distributions in Figure 14.2 are shown as a conditional probability
table, or CPT. (This form of table can be used for discrete variables; other representations,
including those suitable for continuous variables, are described in Section 14.2.) Each row
in a CPT contains the conditional probability of each node value for a conditioning case.
A conditioning case is just a possible combination of values for the parent nodesâ€”a miniature possible world, if you like. Each row must sum to 1, because the entries represent an
exhaustive set of cases for the variable. For Boolean variables, once you know that the probability of a true value is p, the probability of false must be 1 â€“ p, so we often omit the second
number, as in Figure 14.2. In general, a table for a Boolean variable with k Boolean parents
contains 2k independently specifiable probabilities. A node with no parents has only one row,
representing the prior probabilities of each possible value of the variable.
Notice that the network does not have nodes corresponding to Maryâ€™s currently listening
to loud music or to the telephone ringing and confusing John. These factors are summarized
in the uncertainty associated with the links from Alarm to JohnCalls and MaryCalls. This
shows both laziness and ignorance in operation: it would be a lot of work to find out why those
factors would be more or less likely in any particular case, and we have no reasonable way to
obtain the relevant information anyway. The probabilities actually summarize a potentially

Section 14.2.

The Semantics of Bayesian Networks

513

infinite set of circumstances in which the alarm might fail to go off (high humidity, power
failure, dead battery, cut wires, a dead mouse stuck inside the bell, etc.) or John or Mary
might fail to call and report it (out to lunch, on vacation, temporarily deaf, passing helicopter,
etc.). In this way, a small agent can cope with a very large world, at least approximately. The
degree of approximation can be improved if we introduce additional relevant information.

14.2

T HE S EMANTICS OF BAYESIAN N ETWORKS
The previous section described what a network is, but not what it means. There are two
ways in which one can understand the semantics of Bayesian networks. The first is to see
the network as a representation of the joint probability distribution. The second is to view
it as an encoding of a collection of conditional independence statements. The two views are
equivalent, but the first turns out to be helpful in understanding how to construct networks,
whereas the second is helpful in designing inference procedures.

14.2.1 Representing the full joint distribution
Viewed as a piece of â€œsyntax,â€ a Bayesian network is a directed acyclic graph with some
numeric parameters attached to each node. One way to define what the network meansâ€”its
semanticsâ€”is to define the way in which it represents a specific joint distribution over all the
variables. To do this, we first need to retract (temporarily) what we said earlier about the parameters associated with each node. We said that those parameters correspond to conditional
probabilities P(Xi | P arents(Xi )); this is a true statement, but until we assign semantics to
the network as a whole, we should think of them just as numbers Î¸(Xi | P arents(Xi )).
A generic entry in the joint distribution is the probability of a conjunction of particular
assignments to each variable, such as P (X1 = x1 âˆ§ . . . âˆ§ Xn = xn ). We use the notation
P (x1 , . . . , xn ) as an abbreviation for this. The value of this entry is given by the formula
n

Î¸(xi | parents(Xi )) ,
(14.1)
P (x1 , . . . , xn ) =
i=1

where parents(Xi ) denotes the values of P arents(Xi ) that appear in x1 , . . . , xn . Thus,
each entry in the joint distribution is represented by the product of the appropriate elements
of the conditional probability tables (CPTs) in the Bayesian network.
From this definition, it is easy to prove that the parameters Î¸(Xi | P arents(Xi )) are
exactly the conditional probabilities P(Xi | P arents(Xi )) implied by the joint distribution
(see Exercise 14.2). Hence, we can rewrite Equation (14.1) as
n

P (xi | parents(Xi )) .
(14.2)
P (x1 , . . . , xn ) =
i=1

In other words, the tables we have been calling conditional probability tables really are conditional probability tables according to the semantics defined in Equation (14.1).
To illustrate this, we can calculate the probability that the alarm has sounded, but neither
a burglary nor an earthquake has occurred, and both John and Mary call. We multiply entries

514

Chapter 14.

Probabilistic Reasoning

from the joint distribution (using single-letter names for the variables):
P (j, m, a, Â¬b, Â¬e) = P (j | a)P (m | a)P (a | Â¬b âˆ§ Â¬e)P (Â¬b)P (Â¬e)
= 0.90 Ã— 0.70 Ã— 0.001 Ã— 0.999 Ã— 0.998 = 0.000628 .
Section 13.3 explained that the full joint distribution can be used to answer any query about
the domain. If a Bayesian network is a representation of the joint distribution, then it too can
be used to answer any query, by summing all the relevant joint entries. Section 14.4 explains
how to do this, but also describes methods that are much more efficient.
A method for constructing Bayesian networks
Equation (14.2) defines what a given Bayesian network means. The next step is to explain
how to construct a Bayesian network in such a way that the resulting joint distribution is a
good representation of a given domain. We will now show that Equation (14.2) implies certain
conditional independence relationships that can be used to guide the knowledge engineer in
constructing the topology of the network. First, we rewrite the entries in the joint distribution
in terms of conditional probability, using the product rule (see page 486):
P (x1 , . . . , xn ) = P (xn | xnâˆ’1 , . . . , x1 )P (xnâˆ’1 , . . . , x1 ) .
Then we repeat the process, reducing each conjunctive probability to a conditional probability
and a smaller conjunction. We end up with one big product:
P (x1 , . . . , xn ) = P (xn | xnâˆ’1 , . . . , x1 )P (xnâˆ’1 | xnâˆ’2 , . . . , x1 ) Â· Â· Â· P (x2 | x1 )P (x1 )
n

=
P (xi | xiâˆ’1 , . . . , x1 ) .
i=1
CHAIN RULE

This identity is called the chain rule. It holds for any set of random variables. Comparing it
with Equation (14.2), we see that the specification of the joint distribution is equivalent to the
general assertion that, for every variable Xi in the network,
P(Xi | Xiâˆ’1 , . . . , X1 ) = P(Xi | P arents(Xi )) ,

(14.3)

provided that P arents(Xi ) âŠ† {Xiâˆ’1 , . . . , X1 }. This last condition is satisfied by numbering
the nodes in a way that is consistent with the partial order implicit in the graph structure.
What Equation (14.3) says is that the Bayesian network is a correct representation of
the domain only if each node is conditionally independent of its other predecessors in the
node ordering, given its parents. We can satisfy this condition with this methodology:
1. Nodes: First determine the set of variables that are required to model the domain. Now
order them, {X1 , . . . , Xn }. Any order will work, but the resulting network will be more
compact if the variables are ordered such that causes precede effects.
2. Links: For i = 1 to n do:
â€¢ Choose, from X1 , . . . , Xiâˆ’1 , a minimal set of parents for Xi , such that Equation (14.3) is satisfied.
â€¢ For each parent insert a link from the parent to Xi .
â€¢ CPTs: Write down the conditional probability table, P(Xi |Parents(Xi )).

Section 14.2.

The Semantics of Bayesian Networks

515

Intuitively, the parents of node Xi should contain all those nodes in X1 , . . . , Xiâˆ’1 that
directly influence Xi . For example, suppose we have completed the network in Figure 14.2
except for the choice of parents for MaryCalls. MaryCalls is certainly influenced by whether
there is a Burglary or an Earthquake, but not directly influenced. Intuitively, our knowledge
of the domain tells us that these events influence Maryâ€™s calling behavior only through their
effect on the alarm. Also, given the state of the alarm, whether John calls has no influence on
Maryâ€™s calling. Formally speaking, we believe that the following conditional independence
statement holds:
P(MaryCalls | JohnCalls , Alarm, Earthquake, Burglary) = P(MaryCalls | Alarm) .

Thus, Alarm will be the only parent node for MaryCalls.
Because each node is connected only to earlier nodes, this construction method guarantees that the network is acyclic. Another important property of Bayesian networks is that they
contain no redundant probability values. If there is no redundancy, then there is no chance
for inconsistency: it is impossible for the knowledge engineer or domain expert to create a
Bayesian network that violates the axioms of probability.
Compactness and node ordering

LOCALLY
STRUCTURED
SPARSE

As well as being a complete and nonredundant representation of the domain, a Bayesian network can often be far more compact than the full joint distribution. This property is what
makes it feasible to handle domains with many variables. The compactness of Bayesian networks is an example of a general property of locally structured (also called sparse) systems.
In a locally structured system, each subcomponent interacts directly with only a bounded
number of other components, regardless of the total number of components. Local structure
is usually associated with linear rather than exponential growth in complexity. In the case of
Bayesian networks, it is reasonable to suppose that in most domains each random variable
is directly influenced by at most k others, for some constant k. If we assume n Boolean
variables for simplicity, then the amount of information needed to specify each conditional
probability table will be at most 2k numbers, and the complete network can be specified by
n2k numbers. In contrast, the joint distribution contains 2n numbers. To make this concrete,
suppose we have n = 30 nodes, each with five parents (k = 5). Then the Bayesian network
requires 960 numbers, but the full joint distribution requires over a billion.
There are domains in which each variable can be influenced directly by all the others,
so that the network is fully connected. Then specifying the conditional probability tables requires the same amount of information as specifying the joint distribution. In some domains,
there will be slight dependencies that should strictly be included by adding a new link. But
if these dependencies are tenuous, then it may not be worth the additional complexity in the
network for the small gain in accuracy. For example, one might object to our burglary network on the grounds that if there is an earthquake, then John and Mary would not call even
if they heard the alarm, because they assume that the earthquake is the cause. Whether to
add the link from Earthquake to JohnCalls and MaryCalls (and thus enlarge the tables)
depends on comparing the importance of getting more accurate probabilities with the cost of
specifying the extra information.

516

Chapter 14.

Probabilistic Reasoning

MaryCalls

MaryCalls

JohnCalls

JohnCalls

Earthquake

Alarm

Burglary

Burglary
Alarm

Earthquake
(a)

(b)

Figure 14.3 Network structure depends on order of introduction. In each network, we
have introduced nodes in top-to-bottom order.

Even in a locally structured domain, we will get a compact Bayesian network only if
we choose the node ordering well. What happens if we happen to choose the wrong order? Consider the burglary example again. Suppose we decide to add the nodes in the order
MaryCalls, JohnCalls, Alarm, Burglary , Earthquake. We then get the somewhat more
complicated network shown in Figure 14.3(a). The process goes as follows:
â€¢ Adding MaryCalls: No parents.
â€¢ Adding JohnCalls: If Mary calls, that probably means the alarm has gone off, which
of course would make it more likely that John calls. Therefore, JohnCalls needs
MaryCalls as a parent.
â€¢ Adding Alarm: Clearly, if both call, it is more likely that the alarm has gone off than if
just one or neither calls, so we need both MaryCalls and JohnCalls as parents.
â€¢ Adding Burglary: If we know the alarm state, then the call from John or Mary might
give us information about our phone ringing or Maryâ€™s music, but not about burglary:
P(Burglary | Alarm, JohnCalls , MaryCalls) = P(Burglary | Alarm) .
Hence we need just Alarm as parent.
â€¢ Adding Earthquake: If the alarm is on, it is more likely that there has been an earthquake. (The alarm is an earthquake detector of sorts.) But if we know that there has
been a burglary, then that explains the alarm, and the probability of an earthquake would
be only slightly above normal. Hence, we need both Alarm and Burglary as parents.
The resulting network has two more links than the original network in Figure 14.2 and requires three more probabilities to be specified. Whatâ€™s worse, some of the links represent
tenuous relationships that require difficult and unnatural probability judgments, such as as-

Section 14.2.

The Semantics of Bayesian Networks

517

sessing the probability of Earthquake, given Burglary and Alarm. This phenomenon is
quite general and is related to the distinction between causal and diagnostic models introduced in Section 13.5.1 (see also Exercise 8.13). If we try to build a diagnostic model with
links from symptoms to causes (as from MaryCalls to Alarm or Alarm to Burglary), we
end up having to specify additional dependencies between otherwise independent causes (and
often between separately occurring symptoms as well). If we stick to a causal model, we end
up having to specify fewer numbers, and the numbers will often be easier to come up with. In
the domain of medicine, for example, it has been shown by Tversky and Kahneman (1982)
that expert physicians prefer to give probability judgments for causal rules rather than for
diagnostic ones.
Figure 14.3(b) shows a very bad node ordering: MaryCalls, JohnCalls, Earthquake,
Burglary, Alarm. This network requires 31 distinct probabilities to be specifiedâ€”exactly the
same number as the full joint distribution. It is important to realize, however, that any of the
three networks can represent exactly the same joint distribution. The last two versions simply
fail to represent all the conditional independence relationships and hence end up specifying a
lot of unnecessary numbers instead.

14.2.2 Conditional independence relations in Bayesian networks

DESCENDANT

MARKOV BLANKET

We have provided a â€œnumericalâ€ semantics for Bayesian networks in terms of the representation of the full joint distribution, as in Equation (14.2). Using this semantics to derive a
method for constructing Bayesian networks, we were led to the consequence that a node is
conditionally independent of its other predecessors, given its parents. It turns out that we
can also go in the other direction. We can start from a â€œtopologicalâ€ semantics that specifies
the conditional independence relationships encoded by the graph structure, and from this we
can derive the â€œnumericalâ€ semantics. The topological semantics2 specifies that each variable is conditionally independent of its non-descendants, given its parents. For example, in
Figure 14.2, JohnCalls is independent of Burglary, Earthquake, and MaryCalls given the
value of Alarm. The definition is illustrated in Figure 14.4(a). From these conditional independence assertions and the interpretation of the network parameters Î¸(Xi | P arents(Xi ))
as specifications of conditional probabilities P(Xi | P arents(Xi )), the full joint distribution
given in Equation (14.2) can be reconstructed. In this sense, the â€œnumericalâ€ semantics and
the â€œtopologicalâ€ semantics are equivalent.
Another important independence property is implied by the topological semantics: a
node is conditionally independent of all other nodes in the network, given its parents, children,
and childrenâ€™s parentsâ€”that is, given its Markov blanket. (Exercise 14.7 asks you to prove
this.) For example, Burglary is independent of JohnCalls and MaryCalls, given Alarm and
Earthquake. This property is illustrated in Figure 14.4(b).
2 There is also a general topological criterion called d-separation for deciding whether a set of nodes X is
conditionally independent of another set Y, given a third set Z. The criterion is rather complicated and is not
needed for deriving the algorithms in this chapter, so we omit it. Details may be found in Pearl (1988) or Darwiche
(2009). Shachter (1998) gives a more intuitive method of ascertaining d-separation.

518

Chapter 14.

U1

...

Um

X

Z1j

Y1

...
(a)

U1

Z nj

Yn

Probabilistic Reasoning

...

Um

X

Z 1j

Y1

...

Z nj

Yn

(b)

Figure 14.4 (a) A node X is conditionally independent of its non-descendants (e.g., the
Zij s) given its parents (the Ui s shown in the gray area). (b) A node X is conditionally
independent of all other nodes in the network given its Markov blanket (the gray area).

14.3

CANONICAL
DISTRIBUTION

DETERMINISTIC
NODES

NOISY-OR

E FFICIENT R EPRESENTATION OF C ONDITIONAL D ISTRIBUTIONS
Even if the maximum number of parents k is smallish, filling in the CPT for a node requires
up to O(2k ) numbers and perhaps a great deal of experience with all the possible conditioning
cases. In fact, this is a worst-case scenario in which the relationship between the parents and
the child is completely arbitrary. Usually, such relationships are describable by a canonical
distribution that fits some standard pattern. In such cases, the complete table can be specified
by naming the pattern and perhaps supplying a few parametersâ€”much easier than supplying
an exponential number of parameters.
The simplest example is provided by deterministic nodes. A deterministic node has
its value specified exactly by the values of its parents, with no uncertainty. The relationship
can be a logical one: for example, the relationship between the parent nodes Canadian, US ,
Mexican and the child node NorthAmerican is simply that the child is the disjunction of
the parents. The relationship can also be numerical: for example, if the parent nodes are
the prices of a particular model of car at several dealers and the child node is the price that
a bargain hunter ends up paying, then the child node is the minimum of the parent values;
or if the parent nodes are a lakeâ€™s inflows (rivers, runoff, precipitation) and outflows (rivers,
evaporation, seepage) and the child is the change in the water level of the lake, then the value
of the child is the sum of the inflow parents minus the sum of the outflow parents.
Uncertain relationships can often be characterized by so-called noisy logical relationships. The standard example is the noisy-OR relation, which is a generalization of the logical OR. In propositional logic, we might say that Fever is true if and only if Cold , Flu, or
Malaria is true. The noisy-OR model allows for uncertainty about the ability of each parent to cause the child to be trueâ€”the causal relationship between parent and child may be

Section 14.3.

LEAK NODE

Efficient Representation of Conditional Distributions

519

inhibited, and so a patient could have a cold, but not exhibit a fever. The model makes two
assumptions. First, it assumes that all the possible causes are listed. (If some are missing,
we can always add a so-called leak node that covers â€œmiscellaneous causes.â€) Second, it
assumes that inhibition of each parent is independent of inhibition of any other parents: for
example, whatever inhibits Malaria from causing a fever is independent of whatever inhibits
Flu from causing a fever. Given these assumptions, Fever is false if and only if all its true
parents are inhibited, and the probability of this is the product of the inhibition probabilities
q for each parent. Let us suppose these individual inhibition probabilities are as follows:
qcold = P (Â¬fever | cold , Â¬ï¬‚u, Â¬malaria) = 0.6 ,
qï¬‚u = P (Â¬fever | Â¬cold , ï¬‚u, Â¬malaria) = 0.2 ,
qmalaria = P (Â¬fever | Â¬cold , Â¬ï¬‚u, malaria ) = 0.1 .
Then, from this information and the noisy-OR assumptions, the entire CPT can be built. The
general rule is that

qj ,
P (xi | parents(Xi )) = 1 âˆ’
{j:Xj = true}

where the product is taken over the parents that are set to true for that row of the CPT. The
following table illustrates this calculation:
Cold

Flu

F
F
F
F
T
T
T
T

F
F
T
T
F
F
T
T

Malaria P (Fever ) P (Â¬Fever )
F
T
F
T
F
T
F
T

0.0
0.9
0.8
0.98
0.4
0.94
0.88
0.988

1.0
0.1
0.2
0.02 = 0.2 Ã— 0.1
0.6
0.06 = 0.6 Ã— 0.1
0.12 = 0.6 Ã— 0.2
0.012 = 0.6 Ã— 0.2 Ã— 0.1

In general, noisy logical relationships in which a variable depends on k parents can be described using O(k) parameters instead of O(2k ) for the full conditional probability table.
This makes assessment and learning much easier. For example, the CPCS network (Pradhan et al., 1994) uses noisy-OR and noisy-MAX distributions to model relationships among
diseases and symptoms in internal medicine. With 448 nodes and 906 links, it requires only
8,254 values instead of 133,931,430 for a network with full CPTs.
Bayesian nets with continuous variables

DISCRETIZATION

Many real-world problems involve continuous quantities, such as height, mass, temperature,
and money; in fact, much of statistics deals with random variables whose domains are continuous. By definition, continuous variables have an infinite number of possible values, so it is
impossible to specify conditional probabilities explicitly for each value. One possible way to
handle continuous variables is to avoid them by using discretizationâ€”that is, dividing up the

520

Chapter 14.

Subsidy

Probabilistic Reasoning

Harvest
Cost

Buys
Figure 14.5 A simple network with discrete variables (Subsidy and Buys) and continuous
variables (Harvest and Cost ).

PARAMETER

NONPARAMETRIC

HYBRID BAYESIAN
NETWORK

LINEAR GAUSSIAN

possible values into a fixed set of intervals. For example, temperatures could be divided into
(<0o C), (0o Câˆ’100o C), and (>100o C). Discretization is sometimes an adequate solution,
but often results in a considerable loss of accuracy and very large CPTs. The most common solution is to define standard families of probability density functions (see Appendix A)
that are specified by a finite number of parameters. For example, a Gaussian (or normal)
distribution N (Î¼, Ïƒ 2 )(x) has the mean Î¼ and the variance Ïƒ 2 as parameters. Yet another
solutionâ€”sometimes called a nonparametric representationâ€”is to define the conditional
distribution implicitly with a collection of instances, each containing specific values of the
parent and child variables. We explore this approach further in Chapter 18.
A network with both discrete and continuous variables is called a hybrid Bayesian
network. To specify a hybrid network, we have to specify two new kinds of distributions:
the conditional distribution for a continuous variable given discrete or continuous parents;
and the conditional distribution for a discrete variable given continuous parents. Consider the
simple example in Figure 14.5, in which a customer buys some fruit depending on its cost,
which depends in turn on the size of the harvest and whether the governmentâ€™s subsidy scheme
is operating. The variable Cost is continuous and has continuous and discrete parents; the
variable Buys is discrete and has a continuous parent.
For the Cost variable, we need to specify P(Cost | Harvest, Subsidy). The discrete
parent is handled by enumerationâ€”that is, by specifying both P (Cost | Harvest, subsidy)
and P (Cost | Harvest, Â¬subsidy). To handle Harvest, we specify how the distribution over
the cost c depends on the continuous value h of Harvest. In other words, we specify the
parameters of the cost distribution as a function of h. The most common choice is the linear
Gaussian distribution, in which the child has a Gaussian distribution whose mean Î¼ varies
linearly with the value of the parent and whose standard deviation Ïƒ is fixed. We need two
distributions, one for subsidy and one for Â¬subsidy, with different parameters:
P (c | h, subsidy) = N (at h + bt , Ïƒt2 )(c) =
P (c | h, Â¬subsidy) = N (af h + bf , Ïƒf2 )(c) =

1
âˆš

Ïƒt 2Ï€
1
âˆš

âˆ’ 12

â€œ

e

âˆ’ 12

e

câˆ’(at h+bt )
Ïƒt

â€ž

â€2

câˆ’(af h+bf )
Ïƒf

Â«2

.
Ïƒf 2Ï€
For this example, then, the conditional distribution for Cost is specified by naming the linear
Gaussian distribution and providing the parameters at , bt , Ïƒt , af , bf , and Ïƒf . Figures 14.6(a)

Section 14.3.

Efficient Representation of Conditional Distributions

P(c | h, subsidy)
0.4
0.3
0.2
0.1
0
0 2 4
6 8 10
Cost c

(a)

P(c | h, Â¬subsidy)
0.4
0.3
0.2
0.1
12
0
8 10
6
0 2 4
4
2
6 8 10
0 Harvest h
Cost c

1012
68
4
0 2Harvest h

(b)

521

P(c | h)
0.4
0.3
0.2
0.1
0
0 2 4
6 8 10
Cost c

1012
68
4
0 2Harvest h

(c)

Figure 14.6 The graphs in (a) and (b) show the probability distribution over Cost as a
function of Harvest size, with Subsidy true and false, respectively. Graph (c) shows the
distribution P (Cost | Harvest ), obtained by summing over the two subsidy cases.

CONDITIONAL
GAUSSIAN

and (b) show these two relationships. Notice that in each case the slope is negative, because
cost decreases as supply increases. (Of course, the assumption of linearity implies that the
cost becomes negative at some point; the linear model is reasonable only if the harvest size is
limited to a narrow range.) Figure 14.6(c) shows the distribution P (c | h), averaging over the
two possible values of Subsidy and assuming that each has prior probability 0.5. This shows
that even with very simple models, quite interesting distributions can be represented.
The linear Gaussian conditional distribution has some special properties. A network
containing only continuous variables with linear Gaussian distributions has a joint distribution that is a multivariate Gaussian distribution (see Appendix A) over all the variables (Exercise 14.9). Furthermore, the posterior distribution given any evidence also has this property.3
When discrete variables are added as parents (not as children) of continuous variables, the
network defines a conditional Gaussian, or CG, distribution: given any assignment to the
discrete variables, the distribution over the continuous variables is a multivariate Gaussian.
Now we turn to the distributions for discrete variables with continuous parents. Consider, for example, the Buys node in Figure 14.5. It seems reasonable to assume that the
customer will buy if the cost is low and will not buy if it is high and that the probability of
buying varies smoothly in some intermediate region. In other words, the conditional distribution is like a â€œsoftâ€ threshold function. One way to make soft thresholds is to use the integral
of the standard normal distribution:
 x
N (0, 1)(x)dx .
Î¦(x) =
âˆ’âˆž

Then the probability of Buys given Cost might be
P (buys | Cost = c) = Î¦((âˆ’c + Î¼)/Ïƒ) ,
which means that the cost threshold occurs around Î¼, the width of the threshold region is proportional to Ïƒ, and the probability of buying decreases as cost increases. This probit distriIt follows that inference in linear Gaussian networks takes only O(n3 ) time in the worst case, regardless of the
network topology. In Section 14.4, we see that inference for networks of discrete variables is NP-hard.
3

Chapter 14.

1

1

0.8

0.8
P(buys | c)

P(c)

522

0.6
0.4
0.2

Probabilistic Reasoning

Logit
Probit

0.6
0.4
0.2

0

0
0

2

4

6
Cost c

8

10

12

0

(a)

2

4

6
Cost c

8

10

12

(b)

Figure 14.7 (a) A normal (Gaussian) distribution for the cost threshold, centered on
Î¼ = 6.0 with standard deviation Ïƒ = 1.0. (b) Logit and probit distributions for the probability
of buys given cost , for the parameters Î¼ = 6.0 and Ïƒ = 1.0.
PROBIT
DISTRIBUTION

LOGIT DISTRIBUTION
LOGISTIC FUNCTION

bution (pronounced â€œpro-bitâ€ and short for â€œprobability unitâ€) is illustrated in Figure 14.7(a).
The form can be justified by proposing that the underlying decision process has a hard threshold, but that the precise location of the threshold is subject to random Gaussian noise.
An alternative to the probit model is the logit distribution (pronounced â€œlow-jitâ€). It
uses the logistic function 1/(1 + eâˆ’x ) to produce a soft threshold:
1
.
P (buys | Cost = c) =
1 + exp(âˆ’2 âˆ’c+Î¼
Ïƒ )
This is illustrated in Figure 14.7(b). The two distributions look similar, but the logit actually
has much longer â€œtails.â€ The probit is often a better fit to real situations, but the logit is sometimes easier to deal with mathematically. It is used widely in neural networks (Chapter 20).
Both probit and logit can be generalized to handle multiple continuous parents by taking a
linear combination of the parent values.

14.4

E XACT I NFERENCE IN BAYESIAN N ETWORKS

EVENT

HIDDEN VARIABLE

The basic task for any probabilistic inference system is to compute the posterior probability
distribution for a set of query variables, given some observed eventâ€”that is, some assignment of values to a set of evidence variables. To simplify the presentation, we will consider
only one query variable at a time; the algorithms can easily be extended to queries with multiple variables. We will use the notation from Chapter 13: X denotes the query variable; E
denotes the set of evidence variables E1 , . . . , Em , and e is a particular observed event; Y will
denotes the nonevidence, nonquery variables Y1 , . . . , Yl (called the hidden variables). Thus,
the complete set of variables is X = {X} âˆª E âˆª Y. A typical query asks for the posterior
probability distribution P(X | e).

Section 14.4.

Exact Inference in Bayesian Networks

523

In the burglary network, we might observe the event in which JohnCalls = true and
MaryCalls = true. We could then ask for, say, the probability that a burglary has occurred:
P(Burglary | JohnCalls = true, MaryCalls = true) = 0.284, 0.716 .
In this section we discuss exact algorithms for computing posterior probabilities and will
consider the complexity of this task. It turns out that the general case is intractable, so Section 14.5 covers methods for approximate inference.

14.4.1 Inference by enumeration
Chapter 13 explained that any conditional probability can be computed by summing terms
from the full joint distribution. More specifically, a query P(X | e) can be answered using
Equation (13.9), which we repeat here for convenience:

P(X, e, y) .
P(X | e) = Î± P(X, e) = Î±
y

Now, a Bayesian network gives a complete representation of the full joint distribution. More
specifically, Equation (14.2) on page 513 shows that the terms P (x, e, y) in the joint distribution can be written as products of conditional probabilities from the network. Therefore, a
query can be answered using a Bayesian network by computing sums of products of conditional probabilities from the network.
Consider the query P(Burglary | JohnCalls = true, MaryCalls = true). The hidden
variables for this query are Earthquake and Alarm. From Equation (13.9), using initial
letters for the variables to shorten the expressions, we have4

P(B | j, m) = Î± P(B, j, m) = Î±
P(B, j, m, e, a, ) .
e

a

The semantics of Bayesian networks (Equation (14.2)) then gives us an expression in terms
of CPT entries. For simplicity, we do this just for Burglary = true:

P (b)P (e)P (a | b, e)P (j | a)P (m | a) .
P (b | j, m) = Î±
e

a

To compute this expression, we have to add four terms, each computed by multiplying five
numbers. In the worst case, where we have to sum out almost all the variables, the complexity
of the algorithm for a network with n Boolean variables is O(n2n ).
An improvement can be obtained from the following simple observations: the P (b)
term is a constant and can be moved outside the summations over a and e, and the P (e) term
can be moved outside the summation over a. Hence, we have


P (b | j, m) = Î± P (b)
P (e)
P (a | b, e)P (j | a)P (m | a) .
(14.4)
e

a

This expression can be evaluated by looping through the variables in order, multiplying CPT
entries as we go. For each summation, we also need to loop over the variableâ€™s possible
P
An expression such as e P (a, e) means to sum P (A = a, E = e) for all possible values of e. When E is
Boolean, there is an ambiguity in that P (e) is used to mean both P (E = true) and P (E = e), but it should be
clear from context which is intended; in particular, in the context of a sum the latter is intended.

4

524

Chapter 14.

Probabilistic Reasoning

values. The structure of this computation is shown in Figure 14.8. Using the numbers from
Figure 14.2, we obtain P (b | j, m) = Î± Ã— 0.00059224. The corresponding computation for
Â¬b yields Î± Ã— 0.0014919; hence,
P(B | j, m) = Î± 0.00059224, 0.0014919 â‰ˆ 0.284, 0.716 .
That is, the chance of a burglary, given calls from both neighbors, is about 28%.
The evaluation process for the expression in Equation (14.4) is shown as an expression
tree in Figure 14.8. The E NUMERATION -A SK algorithm in Figure 14.9 evaluates such trees
using depth-first recursion. The algorithm is very similar in structure to the backtracking algorithm for solving CSPs (Figure 6.5) and the DPLL algorithm for satisfiability (Figure 7.17).
The space complexity of E NUMERATION -A SK is only linear in the number of variables:
the algorithm sums over the full joint distribution without ever constructing it explicitly. Unfortunately, its time complexity for a network with n Boolean variables is always O(2n )â€”
better than the O(n 2n ) for the simple approach described earlier, but still rather grim.
Note that the tree in Figure 14.8 makes explicit the repeated subexpressions evaluated by the algorithm. The products P (j | a)P (m | a) and P (j | Â¬a)P (m | Â¬a) are computed
twice, once for each value of e. The next section describes a general method that avoids such
wasted computations.

14.4.2 The variable elimination algorithm

VARIABLE
ELIMINATION

The enumeration algorithm can be improved substantially by eliminating repeated calculations of the kind illustrated in Figure 14.8. The idea is simple: do the calculation once and
save the results for later use. This is a form of dynamic programming. There are several versions of this approach; we present the variable elimination algorithm, which is the simplest.
Variable elimination works by evaluating expressions such as Equation (14.4) in right-to-left
order (that is, bottom up in Figure 14.8). Intermediate results are stored, and summations over
each variable are done only for those portions of the expression that depend on the variable.
Let us illustrate this process for the burglary network. We evaluate the expression


P (e)
P(a | B, e) P (j | a) P (m | a) .
P(B | j, m) = Î± P(B)
  
        
  
f1 (B)

FACTOR

e

f2 (E)

a

f3 (A,B,E)

f4 (A)

f5 (A)

Notice that we have annotated each part of the expression with the name of the corresponding
factor; each factor is a matrix indexed by the values of its argument variables. For example,
the factors f4 (A) and f5 (A) corresponding to P (j | a) and P (m | a) depend just on A because
J and M are fixed by the query. They are therefore two-element vectors:

 


 

P (j | a)
0.90
P (m | a)
0.70
=
f5 (A) =
=
.
f4 (A) =
P (j | Â¬a)
0.05
P (m | Â¬a)
0.01
f3 (A, B, E) will be a 2 Ã— 2 Ã— 2 matrix, which is hard to show on the printed page. (The â€œfirstâ€
element is given by P (a | b, e) = 0.95 and the â€œlastâ€ by P (Â¬a | Â¬b, Â¬e) = 0.999.) In terms of
factors, the query expression is written as


f2 (E) Ã—
f3 (A, B, E) Ã— f4 (A) Ã— f5 (A)
P(B | j, m) = Î± f1 (B) Ã—
e

a

Section 14.4.

Exact Inference in Bayesian Networks

525

P(b)
.001
P(e)
.002
P(a|b,e)
.95

P(Â¬e)
.998
P(Â¬a|b,e)
.05

P(a|b,Â¬e)
.94

P(Â¬a|b,Â¬e)
.06

P(j|a)
.90

P( j|Â¬a)
.05

P( j|a)
.90

P( j|Â¬a)
.05

P(m|a)
.70

P(m|Â¬a)
.01

P(m|a)
.70

P(m|Â¬a)
.01

Figure 14.8 The structure of the expression shown in Equation (14.4). The evaluation
proceeds top down, multiplying values along each path and summing at the â€œ+â€ nodes. Notice
the repetition of the paths for j and m.

function E NUMERATION -A SK(X , e, bn) returns a distribution over X
inputs: X , the query variable
e, observed values for variables E
bn, a Bayes net with variables {X} âˆª E âˆª Y /* Y = hidden variables */
Q(X ) â† a distribution over X , initially empty
for each value xi of X do
Q(xi ) â† E NUMERATE -A LL(bn.VARS, exi )
where exi is e extended with X = xi
return N ORMALIZE(Q(X))
function E NUMERATE -A LL(vars, e) returns a real number
if E MPTY ?(vars) then return 1.0
Y â† F IRST(vars)
if Y has value y in e
then return
P (y | parents(Y )) Ã— E NUMERATE -A LL(R EST(vars), e)
else return y P (y | parents(Y )) Ã— E NUMERATE -A LL(R EST(vars), ey )
where ey is e extended with Y = y
Figure 14.9

The enumeration algorithm for answering queries on Bayesian networks.

526

POINTWISE
PRODUCT

Chapter 14.

Probabilistic Reasoning

where the â€œÃ—â€ operator is not ordinary matrix multiplication but instead the pointwise product operation, to be described shortly.
The process of evaluation is a process of summing out variables (right to left) from
pointwise products of factors to produce new factors, eventually yielding a factor that is the
solution, i.e., the posterior distribution over the query variable. The steps are as follows:
â€¢ First, we sum out A from the product of f3 , f4 , and f5 . This gives us a new 2 Ã— 2 factor
f6 (B, E) whose indices range over just B and E:

f3 (A, B, E) Ã— f4 (A) Ã— f5 (A)
f6 (B, E) =
a

= (f3 (a, B, E) Ã— f4 (a) Ã— f5 (a)) + (f3 (Â¬a, B, E) Ã— f4 (Â¬a) Ã— f5 (Â¬a)) .
Now we are left with the expression

f2 (E) Ã— f6 (B, E) .
P(B | j, m) = Î± f1 (B) Ã—
e

â€¢ Next, we sum out E from the product of f2 and f6 :

f2 (E) Ã— f6 (B, E)
f7 (B) =
e

= f2 (e) Ã— f6 (B, e) + f2 (Â¬e) Ã— f6 (B, Â¬e) .
This leaves the expression
P(B | j, m) = Î± f1 (B) Ã— f7 (B)
which can be evaluated by taking the pointwise product and normalizing the result.
Examining this sequence, we see that two basic computational operations are required: pointwise product of a pair of factors, and summing out a variable from a product of factors. The
next section describes each of these operations.
Operations on factors
The pointwise product of two factors f1 and f2 yields a new factor f whose variables are
the union of the variables in f1 and f2 and whose elements are given by the product of the
corresponding elements in the two factors. Suppose the two factors have variables Y1 , . . . , Yk
in common. Then we have
f(X1 . . . Xj , Y1 . . . Yk , Z1 . . . Zl ) = f1 (X1 . . . Xj , Y1 . . . Yk ) f2 (Y1 . . . Yk , Z, . . . Zl ).
If all the variables are binary, then f1 and f2 have 2j+k and 2k+l entries, respectively, and
the pointwise product has 2j+k+l entries. For example, given two factors f1 (A, B) and
f2 (B, C), the pointwise product f1 Ã— f2 = f3 (A, B, C) has 21+1+1 = 8 entries, as illustrated
in Figure 14.10. Notice that the factor resulting from a pointwise product can contain more
variables than any of the factors being multiplied and that the size of a factor is exponential in
the number of variables. This is where both space and time complexity arise in the variable
elimination algorithm.

Section 14.4.

Exact Inference in Bayesian Networks

527

A

B

f1 (A, B)

B

C

f2 (B, C)

A

B

C

f3 (A, B, C)

T
T
F
F

T
F
T
F

.3
.7
.9
.1

T
T
F
F

T
F
T
F

.2
.8
.6
.4

T
T
T
T
F
F
F
F

T
T
F
F
T
T
F
F

T
F
T
F
T
F
T
F

.3 Ã— .2 = .06
.3 Ã— .8 = .24
.7 Ã— .6 = .42
.7 Ã— .4 = .28
.9 Ã— .2 = .18
.9 Ã— .8 = .72
.1 Ã— .6 = .06
.1 Ã— .4 = .04

Figure 14.10

Illustrating pointwise multiplication: f1 (A, B) Ã— f2 (B, C) = f3 (A, B, C).

Summing out a variable from a product of factors is done by adding up the submatrices
formed by fixing the variable to each of its values in turn. For example, to sum out A from
f3 (A, B, C), we write

f3 (A, B, C) = f3 (a, B, C) + f3 (Â¬a, B, C)
f(B, C) =
a



 
 

.06 .24
.18 .72
.24 .96
=
+
=
.
.42 .28
.06 .04
.48 .32
The only trick is to notice that any factor that does not depend on the variable to be summed
out can be moved outside the summation. For example, if we were to sum out E first in the
burglary network, the relevant part of the expression would be


f2 (E) Ã— f3 (A, B, E) Ã— f4 (A) Ã— f5 (A) = f4 (A) Ã— f5 (A) Ã—
f2 (E) Ã— f3 (A, B, E) .
e

e

Now the pointwise product inside the summation is computed, and the variable is summed
out of the resulting matrix.
Notice that matrices are not multiplied until we need to sum out a variable from the
accumulated product. At that point, we multiply just those matrices that include the variable
to be summed out. Given functions for pointwise product and summing out, the variable
elimination algorithm itself can be written quite simply, as shown in Figure 14.11.
Variable ordering and variable relevance
The algorithm in Figure 14.11 includes an unspecified O RDER function to choose an ordering
for the variables. Every choice of ordering yields a valid algorithm, but different orderings
cause different intermediate factors to be generated during the calculation. For example, in
the calculation shown previously, we eliminated A before E; if we do it the other way, the
calculation becomes


f4 (A) Ã— f5 (A) Ã—
f2 (E) Ã— f3 (A, B, E) ,
P(B | j, m) = Î± f1 (B) Ã—
a

e

during which a new factor f6 (A, B) will be generated.
In general, the time and space requirements of variable elimination are dominated by
the size of the largest factor constructed during the operation of the algorithm. This in turn

528

Chapter 14.

Probabilistic Reasoning

function E LIMINATION -A SK(X , e, bn) returns a distribution over X
inputs: X , the query variable
e, observed values for variables E
bn, a Bayesian network specifying joint distribution P(X1 , . . . , Xn )
factors â† [ ]
for each var in O RDER(bn.VARS) do
factors â† [M AKE -FACTOR(var , e)|factors]
if var is a hidden variable then factors â† S UM -O UT(var , factors )
return N ORMALIZE(P OINTWISE -P RODUCT(factors))
Figure 14.11

The variable elimination algorithm for inference in Bayesian networks.

is determined by the order of elimination of variables and by the structure of the network.
It turns out to be intractable to determine the optimal ordering, but several good heuristics
are available. One fairly effective method is a greedy one: eliminate whichever variable
minimizes the size of the next factor to be constructed.
Let us consider one more query: P(JohnCalls | Burglary = true). As usual, the first
step is to write out the nested summation:



P (e)
P (a | b, e)P(J | a)
P (m | a) .
P(J | b) = Î± P (b)
e

a

m


Evaluating this expression from right to left, we notice something interesting: m P (m | a)
is equal to 1 by definition! Hence, there was no need to include it in the first place; the variable M is irrelevant to this query. Another way of saying this is that the result of the query
P (JohnCalls | Burglary = true) is unchanged if we remove MaryCalls from the network
altogether. In general, we can remove any leaf node that is not a query variable or an evidence
variable. After its removal, there may be some more leaf nodes, and these too may be irrelevant. Continuing this process, we eventually find that every variable that is not an ancestor
of a query variable or evidence variable is irrelevant to the query. A variable elimination
algorithm can therefore remove all these variables before evaluating the query.

14.4.3 The complexity of exact inference

SINGLY CONNECTED
POLYTREE

MULTIPLY
CONNECTED

The complexity of exact inference in Bayesian networks depends strongly on the structure of
the network. The burglary network of Figure 14.2 belongs to the family of networks in which
there is at most one undirected path between any two nodes in the network. These are called
singly connected networks or polytrees, and they have a particularly nice property: The time
and space complexity of exact inference in polytrees is linear in the size of the network. Here,
the size is defined as the number of CPT entries; if the number of parents of each node is
bounded by a constant, then the complexity will also be linear in the number of nodes.
For multiply connected networks, such as that of Figure 14.12(a), variable elimination
can have exponential time and space complexity in the worst case, even when the number
of parents per node is bounded. This is not surprising when one considers that because it

Section 14.4.

Exact Inference in Bayesian Networks

529

P(C)=.5
Cloudy
C
t
f

P(S)
.10
.50

P(C)=.5
Rain

Sprinkler

C P(R)
t .80
f .20

Cloudy

Wet
Grass
S
t
t
f
f

R P(W)
t
.99
f
.90
t .90
f .00

(a)

Spr+Rain
S+R
t t
t f
f t
f f

P(W)
.99
.90
.90
.00

P(S+R=x)
tf ft ff

C

tt

t
f

.08 .02 .72 .18
.10 .40 .10 .40

Wet
Grass

(b)

Figure 14.12 (a) A multiply connected network with conditional probability tables. (b) A
clustered equivalent of the multiply connected network.

includes inference in propositional logic as a special case, inference in Bayesian networks is
NP-hard. In fact, it can be shown (Exercise 14.16) that the problem is as hard as that of computing the number of satisfying assignments for a propositional logic formula. This means
that it is #P-hard (â€œnumber-P hardâ€)â€”that is, strictly harder than NP-complete problems.
There is a close connection between the complexity of Bayesian network inference and
the complexity of constraint satisfaction problems (CSPs). As we discussed in Chapter 6,
the difficulty of solving a discrete CSP is related to how â€œtreelikeâ€ its constraint graph is.
Measures such as tree width, which bound the complexity of solving a CSP, can also be
applied directly to Bayesian networks. Moreover, the variable elimination algorithm can be
generalized to solve CSPs as well as Bayesian networks.

14.4.4 Clustering algorithms

CLUSTERING
JOIN TREE

The variable elimination algorithm is simple and efficient for answering individual queries. If
we want to compute posterior probabilities for all the variables in a network, however, it can
be less efficient. For example, in a polytree network, one would need to issue O(n) queries
costing O(n) each, for a total of O(n2 ) time. Using clustering algorithms (also known as
join tree algorithms), the time can be reduced to O(n). For this reason, these algorithms are
widely used in commercial Bayesian network tools.
The basic idea of clustering is to join individual nodes of the network to form cluster nodes in such a way that the resulting network is a polytree. For example, the multiply
connected network shown in Figure 14.12(a) can be converted into a polytree by combining the Sprinkler and Rain node into a cluster node called Sprinkler +Rain, as shown in
Figure 14.12(b). The two Boolean nodes are replaced by a â€œmeganodeâ€ that takes on four
possible values: tt, tf , f t, and f f . The meganode has only one parent, the Boolean variable
Cloudy, so there are two conditioning cases. Although this example doesnâ€™t show it, the
process of clustering often produces meganodes that share some variables.

530

Chapter 14.

Probabilistic Reasoning

Once the network is in polytree form, a special-purpose inference algorithm is required,
because ordinary inference methods cannot handle meganodes that share variables with each
other. Essentially, the algorithm is a form of constraint propagation (see Chapter 6) where the
constraints ensure that neighboring meganodes agree on the posterior probability of any variables that they have in common. With careful bookkeeping, this algorithm is able to compute
posterior probabilities for all the nonevidence nodes in the network in time linear in the size
of the clustered network. However, the NP-hardness of the problem has not disappeared: if a
network requires exponential time and space with variable elimination, then the CPTs in the
clustered network will necessarily be exponentially large.

14.5

MONTE CARLO

A PPROXIMATE I NFERENCE IN BAYESIAN N ETWORKS
Given the intractability of exact inference in large, multiply connected networks, it is essential to consider approximate inference methods. This section describes randomized sampling
algorithms, also called Monte Carlo algorithms, that provide approximate answers whose
accuracy depends on the number of samples generated. Monte Carlo algorithms, of which
simulated annealing (page 126) is an example, are used in many branches of science to estimate quantities that are difficult to calculate exactly. In this section, we are interested in
sampling applied to the computation of posterior probabilities. We describe two families of
algorithms: direct sampling and Markov chain sampling. Two other approachesâ€”variational
methods and loopy propagationâ€”are mentioned in the notes at the end of the chapter.

14.5.1 Direct sampling methods
The primitive element in any sampling algorithm is the generation of samples from a known
probability distribution. For example, an unbiased coin can be thought of as a random variable
Coin with values heads , tails and a prior distribution P(Coin) = 0.5, 0.5. Sampling
from this distribution is exactly like flipping the coin: with probability 0.5 it will return heads,
and with probability 0.5 it will return tails. Given a source of random numbers uniformly
distributed in the range [0, 1], it is a simple matter to sample any distribution on a single
variable, whether discrete or continuous. (See Exercise 14.17.)
The simplest kind of random sampling process for Bayesian networks generates events
from a network that has no evidence associated with it. The idea is to sample each variable
in turn, in topological order. The probability distribution from which the value is sampled is
conditioned on the values already assigned to the variableâ€™s parents. This algorithm is shown
in Figure 14.13. We can illustrate its operation on the network in Figure 14.12(a), assuming
an ordering [Cloudy , Sprinkler , Rain, WetGrass ]:
1.
2.
3.
4.

Sample from P(Cloudy ) = 0.5, 0.5, value is true.
Sample from P(Sprinkler | Cloudy = true) = 0.1, 0.9, value is false.
Sample from P(Rain | Cloudy = true) = 0.8, 0.2, value is true.
Sample from P(WetGrass | Sprinkler = false, Rain = true) = 0.9, 0.1, value is true.

In this case, P RIOR -S AMPLE returns the event [true, false, true, true].

Section 14.5.

Approximate Inference in Bayesian Networks

531

function P RIOR -S AMPLE (bn) returns an event sampled from the prior specified by bn
inputs: bn, a Bayesian network specifying joint distribution P(X1 , . . . , Xn )
x â† an event with n elements
foreach variable Xi in X1 , . . . , Xn do
x[i] â† a random sample from P(Xi | parents(Xi ))
return x
Figure 14.13 A sampling algorithm that generates events from a Bayesian network. Each
variable is sampled according to the conditional distribution given the values already sampled
for the variableâ€™s parents.

It is easy to see that P RIOR -S AMPLE generates samples from the prior joint distribution
specified by the network. First, let SPS (x1 , . . . , xn ) be the probability that a specific event is
generated by the P RIOR -S AMPLE algorithm. Just looking at the sampling process, we have
n

P (xi | parents(Xi ))
SPS (x1 . . . xn ) =
i=1

because each sampling step depends only on the parent values. This expression should look
familiar, because it is also the probability of the event according to the Bayesian netâ€™s representation of the joint distribution, as stated in Equation (14.2). That is, we have
SPS (x1 . . . xn ) = P (x1 . . . xn ) .
This simple fact makes it easy to answer questions by using samples.
In any sampling algorithm, the answers are computed by counting the actual samples
generated. Suppose there are N total samples, and let NPS (x1 , . . . , xn ) be the number of
times the specific event x1 , . . . , xn occurs in the set of samples. We expect this number, as a
fraction of the total, to converge in the limit to its expected value according to the sampling
probability:
NPS (x1 , . . . , xn )
= SPS (x1 , . . . , xn ) = P (x1 , . . . , xn ) .
(14.5)
lim
N â†’âˆž
N
For example, consider the event produced earlier: [true, false, true, true]. The sampling
probability for this event is
SPS (true, false, true, true) = 0.5 Ã— 0.9 Ã— 0.8 Ã— 0.9 = 0.324 .

CONSISTENT

Hence, in the limit of large N , we expect 32.4% of the samples to be of this event.
Whenever we use an approximate equality (â€œâ‰ˆâ€) in what follows, we mean it in exactly
this senseâ€”that the estimated probability becomes exact in the large-sample limit. Such an
estimate is called consistent. For example, one can produce a consistent estimate of the
probability of any partially specified event x1 , . . . , xm , where m â‰¤ n, as follows:
P (x1 , . . . , xm ) â‰ˆ NPS (x1 , . . . , xm )/N .

(14.6)

That is, the probability of the event can be estimated as the fraction of all complete events
generated by the sampling process that match the partially specified event. For example, if

532

Chapter 14.

Probabilistic Reasoning

we generate 1000 samples from the sprinkler network, and 511 of them have Rain = true,
then the estimated probability of rain, written as PÌ‚ (Rain = true), is 0.511.
Rejection sampling in Bayesian networks
REJECTION
SAMPLING

Rejection sampling is a general method for producing samples from a hard-to-sample distribution given an easy-to-sample distribution. In its simplest form, it can be used to compute
conditional probabilitiesâ€”that is, to determine P (X | e). The R EJECTION -S AMPLING algorithm is shown in Figure 14.14. First, it generates samples from the prior distribution specified
by the network. Then, it rejects all those that do not match the evidence. Finally, the estimate
PÌ‚ (X = x | e) is obtained by counting how often X = x occurs in the remaining samples.
Let PÌ‚(X | e) be the estimated distribution that the algorithm returns. From the definition
of the algorithm, we have
PÌ‚(X | e) = Î± NPS (X, e) =

NPS (X, e)
.
NPS (e)

From Equation (14.6), this becomes
PÌ‚(X | e) â‰ˆ

P(X, e)
= P(X | e) .
P (e)

That is, rejection sampling produces a consistent estimate of the true probability.
Continuing with our example from Figure 14.12(a), let us assume that we wish to estimate P(Rain | Sprinkler = true), using 100 samples. Of the 100 that we generate, suppose
that 73 have Sprinkler = false and are rejected, while 27 have Sprinkler = true; of the 27,
8 have Rain = true and 19 have Rain = false. Hence,
P(Rain | Sprinkler = true) â‰ˆ N ORMALIZE (8, 19) = 0.296, 0.704 .
The true answer is 0.3, 0.7. As more samples are collected, the estimate will converge to
the true answer. The standard deviation of the error in each probability will be proportional
âˆš
to 1/ n, where n is the number of samples used in the estimate.
The biggest problem with rejection sampling is that it rejects so many samples! The
fraction of samples consistent with the evidence e drops exponentially as the number of evidence variables grows, so the procedure is simply unusable for complex problems.
Notice that rejection sampling is very similar to the estimation of conditional probabilities directly from the real world. For example, to estimate P(Rain | RedSkyAtNight = true),
one can simply count how often it rains after a red sky is observed the previous eveningâ€”
ignoring those evenings when the sky is not red. (Here, the world itself plays the role of
the sample-generation algorithm.) Obviously, this could take a long time if the sky is very
seldom red, and that is the weakness of rejection sampling.
Likelihood weighting
LIKELIHOOD
WEIGHTING
IMPORTANCE
SAMPLING

Likelihood weighting avoids the inefficiency of rejection sampling by generating only events
that are consistent with the evidence e. It is a particular instance of the general statistical
technique of importance sampling, tailored for inference in Bayesian networks. We begin by

Section 14.5.

Approximate Inference in Bayesian Networks

533

function R EJECTION -S AMPLING(X , e, bn, N ) returns an estimate of P(X|e)
inputs: X , the query variable
e, observed values for variables E
bn, a Bayesian network
N , the total number of samples to be generated
local variables: N, a vector of counts for each value of X , initially zero
for j = 1 to N do
x â† P RIOR -S AMPLE (bn)
if x is consistent with e then
N[x ] â† N[x ]+1 where x is the value of X in x
return N ORMALIZE(N)
Figure 14.14 The rejection-sampling algorithm for answering queries given evidence in a
Bayesian network.

describing how the algorithm works; then we show that it works correctlyâ€”that is, generates
consistent probability estimates.
L IKELIHOOD-W EIGHTING (see Figure 14.15) fixes the values for the evidence variables E and samples only the nonevidence variables. This guarantees that each event generated is consistent with the evidence. Not all events are equal, however. Before tallying the
counts in the distribution for the query variable, each event is weighted by the likelihood that
the event accords to the evidence, as measured by the product of the conditional probabilities
for each evidence variable, given its parents. Intuitively, events in which the actual evidence
appears unlikely should be given less weight.
Let us apply the algorithm to the network shown in Figure 14.12(a), with the query
P(Rain | Cloudy = true, WetGrass = true) and the ordering Cloudy, Sprinkler, Rain, WetGrass. (Any topological ordering will do.) The process goes as follows: First, the weight w
is set to 1.0. Then an event is generated:
1. Cloudy is an evidence variable with value true. Therefore, we set
w â† w Ã— P (Cloudy = true) = 0.5 .
2. Sprinkler is not an evidence variable, so sample from P(Sprinkler | Cloudy = true) =
0.1, 0.9; suppose this returns false.
3. Similarly, sample from P(Rain | Cloudy = true) = 0.8, 0.2; suppose this returns
true.
4. WetGrass is an evidence variable with value true. Therefore, we set
w â† w Ã— P (WetGrass = true | Sprinkler = false, Rain = true) = 0.45 .
Here W EIGHTED -S AMPLE returns the event [true, false, true, true] with weight 0.45, and
this is tallied under Rain = true.
To understand why likelihood weighting works, we start by examining the sampling
probability SWS for W EIGHTED -S AMPLE . Remember that the evidence variables E are fixed

534

Chapter 14.

Probabilistic Reasoning

function L IKELIHOOD-W EIGHTING(X , e, bn, N ) returns an estimate of P(X|e)
inputs: X , the query variable
e, observed values for variables E
bn, a Bayesian network specifying joint distribution P(X1 , . . . , Xn )
N , the total number of samples to be generated
local variables: W, a vector of weighted counts for each value of X , initially zero
for j = 1 to N do
x, w â† W EIGHTED -S AMPLE(bn, e)
W[x ] â† W[x ] + w where x is the value of X in x
return N ORMALIZE(W)
function W EIGHTED -S AMPLE(bn, e) returns an event and a weight
w â† 1; x â† an event with n elements initialized from e
foreach variable Xi in X1 , . . . , Xn do
if Xi is an evidence variable with value xi in e
then w â† w Ã— P (Xi = xi | parents(Xi ))
else x[i] â† a random sample from P(Xi | parents(Xi ))
return x, w
Figure 14.15 The likelihood-weighting algorithm for inference in Bayesian networks. In
W EIGHTED -S AMPLE, each nonevidence variable is sampled according to the conditional
distribution given the values already sampled for the variableâ€™s parents, while a weight is
accumulated based on the likelihood for each evidence variable.

with values e. We call the nonevidence variables Z (including the query variable X). The
algorithm samples each variable in Z given its parent values:
SWS (z, e) =

l


P (zi | parents(Zi )) .

(14.7)

i=1

Notice that P arents(Zi ) can include both nonevidence variables and evidence variables. Unlike the prior distribution P (z), the distribution SWS pays some attention to the evidence: the
sampled values for each Zi will be influenced by evidence among Zi â€™s ancestors. For example, when sampling Sprinkler the algorithm pays attention to the evidence Cloudy = true in
its parent variable. On the other hand, SWS pays less attention to the evidence than does the
true posterior distribution P (z | e), because the sampled values for each Zi ignore evidence
among Zi â€™s non-ancestors.5 For example, when sampling Sprinkler and Rain the algorithm
ignores the evidence in the child variable WetGrass = true; this means it will generate many
samples with Sprinkler = false and Rain = false despite the fact that the evidence actually
rules out this case.
5 Ideally, we would like to use a sampling distribution equal to the true posterior P (z | e), to take all the evidence
into account. This cannot be done efficiently, however. If it could, then we could approximate the desired
probability to arbitrary accuracy with a polynomial number of samples. It can be shown that no such polynomialtime approximation scheme can exist.

Section 14.5.

Approximate Inference in Bayesian Networks

535

The likelihood weight w makes up for the difference between the actual and desired
sampling distributions. The weight for a given sample x, composed from z and e, is the
product of the likelihoods for each evidence variable given its parents (some or all of which
may be among the Zi s):
m

P (ei | parents(Ei )) .
(14.8)
w(z, e) =
i=1

Multiplying Equations (14.7) and (14.8), we see that the weighted probability of a sample has
the particularly convenient form
SWS (z, e)w(z, e) =

l


P (zi | parents(Zi ))

i=1

m


P (ei | parents(Ei ))

i=1

= P (z, e)

(14.9)

because the two products cover all the variables in the network, allowing us to use Equation (14.2) for the joint probability.
Now it is easy to show that likelihood weighting estimates are consistent. For any
particular value x of X, the estimated posterior probability can be calculated as follows:

NWS (x, y, e)w(x, y, e)
from L IKELIHOOD-W EIGHTING
PÌ‚ (x | e) = Î±
y



â‰ˆ Î±



SWS (x, y, e)w(x, y, e)

for large N

y

= Î±



P (x, y, e)

by Equation (14.9)

y

= Î± P (x, e) = P (x | e) .
Hence, likelihood weighting returns consistent estimates.
Because likelihood weighting uses all the samples generated, it can be much more efficient than rejection sampling. It will, however, suffer a degradation in performance as the
number of evidence variables increases. This is because most samples will have very low
weights and hence the weighted estimate will be dominated by the tiny fraction of samples
that accord more than an infinitesimal likelihood to the evidence. The problem is exacerbated
if the evidence variables occur late in the variable ordering, because then the nonevidence
variables will have no evidence in their parents and ancestors to guide the generation of samples. This means the samples will be simulations that bear little resemblance to the reality
suggested by the evidence.

14.5.2 Inference by Markov chain simulation
MARKOV CHAIN
MONTE CARLO

Markov chain Monte Carlo (MCMC) algorithms work quite differently from rejection sampling and likelihood weighting. Instead of generating each sample from scratch, MCMC algorithms generate each sample by making a random change to the preceding sample. It is
therefore helpful to think of an MCMC algorithm as being in a particular current state specifying a value for every variable and generating a next state by making random changes to the

536

GIBBS SAMPLING

Chapter 14.

Probabilistic Reasoning

current state. (If this reminds you of simulated annealing from Chapter 4 or WALK SAT from
Chapter 7, that is because both are members of the MCMC family.) Here we describe a particular form of MCMC called Gibbs sampling, which is especially well suited for Bayesian
networks. (Other forms, some of them significantly more powerful, are discussed in the notes
at the end of the chapter.) We will first describe what the algorithm does, then we will explain
why it works.
Gibbs sampling in Bayesian networks
The Gibbs sampling algorithm for Bayesian networks starts with an arbitrary state (with the
evidence variables fixed at their observed values) and generates a next state by randomly
sampling a value for one of the nonevidence variables Xi . The sampling for Xi is done
conditioned on the current values of the variables in the Markov blanket of Xi . (Recall from
page 517 that the Markov blanket of a variable consists of its parents, children, and childrenâ€™s
parents.) The algorithm therefore wanders randomly around the state spaceâ€”the space of
possible complete assignmentsâ€”flipping one variable at a time, but keeping the evidence
variables fixed.
Consider the query P(Rain | Sprinkler = true, WetGrass = true) applied to the network in Figure 14.12(a). The evidence variables Sprinkler and WetGrass are fixed to their
observed values and the nonevidence variables Cloudy and Rain are initialized randomlyâ€”
let us say to true and false respectively. Thus, the initial state is [true, true, false, true].
Now the nonevidence variables are sampled repeatedly in an arbitrary order. For example:
1. Cloudy is sampled, given the current values of its Markov blanket variables: in this
case, we sample from P(Cloudy | Sprinkler = true, Rain = false). (Shortly, we will
show how to calculate this distribution.) Suppose the result is Cloudy = false. Then
the new current state is [false, true, false, true].
2. Rain is sampled, given the current values of its Markov blanket variables: in this case,
we sample from P(Rain | Cloudy = false, Sprinkler = true, WetGrass = true). Suppose this yields Rain = true. The new current state is [false, true, true, true].
Each state visited during this process is a sample that contributes to the estimate for the query
variable Rain. If the process visits 20 states where Rain is true and 60 states where Rain is
false, then the answer to the query is N ORMALIZE (20, 60) = 0.25, 0.75. The complete
algorithm is shown in Figure 14.16.
Why Gibbs sampling works

TRANSITION
PROBABILITY

We will now show that Gibbs sampling returns consistent estimates for posterior probabilities. The material in this section is quite technical, but the basic claim is straightforward:
the sampling process settles into a â€œdynamic equilibriumâ€ in which the long-run fraction of
time spent in each state is exactly proportional to its posterior probability. This remarkable
property follows from the specific transition probability with which the process moves from
one state to another, as defined by the conditional distribution given the Markov blanket of
the variable being sampled.

Section 14.5.

Approximate Inference in Bayesian Networks

537

function G IBBS -A SK (X , e, bn, N ) returns an estimate of P(X|e)
local variables: N, a vector of counts for each value of X , initially zero
Z, the nonevidence variables in bn
x, the current state of the network, initially copied from e
initialize x with random values for the variables in Z
for j = 1 to N do
for each Zi in Z do
set the value of Zi in x by sampling from P(Zi |mb(Zi ))
N[x ] â† N[x ] + 1 where x is the value of X in x
return N ORMALIZE(N)
Figure 14.16 The Gibbs sampling algorithm for approximate inference in Bayesian networks; this version cycles through the variables, but choosing variables at random also works.

MARKOV CHAIN

Let q(x â†’ x ) be the probability that the process makes a transition from state x to
state x . This transition probability defines what is called a Markov chain on the state space.
(Markov chains also figure prominently in Chapters 15 and 17.) Now suppose that we run
the Markov chain for t steps, and let Ï€t (x) be the probability that the system is in state x at
time t. Similarly, let Ï€t+1 (x ) be the probability of being in state x at time t + 1. Given
Ï€t (x), we can calculate Ï€t+1 (x ) by summing, for all states the system could be in at time t,
the probability of being in that state times the probability of making the transition to x :

Ï€t (x)q(x â†’ x ) .
Ï€t+1 (x ) =
x

STATIONARY
DISTRIBUTION

We say that the chain has reached its stationary distribution if Ï€t = Ï€t+1 . Let us call this
stationary distribution Ï€; its defining equation is therefore

Ï€(x)q(x â†’ x )
for all x .
(14.10)
Ï€(x ) =
x

ERGODIC

Provided the transition probability distribution q is ergodicâ€”that is, every state is reachable
from every other and there are no strictly periodic cyclesâ€”there is exactly one distribution Ï€
satisfying this equation for any given q.
Equation (14.10) can be read as saying that the expected â€œoutflowâ€ from each state (i.e.,
its current â€œpopulationâ€) is equal to the expected â€œinflowâ€ from all the states. One obvious
way to satisfy this relationship is if the expected flow between any pair of states is the same
in both directions; that is,
Ï€(x)q(x â†’ x ) = Ï€(x )q(x â†’ x)

DETAILED BALANCE

for all x, x .

(14.11)

When these equations hold, we say that q(x â†’ x ) is in detailed balance with Ï€(x).
We can show that detailed balance implies stationarity simply by summing over x in
Equation (14.11). We have



Ï€(x)q(x â†’ x ) =
Ï€(x )q(x â†’ x) = Ï€(x )
q(x â†’ x) = Ï€(x )
x

x

x

538

Chapter 14.

Probabilistic Reasoning

where the last step follows because a transition from x is guaranteed to occur.
The transition probability q(x â†’ x ) defined by the sampling step in G IBBS-A SK is
actually a special case of the more general definition of Gibbs sampling, according to which
each variable is sampled conditionally on the current values of all the other variables. We
start by showing that this general definition of Gibbs sampling satisfies the detailed balance
equation with a stationary distribution equal to P (x | e), (the true posterior distribution on
the nonevidence variables). Then, we simply observe that, for Bayesian networks, sampling
conditionally on all variables is equivalent to sampling conditionally on the variableâ€™s Markov
blanket (see page 517).
To analyze the general Gibbs sampler, which samples each Xi in turn with a transition
probability qi that conditions on all the other variables, we define Xi to be these other variables (except the evidence variables); their values in the current state are xi . If we sample a
new value xi for Xi conditionally on all the other variables, including the evidence, we have
qi (x â†’ x ) = qi ((xi , xi ) â†’ (xi , xi )) = P (xi | xi , e) .
Now we show that the transition probability for each step of the Gibbs sampler is in detailed
balance with the true posterior:
Ï€(x)qi (x â†’ x ) = P (x | e)P (xi | xi , e) = P (xi , xi | e)P (xi | xi , e)
= P (xi | xi , e)P (xi | e)P (xi | xi , e)
=
=

P (xi | xi , e)P (xi , xi | e)
Ï€(x )qi (x â†’ x) .

(using the chain rule on the first term)
(using the chain rule backward)

We can think of the loop â€œfor each Zi in Z doâ€ in Figure 14.16 as defining one large transition
probability q that is the sequential composition q1 â—¦ q2 â—¦ Â· Â· Â· â—¦ qn of the transition probabilities
for the individual variables. It is easy to show (Exercise 14.19) that if each of qi and qj has
Ï€ as its stationary distribution, then the sequential composition qi â—¦ qj does too; hence the
transition probability q for the whole loop has P (x | e) as its stationary distribution. Finally,
unless the CPTs contain probabilities of 0 or 1â€”which can cause the state space to become
disconnectedâ€”it is easy to see that q is ergodic. Hence, the samples generated by Gibbs
sampling will eventually be drawn from the true posterior distribution.
The final step is to show how to perform the general Gibbs sampling stepâ€”sampling
Xi from P(Xi | xi , e)â€”in a Bayesian network. Recall from page 517 that a variable is independent of all other variables given its Markov blanket; hence,
P (xi | xi , e) = P (xi | mb(Xi )) ,
where mb(Xi ) denotes the values of the variables in Xi â€™s Markov blanket, M B(Xi ). As
shown in Exercise 14.7, the probability of a variable given its Markov blanket is proportional
to the probability of the variable given its parents times the probability of each child given its
respective parents:

P (yj | parents(Yj )) . (14.12)
P (xi | mb(Xi )) = Î± P (xi | parents(Xi )) Ã—
Yj âˆˆChildren(Xi )

Hence, to flip each variable Xi conditioned on its Markov blanket, the number of multiplications required is equal to the number of Xi â€™s children.

Section 14.6.

Relational and First-Order Probability Models

539

Quality(B2)

Quality(B1)
Quality(B1)
Honesty(C1)

Honesty(C1)

Kindness(C1)

Honesty(C2)

Kindness(C2)

Kindness(C1)

Recommendation(C1, B1)
(a)

Recommendation(C1, B1)

Recommendation(C2, B1)

Recommendation(C1, B2)

Recommendation(C2, B2)
(b)

Figure 14.17 (a) Bayes net for a single customer C1 recommending a single book B1 .
Honest(C1 ) is Boolean, while the other variables have integer values from 1 to 5. (b) Bayes
net with two customers and two books.

14.6

R ELATIONAL AND F IRST-O RDER P ROBABILITY M ODELS
In Chapter 8, we explained the representational advantages possessed by first-order logic in
comparison to propositional logic. First-order logic commits to the existence of objects and
relations among them and can express facts about some or all of the objects in a domain. This
often results in representations that are vastly more concise than the equivalent propositional
descriptions. Now, Bayesian networks are essentially propositional: the set of random variables is fixed and finite, and each has a fixed domain of possible values. This fact limits the
applicability of Bayesian networks. If we can find a way to combine probability theory with
the expressive power of first-order representations, we expect to be able to increase dramatically the range of problems that can be handled.
For example, suppose that an online book retailer would like to provide overall evaluations of products based on recommendations received from its customers. The evaluation
will take the form of a posterior distribution over the quality of the book, given the available evidence. The simplest solution to base the evaluation on the average recommendation,
perhaps with a variance determined by the number of recommendations, but this fails to take
into account the fact that some customers are kinder than others and some are less honest than
others. Kind customers tend to give high recommendations even to fairly mediocre books,
while dishonest customers give very high or very low recommendations for reasons other
than qualityâ€”for example, they might work for a publisher. 6
For a single customer C1 , recommending a single book B1 , the Bayes net might look
like the one shown in Figure 14.17(a). (Just as in Section 9.1, expressions with parentheses
such as Honest(C1 ) are just fancy symbolsâ€”in this case, fancy names for random variables.)
6

A game theorist would advise a dishonest customer to avoid detection by occasionally recommending a good
book from a competitor. See Chapter 17.

540

Chapter 14.

Probabilistic Reasoning

With two customers and two books, the Bayes net looks like the one in Figure 14.17(b). For
larger numbers of books and customers, it becomes completely impractical to specify the
network by hand.
Fortunately, the network has a lot of repeated structure. Each Recommendation (c, b)
variable has as its parents the variables Honest(c), Kindness(c), and Quality(b). Moreover,
the CPTs for all the Recommendation (c, b) variables are identical, as are those for all the
Honest(c) variables, and so on. The situation seems tailor-made for a first-order language.
We would like to say something like
Recommendation (c, b) âˆ¼ RecCPT (Honest(c), Kindness (c), Quality (b))
with the intended meaning that a customerâ€™s recommendation for a book depends on the
customerâ€™s honesty and kindness and the bookâ€™s quality according to some fixed CPT. This
section develops a language that lets us say exactly this, and a lot more besides.

14.6.1 Possible worlds
Recall from Chapter 13 that a probability model defines a set Î© of possible worlds with
a probability P (Ï‰) for each world Ï‰. For Bayesian networks, the possible worlds are assignments of values to variables; for the Boolean case in particular, the possible worlds are
identical to those of propositional logic. For a first-order probability model, then, it seems
we need the possible worlds to be those of first-order logicâ€”that is, a set of objects with
relations among them and an interpretation that maps constant symbols to objects, predicate
symbols to relations, and function symbols to functions on those objects. (See Section 8.2.)
The model also needs to define a probability for each such possible world, just as a Bayesian
network defines a probability for each assignment of values to variables.
Let us suppose, for a moment, that we have figured out how to do this. Then, as usual
(see page 485), we can obtain the probability of any first-order logical sentence Ï† as a sum
over the possible worlds where it is true:

P (Ï‰) .
(14.13)
P (Ï†) =
Ï‰:Ï† is true in Ï‰

Conditional probabilities P (Ï† | e) can be obtained similarly, so we can, in principle, ask any
question we want of our modelâ€”e.g., â€œWhich books are most likely to be recommended
highly by dishonest customers?â€â€”and get an answer. So far, so good.
There is, however, a problem: the set of first-order models is infinite. We saw this
explicitly in Figure 8.4 on page 293, which we show again in Figure 14.18 (top). This means
that (1) the summation in Equation (14.13) could be infeasible, and (2) specifying a complete,
consistent distribution over an infinite set of worlds could be very difficult.
Section 14.6.2 explores one approach to dealing with this problem. The idea is to
borrow not from the standard semantics of first-order logic but from the database semantics defined in Section 8.2.8 (page 299). The database semantics makes the unique names
assumptionâ€”here, we adopt it for the constant symbols. It also assumes domain closureâ€”
there are no more objects than those that are named. We can then guarantee a finite set of
possible worlds by making the set of objects in each world be exactly the set of constant

Section 14.6.

Relational and First-Order Probability Models

R

J

R

R

J

R

J

R

R

J

R
J

J

R

R

J

R
J

541

J

R

R

...

...

R

R

J

R
J

J

J

...

J

...

J

R
J

Figure 14.18 Top: Some members of the set of all possible worlds for a language with two
constant symbols, R and J, and one binary relation symbol, under the standard semantics for
first-order logic. Bottom: the possible worlds under database semantics. The interpretation
of the constant symbols is fixed, and there is a distinct object for each constant symbol.

RELATIONAL
PROBABILITY MODEL

SIBYL
SIBYL ATTACK
EXISTENCE
UNCERTAINTY
IDENTITY
UNCERTAINTY

symbols that are used; as shown in Figure 14.18 (bottom), there is no uncertainty about the
mapping from symbols to objects or about the objects that exist. We will call models defined
in this way relational probability models, or RPMs.7 The most significant difference between the semantics of RPMs and the database semantics introduced in Section 8.2.8 is that
RPMs do not make the closed-world assumptionâ€”obviously, assuming that every unknown
fact is false doesnâ€™t make sense in a probabilistic reasoning system!
When the underlying assumptions of database semantics fail to hold, RPMs wonâ€™t work
well. For example, a book retailer might use an ISBN (International Standard Book Number)
as a constant symbol to name each book, even though a given â€œlogicalâ€ book (e.g., â€œGone
With the Windâ€) may have several ISBNs. It would make sense to aggregate recommendations across multiple ISBNs, but the retailer may not know for sure which ISBNs are really
the same book. (Note that we are not reifying the individual copies of the book, which might
be necessary for used-book sales, car sales, and so on.) Worse still, each customer is identified by a login ID, but a dishonest customer may have thousands of IDs! In the computer
security field, these multiple IDs are called sibyls and their use to confound a reputation system is called a sibyl attack. Thus, even a simple application in a relatively well-defined,
online domain involves both existence uncertainty (what are the real books and customers
underlying the observed data) and identity uncertainty (which symbol really refer to the
same object). We need to bite the bullet and define probability models based on the standard
semantics of first-order logic, for which the possible worlds vary in the objects they contain
and in the mappings from symbols to objects. Section 14.6.3 shows how to do this.
7

The name relational probability model was given by Pfeffer (2000) to a slightly different representation, but
the underlying ideas are the same.

542

Chapter 14.

Probabilistic Reasoning

14.6.2 Relational probability models

TYPE SIGNATURE

Like first-order logic, RPMs have constant, function, and predicate symbols. (It turns out to
be easier to view predicates as functions that return true or false.) We will also assume a
type signature for each function, that is, a specification of the type of each argument and the
functionâ€™s value. If the type of each object is known, many spurious possible worlds are eliminated by this mechanism. For the book-recommendation domain, the types are Customer
and Book , and the type signatures for the functions and predicates are as follows:
Honest : Customer â†’ {true, false}Kindness : Customer â†’ {1, 2, 3, 4, 5}
Quality : Book â†’ {1, 2, 3, 4, 5}
Recommendation : Customer Ã— Book â†’ {1, 2, 3, 4, 5}
The constant symbols will be whatever customer and book names appear in the retailerâ€™s data
set. In the example given earlier (Figure 14.17(b)), these were C1 , C2 and B1 , B2 .
Given the constants and their types, together with the functions and their type signatures, the random variables of the RPM are obtained by instantiating each function with each
possible combination of objects: Honest(C1 ), Quality(B2 ), Recommendation (C1 , B2 ),
and so on. These are exactly the variables appearing in Figure 14.17(b). Because each type
has only finitely many instances, the number of basic random variables is also finite.
To complete the RPM, we have to write the dependencies that govern these random
variables. There is one dependency statement for each function, where each argument of the
function is a logical variable (i.e., a variable that ranges over objects, as in first-order logic):
Honest(c) âˆ¼ 0.99, 0.01
Kindness(c) âˆ¼ 0.1, 0.1, 0.2, 0.3, 0.3
Quality(b) âˆ¼ 0.05, 0.2, 0.4, 0.2, 0.15
Recommendation (c, b) âˆ¼ RecCPT (Honest(c), Kindness (c), Quality (b))

CONTEXT-SPECIFIC
INDEPENDENCE

where RecCPT is a separately defined conditional distribution with 2 Ã— 5 Ã— 5 = 50 rows,
each with 5 entries. The semantics of the RPM can be obtained by instantiating these dependencies for all known constants, giving a Bayesian network (as in Figure 14.17(b)) that
defines a joint distribution over the RPMâ€™s random variables.8
We can refine the model by introducing a context-specific independence to reflect the
fact that dishonest customers ignore quality when giving a recommendation; moreover, kindness plays no role in their decisions. A context-specific independence allows a variable to be
independent of some of its parents given certain values of others; thus, Recommendation (c, b)
is independent of Kindness(c) and Quality(b) when Honest(c) = false:
Recommendation (c, b) âˆ¼

if Honest(c) then
HonestRecCPT (Kindness(c), Quality (b))
else 0.4, 0.1, 0.0, 0.1, 0.4 .

8

Some technical conditions must be observed to guarantee that the RPM defines a proper distribution. First,
the dependencies must be acyclic, otherwise the resulting Bayesian network will have cycles and will not define
a proper distribution. Second, the dependencies must be well-founded, that is, there can be no infinite ancestor
chains, such as might arise from recursive dependencies. Under some circumstances (see Exercise 14.6), a fixedpoint calculation yields a well-defined probability model for a recursive RPM.

Section 14.6.

Relational and First-Order Probability Models

Fan(C1, A1)

Quality(B1)

Honesty(C1)

Recommendation(C1, B1)
Figure 14.19

Fan(C1, A2)

Kindness(C1)

543

Author(B2)

Quality(B2)

Recommendation(C2, B1)

Fragment of the equivalent Bayes net when Author(B2 ) is unknown.

This kind of dependency may look like an ordinary ifâ€“thenâ€“else statement on a programming
language, but there is a key difference: the inference engine doesnâ€™t necessarily know the
value of the conditional test!
We can elaborate this model in endless ways to make it more realistic. For example,
suppose that an honest customer who is a fan of a bookâ€™s author always gives the book a 5,
regardless of quality:
Recommendation (c, b) âˆ¼

if Honest(c) then
if Fan(c, Author (b)) then Exactly(5)
else HonestRecCPT (Kindness(c), Quality (b))
else 0.4, 0.1, 0.0, 0.1, 0.4

MULTIPLEXER

RELATIONAL
UNCERTAINTY

Again, the conditional test Fan(c, Author (b)) is unknown, but if a customer gives only 5s to
a particular authorâ€™s books and is not otherwise especially kind, then the posterior probability
that the customer is a fan of that author will be high. Furthermore, the posterior distribution
will tend to discount the customerâ€™s 5s in evaluating the quality of that authorâ€™s books.
In the preceding example, we implicitly assumed that the value of Author (b) is known
for every b, but this may not be the case. How can the system reason about whether, say, C1
is a fan of Author (B2 ) when Author (B2 ) is unknown? The answer is that the system may
have to reason about all possible authors. Suppose (to keep things simple) that there are just
two authors, A1 and A2 . Then Author (B2 ) is a random variable with two possible values,
A1 and A2 , and it is a parent of Recommendation (C1 , B2 ). The variables Fan(C1 , A1 ) and
Fan(C1 , A2 ) are parents too. The conditional distribution for Recommendation (C1 , B2 ) is
then essentially a multiplexer in which the Author (B2 ) parent acts as a selector to choose
which of Fan(C1 , A1 ) and Fan(C1 , A2 ) actually gets to influence the recommendation. A
fragment of the equivalent Bayes net is shown in Figure 14.19. Uncertainty in the value
of Author (B2 ), which affects the dependency structure of the network, is an instance of
relational uncertainty.
In case you are wondering how the system can possibly work out who the author of
B2 is: consider the possibility that three other customers are fans of A1 (and have no other
favorite authors in common) and all three have given B2 a 5, even though most other customers find it quite dismal. In that case, it is extremely likely that A1 is the author of B2 .

544

UNROLLING

Chapter 14.

Probabilistic Reasoning

The emergence of sophisticated reasoning like this from an RPM model of just a few lines
is an intriguing example of how probabilistic influences spread through the web of interconnections among objects in the model. As more dependencies and more objects are added, the
picture conveyed by the posterior distribution often becomes clearer and clearer.
The next question is how to do inference in RPMs. One approach is to collect the
evidence and query and the constant symbols therein, construct the equivalent Bayes net,
and apply any of the inference methods discussed in this chapter. This technique is called
unrolling. The obvious drawback is that the resulting Bayes net may be very large. Furthermore, if there are many candidate objects for an unknown relation or functionâ€”for example,
the unknown author of B2 â€”then some variables in the network may have many parents.
Fortunately, much can be done to improve on generic inference algorithms. First, the
presence of repeated substructure in the unrolled Bayes net means that many of the factors
constructed during variable elimination (and similar kinds of tables constructed by clustering algorithms) will be identical; effective caching schemes have yielded speedups of three
orders of magnitude for large networks. Second, inference methods developed to take advantage of context-specific independence in Bayes nets find many applications in RPMs. Third,
MCMC inference algorithms have some interesting properties when applied to RPMs with
relational uncertainty. MCMC works by sampling complete possible worlds, so in each state
the relational structure is completely known. In the example given earlier, each MCMC state
would specify the value of Author (B2 ), and so the other potential authors are no longer parents of the recommendation nodes for B2 . For MCMC, then, relational uncertainty causes no
increase in network complexity; instead, the MCMC process includes transitions that change
the relational structure, and hence the dependency structure, of the unrolled network.
All of the methods just described assume that the RPM has to be partially or completely
unrolled into a Bayesian network. This is exactly analogous to the method of propositionalization for first-order logical inference. (See page 322.) Resolution theorem-provers and
logic programming systems avoid propositionalizing by instantiating the logical variables
only as needed to make the inference go through; that is, they lift the inference process above
the level of ground propositional sentences and make each lifted step do the work of many
ground steps. The same idea applied in probabilistic inference. For example, in the variable
elimination algorithm, a lifted factor can represent an entire set of ground factors that assign
probabilities to random variables in the RPM, where those random variables differ only in the
constant symbols used to construct them. The details of this method are beyond the scope of
this book, but references are given at the end of the chapter.

14.6.3 Open-universe probability models
We argued earlier that database semantics was appropriate for situations in which we know
exactly the set of relevant objects that exist and can identify them unambiguously. (In particular, all observations about an object are correctly associated with the constant symbol that
names it.) In many real-world settings, however, these assumptions are simply untenable. We
gave the examples of multiple ISBNs and sibyl attacks in the book-recommendation domain
(to which we will return in a moment), but the phenomenon is far more pervasive:

Section 14.6.

Relational and First-Order Probability Models

545

â€¢ A vision system doesnâ€™t know what exists, if anything, around the next corner, and may
not know if the object it sees now is the same one it saw a few minutes ago.
â€¢ A text-understanding system does not know in advance the entities that will be featured
in a text, and must reason about whether phrases such as â€œMary,â€ â€œDr. Smith,â€ â€œshe,â€
â€œhis cardiologist,â€ â€œhis mother,â€ and so on refer to the same object.
â€¢ An intelligence analyst hunting for spies never knows how many spies there really are
and can only guess whether various pseudonyms, phone numbers, and sightings belong
to the same individual.

OPEN UNIVERSE

In fact, a major part of human cognition seems to require learning what objects exist and
being able to connect observationsâ€”which almost never come with unique IDs attachedâ€”to
hypothesized objects in the world.
For these reasons, we need to be able to write so-called open-universe probability
models or OUPMs based on the standard semantics of first-order logic, as illustrated at the
top of Figure 14.18. A language for OUPMs provides a way of writing such models easily
while guaranteeing a unique, consistent probability distribution over the infinite space of
possible worlds.
The basic idea is to understand how ordinary Bayesian networks and RPMs manage
to define a unique probability model and to transfer that insight to the first-order setting. In
essence, a Bayes net generates each possible world, event by event, in the topological order
defined by the network structure, where each event is an assignment of a value to a variable.
An RPM extends this to entire sets of events, defined by the possible instantiations of the
logical variables in a given predicate or function. OUPMs go further by allowing generative
steps that add objects to the possible world under construction, where the number and type
of objects may depend on the objects that are already in that world. That is, the event being
generated is not the assignment of a value to a variable, but the very existence of objects.
One way to do this in OUPMs is to add statements that define conditional distributions
over the numbers of objects of various kinds. For example, in the book-recommendation
domain, we might want to distinguish between customers (real people) and their login IDs.
Suppose we expect somewhere between 100 and 10,000 distinct customers (whom we cannot
observe directly). We can express this as a prior log-normal distribution9 as follows:
# Customer âˆ¼ LogNormal [6.9, 2.32 ]() .
We expect honest customers to have just one ID, whereas dishonest customers might have
anywhere between 10 and 1000 IDs:
# LoginID (Owner = c) âˆ¼

if Honest(c) then Exactly(1)
else LogNormal [6.9, 2.32 ]() .

ORIGIN FUNCTION

This statement defines the number of login IDs for a given owner, who is a customer. The
Owner function is called an origin function because it says where each generated object
came from. In the formal semantics of B LOG (as distinct from first-order logic), the domain
elements in each possible world are actually generation histories (e.g., â€œthe fourth login ID of
the seventh customerâ€) rather than simple tokens.
9

A distribution LogNormal [Î¼, Ïƒ 2 ](x) is equivalent to a distribution N [Î¼, Ïƒ 2 ](x) over loge (x).

546

Chapter 14.

Probabilistic Reasoning

Subject to technical conditions of acyclicity and well-foundedness similar to those for
RPMs, open-universe models of this kind define a unique distribution over possible worlds.
Furthermore, there exist inference algorithms such that, for every such well-defined model
and every first-order query, the answer returned approaches the true posterior arbitrarily
closely in the limit. There are some tricky issues involved in designing these algorithms.
For example, an MCMC algorithm cannot sample directly in the space of possible worlds
when the size of those worlds is unbounded; instead, it samples finite, partial worlds, relying on the fact that only finitely many objects can be relevant to the query in distinct ways.
Moreover, transitions must allow for merging two objects into one or splitting one into two.
(Details are given in the references at the end of the chapter.) Despite these complications,
the basic principle established in Equation (14.13) still holds: the probability of any sentence
is well defined and can be calculated.
Research in this area is still at an early stage, but already it is becoming clear that firstorder probabilistic reasoning yields a tremendous increase in the effectiveness of AI systems
at handling uncertain information. Potential applications include those mentioned aboveâ€”
computer vision, text understanding, and intelligence analysisâ€”as well as many other kinds
of sensor interpretation.

14.7

OTHER A PPROACHES TO U NCERTAIN R EASONING
Other sciences (e.g., physics, genetics, and economics) have long favored probability as a
model for uncertainty. In 1819, Pierre Laplace said, â€œProbability theory is nothing but common sense reduced to calculation.â€ In 1850, James Maxwell said, â€œThe true logic for this
world is the calculus of Probabilities, which takes account of the magnitude of the probability which is, or ought to be, in a reasonable manâ€™s mind.â€
Given this long tradition, it is perhaps surprising that AI has considered many alternatives to probability. The earliest expert systems of the 1970s ignored uncertainty and used
strict logical reasoning, but it soon became clear that this was impractical for most real-world
domains. The next generation of expert systems (especially in medical domains) used probabilistic techniques. Initial results were promising, but they did not scale up because of the
exponential number of probabilities required in the full joint distribution. (Efficient Bayesian
network algorithms were unknown then.) As a result, probabilistic approaches fell out of
favor from roughly 1975 to 1988, and a variety of alternatives to probability were tried for a
variety of reasons:
â€¢ One common view is that probability theory is essentially numerical, whereas human
judgmental reasoning is more â€œqualitative.â€ Certainly, we are not consciously aware
of doing numerical calculations of degrees of belief. (Neither are we aware of doing
unification, yet we seem to be capable of some kind of logical reasoning.) It might be
that we have some kind of numerical degrees of belief encoded directly in strengths
of connections and activations in our neurons. In that case, the difficulty of conscious
access to those strengths is not surprising. One should also note that qualitative reason-

Section 14.7.

Other Approaches to Uncertain Reasoning

547

ing mechanisms can be built directly on top of probability theory, so the â€œno numbersâ€
argument against probability has little force. Nonetheless, some qualitative schemes
have a good deal of appeal in their own right. One of the best studied is default reasoning, which treats conclusions not as â€œbelieved to a certain degree,â€ but as â€œbelieved
until a better reason is found to believe something else.â€ Default reasoning is covered
in Chapter 12.
â€¢ Rule-based approaches to uncertainty have also been tried. Such approaches hope to
build on the success of logical rule-based systems, but add a sort of â€œfudge factorâ€ to
each rule to accommodate uncertainty. These methods were developed in the mid-1970s
and formed the basis for a large number of expert systems in medicine and other areas.
â€¢ One area that we have not addressed so far is the question of ignorance, as opposed
to uncertainty. Consider the flipping of a coin. If we know that the coin is fair, then
a probability of 0.5 for heads is reasonable. If we know that the coin is biased, but
we do not know which way, then 0.5 for heads is again reasonable. Obviously, the
two cases are different, yet the outcome probability seems not to distinguish them. The
Dempsterâ€“Shafer theory uses interval-valued degrees of belief to represent an agentâ€™s
knowledge of the probability of a proposition.
â€¢ Probability makes the same ontological commitment as logic: that propositions are true
or false in the world, even if the agent is uncertain as to which is the case. Researchers
in fuzzy logic have proposed an ontology that allows vagueness: that a proposition can
be â€œsort ofâ€ true. Vagueness and uncertainty are in fact orthogonal issues.
The next three subsections treat some of these approaches in slightly more depth. We will not
provide detailed technical material, but we cite references for further study.

14.7.1 Rule-based methods for uncertain reasoning
Rule-based systems emerged from early work on practical and intuitive systems for logical
inference. Logical systems in general, and logical rule-based systems in particular, have three
desirable properties:
LOCALITY

DETACHMENT

TRUTHFUNCTIONALITY

â€¢ Locality: In logical systems, whenever we have a rule of the form A â‡’ B, we can
conclude B, given evidence A, without worrying about any other rules. In probabilistic
systems, we need to consider all the evidence.
â€¢ Detachment: Once a logical proof is found for a proposition B, the proposition can be
used regardless of how it was derived. That is, it can be detached from its justification.
In dealing with probabilities, on the other hand, the source of the evidence for a belief
is important for subsequent reasoning.
â€¢ Truth-functionality: In logic, the truth of complex sentences can be computed from
the truth of the components. Probability combination does not work this way, except
under strong global independence assumptions.
There have been several attempts to devise uncertain reasoning schemes that retain these
advantages. The idea is to attach degrees of belief to propositions and rules and to devise
purely local schemes for combining and propagating those degrees of belief. The schemes

548

Chapter 14.

Probabilistic Reasoning

are also truth-functional; for example, the degree of belief in A âˆ¨ B is a function of the belief
in A and the belief in B.
The bad news for rule-based systems is that the properties of locality, detachment, and
truth-functionality are simply not appropriate for uncertain reasoning. Let us look at truthfunctionality first. Let H1 be the event that a fair coin flip comes up heads, let T1 be the event
that the coin comes up tails on that same flip, and let H2 be the event that the coin comes
up heads on a second flip. Clearly, all three events have the same probability, 0.5, and so a
truth-functional system must assign the same belief to the disjunction of any two of them.
But we can see that the probability of the disjunction depends on the events themselves and
not just on their probabilities:
P (A)

P (B)

P (A âˆ¨ B)

P (H1 ) = 0.5 P (H1 âˆ¨ H1 ) = 0.50
P (H1 ) = 0.5 P (T1 ) = 0.5 P (H1 âˆ¨ T1 ) = 1.00
P (H2 ) = 0.5 P (H1 âˆ¨ H2 ) = 0.75
It gets worse when we chain evidence together. Truth-functional systems have rules of the
form A %â†’ B that allow us to compute the belief in B as a function of the belief in the rule
and the belief in A. Both forward- and backward-chaining systems can be devised. The belief
in the rule is assumed to be constant and is usually specified by the knowledge engineerâ€”for
example, as A %â†’0.9 B.
Consider the wet-grass situation from Figure 14.12(a) (page 529). If we wanted to be
able to do both causal and diagnostic reasoning, we would need the two rules
Rain %â†’ WetGrass

and

WetGrass %â†’ Rain .

These two rules form a feedback loop: evidence for Rain increases the belief in WetGrass,
which in turn increases the belief in Rain even more. Clearly, uncertain reasoning systems
need to keep track of the paths along which evidence is propagated.
Intercausal reasoning (or explaining away) is also tricky. Consider what happens when
we have the two rules
Sprinkler %â†’ WetGrass

CERTAINTY FACTOR

and

WetGrass %â†’ Rain .

Suppose we see that the sprinkler is on. Chaining forward through our rules, this increases the
belief that the grass will be wet, which in turn increases the belief that it is raining. But this
is ridiculous: the fact that the sprinkler is on explains away the wet grass and should reduce
the belief in rain. A truth-functional system acts as if it also believes Sprinkler %â†’ Rain.
Given these difficulties, how can truth-functional systems be made useful in practice?
The answer lies in restricting the task and in carefully engineering the rule base so that undesirable interactions do not occur. The most famous example of a truth-functional system
for uncertain reasoning is the certainty factors model, which was developed for the M YCIN
medical diagnosis program and was widely used in expert systems of the late 1970s and
1980s. Almost all uses of certainty factors involved rule sets that were either purely diagnostic (as in M YCIN ) or purely causal. Furthermore, evidence was entered only at the â€œrootsâ€
of the rule set, and most rule sets were singly connected. Heckerman (1986) has shown that,

Section 14.7.

Other Approaches to Uncertain Reasoning

549

under these circumstances, a minor variation on certainty-factor inference was exactly equivalent to Bayesian inference on polytrees. In other circumstances, certainty factors could yield
disastrously incorrect degrees of belief through overcounting of evidence. As rule sets became larger, undesirable interactions between rules became more common, and practitioners
found that the certainty factors of many other rules had to be â€œtweakedâ€ when new rules were
added. For these reasons, Bayesian networks have largely supplanted rule-based methods for
uncertain reasoning.

14.7.2 Representing ignorance: Dempsterâ€“Shafer theory
DEMPSTERâ€“SHAFER
THEORY

BELIEF FUNCTION

MASS

The Dempsterâ€“Shafer theory is designed to deal with the distinction between uncertainty
and ignorance. Rather than computing the probability of a proposition, it computes the
probability that the evidence supports the proposition. This measure of belief is called a
belief function, written Bel(X).
We return to coin flipping for an example of belief functions. Suppose you pick a
coin from a magicianâ€™s pocket. Given that the coin might or might not be fair, what belief
should you ascribe to the event that it comes up heads? Dempsterâ€“Shafer theory says that
because you have no evidence either way, you have to say that the belief Bel (Heads) = 0
and also that Bel (Â¬Heads) = 0. This makes Dempsterâ€“Shafer reasoning systems skeptical
in a way that has some intuitive appeal. Now suppose you have an expert at your disposal
who testifies with 90% certainty that the coin is fair (i.e., he is 90% sure that P (Heads) =
0.5). Then Dempsterâ€“Shafer theory gives Bel(Heads) = 0.9 Ã— 0.5 = 0.45 and likewise
Bel(Â¬Heads) = 0.45. There is still a 10 percentage point â€œgapâ€ that is not accounted for by
the evidence.
The mathematical underpinnings of Dempsterâ€“Shafer theory have a similar flavor to
those of probability theory; the main difference is that, instead of assigning probabilities
to possible worlds, the theory assigns masses to sets of possible world, that is, to events.
The masses still must add to 1 over all possible events. Bel (A) is defined to be the sum of
masses for all events that are subsets of (i.e., that entail) A, including A itself. With this
definition, Bel(A) and Bel (Â¬A) sum to at most 1, and the gapâ€”the interval between Bel(A)
and 1 âˆ’ Bel(Â¬A)â€”is often interpreted as bounding the probability of A.
As with default reasoning, there is a problem in connecting beliefs to actions. Whenever
there is a gap in the beliefs, then a decision problem can be defined such that a Dempsterâ€“
Shafer system is unable to make a decision. In fact, the notion of utility in the Dempsterâ€“
Shafer model is not yet well understood because the meanings of masses and beliefs themselves have yet to be understood. Pearl (1988) has argued that Bel (A) should be interpreted
not as a degree of belief in A but as the probability assigned to all the possible worlds (now
interpreted as logical theories) in which A is provable. While there are cases in which this
quantity might be of interest, it is not the same as the probability that A is true.
A Bayesian analysis of the coin-flipping example would suggest that no new formalism
is necessary to handle such cases. The model would have two variables: the Bias of the coin
(a number between 0 and 1, where 0 is a coin that always shows tails and 1 a coin that always
shows heads) and the outcome of the next Flip. The prior probability distribution for Bias

550

Chapter 14.

Probabilistic Reasoning

would reflect our beliefs based on the source of the coin (the magicianâ€™s pocket): some small
probability that it is fair and some probability that it is heavily biased toward heads or tails.
The conditional distribution P(Flip | Bias) simply defines how the bias operates. If P(Bias)
is symmetric about 0.5, then our prior probability for the flip is
 1
P (Bias = x)P (Flip = heads | Bias = x) dx = 0.5 .
P (Flip = heads) =
0

This is the same prediction as if we believe strongly that the coin is fair, but that does not
mean that probability theory treats the two situations identically. The difference arises after
the flips in computing the posterior distribution for Bias. If the coin came from a bank, then
seeing it come up heads three times running would have almost no effect on our strong prior
belief in its fairness; but if the coin comes from the magicianâ€™s pocket, the same evidence
will lead to a stronger posterior belief that the coin is biased toward heads. Thus, a Bayesian
approach expresses our â€œignoranceâ€ in terms of how our beliefs would change in the face of
future information gathering.

14.7.3 Representing vagueness: Fuzzy sets and fuzzy logic
FUZZY SET THEORY

FUZZY LOGIC

FUZZY CONTROL

Fuzzy set theory is a means of specifying how well an object satisfies a vague description.
For example, consider the proposition â€œNate is tall.â€ Is this true if Nate is 5 10 ? Most
people would hesitate to answer â€œtrueâ€ or â€œfalse,â€ preferring to say, â€œsort of.â€ Note that this
is not a question of uncertainty about the external worldâ€”we are sure of Nateâ€™s height. The
issue is that the linguistic term â€œtallâ€ does not refer to a sharp demarcation of objects into two
classesâ€”there are degrees of tallness. For this reason, fuzzy set theory is not a method for
uncertain reasoning at all. Rather, fuzzy set theory treats Tall as a fuzzy predicate and says
that the truth value of Tall (Nate) is a number between 0 and 1, rather than being just true
or false. The name â€œfuzzy setâ€ derives from the interpretation of the predicate as implicitly
defining a set of its membersâ€”a set that does not have sharp boundaries.
Fuzzy logic is a method for reasoning with logical expressions describing membership
in fuzzy sets. For example, the complex sentence Tall (Nate) âˆ§ Heavy(Nate) has a fuzzy
truth value that is a function of the truth values of its components. The standard rules for
evaluating the fuzzy truth, T , of a complex sentence are
T (A âˆ§ B) = min(T (A), T (B))
T (A âˆ¨ B) = max(T (A), T (B))
T (Â¬A) = 1 âˆ’ T (A) .
Fuzzy logic is therefore a truth-functional systemâ€”a fact that causes serious difficulties.
For example, suppose that T (Tall (Nate)) = 0.6 and T (Heavy(Nate)) = 0.4. Then we have
T (Tall (Nate) âˆ§ Heavy(Nate)) = 0.4, which seems reasonable, but we also get the result
T (Tall (Nate) âˆ§ Â¬Tall (Nate)) = 0.4, which does not. Clearly, the problem arises from the
inability of a truth-functional approach to take into account the correlations or anticorrelations
among the component propositions.
Fuzzy control is a methodology for constructing control systems in which the mapping
between real-valued input and output parameters is represented by fuzzy rules. Fuzzy control has been very successful in commercial products such as automatic transmissions, video

Section 14.8.

Summary

551

cameras, and electric shavers. Critics (see, e.g., Elkan, 1993) argue that these applications
are successful because they have small rule bases, no chaining of inferences, and tunable
parameters that can be adjusted to improve the systemâ€™s performance. The fact that they are
implemented with fuzzy operators might be incidental to their success; the key is simply to
provide a concise and intuitive way to specify a smoothly interpolated, real-valued function.
There have been attempts to provide an explanation of fuzzy logic in terms of probability theory. One idea is to view assertions such as â€œNate is Tallâ€ as discrete observations made
concerning a continuous hidden variable, Nateâ€™s actual Height. The probability model specifies P (Observer says Nate is tall | Height), perhaps using a probit distribution as described
on page 522. A posterior distribution over Nateâ€™s height can then be calculated in the usual
way, for example, if the model is part of a hybrid Bayesian network. Such an approach is not
truth-functional, of course. For example, the conditional distribution
P (Observer says Nate is tall and heavy | Height, Weight)

RANDOM SET

14.8

allows for interactions between height and weight in the causing of the observation. Thus,
someone who is eight feet tall and weighs 190 pounds is very unlikely to be called â€œtall and
heavy,â€ even though â€œeight feetâ€ counts as â€œtallâ€ and â€œ190 poundsâ€ counts as â€œheavy.â€
Fuzzy predicates can also be given a probabilistic interpretation in terms of random
setsâ€”that is, random variables whose possible values are sets of objects. For example, Tall
is a random set whose possible values are sets of people. The probability P (Tall = S1 ),
where S1 is some particular set of people, is the probability that exactly that set would be
identified as â€œtallâ€ by an observer. Then the probability that â€œNate is tallâ€ is the sum of the
probabilities of all the sets of which Nate is a member.
Both the hybrid Bayesian network approach and the random sets approach appear to
capture aspects of fuzziness without introducing degrees of truth. Nonetheless, there remain
many open issues concerning the proper representation of linguistic observations and continuous quantitiesâ€”issues that have been neglected by most outside the fuzzy community.

S UMMARY
This chapter has described Bayesian networks, a well-developed representation for uncertain
knowledge. Bayesian networks play a role roughly analogous to that of propositional logic
for definite knowledge.
â€¢ A Bayesian network is a directed acyclic graph whose nodes correspond to random
variables; each node has a conditional distribution for the node, given its parents.
â€¢ Bayesian networks provide a concise way to represent conditional independence relationships in the domain.
â€¢ A Bayesian network specifies a full joint distribution; each joint entry is defined as the
product of the corresponding entries in the local conditional distributions. A Bayesian
network is often exponentially smaller than an explicitly enumerated joint distribution.
â€¢ Many conditional distributions can be represented compactly by canonical families of

552

Chapter 14.

â€¢

â€¢
â€¢

â€¢

â€¢

Probabilistic Reasoning

distributions. Hybrid Bayesian networks, which include both discrete and continuous
variables, use a variety of canonical distributions.
Inference in Bayesian networks means computing the probability distribution of a set
of query variables, given a set of evidence variables. Exact inference algorithms, such
as variable elimination, evaluate sums of products of conditional probabilities as efficiently as possible.
In polytrees (singly connected networks), exact inference takes time linear in the size
of the network. In the general case, the problem is intractable.
Stochastic approximation techniques such as likelihood weighting and Markov chain
Monte Carlo can give reasonable estimates of the true posterior probabilities in a network and can cope with much larger networks than can exact algorithms.
Probability theory can be combined with representational ideas from first-order logic to
produce very powerful systems for reasoning under uncertainty. Relational probability models (RPMs) include representational restrictions that guarantee a well-defined
probability distribution that can be expressed as an equivalent Bayesian network. Openuniverse probability models handle existence and identity uncertainty, defining probabilty distributions over the infinite space of first-order possible worlds.
Various alternative systems for reasoning under uncertainty have been suggested. Generally speaking, truth-functional systems are not well suited for such reasoning.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

The use of networks to represent probabilistic information began early in the 20th century,
with the work of Sewall Wright on the probabilistic analysis of genetic inheritance and animal growth factors (Wright, 1921, 1934). I. J. Good (1961), in collaboration with Alan
Turing, developed probabilistic representations and Bayesian inference methods that could
be regarded as a forerunner of modern Bayesian networksâ€”although the paper is not often
cited in this context.10 The same paper is the original source for the noisy-OR model.
The influence diagram representation for decision problems, which incorporated a
DAG representation for random variables, was used in decision analysis in the late 1970s
(see Chapter 16), but only enumeration was used for evaluation. Judea Pearl developed the
message-passing method for carrying out inference in tree networks (Pearl, 1982a) and polytree networks (Kim and Pearl, 1983) and explained the importance of causal rather than diagnostic probability models, in contrast to the certainty-factor systems then in vogue.
The first expert system using Bayesian networks was C ONVINCE (Kim, 1983). Early
applications in medicine included the M UNIN system for diagnosing neuromuscular disorders
(Andersen et al., 1989) and the PATHFINDER system for pathology (Heckerman, 1991). The
CPCS system (Pradhan et al., 1994) is a Bayesian network for internal medicine consisting
10 I. J. Good was chief statistician for Turingâ€™s code-breaking team in World War II. In 2001: A Space Odyssey
(Clarke, 1968a), Good and Minsky are credited with making the breakthrough that led to the development of the
HAL 9000 computer.

Bibliographical and Historical Notes

MARKOV NETWORK

NONSERIAL DYNAMIC
PROGRAMMING

553

of 448 nodes, 906 links and 8,254 conditional probability values. (The front cover shows a
portion of the network.)
Applications in engineering include the Electric Power Research Instituteâ€™s work on
monitoring power generators (Morjaria et al., 1995), NASAâ€™s work on displaying timecritical information at Mission Control in Houston (Horvitz and Barry, 1995), and the general
field of network tomography, which aims to infer unobserved local properties of nodes and
links in the Internet from observations of end-to-end message performance (Castro et al.,
2004). Perhaps the most widely used Bayesian network systems have been the diagnosisand-repair modules (e.g., the Printer Wizard) in Microsoft Windows (Breese and Heckerman,
1996) and the Office Assistant in Microsoft Office (Horvitz et al., 1998). Another important application area is biology: Bayesian networks have been used for identifying human
genes by reference to mouse genes (Zhang et al., 2003), inferring cellular networks Friedman
(2004), and many other tasks in bioinformatics. We could go on, but instead weâ€™ll refer you
to Pourret et al. (2008), a 400-page guide to applications of Bayesian networks.
Ross Shachter (1986), working in the influence diagram community, developed the first
complete algorithm for general Bayesian networks. His method was based on goal-directed
reduction of the network using posterior-preserving transformations. Pearl (1986) developed
a clustering algorithm for exact inference in general Bayesian networks, utilizing a conversion
to a directed polytree of clusters in which message passing was used to achieve consistency
over variables shared between clusters. A similar approach, developed by the statisticians
David Spiegelhalter and Steffen Lauritzen (Lauritzen and Spiegelhalter, 1988), is based on
conversion to an undirected form of graphical model called a Markov network. This approach is implemented in the H UGIN system, an efficient and widely used tool for uncertain
reasoning (Andersen et al., 1989). Boutilier et al. (1996) show how to exploit context-specific
independence in clustering algorithms.
The basic idea of variable eliminationâ€”that repeated computations within the overall
sum-of-products expression can be avoided by cachingâ€”appeared in the symbolic probabilistic inference (SPI) algorithm (Shachter et al., 1990). The elimination algorithm we describe
is closest to that developed by Zhang and Poole (1994). Criteria for pruning irrelevant variables were developed by Geiger et al. (1990) and by Lauritzen et al. (1990); the criterion we
give is a simple special case of these. Dechter (1999) shows how the variable elimination idea
is essentially identical to nonserial dynamic programming (Bertele and Brioschi, 1972), an
algorithmic approach that can be applied to solve a range of inference problems in Bayesian
networksâ€”for example, finding the most likely explanation for a set of observations. This
connects Bayesian network algorithms to related methods for solving CSPs and gives a direct
measure of the complexity of exact inference in terms of the tree width of the network. Wexler
and Meek (2009) describe a method of preventing exponential growth in the size of factors
computed in variable elimination; their algorithm breaks down large factors into products of
smaller factors and simultaneously computes an error bound for the resulting approximation.
The inclusion of continuous random variables in Bayesian networks was considered
by Pearl (1988) and Shachter and Kenley (1989); these papers discussed networks containing only continuous variables with linear Gaussian distributions. The inclusion of discrete
variables has been investigated by Lauritzen and Wermuth (1989) and implemented in the

554

VARIATIONAL
APPROXIMATION

VARIATIONAL
PARAMETER

MEAN FIELD

Chapter 14.

Probabilistic Reasoning

cHUGIN system (Olesen, 1993). Further analysis of linear Gaussian models, with connections to many other models used in statistics, appears in Roweis and Ghahramani (1999) The
probit distribution is usually attributed to Gaddum (1933) and Bliss (1934), although it had
been discovered several times in the 19th century. Blissâ€™s work was expanded considerably
by Finney (1947). The probit has been used widely for modeling discrete choice phenomena
and can be extended to handle more than two choices (Daganzo, 1979). The logit model was
introduced by Berkson (1944); initially much derided, it eventually became more popular
than the probit model. Bishop (1995) gives a simple justification for its use.
Cooper (1990) showed that the general problem of inference in unconstrained Bayesian
networks is NP-hard, and Paul Dagum and Mike Luby (1993) showed the corresponding
approximation problem to be NP-hard. Space complexity is also a serious problem in both
clustering and variable elimination methods. The method of cutset conditioning, which was
developed for CSPs in Chapter 6, avoids the construction of exponentially large tables. In a
Bayesian network, a cutset is a set of nodes that, when instantiated, reduces the remaining
nodes to a polytree that can be solved in linear time and space. The query is answered by
summing over all the instantiations of the cutset, so the overall space requirement is still linear (Pearl, 1988). Darwiche (2001) describes a recursive conditioning algorithm that allows
a complete range of space/time tradeoffs.
The development of fast approximation algorithms for Bayesian network inference is
a very active area, with contributions from statistics, computer science, and physics. The
rejection sampling method is a general technique that is long known to statisticians; it was
first applied to Bayesian networks by Max Henrion (1988), who called it logic sampling.
Likelihood weighting, which was developed by Fung and Chang (1989) and Shachter and
Peot (1989), is an example of the well-known statistical method of importance sampling.
Cheng and Druzdzel (2000) describe an adaptive version of likelihood weighting that works
well even when the evidence has very low prior likelihood.
Markov chain Monte Carlo (MCMC) algorithms began with the Metropolis algorithm,
due to Metropolis et al. (1953), which was also the source of the simulated annealing algorithm described in Chapter 4. The Gibbs sampler was devised by Geman and Geman (1984)
for inference in undirected Markov networks. The application of MCMC to Bayesian networks is due to Pearl (1987). The papers collected by Gilks et al. (1996) cover a wide variety
of applications of MCMC, several of which were developed in the well-known B UGS package (Gilks et al., 1994).
There are two very important families of approximation methods that we did not cover
in the chapter. The first is the family of variational approximation methods, which can be
used to simplify complex calculations of all kinds. The basic idea is to propose a reduced
version of the original problem that is simple to work with, but that resembles the original
problem as closely as possible. The reduced problem is described by some variational parameters Î» that are adjusted to minimize a distance function D between the original and
the reduced problem, often by solving the system of equations âˆ‚D/âˆ‚Î» = 0. In many cases,
strict upper and lower bounds can be obtained. Variational methods have long been used in
statistics (Rustagi, 1976). In statistical physics, the mean-field method is a particular variational approximation in which the individual variables making up the model are assumed

Bibliographical and Historical Notes

BELIEF
PROPAGATION

TURBO DECODING

INDEXED RANDOM
VARIABLE

555

to be completely independent. This idea was applied to solve large undirected Markov networks (Peterson and Anderson, 1987; Parisi, 1988). Saul et al. (1996) developed the mathematical foundations for applying variational methods to Bayesian networks and obtained
accurate lower-bound approximations for sigmoid networks with the use of mean-field methods. Jaakkola and Jordan (1996) extended the methodology to obtain both lower and upper
bounds. Since these early papers, variational methods have been applied to many specific
families of models. The remarkable paper by Wainwright and Jordan (2008) provides a unifying theoretical analysis of the literature on variational methods.
A second important family of approximation algorithms is based on Pearlâ€™s polytree
message-passing algorithm (1982a). This algorithm can be applied to general networks, as
suggested by Pearl (1988). The results might be incorrect, or the algorithm might fail to terminate, but in many cases, the values obtained are close to the true values. Little attention
was paid to this so-called belief propagation (or BP) approach until McEliece et al. (1998)
observed that message passing in a multiply connected Bayesian network was exactly the
computation performed by the turbo decoding algorithm (Berrou et al., 1993), which provided a major breakthrough in the design of efficient error-correcting codes. The implication
is that BP is both fast and accurate on the very large and very highly connected networks used
for decoding and might therefore be useful more generally. Murphy et al. (1999) presented a
promising empirical study of BPâ€™s performance, and Weiss and Freeman (2001) established
strong convergence results for BP on linear Gaussian networks. Weiss (2000b) shows how an
approximation called loopy belief propagation works, and when the approximation is correct.
Yedidia et al. (2005) made further connections between loopy propagation and ideas from
statistical physics.
The connection between probability and first-order languages was first studied by Carnap (1950). Gaifman (1964) and Scott and Krauss (1966) defined a language in which probabilities could be associated with first-order sentences and for which models were probability
measures on possible worlds. Within AI, this idea was developed for propositional logic
by Nilsson (1986) and for first-order logic by Halpern (1990). The first extensive investigation of knowledge representation issues in such languages was carried out by Bacchus
(1990). The basic idea is that each sentence in the knowledge base expressed a constraint on
the distribution over possible worlds; one sentence entails another if it expresses a stronger
constraint. For example, the sentence âˆ€ x P (Hungry(x)) > 0.2 rules out distributions
in which any object is hungry with probability less than 0.2; thus, it entails the sentence
âˆ€ x P (Hungry(x)) > 0.1. It turns out that writing a consistent set of sentences in these
languages is quite difficult and constructing a unique probability model nearly impossible
unless one adopts the representation approach of Bayesian networks by writing suitable sentences about conditional probabilities.
Beginning in the early 1990s, researchers working on complex applications noticed
the expressive limitations of Bayesian networks and developed various languages for writing
â€œtemplatesâ€ with logical variables, from which large networks could be constructed automatically for each problem instance (Breese, 1992; Wellman et al., 1992). The most important
such language was B UGS (Bayesian inference Using Gibbs Sampling) (Gilks et al., 1994),
which combined Bayesian networks with the indexed random variable notation common in

556

RECORD LINKAGE

Chapter 14.

Probabilistic Reasoning

statistics. (In B UGS, an indexed random variable looks like X[i], where i has a defined integer
range.) These languages inherited the key property of Bayesian networks: every well-formed
knowledge base defines a unique, consistent probability model. Languages with well-defined
semantics based on unique names and domain closure drew on the representational capabilities of logic programming (Poole, 1993; Sato and Kameya, 1997; Kersting et al., 2000)
and semantic networks (Koller and Pfeffer, 1998; Pfeffer, 2000). Pfeffer (2007) went on to
develop I BAL , which represents first-order probability models as probabilistic programs in a
programming language extended with a randomization primitive. Another important thread
was the combination of relational and first-order notations with (undirected) Markov networks (Taskar et al., 2002; Domingos and Richardson, 2004), where the emphasis has been
less on knowledge representation and more on learning from large data sets.
Initially, inference in these models was performed by generating an equivalent Bayesian
network. Pfeffer et al. (1999) introduced a variable elimination algorithm that cached each
computed factor for reuse by later computations involving the same relations but different
objects, thereby realizing some of the computational gains of lifting. The first truly lifted
inference algorithm was a lifted form of variable elimination described by Poole (2003) and
subsequently improved by de Salvo Braz et al. (2007). Further advances, including cases
where certain aggregate probabilities can be computed in closed form, are described by Milch
et al. (2008) and Kisynski and Poole (2009). Pasula and Russell (2001) studied the application
of MCMC to avoid building the complete equivalent Bayes net in cases of relational and
identity uncertainty. Getoor and Taskar (2007) collect many important papers on first-order
probability models and their use in machine learning.
Probabilistic reasoning about identity uncertainty has two distinct origins. In statistics, the problem of record linkage arises when data records do not contain standard unique
identifiersâ€”for example, various citations of this book might name its first author â€œStuart
Russellâ€ or â€œS. J. Russellâ€ or even â€œStewart Russle,â€ and other authors may use the some of
the same names. Literally hundreds of companies exist solely to solve record linkage problems in financial, medical, census, and other data. Probabilistic analysis goes back to work
by Dunn (1946); the Fellegiâ€“Sunter model (1969), which is essentially naive Bayes applied
to matching, still dominates current practice. The second origin for work on identity uncertainty is multitarget tracking (Sittler, 1964), which we cover in Chapter 15. For most of its
history, work in symbolic AI assumed erroneously that sensors could supply sentences with
unique identifiers for objects. The issue was studied in the context of language understanding
by Charniak and Goldman (1992) and in the context of surveillance by (Huang and Russell,
1998) and Pasula et al. (1999). Pasula et al. (2003) developed a complex generative model
for authors, papers, and citation strings, involving both relational and identity uncertainty,
and demonstrated high accuracy for citation information extraction. The first formally defined language for open-universe probability models was B LOG (Milch et al., 2005), which
came with a complete (albeit slow) MCMC inference algorithm for all well-defined mdoels.
(The program code faintly visible on the front cover of this book is part of a B LOG model
for detecting nuclear explosions from seismic signals as part of the UN Comprehensive Test
Ban Treaty verification regime.) Laskey (2008) describes another open-universe modeling
language called multi-entity Bayesian networks.

Bibliographical and Historical Notes

POSSIBILITY THEORY

557

As explained in Chapter 13, early probabilistic systems fell out of favor in the early
1970s, leaving a partial vacuum to be filled by alternative methods. Certainty factors were
invented for use in the medical expert system M YCIN (Shortliffe, 1976), which was intended
both as an engineering solution and as a model of human judgment under uncertainty. The
collection Rule-Based Expert Systems (Buchanan and Shortliffe, 1984) provides a complete
overview of M YCIN and its descendants (see also Stefik, 1995). David Heckerman (1986)
showed that a slightly modified version of certainty factor calculations gives correct probabilistic results in some cases, but results in serious overcounting of evidence in other cases.
The P ROSPECTOR expert system (Duda et al., 1979) used a rule-based approach in which the
rules were justified by a (seldom tenable) global independence assumption.
Dempsterâ€“Shafer theory originates with a paper by Arthur Dempster (1968) proposing
a generalization of probability to interval values and a combination rule for using them. Later
work by Glenn Shafer (1976) led to the Dempster-Shafer theoryâ€™s being viewed as a competing approach to probability. Pearl (1988) and Ruspini et al. (1992) analyze the relationship
between the Dempsterâ€“Shafer theory and standard probability theory.
Fuzzy sets were developed by Lotfi Zadeh (1965) in response to the perceived difficulty
of providing exact inputs to intelligent systems. The text by Zimmermann (2001) provides
a thorough introduction to fuzzy set theory; papers on fuzzy applications are collected in
Zimmermann (1999). As we mentioned in the text, fuzzy logic has often been perceived
incorrectly as a direct competitor to probability theory, whereas in fact it addresses a different
set of issues. Possibility theory (Zadeh, 1978) was introduced to handle uncertainty in fuzzy
systems and has much in common with probability. Dubois and Prade (1994) survey the
connections between possibility theory and probability theory.
The resurgence of probability depended mainly on Pearlâ€™s development of Bayesian
networks as a method for representing and using conditional independence information. This
resurgence did not come without a fight; Peter Cheesemanâ€™s (1985) pugnacious â€œIn Defense
of Probabilityâ€ and his later article â€œAn Inquiry into Computer Understandingâ€ (Cheeseman,
1988, with commentaries) give something of the flavor of the debate. Eugene Charniak
helped present the ideas to AI researchers with a popular article, â€œBayesian networks without tearsâ€11 (1991), and book (1993). The book by Dean and Wellman (1991) also helped
introduce Bayesian networks to AI researchers. One of the principal philosophical objections
of the logicists was that the numerical calculations that probability theory was thought to require were not apparent to introspection and presumed an unrealistic level of precision in our
uncertain knowledge. The development of qualitative probabilistic networks (Wellman,
1990a) provided a purely qualitative abstraction of Bayesian networks, using the notion of
positive and negative influences between variables. Wellman shows that in many cases such
information is sufficient for optimal decision making without the need for the precise specification of probability values. Goldszmidt and Pearl (1996) take a similar approach. Work
by Adnan Darwiche and Matt Ginsberg (1992) extracts the basic properties of conditioning
and evidence combination from probability theory and shows that they can also be applied in
logical and default reasoning. Often, programs speak louder than words, and the ready avail11

The title of the original version of the article was â€œPearl for swine.â€

558

Chapter 14.

Probabilistic Reasoning

ability of high-quality software such as the Bayes Net toolkit (Murphy, 2001) accelerated the
adoption of the technology.
The most important single publication in the growth of Bayesian networks was undoubtedly the text Probabilistic Reasoning in Intelligent Systems (Pearl, 1988). Several excellent
texts (Lauritzen, 1996; Jensen, 2001; Korb and Nicholson, 2003; Jensen, 2007; Darwiche,
2009; Koller and Friedman, 2009) provide thorough treatments of the topics we have covered in this chapter. New research on probabilistic reasoning appears both in mainstream
AI journals, such as Artificial Intelligence and the Journal of AI Research, and in more specialized journals, such as the International Journal of Approximate Reasoning. Many papers
on graphical models, which include Bayesian networks, appear in statistical journals. The
proceedings of the conferences on Uncertainty in Artificial Intelligence (UAI), Neural Information Processing Systems (NIPS), and Artificial Intelligence and Statistics (AISTATS) are
excellent sources for current research.

E XERCISES
14.1 We have a bag of three biased coins a, b, and c with probabilities of coming up heads
of 20%, 60%, and 80%, respectively. One coin is drawn randomly from the bag (with equal
likelihood of drawing each of the three coins), and then the coin is flipped three times to
generate the outcomes X1 , X2 , and X3 .
a. Draw the Bayesian network corresponding to this setup and define the necessary CPTs.
b. Calculate which coin was most likely to have been drawn from the bag if the observed
flips come out heads twice and tails once.
14.2 Equation (14.1) on page 513 defines the joint distribution represented by a Bayesian
network in terms of the parameters Î¸(Xi | P arents(Xi )). This exercise asks you to derive the
equivalence between the parameters and the conditional probabilities P(Xi | P arents(Xi ))
from this definition.
a. Consider a simple network X â†’ Y â†’ Z with three Boolean variables. Use Equations (13.3) and (13.6) (pages 485 and 492) to express the conditional probability
P (z | y) as the ratio of two sums, each over entries in the joint distribution P(X, Y, Z).
b. Now use Equation (14.1) to write this expression in terms of the network parameters
Î¸(X), Î¸(Y | X), and Î¸(Z | Y ).
c. Next, expand out the summations in your expression from part (b), writing out explicitly
the terms for the true and false values of 
each summed variable. Assuming that all
network parameters satisfy the constraint xi Î¸(xi | parents(Xi )) = 1, show that the
resulting expression reduces to Î¸(x | y).
d. Generalize this derivation to show that Î¸(Xi | P arents(Xi )) = P(Xi | P arents(Xi ))
for any Bayesian network.

Exercises
ARC REVERSAL

559
14.3 The operation of arc reversal in a Bayesian network allows us to change the direction
of an arc X â†’ Y while preserving the joint probability distribution that the network represents (Shachter, 1986). Arc reversal may require introducing new arcs: all the parents of X
also become parents of Y , and all parents of Y also become parents of X.
a. Assume that X and Y start with m and n parents, respectively, and that all variables
have k values. By calculating the change in size for the CPTs of X and Y , show that the
total number of parameters in the network cannot decrease during arc reversal. (Hint:
the parents of X and Y need not be disjoint.)
b. Under what circumstances can the total number remain constant?
c. Let the parents of X be U âˆª V and the parents of Y be V âˆª W, where U and W are
disjoint. The formulas for the new CPTs after arc reversal are as follows:

P(Y | V, W, x)P(x | U, V)
P(Y | U, V, W) =
x

P(X | U, V, W, Y ) = P(Y | X, V, W)P(X | U, V)/P(Y | U, V, W) .
Prove that the new network expresses the same joint distribution over all variables as
the original network.
14.4

Consider the Bayesian network in Figure 14.2.

a. If no evidence is observed, are Burglary and Earthquake independent? Prove this from
the numerical semantics and from the topological semantics.
b. If we observe Alarm = true, are Burglary and Earthquake independent? Justify your
answer by calculating whether the probabilities involved satisfy the definition of conditional independence.
14.5 Suppose that in a Bayesian network containing an unobserved variable Y , all the variables in the Markov blanket MB(Y ) have been observed.
a. Prove that removing the node Y from the network will not affect the posterior distribution for any other unobserved variable in the network.
b. Discuss whether we can remove Y if we are planning to use (i) rejection sampling and
(ii) likelihood weighting.
14.6 Let Hx be a random variable denoting the handedness of an individual x, with possible
values l or r. A common hypothesis is that left- or right-handedness is inherited by a simple
mechanism; that is, perhaps there is a gene Gx , also with values l or r, and perhaps actual
handedness turns out mostly the same (with some probability s) as the gene an individual
possesses. Furthermore, perhaps the gene itself is equally likely to be inherited from either
of an individualâ€™s parents, with a small nonzero probability m of a random mutation flipping
the handedness.
a. Which of the three networks in Figure 14.20 claim that P(Gfather , Gmother , Gchild ) =
P(Gfather )P(Gmother )P(Gchild )?
b. Which of the three networks make independence claims that are consistent with the
hypothesis about the inheritance of handedness?

560

Chapter 14.

Probabilistic Reasoning

Gmother

Gfather

Gmother

Gfather

Gmother

Gfather

Hmother

Hfather

Hmother

Hfather

Hmother

Hfather

Gchild

Gchild

Gchild

Hchild

Hchild

Hchild

(a)

(b)

(c)

Figure 14.20 Three possible structures for a Bayesian network describing genetic inheritance of handedness.

c. Which of the three networks is the best description of the hypothesis?
d. Write down the CPT for the Gchild node in network (a), in terms of s and m.
e. Suppose that P (Gfather = l) = P (Gmother = l) = q. In network (a), derive an expression for P (Gchild = l) in terms of m and q only, by conditioning on its parent nodes.
f. Under conditions of genetic equilibrium, we expect the distribution of genes to be the
same across generations. Use this to calculate the value of q, and, given what you know
about handedness in humans, explain why the hypothesis described at the beginning of
this question must be wrong.
14.7 The Markov blanket of a variable is defined on page 517. Prove that a variable
is independent of all other variables in the network, given its Markov blanket and derive
Equation (14.12) (page 538).
Battery

Radio

Ignition

Gas

Starts

Moves

Figure 14.21 A Bayesian network describing some features of a carâ€™s electrical system
and engine. Each variable is Boolean, and the true value indicates that the corresponding
aspect of the vehicle is in working order.

Exercises

561
14.8

Consider the network for car diagnosis shown in Figure 14.21.

a. Extend the network with the Boolean variables IcyWeather and StarterMotor .
b. Give reasonable conditional probability tables for all the nodes.
c. How many independent values are contained in the joint probability distribution for
eight Boolean nodes, assuming that no conditional independence relations are known
to hold among them?
d. How many independent probability values do your network tables contain?
e. The conditional distribution for Starts could be described as a noisy-AND distribution.
Define this family in general and relate it to the noisy-OR distribution.
14.9

Consider the family of linear Gaussian networks, as defined on page 520.

a. In a two-variable network, let X1 be the parent of X2 , let X1 have a Gaussian prior,
and let P(X2 | X1 ) be a linear Gaussian distribution. Show that the joint distribution
P (X1 , X2 ) is a multivariate Gaussian, and calculate its covariance matrix.
b. Prove by induction that the joint distribution for a general linear Gaussian network on
X1 , . . . , Xn is also a multivariate Gaussian.
14.10 The probit distribution defined on page 522 describes the probability distribution for
a Boolean child, given a single continuous parent.
a. How might the definition be extended to cover multiple continuous parents?
b. How might it be extended to handle a multivalued child variable? Consider both cases
where the childâ€™s values are ordered (as in selecting a gear while driving, depending
on speed, slope, desired acceleration, etc.) and cases where they are unordered (as in
selecting bus, train, or car to get to work). (Hint: Consider ways to divide the possible
values into two sets, to mimic a Boolean variable.)
14.11 In your local nuclear power station, there is an alarm that senses when a temperature
gauge exceeds a given threshold. The gauge measures the temperature of the core. Consider
the Boolean variables A (alarm sounds), FA (alarm is faulty), and FG (gauge is faulty) and
the multivalued nodes G (gauge reading) and T (actual core temperature).
a. Draw a Bayesian network for this domain, given that the gauge is more likely to fail
when the core temperature gets too high.
b. Is your network a polytree? Why or why not?
c. Suppose there are just two possible actual and measured temperatures, normal and high;
the probability that the gauge gives the correct temperature is x when it is working, but
y when it is faulty. Give the conditional probability table associated with G.
d. Suppose the alarm works correctly unless it is faulty, in which case it never sounds.
Give the conditional probability table associated with A.
e. Suppose the alarm and gauge are working and the alarm sounds. Calculate an expression for the probability that the temperature of the core is too high, in terms of the
various conditional probabilities in the network.

562

Chapter 14.

F1

F2

M1

M2

F1

F2

N

Figure 14.22

M2

M1

N

M2

M1

F1

N
(i)

Probabilistic Reasoning

(ii)

F2
(iii)

Three possible networks for the telescope problem.

14.12 Two astronomers in different parts of the world make measurements M1 and M2 of
the number of stars N in some small region of the sky, using their telescopes. Normally, there
is a small possibility e of error by up to one star in each direction. Each telescope can also
(with a much smaller probability f ) be badly out of focus (events F1 and F2 ), in which case
the scientist will undercount by three or more stars (or if N is less than 3, fail to detect any
stars at all). Consider the three networks shown in Figure 14.22.
a. Which of these Bayesian networks are correct (but not necessarily efficient) representations of the preceding information?
b. Which is the best network? Explain.
c. Write out a conditional distribution for P(M1 | N ), for the case where N âˆˆ {1, 2, 3} and
M1 âˆˆ {0, 1, 2, 3, 4}. Each entry in the conditional distribution should be expressed as a
function of the parameters e and/or f .
d. Suppose M1 = 1 and M2 = 3. What are the possible numbers of stars if you assume no
prior constraint on the values of N ?
e. What is the most likely number of stars, given these observations? Explain how to
compute this, or if it is not possible to compute, explain what additional information is
needed and how it would affect the result.
14.13 Consider the network shown in Figure 14.22(ii), and assume that the two telescopes
work identically. N âˆˆ {1, 2, 3} and M1 , M2 âˆˆ {0, 1, 2, 3, 4}, with the symbolic CPTs as described in Exercise 14.12. Using the enumeration algorithm (Figure 14.9 on page 525), calculate the probability distribution P(N | M1 = 2, M2 = 2).
14.14

Consider the Bayes net shown in Figure 14.23.

a. Which of the following are asserted by the network structure?
(i) P(B, I, M ) = P(B)P(I)P(M ).
(ii) P(J | G) = P(J | G, I).
(iii) P(M | G, B, I) = P(M | G, B, I, J).

Exercises

563
B M

t
t
f
f

P(B)

P(I )

t
f
t
f

.9
.5
.5
.1

P(M)

.9

.1

B

M

I

B

I

M

P(G)

t
t
t
t
f
f
f
f

t
t
f
f
t
t
f
f

t
f
t
f
t
f
t
f

.9
.8
.0
.0
.2
.1
.0
.0

G
J

G

P(J)

t
f

.9
.0

Figure 14.23 A simple Bayes net with Boolean variables B = BrokeElectionLaw ,
I = Indicted , M = PoliticallyMotivatedProsecutor , G = FoundGuilty, J = Jailed .

b. Calculate the value of P (b, i, Â¬m, g, j).
c. Calculate the probability that someone goes to jail given that they broke the law, have
been indicted, and face a politically motivated prosecutor.
d. A context-specific independence (see page 542) allows a variable to be independent
of some of its parents given certain values of others. In addition to the usual conditional
independences given by the graph structure, what context-specific independences exist
in the Bayes net in Figure 14.23?
e. Suppose we want to add the variable P = PresidentialPardon to the network; draw the
new network and briefly explain any links you add.
14.15

Consider the variable elimination algorithm in Figure 14.11 (page 528).

a. Section 14.4 applies variable elimination to the query
P(Burglary | JohnCalls = true, MaryCalls = true) .
Perform the calculations indicated and check that the answer is correct.
b. Count the number of arithmetic operations performed, and compare it with the number
performed by the enumeration algorithm.
c. Suppose a network has the form of a chain: a sequence of Boolean variables X1 , . . . , Xn
where P arents(Xi ) = {Xiâˆ’1 } for i = 2, . . . , n. What is the complexity of computing
P(X1 | Xn = true) using enumeration? Using variable elimination?
d. Prove that the complexity of running variable elimination on a polytree network is linear
in the size of the tree for any variable ordering consistent with the network structure.
14.16

Investigate the complexity of exact inference in general Bayesian networks:

a. Prove that any 3-SAT problem can be reduced to exact inference in a Bayesian network
constructed to represent the particular problem and hence that exact inference is NP-

564

Chapter 14.

Probabilistic Reasoning

hard. (Hint: Consider a network with one variable for each proposition symbol, one for
each clause, and one for the conjunction of clauses.)
b. The problem of counting the number of satisfying assignments for a 3-SAT problem is
#P-complete. Show that exact inference is at least as hard as this.
14.17 Consider the problem of generating a random sample from a specified distribution
on a single variable. Assume you have a random number generator that returns a random
number uniformly distributed between 0 and 1.
CUMULATIVE
DISTRIBUTION

a. Let X be a discrete variable with P (X = xi ) = pi for i âˆˆ {1, . . . , k}. The cumulative
distribution of X gives the probability that X âˆˆ {x1 , . . . , xj } for each possible j. (See
also Appendix A.) Explain how to calculate the cumulative distribution in O(k) time
and how to generate a single sample of X from it. Can the latter be done in less than
O(k) time?
b. Now suppose we want to generate N samples of X, where N & k. Explain how to do
this with an expected run time per sample that is constant (i.e., independent of k).
c. Now consider a continuous-valued variable with a parameterized distribution (e.g.,
Gaussian). How can samples be generated from such a distribution?
d. Suppose you want to query a continuous-valued variable and you are using a sampling
algorithm such as L IKELIHOODW EIGHTING to do the inference. How would you have
to modify the query-answering process?
14.18 Consider the query P(Rain | Sprinkler = true, WetGrass = true) in Figure 14.12(a)
(page 529) and how Gibbs sampling can answer it.
a.
b.
c.
d.
e.
14.19

How many states does the Markov chain have?
Calculate the transition matrix Q containing q(y â†’ y ) for all y, y .
What does Q2 , the square of the transition matrix, represent?
What about Qn as n â†’ âˆž?
Explain how to do probabilistic inference in Bayesian networks, assuming that Qn is
available. Is this a practical way to do inference?
This exercise explores the stationary distribution for Gibbs sampling methods.

a. The convex composition [Î±, q1 ; 1 âˆ’ Î±, q2 ] of q1 and q2 is a transition probability distribution that first chooses one of q1 and q2 with probabilities Î± and 1 âˆ’ Î±, respectively,
and then applies whichever is chosen. Prove that if q1 and q2 are in detailed balance
with Ï€, then their convex composition is also in detailed balance with Ï€. (Note: this
result justifies a variant of G IBBS-A SK in which variables are chosen at random rather
than sampled in a fixed sequence.)
b. Prove that if each of q1 and q2 has Ï€ as its stationary distribution, then the sequential
composition q = q1 â—¦ q2 also has Ï€ as its stationary distribution.
METROPOLISâ€“
HASTINGS

14.20 The Metropolisâ€“Hastings algorithm is a member of the MCMC family; as such, it is
designed to generate samples x (eventually) according to target probabilities Ï€(x). (Typically

Exercises

PROPOSAL
DISTRIBUTION
ACCEPTANCE
PROBABILITY

565
we are interested in sampling from Ï€(x) = P (x | e).) Like simulated annealing, Metropolisâ€“
Hastings operates in two stages. First, it samples a new state x from a proposal distribution
q(x | x), given the current state x. Then, it probabilistically accepts or rejects x according to
the acceptance probability


Ï€(x )q(x | x )

.
Î±(x | x) = min 1,
Ï€(x)q(x | x)
If the proposal is rejected, the state remains at x.
a. Consider an ordinary Gibbs sampling step for a specific variable Xi . Show that this
step, considered as a proposal, is guaranteed to be accepted by Metropolisâ€“Hastings.
(Hence, Gibbs sampling is a special case of Metropolisâ€“Hastings.)
b. Show that the two-step process above, viewed as a transition probability distribution, is
in detailed balance with Ï€.
14.21 Three soccer teams A, B, and C, play each other once. Each match is between two
teams, and can be won, drawn, or lost. Each team has a fixed, unknown degree of qualityâ€”
an integer ranging from 0 to 3â€”and the outcome of a match depends probabilistically on the
difference in quality between the two teams.
a. Construct a relational probability model to describe this domain, and suggest numerical
values for all the necessary probability distributions.
b. Construct the equivalent Bayesian network for the three matches.
c. Suppose that in the first two matches A beats B and draws with C. Using an exact
inference algorithm of your choice, compute the posterior distribution for the outcome
of the third match.
d. Suppose there are n teams in the league and we have the results for all but the last
match. How does the complexity of predicting the last game vary with n?
e. Investigate the application of MCMC to this problem. How quickly does it converge in
practice and how well does it scale?

15

PROBABILISTIC
REASONING OVER TIME

In which we try to interpret the present, understand the past, and perhaps predict
the future, even when very little is crystal clear.

Agents in partially observable environments must be able to keep track of the current state, to
the extent that their sensors allow. In Section 4.4 we showed a methodology for doing that: an
agent maintains a belief state that represents which states of the world are currently possible.
From the belief state and a transition model, the agent can predict how the world might
evolve in the next time step. From the percepts observed and a sensor model, the agent can
update the belief state. This is a pervasive idea: in Chapter 4 belief states were represented by
explicitly enumerated sets of states, whereas in Chapters 7 and 11 they were represented by
logical formulas. Those approaches defined belief states in terms of which world states were
possible, but could say nothing about which states were likely or unlikely. In this chapter, we
use probability theory to quantify the degree of belief in elements of the belief state.
As we show in Section 15.1, time itself is handled in the same way as in Chapter 7: a
changing world is modeled using a variable for each aspect of the world state at each point in
time. The transition and sensor models may be uncertain: the transition model describes the
probability distribution of the variables at time t, given the state of the world at past times,
while the sensor model describes the probability of each percept at time t, given the current
state of the world. Section 15.2 defines the basic inference tasks and describes the general structure of inference algorithms for temporal models. Then we describe three specific
kinds of models: hidden Markov models, Kalman filters, and dynamic Bayesian networks (which include hidden Markov models and Kalman filters as special cases). Finally,
Section 15.6 examines the problems faced when keeping track of more than one thing.

15.1

T IME AND U NCERTAINTY
We have developed our techniques for probabilistic reasoning in the context of static worlds,
in which each random variable has a single fixed value. For example, when repairing a car,
we assume that whatever is broken remains broken during the process of diagnosis; our job
is to infer the state of the car from observed evidence, which also remains fixed.
566

Section 15.1.

Time and Uncertainty

567

Now consider a slightly different problem: treating a diabetic patient. As in the case of
car repair, we have evidence such as recent insulin doses, food intake, blood sugar measurements, and other physical signs. The task is to assess the current state of the patient, including
the actual blood sugar level and insulin level. Given this information, we can make a decision about the patientâ€™s food intake and insulin dose. Unlike the case of car repair, here the
dynamic aspects of the problem are essential. Blood sugar levels and measurements thereof
can change rapidly over time, depending on recent food intake and insulin doses, metabolic
activity, the time of day, and so on. To assess the current state from the history of evidence
and to predict the outcomes of treatment actions, we must model these changes.
The same considerations arise in many other contexts, such as tracking the location of
a robot, tracking the economic activity of a nation, and making sense of a spoken or written
sequence of words. How can dynamic situations like these be modeled?

15.1.1 States and observations
TIME SLICE

We view the world as a series of snapshots, or time slices, each of which contains a set of
random variables, some observable and some not. 1 For simplicity, we will assume that the
same subset of variables is observable in each time slice (although this is not strictly necessary
in anything that follows). We will use Xt to denote the set of state variables at time t, which
are assumed to be unobservable, and Et to denote the set of observable evidence variables.
The observation at time t is Et = et for some set of values et .
Consider the following example: You are the security guard stationed at a secret underground installation. You want to know whether itâ€™s raining today, but your only access to the
outside world occurs each morning when you see the director coming in with, or without, an
umbrella. For each day t, the set Et thus contains a single evidence variable Umbrella t or Ut
for short (whether the umbrella appears), and the set Xt contains a single state variable Rain t
or Rt for short (whether it is raining). Other problems can involve larger sets of variables. In
the diabetes example, we might have evidence variables, such as MeasuredBloodSugar t and
PulseRate t , and state variables, such as BloodSugar t and StomachContents t . (Notice that
BloodSugar t and MeasuredBloodSugar t are not the same variable; this is how we deal with
noisy measurements of actual quantities.)
The interval between time slices also depends on the problem. For diabetes monitoring,
a suitable interval might be an hour rather than a day. In this chapter we assume the interval
between slices is fixed, so we can label times by integers. We will assume that the state
sequence starts at t = 0; for various uninteresting reasons, we will assume that evidence starts
arriving at t = 1 rather than t = 0. Hence, our umbrella world is represented by state variables
R0 , R1 , R2 , . . . and evidence variables U1 , U2 , . . .. We will use the notation a:b to denote
the sequence of integers from a to b (inclusive), and the notation Xa:b to denote the set of
variables from Xa to Xb . For example, U1:3 corresponds to the variables U1 , U2 , U3 .
1

Uncertainty over continuous time can be modeled by stochastic differential equations (SDEs). The models
studied in this chapter can be viewed as discrete-time approximations to SDEs.

568

Chapter 15.

Probabilistic Reasoning over Time

(a)

Xtâ€“2

Xtâ€“1

Xt

Xt+1

Xt+2

(b)

Xtâ€“2

Xtâ€“1

Xt

Xt+1

Xt+2

Figure 15.1 (a) Bayesian network structure corresponding to a first-order Markov process
with state defined by the variables Xt . (b) A second-order Markov process.

15.1.2 Transition and sensor models

MARKOV
ASSUMPTION

MARKOV PROCESS
FIRST-ORDER
MARKOV PROCESS

With the set of state and evidence variables for a given problem decided on, the next step is
to specify how the world evolves (the transition model) and how the evidence variables get
their values (the sensor model).
The transition model specifies the probability distribution over the latest state variables,
given the previous values, that is, P(Xt | X0:tâˆ’1 ). Now we face a problem: the set X0:tâˆ’1 is
unbounded in size as t increases. We solve the problem by making a Markov assumptionâ€”
that the current state depends on only a finite fixed number of previous states. Processes satisfying this assumption were first studied in depth by the Russian statistician Andrei Markov
(1856â€“1922) and are called Markov processes or Markov chains. They come in various flavors; the simplest is the first-order Markov process, in which the current state depends only
on the previous state and not on any earlier states. In other words, a state provides enough
information to make the future conditionally independent of the past, and we have
P(Xt | X0:tâˆ’1 ) = P(Xt | Xtâˆ’1 ) .

STATIONARY
PROCESS

SENSOR MARKOV
ASSUMPTION

(15.1)

Hence, in a first-order Markov process, the transition model is the conditional distribution
P(Xt | Xtâˆ’1 ). The transition model for a second-order Markov process is the conditional
distribution P(Xt | Xtâˆ’2 , Xtâˆ’1 ). Figure 15.1 shows the Bayesian network structures corresponding to first-order and second-order Markov processes.
Even with the Markov assumption there is still a problem: there are infinitely many
possible values of t. Do we need to specify a different distribution for each time step? We
avoid this problem by assuming that changes in the world state are caused by a stationary
processâ€”that is, a process of change that is governed by laws that do not themselves change
over time. (Donâ€™t confuse stationary with static: in a static process, the state itself does not
change.) In the umbrella world, then, the conditional probability of rain, P(Rt | Rtâˆ’1 ), is the
same for all t, and we only have to specify one conditional probability table.
Now for the sensor model. The evidence variables Et could depend on previous variables as well as the current state variables, but any state thatâ€™s worth its salt should suffice to
generate the current sensor values. Thus, we make a sensor Markov assumption as follows:
P(Et | X0:t , E0:tâˆ’1 ) = P(Et | Xt ) .

(15.2)

Thus, P(Et | Xt ) is our sensor model (sometimes called the observation model). Figure 15.2
shows both the transition model and the sensor model for the umbrella example. Notice the

Section 15.1.

Time and Uncertainty

569
Rt -1
t
f

Raintâ€“1

P(Rt )
0.7
0.3

Rt
t
f

Umbrellatâ€“1

Raint+1

Raint
P(U t )
0.9
0.2

Umbrellat

Umbrellat+1

Figure 15.2 Bayesian network structure and conditional distributions describing the
umbrella world. The transition model is P (Rain t | Rain tâˆ’1 ) and the sensor model is
P (Umbrella t | Rain t ).

direction of the dependence between state and sensors: the arrows go from the actual state
of the world to sensor values because the state of the world causes the sensors to take on
particular values: the rain causes the umbrella to appear. (The inference process, of course,
goes in the other direction; the distinction between the direction of modeled dependencies
and the direction of inference is one of the principal advantages of Bayesian networks.)
In addition to specifying the transition and sensor models, we need to say how everything gets startedâ€”the prior probability distribution at time 0, P(X0 ). With that, we have a
specification of the complete joint distribution over all the variables, using Equation (14.2).
For any t,
P(X0:t , E1:t ) = P(X0 )

t


P(Xi | Xiâˆ’1 ) P(Ei | Xi ) .

(15.3)

i=1

The three terms on the right-hand side are the initial state model P(X0 ), the transition model
P(Xi | Xiâˆ’1 ), and the sensor model P(Ei | Xi ).
The structure in Figure 15.2 is a first-order Markov processâ€”the probability of rain is
assumed to depend only on whether it rained the previous day. Whether such an assumption
is reasonable depends on the domain itself. The first-order Markov assumption says that the
state variables contain all the information needed to characterize the probability distribution
for the next time slice. Sometimes the assumption is exactly trueâ€”for example, if a particle
is executing a random walk along the x-axis, changing its position by Â±1 at each time step,
then using the x-coordinate as the state gives a first-order Markov process. Sometimes the
assumption is only approximate, as in the case of predicting rain only on the basis of whether
it rained the previous day. There are two ways to improve the accuracy of the approximation:
1. Increasing the order of the Markov process model. For example, we could make a
second-order model by adding Rain tâˆ’2 as a parent of Rain t , which might give slightly
more accurate predictions. For example, in Palo Alto, California, it very rarely rains
more than two days in a row.
2. Increasing the set of state variables. For example, we could add Season t to allow

570

Chapter 15.

Probabilistic Reasoning over Time

us to incorporate historical records of rainy seasons, or we could add Temperature t ,
Humidity t and Pressure t (perhaps at a range of locations) to allow us to use a physical
model of rainy conditions.
Exercise 15.1 asks you to show that the first solutionâ€”increasing the orderâ€”can always be
reformulated as an increase in the set of state variables, keeping the order fixed. Notice that
adding state variables might improve the systemâ€™s predictive power but also increases the
prediction requirements: we now have to predict the new variables as well. Thus, we are
looking for a â€œself-sufficientâ€ set of variables, which really means that we have to understand
the â€œphysicsâ€ of the process being modeled. The requirement for accurate modeling of the
process is obviously lessened if we can add new sensors (e.g., measurements of temperature
and pressure) that provide information directly about the new state variables.
Consider, for example, the problem of tracking a robot wandering randomly on the Xâ€“Y
plane. One might propose that the position and velocity are a sufficient set of state variables:
one can simply use Newtonâ€™s laws to calculate the new position, and the velocity may change
unpredictably. If the robot is battery-powered, however, then battery exhaustion would tend to
have a systematic effect on the change in velocity. Because this in turn depends on how much
power was used by all previous maneuvers, the Markov property is violated. We can restore
the Markov property by including the charge level Battery t as one of the state variables that
make up Xt . This helps in predicting the motion of the robot, but in turn requires a model
for predicting Battery t from Battery tâˆ’1 and the velocity. In some cases, that can be done
reliably, but more often we find that error accumulates over time. In that case, accuracy can
be improved by adding a new sensor for the battery level.

15.2

I NFERENCE IN T EMPORAL M ODELS
Having set up the structure of a generic temporal model, we can formulate the basic inference
tasks that must be solved:
â€¢ Filtering: This is the task of computing the belief stateâ€”the posterior distribution
over the most recent stateâ€”given all evidence to date. Filtering2 is also called state
estimation. In our example, we wish to compute P(Xt | e1:t ). In the umbrella example,
this would mean computing the probability of rain today, given all the observations of
the umbrella carrier made so far. Filtering is what a rational agent does to keep track
of the current state so that rational decisions can be made. It turns out that an almost
identical calculation provides the likelihood of the evidence sequence, P (e1:t ).
â€¢ Prediction: This is the task of computing the posterior distribution over the future state,
given all evidence to date. That is, we wish to compute P(Xt+k | e1:t ) for some k > 0.
In the umbrella example, this might mean computing the probability of rain three days
from now, given all the observations to date. Prediction is useful for evaluating possible
courses of action based on their expected outcomes.

FILTERING
BELIEF STATE
STATE ESTIMATION

PREDICTION

2

The term â€œfilteringâ€ refers to the roots of this problem in early work on signal processing, where the problem
is to filter out the noise in a signal by estimating its underlying properties.

Section 15.2.

Inference in Temporal Models

571

â€¢ Smoothing: This is the task of computing the posterior distribution over a past state,
given all evidence up to the present. That is, we wish to compute P(Xk | e1:t ) for some k
such that 0 â‰¤ k < t. In the umbrella example, it might mean computing the probability
that it rained last Wednesday, given all the observations of the umbrella carrier made
up to today. Smoothing provides a better estimate of the state than was available at the
time, because it incorporates more evidence. 3
â€¢ Most likely explanation: Given a sequence of observations, we might wish to find the
sequence of states that is most likely to have generated those observations. That is, we
wish to compute argmaxx1:t P (x1:t | e1:t ). For example, if the umbrella appears on each
of the first three days and is absent on the fourth, then the most likely explanation is that
it rained on the first three days and did not rain on the fourth. Algorithms for this task
are useful in many applications, including speech recognitionâ€”where the aim is to find
the most likely sequence of words, given a series of soundsâ€”and the reconstruction of
bit strings transmitted over a noisy channel.

SMOOTHING

In addition to these inference tasks, we also have
â€¢ Learning: The transition and sensor models, if not yet known, can be learned from
observations. Just as with static Bayesian networks, dynamic Bayes net learning can be
done as a by-product of inference. Inference provides an estimate of what transitions
actually occurred and of what states generated the sensor readings, and these estimates
can be used to update the models. The updated models provide new estimates, and the
process iterates to convergence. The overall process is an instance of the expectationmaximization or EM algorithm. (See Section 20.3.)
Note that learning requires smoothing, rather than filtering, because smoothing provides better estimates of the states of the process. Learning with filtering can fail to converge correctly;
consider, for example, the problem of learning to solve murders: unless you are an eyewitness, smoothing is always required to infer what happened at the murder scene from the
observable variables.
The remainder of this section describes generic algorithms for the four inference tasks,
independent of the particular kind of model employed. Improvements specific to each model
are described in subsequent sections.

15.2.1 Filtering and prediction
As we pointed out in Section 7.7.3, a useful filtering algorithm needs to maintain a current
state estimate and update it, rather than going back over the entire history of percepts for each
update. (Otherwise, the cost of each update increases as time goes by.) In other words, given
the result of filtering up to time t, the agent needs to compute the result for t + 1 from the
new evidence et+1 ,
P(Xt+1 | e1:t+1 ) = f (et+1 , P(Xt | e1:t )) ,
RECURSIVE
ESTIMATION

for some function f . This process is called recursive estimation. We can view the calculation
3

In particular, when tracking a moving object with inaccurate position observations, smoothing gives a smoother
estimated trajectory than filteringâ€”hence the name.

572

Chapter 15.

Probabilistic Reasoning over Time

as being composed of two parts: first, the current state distribution is projected forward from
t to t + 1; then it is updated using the new evidence et+1 . This two-part process emerges quite
simply when the formula is rearranged:
P(Xt+1 | e1:t+1 ) = P(Xt+1 | e1:t , et+1 ) (dividing up the evidence)
= Î± P(et+1 | Xt+1 , e1:t ) P(Xt+1 | e1:t ) (using Bayesâ€™ rule)
(15.4)
= Î± P(et+1 | Xt+1 ) P(Xt+1 | e1:t ) (by the sensor Markov assumption).
Here and throughout this chapter, Î± is a normalizing constant used to make probabilities sum
up to 1. The second term, P(Xt+1 | e1:t ) represents a one-step prediction of the next state,
and the first term updates this with the new evidence; notice that P(et+1 | Xt+1 ) is obtainable
directly from the sensor model. Now we obtain the one-step prediction for the next state by
conditioning on the current state Xt :

P(Xt+1 | xt , e1:t )P (xt | e1:t )
P(Xt+1 | e1:t+1 ) = Î± P(et+1 | Xt+1 )
= Î± P(et+1 | Xt+1 )



xt

P(Xt+1 | xt )P (xt | e1:t ) (Markov assumption).

(15.5)

xt

Within the summation, the first factor comes from the transition model and the second comes
from the current state distribution. Hence, we have the desired recursive formulation. We can
think of the filtered estimate P(Xt | e1:t ) as a â€œmessageâ€ f1:t that is propagated forward along
the sequence, modified by each transition and updated by each new observation. The process
is given by
f1:t+1 = Î± F ORWARD (f1:t , et+1 ) ,
where F ORWARD implements the update described in Equation (15.5) and the process begins
with f1:0 = P(X0 ). When all the state variables are discrete, the time for each update is
constant (i.e., independent of t), and the space required is also constant. (The constants
depend, of course, on the size of the state space and the specific type of the temporal model
in question.) The time and space requirements for updating must be constant if an agent with
limited memory is to keep track of the current state distribution over an unbounded sequence
of observations.
Let us illustrate the filtering process for two steps in the basic umbrella example (Figure 15.2.) That is, we will compute P(R2 | u1:2 ) as follows:
â€¢ On day 0, we have no observations, only the security guardâ€™s prior beliefs; letâ€™s assume
that consists of P(R0 ) = 0.5, 0.5.
â€¢ On day 1, the umbrella appears, so U1 = true. The prediction from t = 0 to t = 1 is

P(R1 | r0 )P (r0 )
P(R1 ) =
r0

= 0.7, 0.3 Ã— 0.5 + 0.3, 0.7 Ã— 0.5 = 0.5, 0.5 .
Then the update step simply multiplies by the probability of the evidence for t = 1 and
normalizes, as shown in Equation (15.4):
P(R1 | u1 ) = Î± P(u1 | R1 )P(R1 ) = Î± 0.9, 0.20.5, 0.5
= Î± 0.45, 0.1 â‰ˆ 0.818, 0.182 .

Section 15.2.

Inference in Temporal Models

573

â€¢ On day 2, the umbrella appears, so U2 = true. The prediction from t = 1 to t = 2 is

P(R2 | r1 )P (r1 | u1 )
P(R2 | u1 ) =
r1

= 0.7, 0.3 Ã— 0.818 + 0.3, 0.7 Ã— 0.182 â‰ˆ 0.627, 0.373 ,
and updating it with the evidence for t = 2 gives
P(R2 | u1 , u2 ) = Î± P(u2 | R2 )P(R2 | u1 ) = Î± 0.9, 0.20.627, 0.373
= Î± 0.565, 0.075 â‰ˆ 0.883, 0.117 .
Intuitively, the probability of rain increases from day 1 to day 2 because rain persists. Exercise 15.2(a) asks you to investigate this tendency further.
The task of prediction can be seen simply as filtering without the addition of new
evidence. In fact, the filtering process already incorporates a one-step prediction, and it is
easy to derive the following recursive computation for predicting the state at t + k + 1 from
a prediction for t + k:

P(Xt+k+1 | xt+k )P (xt+k | e1:t ) .
(15.6)
P(Xt+k+1 | e1:t ) =
xt+k

MIXING TIME

Naturally, this computation involves only the transition model and not the sensor model.
It is interesting to consider what happens as we try to predict further and further into
the future. As Exercise 15.2(b) shows, the predicted distribution for rain converges to a
fixed point 0.5, 0.5, after which it remains constant for all time. This is the stationary
distribution of the Markov process defined by the transition model. (See also page 537.) A
great deal is known about the properties of such distributions and about the mixing timeâ€”
roughly, the time taken to reach the fixed point. In practical terms, this dooms to failure any
attempt to predict the actual state for a number of steps that is more than a small fraction of
the mixing time, unless the stationary distribution itself is strongly peaked in a small area of
the state space. The more uncertainty there is in the transition model, the shorter will be the
mixing time and the more the future is obscured.
In addition to filtering and prediction, we can use a forward recursion to compute the
likelihood of the evidence sequence, P (e1:t ). This is a useful quantity if we want to compare
different temporal models that might have produced the same evidence sequence (e.g., two
different models for the persistence of rain). For this recursion, we use a likelihood message
1:t (Xt ) = P(Xt , e1:t ). It is a simple exercise to show that the message calculation is identical
to that for filtering:
1:t+1 = F ORWARD (1:t , et+1 ) .
Having computed 1:t , we obtain the actual likelihood by summing out Xt :

1:t (xt ) .
L1:t = P (e1:t ) =

(15.7)

xt

Notice that the likelihood message represents the probabilities of longer and longer evidence
sequences as time goes by and so becomes numerically smaller and smaller, leading to underflow problems with floating-point arithmetic. This is an important problem in practice, but
we shall not go into solutions here.

574

Chapter 15.

X0

Probabilistic Reasoning over Time

X1

Xk

Xt

E1

Ek

Et

Figure 15.3 Smoothing computes P(Xk | e1:t ), the posterior distribution of the state at
some past time k given a complete sequence of observations from 1 to t.

15.2.2 Smoothing
As we said earlier, smoothing is the process of computing the distribution over past states
given evidence up to the present; that is, P(Xk | e1:t ) for 0 â‰¤ k < t. (See Figure 15.3.)
In anticipation of another recursive message-passing approach, we can split the computation
into two partsâ€”the evidence up to k and the evidence from k + 1 to t,
P(Xk | e1:t ) = P(Xk | e1:k , ek+1:t )
= Î± P(Xk | e1:k )P(ek+1:t | Xk , e1:k ) (using Bayesâ€™ rule)
= Î± P(Xk | e1:k )P(ek+1:t | Xk )

(using conditional independence)

= Î± f1:k Ã— bk+1:t .

(15.8)

where â€œÃ—â€ represents pointwise multiplication of vectors. Here we have defined a â€œbackwardâ€ message bk+1:t = P(ek+1:t | Xk ), analogous to the forward message f1:k . The forward
message f1:k can be computed by filtering forward from 1 to k, as given by Equation (15.5).
It turns out that the backward message bk+1:t can be computed by a recursive process that
runs backward from t:

P(ek+1:t | Xk , xk+1 )P(xk+1 | Xk ) (conditioning on Xk+1 )
P(ek+1:t | Xk ) =
xk+1

=



P (ek+1:t | xk+1 )P(xk+1 | Xk )

(by conditional independence)

xk+1

=



P (ek+1 , ek+2:t | xk+1 )P(xk+1 | Xk )

xk+1

=



P (ek+1 | xk+1 )P (ek+2:t | xk+1 )P(xk+1 | Xk ) ,

(15.9)

xk+1

where the last step follows by the conditional independence of ek+1 and ek+2:t , given Xk+1 .
Of the three factors in this summation, the first and third are obtained directly from the model,
and the second is the â€œrecursive call.â€ Using the message notation, we have
bk+1:t = BACKWARD(bk+2:t , ek+1 ) ,
where BACKWARD implements the update described in Equation (15.9). As with the forward
recursion, the time and space needed for each update are constant and thus independent of t.
We can now see that the two terms in Equation (15.8) can both be computed by recursions through time, one running forward from 1 to k and using the filtering equation (15.5)

Section 15.2.

Inference in Temporal Models

575

and the other running backward from t to k + 1 and using Equation (15.9). Note that the
backward phase is initialized with bt+1:t = P(et+1:t | Xt ) = P( | Xt )1, where 1 is a vector of
1s. (Because et+1:t is an empty sequence, the probability of observing it is 1.)
Let us now apply this algorithm to the umbrella example, computing the smoothed
estimate for the probability of rain at time k = 1, given the umbrella observations on days 1
and 2. From Equation (15.8), this is given by
P(R1 | u1 , u2 ) = Î± P(R1 | u1 ) P(u2 | R1 ) .

(15.10)

The first term we already know to be .818, .182, from the forward filtering process described earlier. The second term can be computed by applying the backward recursion in
Equation (15.9):

P (u2 | r2 )P ( | r2 )P(r2 | R1 )
P(u2 | R1 ) =
r2

= (0.9 Ã— 1 Ã— 0.7, 0.3) + (0.2 Ã— 1 Ã— 0.3, 0.7) = 0.69, 0.41 .
Plugging this into Equation (15.10), we find that the smoothed estimate for rain on day 1 is
P(R1 | u1 , u2 ) = Î± 0.818, 0.182 Ã— 0.69, 0.41 â‰ˆ 0.883, 0.117 .

FORWARDâ€“
BACKWARD
ALGORITHM

Thus, the smoothed estimate for rain on day 1 is higher than the filtered estimate (0.818) in
this case. This is because the umbrella on day 2 makes it more likely to have rained on day
2; in turn, because rain tends to persist, that makes it more likely to have rained on day 1.
Both the forward and backward recursions take a constant amount of time per step;
hence, the time complexity of smoothing with respect to evidence e1:t is O(t). This is the
complexity for smoothing at a particular time step k. If we want to smooth the whole sequence, one obvious method is simply to run the whole smoothing process once for each
time step to be smoothed. This results in a time complexity of O(t2 ). A better approach
uses a simple application of dynamic programming to reduce the complexity to O(t). A clue
appears in the preceding analysis of the umbrella example, where we were able to reuse the
results of the forward-filtering phase. The key to the linear-time algorithm is to record the
results of forward filtering over the whole sequence. Then we run the backward recursion
from t down to 1, computing the smoothed estimate at each step k from the computed backward message bk+1:t and the stored forward message f1:k . The algorithm, aptly called the
forwardâ€“backward algorithm, is shown in Figure 15.4.
The alert reader will have spotted that the Bayesian network structure shown in Figure 15.3 is a polytree as defined on page 528. This means that a straightforward application
of the clustering algorithm also yields a linear-time algorithm that computes smoothed estimates for the entire sequence. It is now understood that the forwardâ€“backward algorithm
is in fact a special case of the polytree propagation algorithm used with clustering methods
(although the two were developed independently).
The forwardâ€“backward algorithm forms the computational backbone for many applications that deal with sequences of noisy observations. As described so far, it has two practical
drawbacks. The first is that its space complexity can be too high when the state space is large
and the sequences are long. It uses O(|f|t) space where |f| is the size of the representation of
the forward message. The space requirement can be reduced to O(|f| log t) with a concomi-

576

FIXED-LAG
SMOOTHING

Chapter 15.

Probabilistic Reasoning over Time

tant increase in the time complexity by a factor of log t, as shown in Exercise 15.3. In some
cases (see Section 15.3), a constant-space algorithm can be used.
The second drawback of the basic algorithm is that it needs to be modified to work
in an online setting where smoothed estimates must be computed for earlier time slices as
new observations are continuously added to the end of the sequence. The most common
requirement is for fixed-lag smoothing, which requires computing the smoothed estimate
P(Xtâˆ’d | e1:t ) for fixed d. That is, smoothing is done for the time slice d steps behind the
current time t; as t increases, the smoothing has to keep up. Obviously, we can run the
forwardâ€“backward algorithm over the d-step â€œwindowâ€ as each new observation is added,
but this seems inefficient. In Section 15.3, we will see that fixed-lag smoothing can, in some
cases, be done in constant time per update, independent of the lag d.

15.2.3 Finding the most likely sequence
Suppose that [true, true, false, true, true] is the umbrella sequence for the security guardâ€™s
first five days on the job. What is the weather sequence most likely to explain this? Does
the absence of the umbrella on day 3 mean that it wasnâ€™t raining, or did the director forget
to bring it? If it didnâ€™t rain on day 3, perhaps (because weather tends to persist) it didnâ€™t
rain on day 4 either, but the director brought the umbrella just in case. In all, there are 25
possible weather sequences we could pick. Is there a way to find the most likely one, short of
enumerating all of them?
We could try this linear-time procedure: use smoothing to find the posterior distribution
for the weather at each time step; then construct the sequence, using at each step the weather
that is most likely according to the posterior. Such an approach should set off alarm bells
in the readerâ€™s head, because the posterior distributions computed by smoothing are distri-

function F ORWARD -BACKWARD (ev, prior ) returns a vector of probability distributions
inputs: ev, a vector of evidence values for steps 1, . . . , t
prior , the prior distribution on the initial state, P(X0 )
local variables: fv, a vector of forward messages for steps 0, . . . , t
b, a representation of the backward message, initially all 1s
sv, a vector of smoothed estimates for steps 1, . . . , t
fv[0] â† prior
for i = 1 to t do
fv[i] â† F ORWARD (fv[i âˆ’ 1], ev[i])
for i = t downto 1 do
sv[i] â† N ORMALIZE(fv[i] Ã— b)
b â† BACKWARD (b, ev[i])
return sv
Figure 15.4 The forwardâ€“backward algorithm for smoothing: computing posterior probabilities of a sequence of states given a sequence of observations. The F ORWARD and
BACKWARD operators are defined by Equations (15.5) and (15.9), respectively.

Section 15.2.

Inference in Temporal Models

577

Rain 1

Rain 2

Rain 3

Rain 4

Rain 5

true

true

true

true

true

false

false

false

false

false

Umbrella t true

true

false

true

true

.8182

.5155

.0361

.0334

.0210

.1818

.0491

.1237

.0173

.0024

m1:1

m1:2

m1:3

m1:4

m1:5

(a)

(b)

Figure 15.5 (a) Possible state sequences for Rain t can be viewed as paths through a graph
of the possible states at each time step. (States are shown as rectangles to avoid confusion
with nodes in a Bayes net.) (b) Operation of the Viterbi algorithm for the umbrella observation sequence [true, true, false, true, true]. For each t, we have shown the values of the
message m1:t , which gives the probability of the best sequence reaching each state at time t.
Also, for each state, the bold arrow leading into it indicates its best predecessor as measured
by the product of the preceding sequence probability and the transition probability. Following
the bold arrows back from the most likely state in m1:5 gives the most likely sequence.

butions over single time steps, whereas to find the most likely sequence we must consider
joint probabilities over all the time steps. The results can in fact be quite different. (See
Exercise 15.4.)
There is a linear-time algorithm for finding the most likely sequence, but it requires a
little more thought. It relies on the same Markov property that yielded efficient algorithms for
filtering and smoothing. The easiest way to think about the problem is to view each sequence
as a path through a graph whose nodes are the possible states at each time step. Such a
graph is shown for the umbrella world in Figure 15.5(a). Now consider the task of finding
the most likely path through this graph, where the likelihood of any path is the product of
the transition probabilities along the path and the probabilities of the given observations at
each state. Letâ€™s focus in particular on paths that reach the state Rain 5 = true. Because of
the Markov property, it follows that the most likely path to the state Rain 5 = true consists of
the most likely path to some state at time 4 followed by a transition to Rain 5 = true; and the
state at time 4 that will become part of the path to Rain 5 = true is whichever maximizes the
likelihood of that path. In other words, there is a recursive relationship between most likely
paths to each state xt+1 and most likely paths to each state xt . We can write this relationship
as an equation connecting the probabilities of the paths:
max P(x1 , . . . , xt , Xt+1 | e1:t+1 )


= Î± P(et+1 | Xt+1 ) max P(Xt+1 | xt ) max P (x1 , . . . , xtâˆ’1 , xt | e1:t ) . (15.11)

x1 ...xt

xt

x1 ...xtâˆ’1

Equation (15.11) is identical to the filtering equation (15.5) except that

578

Chapter 15.

Probabilistic Reasoning over Time

1. The forward message f1:t = P(Xt | e1:t ) is replaced by the message
m1:t = max P(x1 , . . . , xtâˆ’1 , Xt | e1:t ) ,
x1 ...xtâˆ’1

that is, the probabilities of the most likely path to each state xt ; and
2. the summation over xt in Equation (15.5) is replaced by the maximization over xt in
Equation (15.11).

VITERBI ALGORITHM

15.3

HIDDEN MARKOV
MODEL

Thus, the algorithm for computing the most likely sequence is similar to filtering: it runs forward along the sequence, computing the m message at each time step, using Equation (15.11).
The progress of this computation is shown in Figure 15.5(b). At the end, it will have the
probability for the most likely sequence reaching each of the final states. One can thus easily
select the most likely sequence overall (the states outlined in bold). In order to identify the
actual sequence, as opposed to just computing its probability, the algorithm will also need to
record, for each state, the best state that leads to it; these are indicated by the bold arrows in
Figure 15.5(b). The optimal sequence is identified by following these bold arrows backwards
from the best final state.
The algorithm we have just described is called the Viterbi algorithm, after its inventor.
Like the filtering algorithm, its time complexity is linear in t, the length of the sequence.
Unlike filtering, which uses constant space, its space requirement is also linear in t. This
is because the Viterbi algorithm needs to keep the pointers that identify the best sequence
leading to each state.

H IDDEN M ARKOV M ODELS
The preceding section developed algorithms for temporal probabilistic reasoning using a general framework that was independent of the specific form of the transition and sensor models.
In this and the next two sections, we discuss more concrete models and applications that
illustrate the power of the basic algorithms and in some cases allow further improvements.
We begin with the hidden Markov model, or HMM. An HMM is a temporal probabilistic model in which the state of the process is described by a single discrete random variable. The possible values of the variable are the possible states of the world. The umbrella
example described in the preceding section is therefore an HMM, since it has just one state
variable: Rain t . What happens if you have a model with two or more state variables? You can
still fit it into the HMM framework by combining the variables into a single â€œmegavariableâ€
whose values are all possible tuples of values of the individual state variables. We will see
that the restricted structure of HMMs allows for a simple and elegant matrix implementation
of all the basic algorithms.4
4

The reader unfamiliar with basic operations on vectors and matrices might wish to consult Appendix A before
proceeding with this section.

Section 15.3.

Hidden Markov Models

579

15.3.1 Simplified matrix algorithms
With a single, discrete state variable Xt , we can give concrete form to the representations
of the transition model, the sensor model, and the forward and backward messages. Let the
state variable Xt have values denoted by integers 1, . . . , S, where S is the number of possible
states. The transition model P(Xt | Xtâˆ’1 ) becomes an S Ã— S matrix T, where
Tij = P (Xt = j | Xtâˆ’1 = i) .
That is, Tij is the probability of a transition from state i to state j. For example, the transition
matrix for the umbrella world

 is

0.7 0.3
.
T = P(Xt | Xtâˆ’1 ) =
0.3 0.7
We also put the sensor model in matrix form. In this case, because the value of the evidence
variable Et is known at time t (call it et ), we need only specify, for each state, how likely it
is that the state causes et to appear: we need P (et | Xt = i) for each state i. For mathematical
convenience we place these values into an S Ã— S diagonal matrix, Ot whose ith diagonal
entry is P (et | Xt = i) and whose other entries are 0. For example, on day 1 in the umbrella
world of Figure 15.5, U1 = true, and on day 3, U3 = false, so, from Figure 15.2, we have




0.9 0
0.1 0
;
O3 =
.
O1 =
0 0.2
0 0.8
Now, if we use column vectors to represent the forward and backward messages, all the computations become simple matrixâ€“vector operations. The forward equation (15.5) becomes
f1:t+1 = Î± Ot+1 T f1:t

(15.12)

and the backward equation (15.9) becomes
bk+1:t = TOk+1 bk+2:t .

(15.13)

From these equations, we can see that the time complexity of the forwardâ€“backward algorithm (Figure 15.4) applied to a sequence of length t is O(S 2 t), because each step requires
multiplying an S-element vector by an S Ã— S matrix. The space requirement is O(St), because the forward pass stores t vectors of size S.
Besides providing an elegant description of the filtering and smoothing algorithms for
HMMs, the matrix formulation reveals opportunities for improved algorithms. The first is
a simple variation on the forwardâ€“backward algorithm that allows smoothing to be carried
out in constant space, independently of the length of the sequence. The idea is that smoothing for any particular time slice k requires the simultaneous presence of both the forward and
backward messages, f1:k and bk+1:t , according to Equation (15.8). The forwardâ€“backward algorithm achieves this by storing the fs computed on the forward pass so that they are available
during the backward pass. Another way to achieve this is with a single pass that propagates
both f and b in the same direction. For example, the â€œforwardâ€ message f can be propagated
backward if we manipulate Equation (15.12) to work in the other direction:
f1:t = Î± (T )âˆ’1 Oâˆ’1
t+1 f1:t+1 .
The modified smoothing algorithm works by first running the standard forward pass to compute ft:t (forgetting all the intermediate results) and then running the backward pass for both

580

Chapter 15.

Probabilistic Reasoning over Time

function F IXED -L AG -S MOOTHING(et , hmm, d ) returns a distribution over Xtâˆ’d
inputs: et , the current evidence for time step t
hmm, a hidden Markov model with S Ã— S transition matrix T
d , the length of the lag for smoothing
persistent: t , the current time, initially 1
f, the forward message P(Xt |e1:t ), initially hmm.P RIOR
B, the d-step backward transformation matrix, initially the identity matrix
etâˆ’d:t , double-ended list of evidence from t âˆ’ d to t, initially empty
local variables: Otâˆ’d , Ot , diagonal matrices containing the sensor model information
add et to the end of etâˆ’d:t
Ot â† diagonal matrix containing P(et |Xt )
if t > d then
f â† F ORWARD(f, et )
remove etâˆ’dâˆ’1 from the beginning of etâˆ’d:t
Otâˆ’d â† diagonal matrix containing P(etâˆ’d |Xtâˆ’d )
âˆ’1
B â† Oâˆ’1
BTOt
tâˆ’d T
else B â† BTOt
t â†t + 1
if t > d then return N ORMALIZE(f Ã— B1) else return null
Figure 15.6 An algorithm for smoothing with a fixed time lag of d steps, implemented
as an online algorithm that outputs the new smoothed estimate given the observation for a
new time step. Notice that the final output N ORMALIZE(f Ã— B1) is just Î± f Ã— b, by Equation (15.14).

b and f together, using them to compute the smoothed estimate at each step. Since only one
copy of each message is needed, the storage requirements are constant (i.e., independent of
t, the length of the sequence). There are two significant restrictions on this algorithm: it requires that the transition matrix be invertible and that the sensor model have no zeroesâ€”that
is, that every observation be possible in every state.
A second area in which the matrix formulation reveals an improvement is in online
smoothing with a fixed lag. The fact that smoothing can be done in constant space suggests
that there should exist an efficient recursive algorithm for online smoothingâ€”that is, an algorithm whose time complexity is independent of the length of the lag. Let us suppose that
the lag is d; that is, we are smoothing at time slice t âˆ’ d, where the current time is t. By
Equation (15.8), we need to compute
Î± f1:tâˆ’d Ã— btâˆ’d+1:t
for slice t âˆ’ d. Then, when a new observation arrives, we need to compute
Î± f1:tâˆ’d+1 Ã— btâˆ’d+2:t+1
for slice t âˆ’ d + 1. How can this be done incrementally? First, we can compute f1:tâˆ’d+1 from
f1:tâˆ’d , using the standard filtering process, Equation (15.5).

Section 15.3.

Hidden Markov Models

581

Computing the backward message incrementally is trickier, because there is no simple
relationship between the old backward message btâˆ’d+1:t and the new backward message
btâˆ’d+2:t+1 . Instead, we will examine the relationship between the old backward message
btâˆ’d+1:t and the backward message at the front of the sequence, bt+1:t . To do this, we apply
Equation (15.13) d times to get

t

TOi bt+1:t = Btâˆ’d+1:t 1 ,
(15.14)
btâˆ’d+1:t =
i = tâˆ’d+1

where the matrix Btâˆ’d+1:t is the product of the sequence of T and O matrices. B can be
thought of as a â€œtransformation operatorâ€ that transforms a later backward message into an
earlier one. A similar equation holds for the new backward messages after the next observation arrives:
 t+1

TOi bt+2:t+1 = Btâˆ’d+2:t+1 1 .
(15.15)
btâˆ’d+2:t+1 =
i = tâˆ’d+2

Examining the product expressions in Equations (15.14) and (15.15), we see that they have a
simple relationship: to get the second product, â€œdivideâ€ the first product by the first element
TOtâˆ’d+1 , and multiply by the new last element TOt+1 . In matrix language, then, there is a
simple relationship between the old and new B matrices:
âˆ’1
Btâˆ’d+2:t+1 = Oâˆ’1
tâˆ’d+1 T Btâˆ’d+1:t TOt+1 .

(15.16)

This equation provides an incremental update for the B matrix, which in turn (through Equation (15.15)) allows us to compute the new backward message btâˆ’d+2:t+1 . The complete
algorithm, which requires storing and updating f and B, is shown in Figure 15.6.

15.3.2 Hidden Markov model example: Localization
On page 145, we introduced a simple form of the localization problem for the vacuum world.
In that version, the robot had a single nondeterministic Move action and its sensors reported
perfectly whether or not obstacles lay immediately to the north, south, east, and west; the
robotâ€™s belief state was the set of possible locations it could be in.
Here we make the problem slightly more realistic by including a simple probability
model for the robotâ€™s motion and by allowing for noise in the sensors. The state variable Xt
represents the location of the robot on the discrete grid; the domain of this variable is the
set of empty squares {s1 , . . . , sn }. Let N EIGHBORS(s) be the set of empty squares that are
adjacent to s and let N (s) be the size of that set. Then the transition model for Move action
says that the robot is equally likely to end up at any neighboring square:
P (Xt+1 = j | Xt = i) = Tij = (1/N (i) if j âˆˆ N EIGHBORS (i) else 0) .
We donâ€™t know where the robot starts, so we will assume a uniform distribution over all the
squares; that is, P (X0 = i) = 1/n. For the particular environment we consider (Figure 15.7),
n = 42 and the transition matrix T has 42 Ã— 42 = 1764 entries.
The sensor variable Et has 16 possible values, each a four-bit sequence giving the presence or absence of an obstacle in a particular compass direction. We will use the notation

582

Chapter 15.

Probabilistic Reasoning over Time

(a) Posterior distribution over robot location after E 1 = N SW

(b) Posterior distribution over robot location after E 1 = N SW, E 2 = N S
Figure 15.7 Posterior distribution over robot location: (a) one observation E1 = N SW ;
(b) after a second observation E2 = N S. The size of each disk corresponds to the probability
that the robot is at that location. The sensor error rate is  = 0.2.

N S, for example, to mean that the north and south sensors report an obstacle and the east and
west do not. Suppose that each sensorâ€™s error rate is  and that errors occur independently for
the four sensor directions. In that case, the probability of getting all four bits right is (1 âˆ’ )4
and the probability of getting them all wrong is 4 . Furthermore, if dit is the discrepancyâ€”the
number of bits that are differentâ€”between the true values for square i and the actual reading
et , then the probability that a robot in square i would receive a sensor reading et is
P (Et = et | Xt = i) = Otii = (1 âˆ’ )4âˆ’dit dit .
For example, the probability that a square with obstacles to the north and south would produce
a sensor reading NSE is (1 âˆ’ )3 1 .
Given the matrices T and Ot , the robot can use Equation (15.12) to compute the posterior distribution over locationsâ€”that is, to work out where it is. Figure 15.7 shows the
distributions P(X1 | E1 = N SW ) and P(X2 | E1 = N SW, E2 = N S). This is the same maze
we saw before in Figure 4.18 (page 146), but there we used logical filtering to find the locations that were possible, assuming perfect sensing. Those same locations are still the most
likely with noisy sensing, but now every location has some nonzero probability.
In addition to filtering to estimate its current location, the robot can use smoothing
(Equation (15.13)) to work out where it was at any given past timeâ€”for example, where it
began at time 0â€”and it can use the Viterbi algorithm to work out the most likely path it has

Hidden Markov Models

6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5

583

Îµ = 0.20
Îµ = 0.10
Îµ = 0.05
Îµ = 0.02
Îµ = 0.00

0

5

10 15 20 25 30
Number of observations

(a)

Path accuracy

Localization error

Section 15.3.

35

40

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

Îµ = 0.00
Îµ = 0.02
Îµ = 0.05
Îµ = 0.10
Îµ = 0.20

0

5

10 15 20 25 30
Number of observations

35

40

(b)

Figure 15.8 Performance of HMM localization as a function of the length of the observation sequence for various different values of the sensor error probability ; data averaged over
400 runs. (a) The localization error, defined as the Manhattan distance from the true location.
(b) The Viterbi path accuracy, defined as the fraction of correct states on the Viterbi path.

taken to get where it is now. Figure 15.8 shows the localization error and Viterbi path accuracy
for various values of the per-bit sensor error rate . Even when  is 20%â€”which means that
the overall sensor reading is wrong 59% of the timeâ€”the robot is usually able to work out its
location within two squares after 25 observations. This is because of the algorithmâ€™s ability
to integrate evidence over time and to take into account the probabilistic constraints imposed
on the location sequence by the transition model. When  is 10%, the performance after
a half-dozen observations is hard to distinguish from the performance with perfect sensing.
Exercise 15.7 asks you to explore how robust the HMM localization algorithm is to errors in
the prior distribution P(X0 ) and in the transition model itself. Broadly speaking, high levels
of localization and path accuracy are maintained even in the face of substantial errors in the
models used.
The state variable for the example we have considered in this section is a physical
location in the world. Other problems can, of course, include other aspects of the world.
Exercise 15.8 asks you to consider a version of the vacuum robot that has the policy of going
straight for as long as it can; only when it encounters an obstacle does it change to a new
(randomly selected) heading. To model this robot, each state in the model consists of a
(location, heading) pair. For the environment in Figure 15.7, which has 42 empty squares,
this leads to 168 states and a transition matrix with 1682 = 28, 224 entriesâ€”still a manageable
number. If we add the possibility of dirt in the squares, the number of states is multiplied by
242 and the transition matrix ends up with more than 1029 entriesâ€”no longer a manageable
number; Section 15.5 shows how to use dynamic Bayesian networks to model domains with
many state variables. If we allow the robot to move continuously rather than in a discrete
grid, the number of states becomes infinite; the next section shows how to handle this case.

584

15.4

Chapter 15.

K ALMAN F ILTERS

KALMAN FILTERING

MULTIVARIATE
GAUSSIAN

Probabilistic Reasoning over Time

Imagine watching a small bird flying through dense jungle foliage at dusk: you glimpse
brief, intermittent flashes of motion; you try hard to guess where the bird is and where it will
appear next so that you donâ€™t lose it. Or imagine that you are a World War II radar operator
peering at a faint, wandering blip that appears once every 10 seconds on the screen. Or, going
back further still, imagine you are Kepler trying to reconstruct the motions of the planets
from a collection of highly inaccurate angular observations taken at irregular and imprecisely
measured intervals. In all these cases, you are doing filtering: estimating state variables (here,
position and velocity) from noisy observations over time. If the variables were discrete, we
could model the system with a hidden Markov model. This section examines methods for
handling continuous variables, using an algorithm called Kalman filtering, after one of its
inventors, Rudolf E. Kalman.
The birdâ€™s flight might be specified by six continuous variables at each time point; three
for position (Xt , Yt , Zt ) and three for velocity (XÌ‡t , YÌ‡t , ZÌ‡t ). We will need suitable conditional
densities to represent the transition and sensor models; as in Chapter 14, we will use linear
Gaussian distributions. This means that the next state Xt+1 must be a linear function of the
current state Xt , plus some Gaussian noise, a condition that turns out to be quite reasonable in
practice. Consider, for example, the X-coordinate of the bird, ignoring the other coordinates
for now. Let the time interval between observations be Î”, and assume constant velocity
during the interval; then the position update is given by Xt+Î” = Xt + XÌ‡ Î”. Adding Gaussian
noise (to account for wind variation, etc.), we obtain a linear Gaussian transition model:
P (Xt+Î” = xt+Î” | Xt = xt , XÌ‡t = xÌ‡t ) = N (xt + xÌ‡t Î”, Ïƒ 2 )(xt+Î” ) .
The Bayesian network structure for a system with position vector Xt and velocity XÌ‡t is shown
in Figure 15.9. Note that this is a very specific form of linear Gaussian model; the general
form will be described later in this section and covers a vast array of applications beyond the
simple motion examples of the first paragraph. The reader might wish to consult Appendix A
for some of the mathematical properties of Gaussian distributions; for our immediate purposes, the most important is that a multivariate Gaussian distribution for d variables is
specified by a d-element mean Î¼ and a d Ã— d covariance matrix Î£.

15.4.1 Updating Gaussian distributions
In Chapter 14 on page 521, we alluded to a key property of the linear Gaussian family of distributions: it remains closed under the standard Bayesian network operations. Here, we make
this claim precise in the context of filtering in a temporal probability model. The required
properties correspond to the two-step filtering calculation in Equation (15.5):
1. If the current distribution P(Xt | e1:t ) is Gaussian and the transition model P(Xt+1 | xt )
is linear Gaussian, then the one-step predicted distribution given by

P(Xt+1 | xt )P (xt | e1:t ) dxt
(15.17)
P(Xt+1 | e1:t ) =
xt

is also a Gaussian distribution.

Section 15.4.

Kalman Filters

585

Xt

X t+1

Xt

X t+1

Zt

Zt+1

Figure 15.9 Bayesian network structure for a linear dynamical system with position Xt ,
velocity XÌ‡t , and position measurement Zt .

2. If the prediction P(Xt+1 | e1:t ) is Gaussian and the sensor model P(et+1 | Xt+1 ) is linear
Gaussian, then, after conditioning on the new evidence, the updated distribution
P(Xt+1 | e1:t+1 ) = Î± P(et+1 | Xt+1 )P(Xt+1 | e1:t )

(15.18)

is also a Gaussian distribution.
Thus, the F ORWARD operator for Kalman filtering takes a Gaussian forward message f1:t ,
specified by a mean Î¼t and covariance matrix Î£t , and produces a new multivariate Gaussian
forward message f1:t+1 , specified by a mean Î¼t+1 and covariance matrix Î£t+1 . So, if we
start with a Gaussian prior f1:0 = P(X0 ) = N (Î¼0 , Î£0 ), filtering with a linear Gaussian model
produces a Gaussian state distribution for all time.
This seems to be a nice, elegant result, but why is it so important? The reason is that,
except for a few special cases such as this, filtering with continuous or hybrid (discrete and
continuous) networks generates state distributions whose representation grows without bound
over time. This statement is not easy to prove in general, but Exercise 15.10 shows what
happens for a simple example.

15.4.2 A simple one-dimensional example
We have said that the F ORWARD operator for the Kalman filter maps a Gaussian into a new
Gaussian. This translates into computing a new mean and covariance matrix from the previous mean and covariance matrix. Deriving the update rule in the general (multivariate) case
requires rather a lot of linear algebra, so we will stick to a very simple univariate case for now;
and later give the results for the general case. Even for the univariate case, the calculations
are somewhat tedious, but we feel that they are worth seeing because the usefulness of the
Kalman filter is tied so intimately to the mathematical properties of Gaussian distributions.
The temporal model we consider describes a random walk of a single continuous state
variable Xt with a noisy observation Zt . An example might be the â€œconsumer confidenceâ€ index, which can be modeled as undergoing a random Gaussian-distributed change each month
and is measured by a random consumer survey that also introduces Gaussian sampling noise.

586

Chapter 15.

Probabilistic Reasoning over Time

The prior distribution is assumed to be Gaussian with variance Ïƒ02 :
âˆ’ 12

â€ž

P (x0 ) = Î± e

(x0 âˆ’Î¼0 )2
Ïƒ2
0

Â«

.

(For simplicity, we use the same symbol Î± for all normalizing constants in this section.) The
transition model adds a Gaussian perturbation of constant variance Ïƒx2 to the current state:
âˆ’ 21

â€ž

P (xt+1 | xt ) = Î± e

Â«

(xt+1 âˆ’xt )2
2
Ïƒx

.

The sensor model assumes Gaussian noise with variance Ïƒz2 :
âˆ’ 12

â€ž

P (zt | xt ) = Î± e

(zt âˆ’xt )2
2
Ïƒz

Â«

.

Now, given the prior P(X0 ), the one-step predicted distribution comes from Equation (15.17):
â€ž
Â«
â€ž
Â«
2
 âˆž
 âˆž
(x1 âˆ’x0 )2
1 (x0 âˆ’Î¼0 )
âˆ’ 21
âˆ’
2
2
2
Ïƒx
Ïƒ0
P (x1 | x0 )P (x0 ) dx0 = Î±
e
e
dx0
P (x1 ) =
âˆ’âˆž



âˆž

âˆ’ 12

= Î±

e

â€ž

âˆ’âˆž
Â«

2 (x âˆ’x )2 +Ïƒ 2 (x âˆ’Î¼ )2
Ïƒ0
1
0
0
x 0
2
Ïƒ 2 Ïƒx
0

dx0 .

âˆ’âˆž

COMPLETING THE
SQUARE

This integral looks rather complicated. The key to progress is to notice that the exponent is the
sum of two expressions that are quadratic in x0 and hence is itself a quadratic in x0 . A simple
trick known as completing the square allows the rewriting of any quadratic ax20 + bx0 + c
b2
2
as the sum of a squared term a(x0 âˆ’ âˆ’b
2a ) and a residual term c âˆ’ 4a that is independent of
x0 . The residual term can be taken outside the integral, giving us
â€œ
â€ âˆž
b2
âˆ’b 2
1
âˆ’ 21 câˆ’ 4a
eâˆ’ 2 (a(x0 âˆ’ 2a ) ) dx0 .
P (x1 ) = Î± e
âˆ’âˆž

Now the integral is just the integral of a Gaussian over its full range, which is simply 1. Thus,
we are left with only the residual term from the quadratic. Then, we notice that the residual
term is a quadratic in x1 ; in fact, after simplification, we obtain
âˆ’ 21

â€ž

P (x1 ) = Î± e

(x1 âˆ’Î¼0 )2
2 +Ïƒ 2
Ïƒ0
x

Â«

.

That is, the one-step predicted distribution is a Gaussian with the same mean Î¼0 and a variance
equal to the sum of the original variance Ïƒ02 and the transition variance Ïƒx2 .
To complete the update step, we need to condition on the observation at the first time
step, namely, z1 . From Equation (15.18), this is given by
P (x1 | z1 ) = Î± P (z1 | x1 )P (x1 )
âˆ’ 12

â€ž

= Î±e

(z1 âˆ’x1 )2
2
Ïƒz

Â«

âˆ’ 12

e

â€ž

(x1 âˆ’Î¼0 )2
2 +Ïƒ 2
Ïƒ0
x

Â«

.

Once again, we combine the exponents and complete the square (Exercise 15.11), obtaining
0

B
âˆ’ 12 B
@

P (x1 | z1 ) = Î± e

1

2 +Ïƒ 2 )z +Ïƒ 2 Î¼
(Ïƒ0
x 1
z 0 )2
2 +Ïƒ 2
C
Ïƒ 2 +Ïƒx
z
0
C
2 +Ïƒ 2 )Ïƒ 2 /(Ïƒ 2 +Ïƒ 2 +Ïƒ 2 ) A
(Ïƒ0
z
x
x z
0

(x1 âˆ’

.

(15.19)

Kalman Filters

587

0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

P(x)

Section 15.4.

P(x1 | z1 = 2.5)
P(x0)

P(x1)

-10

-5

0
x position

z*1

5

10

Figure 15.10 Stages in the Kalman filter update cycle for a random walk with a prior
given by Î¼0 = 0.0 and Ïƒ0 = 1.0, transition noise given by Ïƒx = 2.0, sensor noise given by
Ïƒz = 1.0, and a first observation z1 = 2.5 (marked on the x-axis). Notice how the prediction
P (x1 ) is flattened out, relative to P (x0 ), by the transition noise. Notice also that the mean
of the posterior distribution P (x1 | z1 ) is slightly to the left of the observation z1 because the
mean is a weighted average of the prediction and the observation.

Thus, after one update cycle, we have a new Gaussian distribution for the state variable.
From the Gaussian formula in Equation (15.19), we see that the new mean and standard
deviation can be calculated from the old mean and standard deviation as follows:
(Ïƒ 2 + Ïƒ 2 )zt+1 + Ïƒz2 Î¼t
(Ïƒt2 + Ïƒx2 )Ïƒz2
2
and
Ïƒ
=
.
(15.20)
Î¼t+1 = t 2 x 2
t+1
Ïƒt + Ïƒx + Ïƒz2
Ïƒt2 + Ïƒx2 + Ïƒz2
Figure 15.10 shows one update cycle for particular values of the transition and sensor models.
Equation (15.20) plays exactly the same role as the general filtering equation (15.5) or
the HMM filtering equation (15.12). Because of the special nature of Gaussian distributions,
however, the equations have some interesting additional properties. First, we can interpret
the calculation for the new mean Î¼t+1 as simply a weighted mean of the new observation
zt+1 and the old mean Î¼t . If the observation is unreliable, then Ïƒz2 is large and we pay more
attention to the old mean; if the old mean is unreliable (Ïƒt2 is large) or the process is highly
unpredictable (Ïƒx2 is large), then we pay more attention to the observation. Second, notice
2
is independent of the observation. We can therefore
that the update for the variance Ïƒt+1
compute in advance what the sequence of variance values will be. Third, the sequence of
variance values converges quickly to a fixed value that depends only on Ïƒx2 and Ïƒz2 , thereby
substantially simplifying the subsequent calculations. (See Exercise 15.12.)

15.4.3 The general case
The preceding derivation illustrates the key property of Gaussian distributions that allows
Kalman filtering to work: the fact that the exponent is a quadratic form. This is true not just
for the univariate case; the full multivariate Gaussian distribution has the form
â€œ
â€
âˆ’1
âˆ’ 21 (xâˆ’Î¼) Î£ (xâˆ’Î¼)

N (Î¼, Î£)(x) = Î± e

.

588

Chapter 15.

Probabilistic Reasoning over Time

Multiplying out the terms in the exponent makes it clear that the exponent is also a quadratic
function of the values xi in x. As in the univariate case, the filtering update preserves the
Gaussian nature of the state distribution.
Let us first define the general temporal model used with Kalman filtering. Both the transition model and the sensor model allow for a linear transformation with additive Gaussian
noise. Thus, we have
P (xt+1 | xt ) = N (Fxt , Î£x )(xt+1 )
(15.21)
P (zt | xt ) = N (Hxt , Î£z )(zt ) ,
where F and Î£x are matrices describing the linear transition model and transition noise covariance, and H and Î£z are the corresponding matrices for the sensor model. Now the update
equations for the mean and covariance, in their full, hairy horribleness, are
Î¼t+1 = FÎ¼t + Kt+1 (zt+1 âˆ’ HFÎ¼t )
(15.22)
Î£t+1 = (I âˆ’ Kt+1 H)(FÎ£t F + Î£x ) ,
KALMAN GAIN
MATRIX

where Kt+1 = (FÎ£t F + Î£x )H (H(FÎ£t F + Î£x )H + Î£z )âˆ’1 is called the Kalman gain
matrix. Believe it or not, these equations make some intuitive sense. For example, consider
the update for the mean state estimate Î¼. The term FÎ¼t is the predicted state at t + 1, so
HFÎ¼t is the predicted observation. Therefore, the term zt+1 âˆ’ HFÎ¼t represents the error in
the predicted observation. This is multiplied by Kt+1 to correct the predicted state; hence,
Kt+1 is a measure of how seriously to take the new observation relative to the prediction. As
in Equation (15.20), we also have the property that the variance update is independent of the
observations. The sequence of values for Î£t and Kt can therefore be computed offline, and
the actual calculations required during online tracking are quite modest.
To illustrate these equations at work, we have applied them to the problem of tracking
an object moving on the Xâ€“Y plane. The state variables are X = (X, Y, XÌ‡, YÌ‡ ) , so F, Î£x ,
H, and Î£z are 4 Ã— 4 matrices. Figure 15.11(a) shows the true trajectory, a series of noisy
observations, and the trajectory estimated by Kalman filtering, along with the covariances
indicated by the one-standard-deviation contours. The filtering process does a good job of
tracking the actual motion, and, as expected, the variance quickly reaches a fixed point.
We can also derive equations for smoothing as well as filtering with linear Gaussian
models. The smoothing results are shown in Figure 15.11(b). Notice how the variance in the
position estimate is sharply reduced, except at the ends of the trajectory (why?), and that the
estimated trajectory is much smoother.

15.4.4 Applicability of Kalman filtering
The Kalman filter and its elaborations are used in a vast array of applications. The â€œclassicalâ€
application is in radar tracking of aircraft and missiles. Related applications include acoustic
tracking of submarines and ground vehicles and visual tracking of vehicles and people. In a
slightly more esoteric vein, Kalman filters are used to reconstruct particle trajectories from
bubble-chamber photographs and ocean currents from satellite surface measurements. The
range of application is much larger than just the tracking of motion: any system characterized
by continuous state variables and noisy measurements will do. Such systems include pulp
mills, chemical plants, nuclear reactors, plant ecosystems, and national economies.

Section 15.4.

Kalman Filters

589

2D filtering

2D smoothing

12

12

true
observed
smoothed

11

10

10

Y 9

Y 9

8

8

7

7

6
8

10

12

14

16

X

18

(a)

20

22

24

true
observed
smoothed

11

26

6
8

10

12

14

16

X

18

20

22

24

26

(b)

Figure 15.11 (a) Results of Kalman filtering for an object moving on the Xâ€“Y plane,
showing the true trajectory (left to right), a series of noisy observations, and the trajectory
estimated by Kalman filtering. Variance in the position estimate is indicated by the ovals. (b)
The results of Kalman smoothing for the same observation sequence.

EXTENDED KALMAN
FILTER (EKF)
NONLINEAR

SWITCHING KALMAN
FILTER

The fact that Kalman filtering can be applied to a system does not mean that the results will be valid or useful. The assumptions madeâ€”a linear Gaussian transition and sensor
modelsâ€”are very strong. The extended Kalman filter (EKF) attempts to overcome nonlinearities in the system being modeled. A system is nonlinear if the transition model cannot
be described as a matrix multiplication of the state vector, as in Equation (15.21). The EKF
works by modeling the system as locally linear in xt in the region of xt = Î¼t , the mean of the
current state distribution. This works well for smooth, well-behaved systems and allows the
tracker to maintain and update a Gaussian state distribution that is a reasonable approximation
to the true posterior. A detailed example is given in Chapter 25.
What does it mean for a system to be â€œunsmoothâ€ or â€œpoorly behavedâ€? Technically,
it means that there is significant nonlinearity in system response within the region that is
â€œcloseâ€ (according to the covariance Î£t ) to the current mean Î¼t . To understand this idea
in nontechnical terms, consider the example of trying to track a bird as it flies through the
jungle. The bird appears to be heading at high speed straight for a tree trunk. The Kalman
filter, whether regular or extended, can make only a Gaussian prediction of the location of the
bird, and the mean of this Gaussian will be centered on the trunk, as shown in Figure 15.12(a).
A reasonable model of the bird, on the other hand, would predict evasive action to one side or
the other, as shown in Figure 15.12(b). Such a model is highly nonlinear, because the birdâ€™s
decision varies sharply depending on its precise location relative to the trunk.
To handle examples like these, we clearly need a more expressive language for representing the behavior of the system being modeled. Within the control theory community, for
which problems such as evasive maneuvering by aircraft raise the same kinds of difficulties,
the standard solution is the switching Kalman filter. In this approach, multiple Kalman fil-

590

Chapter 15.

(a)

Probabilistic Reasoning over Time

(b)

Figure 15.12 A bird flying toward a tree (top views). (a) A Kalman filter will predict the
location of the bird using a single Gaussian centered on the obstacle. (b) A more realistic
model allows for the birdâ€™s evasive action, predicting that it will fly to one side or the other.

ters run in parallel, each using a different model of the systemâ€”for example, one for straight
flight, one for sharp left turns, and one for sharp right turns. A weighted sum of predictions
is used, where the weight depends on how well each filter fits the current data. We will see
in the next section that this is simply a special case of the general dynamic Bayesian network model, obtained by adding a discrete â€œmaneuverâ€ state variable to the network shown
in Figure 15.9. Switching Kalman filters are discussed further in Exercise 15.10.

15.5

DYNAMIC BAYESIAN N ETWORKS

DYNAMIC BAYESIAN
NETWORK

A dynamic Bayesian network, or DBN, is a Bayesian network that represents a temporal
probability model of the kind described in Section 15.1. We have already seen examples of
DBNs: the umbrella network in Figure 15.2 and the Kalman filter network in Figure 15.9. In
general, each slice of a DBN can have any number of state variables Xt and evidence variables
Et . For simplicity, we assume that the variables and their links are exactly replicated from
slice to slice and that the DBN represents a first-order Markov process, so that each variable
can have parents only in its own slice or the immediately preceding slice.
It should be clear that every hidden Markov model can be represented as a DBN with
a single state variable and a single evidence variable. It is also the case that every discretevariable DBN can be represented as an HMM; as explained in Section 15.3, we can combine
all the state variables in the DBN into a single state variable whose values are all possible
tuples of values of the individual state variables. Now, if every HMM is a DBN and every
DBN can be translated into an HMM, whatâ€™s the difference? The difference is that, by de-

Section 15.5.

Dynamic Bayesian Networks

591

composing the state of a complex system into its constituent variables, the can take advantage
of sparseness in the temporal probability model. Suppose, for example, that a DBN has 20
Boolean state variables, each of which has three parents in the preceding slice. Then the
DBN transition model has 20 Ã— 23 = 160 probabilities, whereas the corresponding HMM has
220 states and therefore 240 , or roughly a trillion, probabilities in the transition matrix. This
is bad for at least three reasons: first, the HMM itself requires much more space; second,
the huge transition matrix makes HMM inference much more expensive; and third, the problem of learning such a huge number of parameters makes the pure HMM model unsuitable
for large problems. The relationship between DBNs and HMMs is roughly analogous to the
relationship between ordinary Bayesian networks and full tabulated joint distributions.
We have already explained that every Kalman filter model can be represented in a
DBN with continuous variables and linear Gaussian conditional distributions (Figure 15.9).
It should be clear from the discussion at the end of the preceding section that not every DBN
can be represented by a Kalman filter model. In a Kalman filter, the current state distribution
is always a single multivariate Gaussian distributionâ€”that is, a single â€œbumpâ€ in a particular
location. DBNs, on the other hand, can model arbitrary distributions. For many real-world
applications, this flexibility is essential. Consider, for example, the current location of my
keys. They might be in my pocket, on the bedside table, on the kitchen counter, dangling
from the front door, or locked in the car. A single Gaussian bump that included all these
places would have to allocate significant probability to the keys being in mid-air in the front
hall. Aspects of the real world such as purposive agents, obstacles, and pockets introduce
â€œnonlinearitiesâ€ that require combinations of discrete and continuous variables in order to get
reasonable models.

15.5.1 Constructing DBNs
To construct a DBN, one must specify three kinds of information: the prior distribution over
the state variables, P(X0 ); the transition model P(Xt+1 | Xt ); and the sensor model P(Et | Xt ).
To specify the transition and sensor models, one must also specify the topology of the connections between successive slices and between the state and evidence variables. Because
the transition and sensor models are assumed to be stationaryâ€”the same for all tâ€”it is most
convenient simply to specify them for the first slice. For example, the complete DBN specification for the umbrella world is given by the three-node network shown in Figure 15.13(a).
From this specification, the complete DBN with an unbounded number of time slices can be
constructed as needed by copying the first slice.
Let us now consider a more interesting example: monitoring a battery-powered robot
moving in the Xâ€“Y plane, as introduced at the end of Section 15.1. First, we need state
variables, which will include both Xt = (Xt , Yt ) for position and XÌ‡t = (XÌ‡t , YÌ‡t ) for velocity.
We assume some method of measuring positionâ€”perhaps a fixed camera or onboard GPS
(Global Positioning System)â€”yielding measurements Zt . The position at the next time step
depends on the current position and velocity, as in the standard Kalman filter model. The
velocity at the next step depends on the current velocity and the state of the battery. We
add Battery t to represent the actual battery charge level, which has as parents the previous

592

Chapter 15.

Probabilistic Reasoning over Time

BMeter1
P(R 0)
0.7

R0

t
f

P(R1)
0.7
0.3

Rain 0

Rain1
R1

P(U1 )

t
f

0.9
0.2

Battery 0

Battery 1

X0

X1

XX
t0

X1
Z1

Umbrella 1
(a)

(b)

Figure 15.13 (a) Specification of the prior, transition model, and sensor model for the
umbrella DBN. All subsequent slices are assumed to be copies of slice 1. (b) A simple DBN
for robot motion in the Xâ€“Y plane.

GAUSSIAN ERROR
MODEL

TRANSIENT FAILURE

battery level and the velocity, and we add BMeter t , which measures the battery charge level.
This gives us the basic model shown in Figure 15.13(b).
It is worth looking in more depth at the nature of the sensor model for BMeter t . Let
us suppose, for simplicity, that both Battery t and BMeter t can take on discrete values 0
through 5. If the meter is always accurate, then the CPT P(BMeter t | Battery t ) should have
probabilities of 1.0 â€œalong the diagonalâ€ and probabilities of 0.0 elsewhere. In reality, noise
always creeps into measurements. For continuous measurements, a Gaussian distribution
with a small variance might be used.5 For our discrete variables, we can approximate a
Gaussian using a distribution in which the probability of error drops off in the appropriate
way, so that the probability of a large error is very small. We use the term Gaussian error
model to cover both the continuous and discrete versions.
Anyone with hands-on experience of robotics, computerized process control, or other
forms of automatic sensing will readily testify to the fact that small amounts of measurement
noise are often the least of oneâ€™s problems. Real sensors fail. When a sensor fails, it does
not necessarily send a signal saying, â€œOh, by the way, the data Iâ€™m about to send you is a
load of nonsense.â€ Instead, it simply sends the nonsense. The simplest kind of failure is
called a transient failure, where the sensor occasionally decides to send some nonsense. For
example, the battery level sensor might have a habit of sending a zero when someone bumps
the robot, even if the battery is fully charged.
Letâ€™s see what happens when a transient failure occurs with a Gaussian error model that
doesnâ€™t accommodate such failures. Suppose, for example, that the robot is sitting quietly and
observes 20 consecutive battery readings of 5. Then the battery meter has a temporary seizure
5

Strictly speaking, a Gaussian distribution is problematic because it assigns nonzero probability to large negative charge levels. The beta distribution is sometimes a better choice for a variable whose range is restricted.

Section 15.5.

Dynamic Bayesian Networks

593

and the next reading is BMeter 21 = 0. What will the simple Gaussian error model lead us to
believe about Battery 21 ? According to Bayesâ€™ rule, the answer depends on both the sensor
model P(BMeter 21 = 0 | Battery 21 ) and the prediction P(Battery 21 | BMeter 1:20 ). If the
probability of a large sensor error is significantly less likely than the probability of a transition
to Battery 21 = 0, even if the latter is very unlikely, then the posterior distribution will assign
a high probability to the batteryâ€™s being empty. A second reading of 0 at t = 22 will make
this conclusion almost certain. If the transient failure then disappears and the reading returns
to 5 from t = 23 onwards, the estimate for the battery level will quickly return to 5, as if by
magic. This course of events is illustrated in the upper curve of Figure 15.14(a), which shows
the expected value of Battery t over time, using a discrete Gaussian error model.
Despite the recovery, there is a time (t = 22) when the robot is convinced that its battery
is empty; presumably, then, it should send out a mayday signal and shut down. Alas, its
oversimplified sensor model has led it astray. How can this be fixed? Consider a familiar
example from everyday human driving: on sharp curves or steep hills, oneâ€™s â€œfuel tank emptyâ€
warning light sometimes turns on. Rather than looking for the emergency phone, one simply
recalls that the fuel gauge sometimes gives a very large error when the fuel is sloshing around
in the tank. The moral of the story is the following: for the system to handle sensor failure
properly, the sensor model must include the possibility of failure.
The simplest kind of failure model for a sensor allows a certain probability that the
sensor will return some completely incorrect value, regardless of the true state of the world.
For example, if the battery meter fails by returning 0, we might say that
P (BMeter t = 0 | Battery t = 5) = 0.03 ,
TRANSIENT FAILURE
MODEL

PERSISTENT
FAILURE MODEL

which is presumably much larger than the probability assigned by the simple Gaussian error
model. Letâ€™s call this the transient failure model. How does it help when we are faced
with a reading of 0? Provided that the predicted probability of an empty battery, according
to the readings so far, is much less than 0.03, then the best explanation of the observation
BMeter 21 = 0 is that the sensor has temporarily failed. Intuitively, we can think of the belief
about the battery level as having a certain amount of â€œinertiaâ€ that helps to overcome temporary blips in the meter reading. The upper curve in Figure 15.14(b) shows that the transient
failure model can handle transient failures without a catastrophic change in beliefs.
So much for temporary blips. What about a persistent sensor failure? Sadly, failures of
this kind are all too common. If the sensor returns 20 readings of 5 followed by 20 readings
of 0, then the transient sensor failure model described in the preceding paragraph will result
in the robot gradually coming to believe that its battery is empty when in fact it may be that
the meter has failed. The lower curve in Figure 15.14(b) shows the belief â€œtrajectoryâ€ for
this case. By t = 25â€”five readings of 0â€”the robot is convinced that its battery is empty.
Obviously, we would prefer the robot to believe that its battery meter is brokenâ€”if indeed
this is the more likely event.
Unsurprisingly, to handle persistent failure, we need a persistent failure model that
describes how the sensor behaves under normal conditions and after failure. To do this, we
need to augment the state of the system with an additional variable, say, BMBroken, that
describes the status of the battery meter. The persistence of failure must be modeled by an

594

Chapter 15.

E(Batteryt |...5555005555...)

5

E(Batteryt |...5555005555...)

5
4
E(Batteryt)

4
E(Batteryt)

Probabilistic Reasoning over Time

3
2
1

3
2
1

0

0

E(Batteryt |...5555000000...)

-1
15

20
25
Time step t

E(Batteryt |...5555000000...)

-1

30

15

(a)

20
25
Time step

30

(b)

Figure 15.14 (a) Upper curve: trajectory of the expected value of Battery t for an observation sequence consisting of all 5s except for 0s at t = 21 and t = 22, using a simple Gaussian
error model. Lower curve: trajectory when the observation remains at 0 from t = 21 onwards.
(b) The same experiment run with the transient failure model. Notice that the transient failure is handled well, but the persistent failure results in excessive pessimism about the battery
charge.

P(B1)

t
f

5

1.000
0.001

4

BMBroken 1

BMBroken 0

E(Batteryt)

B0

E(Batteryt |...5555005555...)

E(Batteryt |...5555000000...)

3
2

P(BMBrokent |...5555000000...)

1

BMeter1

0

P(BMBrokent |...5555005555...)

-1

Battery 1

Battery 0

(a)

15

20
25
Time step

30

(b)

Figure 15.15 (a) A DBN fragment showing the sensor status variable required for modeling persistent failure of the battery sensor. (b) Upper curves: trajectories of the expected
value of Battery t for the â€œtransient failureâ€ and â€œpermanent failureâ€ observations sequences.
Lower curves: probability trajectories for BMBroken given the two observation sequences.

PERSISTENCE ARC

arc linking BMBroken 0 to BMBroken 1 . This persistence arc has a CPT that gives a small
probability of failure in any given time step, say, 0.001, but specifies that the sensor stays
broken once it breaks. When the sensor is OK, the sensor model for BMeter is identical to
the transient failure model; when the sensor is broken, it says BMeter is always 0, regardless
of the actual battery charge.

Section 15.5.

Dynamic Bayesian Networks

P(R 0 )
0.7
Rain0

R0
t
f

P(R1 )
0.7
0.3
Rain1

595

P(R 0 )
0.7
Rain0

Umbrella1
R1
t
f

P(U1 )
0.9
0.2

R0
t
f

P(R1 )
0.7
0.3

R1
t
f

P(R2 )
0.7
0.3

R2
t
f

P(R3 )
0.7
0.3

R3
t
f

P(R4 )
0.7
0.3

Rain1

Rain2

Rain3

Rain4

Umbrella1

Umbrella2

Umbrella3

Umbrella4

R1
t
f

P(U1 )
0.9
0.2

R2
t
f

P(U2 )
0.9
0.2

R3
t
f

P(U3 )
0.9
0.2

R4
t
f

P(U4 )
0.9
0.2

Figure 15.16 Unrolling a dynamic Bayesian network: slices are replicated to accommodate the observation sequence Umbrella 1:3 . Further slices have no effect on inferences within
the observation period.

The persistent failure model for the battery sensor is shown in Figure 15.15(a). Its
performance on the two data sequences (temporary blip and persistent failure) is shown in
Figure 15.15(b). There are several things to notice about these curves. First, in the case
of the temporary blip, the probability that the sensor is broken rises significantly after the
second 0 reading, but immediately drops back to zero once a 5 is observed. Second, in the
case of persistent failure, the probability that the sensor is broken rises quickly to almost 1
and stays there. Finally, once the sensor is known to be broken, the robot can only assume
that its battery discharges at the â€œnormalâ€ rate, as shown by the gradually descending level of
E(Battery t | . . . ).
So far, we have merely scratched the surface of the problem of representing complex
processes. The variety of transition models is huge, encompassing topics as disparate as
modeling the human endocrine system and modeling multiple vehicles driving on a freeway.
Sensor modeling is also a vast subfield in itself, but even subtle phenomena, such as sensor
drift, sudden decalibration, and the effects of exogenous conditions (such as weather) on
sensor readings, can be handled by explicit representation within dynamic Bayesian networks.

15.5.2 Exact inference in DBNs
Having sketched some ideas for representing complex processes as DBNs, we now turn to
the question of inference. In a sense, this question has already been answered: dynamic
Bayesian networks are Bayesian networks, and we already have algorithms for inference in
Bayesian networks. Given a sequence of observations, one can construct the full Bayesian
network representation of a DBN by replicating slices until the network is large enough to
accommodate the observations, as in Figure 15.16. This technique, mentioned in Chapter 14
in the context of relational probability models, is called unrolling. (Technically, the DBN is
equivalent to the semi-infinite network obtained by unrolling forever. Slices added beyond
the last observation have no effect on inferences within the observation period and can be
omitted.) Once the DBN is unrolled, one can use any of the inference algorithmsâ€”variable
elimination, clustering methods, and so onâ€”described in Chapter 14.
Unfortunately, a naive application of unrolling would not be particularly efficient. If
we want to perform filtering or smoothing with a long sequence of observations e1:t , the

596

Chapter 15.

Probabilistic Reasoning over Time

unrolled network would require O(t) space and would thus grow without bound as more
observations were added. Moreover, if we simply run the inference algorithm anew each
time an observation is added, the inference time per update will also increase as O(t).
Looking back to Section 15.2.1, we see that constant time and space per filtering update
can be achieved if the computation can be done recursively. Essentially, the filtering update
in Equation (15.5) works by summing out the state variables of the previous time step to get
the distribution for the new time step. Summing out variables is exactly what the variable
elimination (Figure 14.11) algorithm does, and it turns out that running variable elimination
with the variables in temporal order exactly mimics the operation of the recursive filtering
update in Equation (15.5). The modified algorithm keeps at most two slices in memory at
any one time: starting with slice 0, we add slice 1, then sum out slice 0, then add slice 2, then
sum out slice 1, and so on. In this way, we can achieve constant space and time per filtering
update. (The same performance can be achieved by suitable modifications to the clustering
algorithm.) Exercise 15.17 asks you to verify this fact for the umbrella network.
So much for the good news; now for the bad news: It turns out that the â€œconstantâ€ for
the per-update time and space complexity is, in almost all cases, exponential in the number of
state variables. What happens is that, as the variable elimination proceeds, the factors grow
to include all the state variables (or, more precisely, all those state variables that have parents
in the previous time slice). The maximum factor size is O(dn+k ) and the total update cost per
step is O(ndn+k ), where d is the domain size of the variables and k is the maximum number
of parents of any state variable.
Of course, this is much less than the cost of HMM updating, which is O(d2n ), but it
is still infeasible for large numbers of variables. This grim fact is somewhat hard to accept.
What it means is that even though we can use DBNs to represent very complex temporal
processes with many sparsely connected variables, we cannot reason efficiently and exactly
about those processes. The DBN model itself, which represents the prior joint distribution
over all the variables, is factorable into its constituent CPTs, but the posterior joint distribution conditioned on an observation sequenceâ€”that is, the forward messageâ€”is generally not
factorable. So far, no one has found a way around this problem, despite the fact that many
important areas of science and engineering would benefit enormously from its solution. Thus,
we must fall back on approximate methods.

15.5.3 Approximate inference in DBNs
Section 14.5 described two approximation algorithms: likelihood weighting (Figure 14.15)
and Markov chain Monte Carlo (MCMC, Figure 14.16). Of the two, the former is most easily
adapted to the DBN context. (An MCMC filtering algorithm is described briefly in the notes
at the end of the chapter.) We will see, however, that several improvements are required over
the standard likelihood weighting algorithm before a practical method emerges.
Recall that likelihood weighting works by sampling the nonevidence nodes of the network in topological order, weighting each sample by the likelihood it accords to the observed
evidence variables. As with the exact algorithms, we could apply likelihood weighting directly to an unrolled DBN, but this would suffer from the same problems of increasing time

Section 15.5.

PARTICLE FILTERING

Dynamic Bayesian Networks

597

and space requirements per update as the observation sequence grows. The problem is that
the standard algorithm runs each sample in turn, all the way through the network. Instead,
we can simply run all N samples together through the DBN, one slice at a time. The modified algorithm fits the general pattern of filtering algorithms, with the set of N samples as
the forward message. The first key innovation, then, is to use the samples themselves as an
approximate representation of the current state distribution. This meets the requirement of a
â€œconstantâ€ time per update, although the constant depends on the number of samples required
to maintain an accurate approximation. There is also no need to unroll the DBN, because we
need to have in memory only the current slice and the next slice.
In our discussion of likelihood weighting in Chapter 14, we pointed out that the algorithmâ€™s accuracy suffers if the evidence variables are â€œdownstreamâ€ from the variables
being sampled, because in that case the samples are generated without any influence from
the evidence. Looking at the typical structure of a DBNâ€”say, the umbrella DBN in Figure 15.16â€”we see that indeed the early state variables will be sampled without the benefit of
the later evidence. In fact, looking more carefully, we see that none of the state variables has
any evidence variables among its ancestors! Hence, although the weight of each sample will
depend on the evidence, the actual set of samples generated will be completely independent
of the evidence. For example, even if the boss brings in the umbrella every day, the sampling process could still hallucinate endless days of sunshine. What this means in practice is
that the fraction of samples that remain reasonably close to the actual series of events (and
therefore have nonnegligible weights) drops exponentially with t, the length of the observation sequence. In other words, to maintain a given level of accuracy, we need to increase the
number of samples exponentially with t. Given that a filtering algorithm that works in real
time can use only a fixed number of samples, what happens in practice is that the error blows
up after a very small number of update steps.
Clearly, we need a better solution. The second key innovation is to focus the set of
samples on the high-probability regions of the state space. This can be done by throwing
away samples that have very low weight, according to the observations, while replicating
those that have high weight. In that way, the population of samples will stay reasonably close
to reality. If we think of samples as a resource for modeling the posterior distribution, then it
makes sense to use more samples in regions of the state space where the posterior is higher.
A family of algorithms called particle filtering is designed to do just that. Particle
filtering works as follows: First, a population of N initial-state samples is created by sampling
from the prior distribution P(X0 ). Then the update cycle is repeated for each time step:
1. Each sample is propagated forward by sampling the next state value xt+1 given the
current value xt for the sample, based on the transition model P(Xt+1 | xt ).
2. Each sample is weighted by the likelihood it assigns to the new evidence, P (et+1 | xt+1 ).
3. The population is resampled to generate a new population of N samples. Each new
sample is selected from the current population; the probability that a particular sample
is selected is proportional to its weight. The new samples are unweighted.
The algorithm is shown in detail in Figure 15.17, and its operation for the umbrella DBN is
illustrated in Figure 15.18.

598

Chapter 15.

Probabilistic Reasoning over Time

function PARTICLE -F ILTERING(e, N , dbn) returns a set of samples for the next time step
inputs: e, the new incoming evidence
N , the number of samples to be maintained
dbn, a DBN with prior P(X0 ), transition model P(X1 |X0 ), sensor model P(E1 |X1 )
persistent: S , a vector of samples of size N , initially generated from P(X0 )
local variables: W , a vector of weights of size N
for i = 1 to N do
S [i] â† sample from P(X1 | X0 = S [i]) /* step 1 */
/* step 2 */
W [i] â† P(e | X1 = S[i])
S â† W EIGHTED -S AMPLE -W ITH -R EPLACEMENT(N , S , W )
return S

/* step 3 */

Figure 15.17 The particle filtering algorithm implemented as a recursive update operation with state (the set of samples). Each of the sampling operations involves sampling the relevant slice variables in topological order, much as in P RIOR -S AMPLE . The
W EIGHTED -S AMPLE -W ITH -R EPLACEMENT operation can be implemented to run in O(N )
expected time. The step numbers refer to the description in the text.

Rain t

Rain t+1

Rain t+1

Rain t+1

true
false
(a) Propagate

(b) Weight

(c) Resample

Figure 15.18 The particle filtering update cycle for the umbrella DBN with N = 10, showing the sample populations of each state. (a) At time t, 8 samples indicate rain and 2 indicate
Â¬rain. Each is propagated forward by sampling the next state through the transition model.
At time t + 1, 6 samples indicate rain and 4 indicate Â¬rain. (b) Â¬umbrella is observed at
t + 1. Each sample is weighted by its likelihood for the observation, as indicated by the size
of the circles. (c) A new set of 10 samples is generated by weighted random selection from
the current set, resulting in 2 samples that indicate rain and 8 that indicate Â¬rain.

We can show that this algorithm is consistentâ€”gives the correct probabilities as N tends
to infinityâ€”by considering what happens during one update cycle. We assume that the sample
population starts with a correct representation of the forward message f1:t = P(Xt | e1:t ) at
time t. Writing N (xt | e1:t ) for the number of samples occupying state xt after observations
e1:t have been processed, we therefore have
N (xt | e1:t )/N = P (xt | e1:t )

(15.23)

for large N . Now we propagate each sample forward by sampling the state variables at t + 1,
given the values for the sample at t. The number of samples reaching state xt+1 from each

Section 15.6.

Keeping Track of Many Objects

599

xt is the transition probability times the population of xt ; hence, the total number of samples
reaching xt+1 is

P (xt+1 | xt )N (xt | e1:t ) .
N (xt+1 | e1:t ) =
xt

Now we weight each sample by its likelihood for the evidence at t + 1. A sample in state xt+1
receives weight P (et+1 | xt+1 ). The total weight of the samples in xt+1 after seeing et+1 is
therefore
W (xt+1 | e1:t+1 ) = P (et+1 | xt+1 )N (xt+1 | e1:t ) .
Now for the resampling step. Since each sample is replicated with probability proportional
to its weight, the number of samples in state xt+1 after resampling is proportional to the total
weight in xt+1 before resampling:
N (xt+1 | e1:t+1 )/N = Î± W (xt+1 | e1:t+1 )
= Î± P (et+1 | xt+1 )N (xt+1 | e1:t )

P (xt+1 | xt )N (xt | e1:t )
= Î± P (et+1 | xt+1 )
xt

= Î± N P (et+1 | xt+1 )



P (xt+1 | xt )P (xt | e1:t ) (by 15.23)

xt

= Î± P (et+1 | xt+1 )



P (xt+1 | xt )P (xt | e1:t )

xt

= P (xt+1 | e1:t+1 ) (by 15.5).
Therefore the sample population after one update cycle correctly represents the forward message at time t + 1.
Particle filtering is consistent, therefore, but is it efficient? In practice, it seems that the
answer is yes: particle filtering seems to maintain a good approximation to the true posterior
using a constant number of samples. Under certain assumptionsâ€”in particular, that the probabilities in the transition and sensor models are strictly greater than 0 and less than 1â€”it is
possible to prove that the approximation maintains bounded error with high probability. On
the practical side, the range of applications has grown to include many fields of science and
engineering; some references are given at the end of the chapter.

15.6

K EEPING T RACK OF M ANY O BJECTS

DATA ASSOCIATION

The preceding sections have consideredâ€”without mentioning itâ€”state estimation problems
involving a single object. In this section, we see what happens when two or more objects
generate the observations. What makes this case different from plain old state estimation is
that there is now the possibility of uncertainty about which object generated which observation. This is the identity uncertainty problem of Section 14.6.3 (page 544), now viewed in a
temporal context. In the control theory literature, this is the data association problemâ€”that
is, the problem of associating observation data with the objects that generated them.

600

Chapter 15.

1

5
2
3
2
3

1

1

5
2

4

4

3
2

4

4

3

1
5

(a)

Probabilistic Reasoning over Time

5

(b)
track termination

1

5
2
3
2

1

3

5
2

4

4
detection
failure

3
2

4

false alarm

(d)

4

3

1
5

(c)

1

track
initiation

5

Figure 15.19 (a) Observations made of object locations in 2D space over five time steps.
Each observation is labeled with the time step but does not identify the object that produced
it. (bâ€“c) Possible hypotheses about the underlying object tracks. (d) A hypothesis for the
case in which false alarms, detection failures, and track initiation/termination are possible.

The data association problem was studied originally in the context of radar tracking,
where reflected pulses are detected at fixed time intervals by a rotating radar antenna. At each
time step, multiple blips may appear on the screen, but there is no direct observation of which
blips at time t belong to which blips at time t âˆ’ 1. Figure 15.19(a) shows a simple example
with two blips per time step for five steps. Let the two blip locations at time t be e1t and e2t .
(The labeling of blips within a time step as â€œ1â€ and â€œ2â€ is completely arbitrary and carries no
information.) Let us assume, for the time being, that exactly two aircraft, A and B, generated
the blips; their true positions are XtA and XtB . Just to keep things simple, weâ€™ll also assume
that the each aircraft moves independently according to a known transition modelâ€”e.g., a
linear Gaussian model as used in the Kalman filter (Section 15.4).
Suppose we try to write down the overall probability model for this scenario, just as
we did for general temporal processes in Equation (15.3) on page 569. As usual, the joint
distribution factors into contributions for each time step as follows:
B
1
2
P (xA
0:t , x0:t , e1:t , e1:t ) =
t

B
A
B
B
1 2
A B
)P
(x
)
P (xA
P (xA
0
0
i | xiâˆ’1 )P (xi | xiâˆ’1 ) P (ei , ei | xi , xi ) .

(15.24)

i=1
B
We would like to factor the observation term P (e1i , e2i | xA
i , xi ) into a product of two terms,
one for each object, but this would require knowing which observation was generated by
which object. Instead, we have to sum over all possible ways of associating the observations

Section 15.6.

Keeping Track of Many Objects

601

with the objects. Some of those ways are shown in Figure 15.19(bâ€“c); in general, for n
objects and T time steps, there are (n!)T ways of doing itâ€”an awfully large number.
Mathematically speaking, the â€œway of associating the observations with the objectsâ€
is a collection of unobserved random variable that identify the source of each observation.
Weâ€™ll write Ï‰t to denote the one-to-one mapping from objects to observations at time t, with
Ï‰t (A) and Ï‰t (B) denoting the specific observations (1 or 2) that Ï‰t assigns to A and B.
(For n objects, Ï‰t will have n! possible values; here, n! = 2.) Because the labels â€œ1â€ ad
â€œ2â€ on the observations are assigned arbitrarily, the prior on Ï‰t is uniform and Ï‰t is indeB
pendent of the states of the objects, xA
t and xt ). So we can condition the observation term
1
2
A
B
P (ei , ei | xi , xi ) on Ï‰t and then simplify:

B
B
A B
P (e1i , e2i | xA
P (e1i , e2i | xA
i , xi ) =
i , xi , Ï‰i )P (Ï‰i | xi , xi )
Ï‰i

=



Ï‰ (A)

P (ei i

Ï‰ (B)

i
| xA
i )P (ei

A B
| xB
i )P (Ï‰i | xi , xi )

Ï‰i

1
Ï‰ (A)
Ï‰i (B)
P (ei i | xA
| xB
=
i )P (ei
i ).
2 Ï‰
i

NEAREST-NEIGHBOR
FILTER

HUNGARIAN
ALGORITHM

Plugging this into Equation (15.24), we get an expression that is only in terms of transition
and sensor models for individual objects and observations.
As for all probability models, inference means summing out the variables other than
the query and the evidence. For filtering in HMMs and DBNs, we were able to sum out the
state variables from 1 to t âˆ’ 1 by a simple dynamic programming trick; for Kalman filters, we
took advantage of special properties of Gaussians. For data association, we are less fortunate.
There is no (known) efficient exact algorithm, for the same reason that there is none for the
1
2
switching Kalman filter (page 589): the filtering distribution P (xA
t | e1:t , e1:t ) for object A
ends up as a mixture of exponentially many distributions, one for each way of picking a
sequence of observations to assign to A.
As a result of the complexity of exact inference, many different approximate methods
have been used. The simplest approach is to choose a single â€œbestâ€ assignment at each time
step, given the predicted positions of the objects at the current time step. This assignment
associates observations with objects and enables the track of each object to be updated and
a prediction made for the next time step. For choosing the â€œbestâ€ assignment, it is common
to use the so-called nearest-neighbor filter, which repeatedly chooses the closest pairing
of predicted position and observation and adds that pairing to the assignment. The nearestneighbor filter works well when the objects are well separated in state space and the prediction
uncertainty and observation error are smallâ€”in other words, when there is no possibility of
confusion. When there is more uncertainty as to the correct assignment, a better approach
is to choose the assignment that maximizes the joint probability of the current observations
given the predicted positions. This can be done very efficiently using the Hungarian algorithm (Kuhn, 1955), even though there are n! assignments to choose from.
Any method that commits to a single best assignment at each time step fails miserably
under more difficult conditions. In particular, if the algorithm commits to an incorrect assignment, the prediction at the next time step may be significantly wrong, leading to more

602

Chapter 15.

(a)

Probabilistic Reasoning over Time

(b)

Figure 15.20 Images from (a) upstream and (b) downstream surveillance cameras roughly
two miles apart on Highway 99 in Sacramento, California. The boxed vehicle has been
identified at both cameras.

FALSE ALARM
CLUTTER
DETECTION FAILURE

incorrect assignments, and so on. Two modern approaches turn out to be much more effective. A particle filtering algorithm (see page 598) for data association works by maintaining
a large collection of possible current assignments. An MCMC algorithm explores the space
of assignment historiesâ€”for example, Figure 15.19(bâ€“c) might be states in the MCMC state
spaceâ€”and can change its mind about previous assignment decisions. Current MCMC data
association methods can handle many hundreds of objects in real time while giving a good
approximation to the true posterior distributions.
The scenario described so far involved n known objects generating n observations at
each time step. Real application of data association are typically much more complicated.
Often, the reported observations include false alarms (also known as clutter), which are not
caused by real objects. Detection failures can occur, meaning that no observation is reported
for a real object. Finally, new objects arrive and old ones disappear. These phenomena, which
create even more possible worlds to worry about, are illustrated in Figure 15.19(d).
Figure 15.20 shows two images from widely separated cameras on a California freeway.
In this application, we are interested in two goals: estimating the time it takes, under current
traffic conditions, to go from one place to another in the freeway system; and measuring
demand, i.e., how many vehicles travel between any two points in the system at particular
times of the day and on particular days of the week. Both goals require solving the data
association problem over a wide area with many cameras and tens of thousands of vehicles
per hour. With visual surveillance, false alarms are caused by moving shadows, articulated
vehicles, reflections in puddles, etc.; detection failures are caused by occlusion, fog, darkness,
and lack of visual contrast; and vehicles are constantly entering and leaving the freeway
system. Furthermore, the appearance of any given vehicle can change dramatically between
cameras depending on lighting conditions and vehicle pose in the image, and the transition
model changes as traffic jams come and go. Despite these problems, modern data association
algorithms have been successful in estimating traffic parameters in real-world settings.

Section 15.7.

Summary

603

Data association is an essential foundation for keeping track of a complex world, because without it there is no way to combine multiple observations of any given object. When
objects in the world interact with each other in complex activities, understanding the world
requires combining data association with the relational and open-universe probability models
of Section 14.6.3. This is currently an active area of research.

15.7

S UMMARY
This chapter has addressed the general problem of representing and reasoning about probabilistic temporal processes. The main points are as follows:
â€¢ The changing state of the world is handled by using a set of random variables to represent the state at each point in time.
â€¢ Representations can be designed to satisfy the Markov property, so that the future
is independent of the past given the present. Combined with the assumption that the
process is stationaryâ€”that is, the dynamics do not change over timeâ€”this greatly
simplifies the representation.
â€¢ A temporal probability model can be thought of as containing a transition model describing the state evolution and a sensor model describing the observation process.
â€¢ The principal inference tasks in temporal models are filtering, prediction, smoothing, and computing the most likely explanation. Each of these can be achieved using
simple, recursive algorithms whose run time is linear in the length of the sequence.
â€¢ Three families of temporal models were studied in more depth: hidden Markov models, Kalman filters, and dynamic Bayesian networks (which include the other two as
special cases).
â€¢ Unless special assumptions are made, as in Kalman filters, exact inference with many
state variables is intractable. In practice, the particle filtering algorithm seems to be an
effective approximation algorithm.
â€¢ When trying to keep track of many objects, uncertainty arises as to which observations
belong to which objectsâ€”the data association problem. The number of association
hypotheses is typically intractably large, but MCMC and particle filtering algorithms
for data association work well in practice.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Many of the basic ideas for estimating the state of dynamical systems came from the mathematician C. F. Gauss (1809), who formulated a deterministic least-squares algorithm for the
problem of estimating orbits from astronomical observations. A. A. Markov (1913) developed what was later called the Markov assumption in his analysis of stochastic processes;

604

Chapter 15.

Probabilistic Reasoning over Time

he estimated a first-order Markov chain on letters from the text of Eugene Onegin. The general theory of Markov chains and their mixing times is covered by Levin et al. (2008).
Significant classified work on filtering was done during World War II by Wiener (1942)
for continuous-time processes and by Kolmogorov (1941) for discrete-time processes. Although this work led to important technological developments over the next 20 years, its
use of a frequency-domain representation made many calculations quite cumbersome. Direct state-space modeling of the stochastic process turned out to be simpler, as shown by
Peter Swerling (1959) and Rudolf Kalman (1960). The latter paper described what is now
known as the Kalman filter for forward inference in linear systems with Gaussian noise;
Kalmanâ€™s results had, however, been obtained previously by the Danish statistician Thorvold
Thiele (1880) and by the Russian mathematician Ruslan Stratonovich (1959), whom Kalman
met in Moscow in 1960. After a visit to NASA Ames Research Center in 1960, Kalman
saw the applicability of the method to the tracking of rocket trajectories, and the filter was
later implemented for the Apollo missions. Important results on smoothing were derived by
Rauch et al. (1965), and the impressively named Rauchâ€“Tungâ€“Striebel smoother is still a
standard technique today. Many early results are gathered in Gelb (1974). Bar-Shalom and
Fortmann (1988) give a more modern treatment with a Bayesian flavor, as well as many references to the vast literature on the subject. Chatfield (1989) and Box et al. (1994) cover the
control theory approach to time series analysis.
The hidden Markov model and associated algorithms for inference and learning, including the forwardâ€“backward algorithm, were developed by Baum and Petrie (1966). The
Viterbi algorithm first appeared in (Viterbi, 1967). Similar ideas also appeared independently
in the Kalman filtering community (Rauch et al., 1965). The forwardâ€“backward algorithm
was one of the main precursors of the general formulation of the EM algorithm (Dempster
et al., 1977); see also Chapter 20. Constant-space smoothing appears in Binder et al. (1997b),
as does the divide-and-conquer algorithm developed in Exercise 15.3. Constant-time fixedlag smoothing for HMMs first appeared in Russell and Norvig (2003). HMMs have found
many applications in language processing (Charniak, 1993), speech recognition (Rabiner and
Juang, 1993), machine translation (Och and Ney, 2003), computational biology (Krogh et al.,
1994; Baldi et al., 1994), financial economics Bhar and Hamori (2004) and other fields. There
have been several extensions to the basic HMM model, for example the Hierarchical HMM
(Fine et al., 1998) and Layered HMM (Oliver et al., 2004) introduce structure back into the
model, replacing the single state variable of HMMs.
Dynamic Bayesian networks (DBNs) can be viewed as a sparse encoding of a Markov
process and were first used in AI by Dean and Kanazawa (1989b), Nicholson and Brady
(1992), and Kjaerulff (1992). The last work extends the H UGIN Bayes net system to accommodate dynamic Bayesian networks. The book by Dean and Wellman (1991) helped
popularize DBNs and the probabilistic approach to planning and control within AI. Murphy
(2002) provides a thorough analysis of DBNs.
Dynamic Bayesian networks have become popular for modeling a variety of complex motion processes in computer vision (Huang et al., 1994; Intille and Bobick, 1999).
Like HMMs, they have found applications in speech recognition (Zweig and Russell, 1998;
Richardson et al., 2000; Stephenson et al., 2000; Nefian et al., 2002; Livescu et al., 2003), ge-

Bibliographical and Historical Notes

EVIDENCE
REVERSAL

RAOBLACKWELLIZED
PARTICLE FILTER

SEQUENTIAL MONTE
CARLO

DECAYED MCMC

ASSUMED-DENSITY
FILTER

FACTORED
FRONTIER

FACTORIAL HMM

605

nomics (Murphy and Mian, 1999; Perrin et al., 2003; Husmeier, 2003) and robot localization
(Theocharous et al., 2004). The link between HMMs and DBNs, and between the forwardâ€“
backward algorithm and Bayesian network propagation, was made explicitly by Smyth et
al. (1997). A further unification with Kalman filters (and other statistical models) appears in
Roweis and Ghahramani (1999). Procedures exist for learning the parameters (Binder et al.,
1997a; Ghahramani, 1998) and structures (Friedman et al., 1998) of DBNs.
The particle filtering algorithm described in Section 15.5 has a particularly interesting
history. The first sampling algorithms for particle filtering (also called sequential Monte Carlo
methods) were developed in the control theory community by Handschin and Mayne (1969),
and the resampling idea that is the core of particle filtering appeared in a Russian control
journal (Zaritskii et al., 1975). It was later reinvented in statistics as sequential importancesampling resampling, or SIR (Rubin, 1988; Liu and Chen, 1998), in control theory as particle filtering (Gordon et al., 1993; Gordon, 1994), in AI as survival of the fittest (Kanazawa
et al., 1995), and in computer vision as condensation (Isard and Blake, 1996). The paper by
Kanazawa et al. (1995) includes an improvement called evidence reversal whereby the state
at time t + 1 is sampled conditional on both the state at time t and the evidence at time t + 1.
This allows the evidence to influence sample generation directly and was proved by Doucet
(1997) and Liu and Chen (1998) to reduce the approximation error. Particle filtering has been
applied in many areas, including tracking complex motion patterns in video (Isard and Blake,
1996), predicting the stock market (de Freitas et al., 2000), and diagnosing faults on planetary rovers (Verma et al., 2004). A variant called the Rao-Blackwellized particle filter or
RBPF (Doucet et al., 2000; Murphy and Russell, 2001) applies particle filtering to a subset
of state variables and, for each particle, performs exact inference on the remaining variables
conditioned on the value sequence in the particle. In some cases RBPF works well with thousands of state variables. An application of RBPF to localization and mapping in robotics is
described in Chapter 25. The book by Doucet et al. (2001) collects many important papers on
sequential Monte Carlo (SMC) algorithms, of which particle filtering is the most important
instance. Pierre Del Moral and colleagues have performed extensive theoretical analyses of
SMC algorithms (Del Moral, 2004; Del Moral et al., 2006).
MCMC methods (see Section 14.5.2) can be applied to the filtering problem; for example, Gibbs sampling can be applied directly to an unrolled DBN. To avoid the problem of
increasing update times as the unrolled network grows, the decayed MCMC filter (Marthi
et al., 2002) prefers to sample more recent state variables, with a probability that decays as
1/k2 for a variable k steps into the past. Decayed MCMC is a provably nondivergent filter.
Nondivergence theorems can also be obtained for certain types of assumed-density filter.
An assumed-density filter assumes that the posterior distribution over states at time t belongs
to a particular finitely parameterized family; if the projection and update steps take it outside
this family, the distribution is projected back to give the best approximation within the family. For DBNs, the Boyenâ€“Koller algorithm (Boyen et al., 1999) and the factored frontier
algorithm (Murphy and Weiss, 2001) assume that the posterior distribution can be approximated well by a product of small factors. Variational techniques (see Chapter 14) have also
been developed for temporal models. Ghahramani and Jordan (1997) discuss an approximation algorithm for the factorial HMM, a DBN in which two or more independently evolving

606

Chapter 15.

Probabilistic Reasoning over Time

Markov chains are linked by a shared observation stream. Jordan et al. (1998) cover a number
of other applications.
Data association for multitarget tracking was first described in a probabilistic setting
by Sittler (1964). The first practical algorithm for large-scale problems was the â€œmultiple
hypothesis trackerâ€ or MHT algorithm (Reid, 1979). Many important papers are collected by
Bar-Shalom and Fortmann (1988) and Bar-Shalom (1992). The development of an MCMC
algorithm for data association is due to Pasula et al. (1999), who applied it to traffic surveillance problems. Oh et al. (2009) provide a formal analysis and extensive experimental comparisons to other methods. Schulz et al. (2003) describe a data association method based on
particle filtering. Ingemar Cox analyzed the complexity of data association (Cox, 1993; Cox
and Hingorani, 1994) and brought the topic to the attention of the vision community. He also
noted the applicability of the polynomial-time Hungarian algorithm to the problem of finding most-likely assignments, which had long been considered an intractable problem in the
tracking community. The algorithm itself was published by Kuhn (1955), based on translations of papers published in 1931 by two Hungarian mathematicians, DeÌnes KoÌˆnig and JenoÌˆ
EgervaÌry. The basic theorem had been derived previously, however, in an unpublished Latin
manuscript by the famous Prussian mathematician Carl Gustav Jacobi (1804â€“1851).

E XERCISES
15.1 Show that any second-order Markov process can be rewritten as a first-order Markov
process with an augmented set of state variables. Can this always be done parsimoniously,
i.e., without increasing the number of parameters needed to specify the transition model?
15.2 In this exercise, we examine what happens to the probabilities in the umbrella world
in the limit of long time sequences.
a. Suppose we observe an unending sequence of days on which the umbrella appears.
Show that, as the days go by, the probability of rain on the current day increases monotonically toward a fixed point. Calculate this fixed point.
b. Now consider forecasting further and further into the future, given just the first two
umbrella observations. First, compute the probability P (r2+k |u1 , u2 ) for k = 1 . . . 20
and plot the results. You should see that the probability converges towards a fixed point.
Prove that the exact value of this fixed point is 0.5.
15.3 This exercise develops a space-efficient variant of the forwardâ€“backward algorithm
described in Figure 15.4 (page 576). We wish to compute P(Xk |e1:t ) for k = 1, . . . , t. This
will be done with a divide-and-conquer approach.
a. Suppose, for simplicity, that t is odd, and let the halfway point be h = (t + 1)/2. Show
that P(Xk |e1:t ) can be computed for k = 1, . . . , h given just the initial forward message
f1:0 , the backward message bh+1:t , and the evidence e1:h .
b. Show a similar result for the second half of the sequence.

Exercises

607
c. Given the results of (a) and (b), a recursive divide-and-conquer algorithm can be constructed by first running forward along the sequence and then backward from the end,
storing just the required messages at the middle and the ends. Then the algorithm is
called on each half. Write out the algorithm in detail.
d. Compute the time and space complexity of the algorithm as a function of t, the length of
the sequence. How does this change if we divide the input into more than two pieces?
15.4 On page 577, we outlined a flawed procedure for finding the most likely state sequence,
given an observation sequence. The procedure involves finding the most likely state at each
time step, using smoothing, and returning the sequence composed of these states. Show that,
for some temporal probability models and observation sequences, this procedure returns an
impossible state sequence (i.e., the posterior probability of the sequence is zero).
15.5 Equation (15.12) describes the filtering process for the matrix formulation of HMMs.
Give a similar equation for the calculation of likelihoods, which was described generically in
Equation (15.7).
15.6 Consider the vacuum worlds of Figure 4.18 (perfect sensing) and Figure 15.7 (noisy
sensing). Suppose that the robot receives an observation sequence such that, with perfect
sensing, there is exactly one possible location it could be in. Is this location necessarily the
most probable location under noisy sensing for sufficiently small noise probability ? Prove
your claim or find a counterexample.
15.7 In Section 15.3.2, the prior distribution over locations is uniform and the transition
model assumes an equal probability of moving to any neighboring square. What if those
assumptions are wrong? Suppose that the initial location is actually chosen uniformly from
the northwest quadrant of the room and the Move action actually tends to move southeast.
Keeping the HMM model fixed, explore the effect on localization and path accuracy as the
southeasterly tendency increases, for different values of .
15.8 Consider a version of the vacuum robot (page 582) that has the policy of going straight
for as long as it can; only when it encounters an obstacle does it change to a new (randomly
selected) heading. To model this robot, each state in the model consists of a (location, heading) pair. Implement this model and see how well the Viterbi algorithm can track a robot with
this model. The robotâ€™s policy is more constrained than the random-walk robot; does that
mean that predictions of the most likely path are more accurate?
15.9 This exercise is concerned with filtering in an environment with no landmarks. Consider a vacuum robot in an empty room, represented by an nÃ—m rectangular grid. The robotâ€™s
location is hidden; the only evidence available to the observer is a noisy location sensor that
gives an approximation to the robotâ€™s location. If the robot is at location (x, y) then with
probability .1 the sensor gives the correct location, with probability .05 each it reports one
of the 8 locations immediately surrounding (x, y), with probability .025 each it reports one
of the 16 locations that surround those 8, and with the remaining probability of .1 it reports
â€œno reading.â€ The robotâ€™s policy is to pick a direction and follow it with probability .8 on
each step; the robot switches to a randomly selected new heading with probability .2 (or with

608

Chapter 15.

Probabilistic Reasoning over Time

St

St+1

Xt

Xt+1

Zt

Zt+1

Figure 15.21 A Bayesian network representation of a switching Kalman filter. The
switching variable St is a discrete state variable whose value determines the transition
model for the continuous state variables Xt . For any discrete state i, the transition model
P(Xt+1 |Xt , St = i) is a linear Gaussian model, just as in a regular Kalman filter. The transition model for the discrete state, P(St+1 |St ), can be thought of as a matrix, as in a hidden
Markov model.

probability 1 if it encounters a wall). Implement this as an HMM and do filtering to track the
robot. How accurately can we track the robotâ€™s path?
15.10 Often, we wish to monitor a continuous-state system whose behavior switches unpredictably among a set of k distinct â€œmodes.â€ For example, an aircraft trying to evade a missile
can execute a series of distinct maneuvers that the missile may attempt to track. A Bayesian
network representation of such a switching Kalman filter model is shown in Figure 15.21.
a. Suppose that the discrete state St has k possible values and that the prior continuous
state estimate P(X0 ) is a multivariate Gaussian distribution. Show that the prediction
P(X1 ) is a mixture of Gaussiansâ€”that is, a weighted sum of Gaussians such that the
weights sum to 1.
b. Show that if the current continuous state estimate P(Xt |e1:t ) is a mixture of m Gaussians, then in the general case the updated state estimate P(Xt+1 |e1:t+1 ) will be a mixture of km Gaussians.
c. What aspect of the temporal process do the weights in the Gaussian mixture represent?
The results in (a) and (b) show that the representation of the posterior grows without limit even
for switching Kalman filters, which are among the simplest hybrid dynamic models.
15.11 Complete the missing step in the derivation of Equation (15.19) on page 586, the first
update step for the one-dimensional Kalman filter.
15.12

Let us examine the behavior of the variance update in Equation (15.20) (page 587).

a. Plot the value of Ïƒt2 as a function of t, given various values for Ïƒx2 and Ïƒz2 .
b. Show that the update has a fixed point Ïƒ 2 such that Ïƒt2 â†’ Ïƒ 2 as t â†’ âˆž, and calculate
the value of Ïƒ 2 .
c. Give a qualitative explanation for what happens as Ïƒx2 â†’ 0 and as Ïƒz2 â†’ 0.

Exercises

609
15.13 A professor wants to know if students are getting enough sleep. Each day, the professor observes whether the students sleep in class, and whether they have red eyes. The
professor has the following domain theory:
â€¢ The prior probability of getting enough sleep, with no observations, is 0.7.
â€¢ The probability of getting enough sleep on night t is 0.8 given that the student got
enough sleep the previous night, and 0.3 if not.
â€¢ The probability of having red eyes is 0.2 if the student got enough sleep, and 0.7 if not.
â€¢ The probability of sleeping in class is 0.1 if the student got enough sleep, and 0.3 if not.
Formulate this information as a dynamic Bayesian network that the professor could use to
filter or predict from a sequence of observations. Then reformulate it as a hidden Markov
model that has only a single observation variable. Give the complete probability tables for
the model.
15.14

For the DBN specified in Exercise 15.13 and for the evidence values
e1 = not red eyes, not sleeping in class
e2 = red eyes, not sleeping in class
e3 = red eyes, sleeping in class

perform the following computations:
a. State estimation: Compute P (EnoughSleep t |e1:t ) for each of t = 1, 2, 3.
b. Smoothing: Compute P (EnoughSleep t |e1:3 ) for each of t = 1, 2, 3.
c. Compare the filtered and smoothed probabilities for t = 1 and t = 2.
15.15 Suppose that a particular student shows up with red eyes and sleeps in class every day.
Given the model described in Exercise 15.13, explain why the probability that the student had
enough sleep the previous night converges to a fixed point rather than continuing to go down
as we gather more days of evidence. What is the fixed point? Answer this both numerically
(by computation) and analytically.
15.16 This exercise analyzes in more detail the persistent-failure model for the battery sensor in Figure 15.15(a) (page 594).
a. Figure 15.15(b) stops at t = 32. Describe qualitatively what should happen as t â†’ âˆž
if the sensor continues to read 0.
b. Suppose that the external temperature affects the battery sensor in such a way that transient failures become more likely as temperature increases. Show how to augment the
DBN structure in Figure 15.15(a), and explain any required changes to the CPTs.
c. Given the new network structure, can battery readings be used by the robot to infer the
current temperature?
15.17 Consider applying the variable elimination algorithm to the umbrella DBN unrolled
for three slices, where the query is P(R3 |u1 , u2 , u3 ). Show that the space complexity of the
algorithmâ€”the size of the largest factorâ€”is the same, regardless of whether the rain variables
are eliminated in forward or backward order.

16

MAKING SIMPLE
DECISIONS

In which we see how an agent should make decisions so that it gets what it wantsâ€”
on average, at least.

In this chapter, we fill in the details of how utility theory combines with probability theory to
yield a decision-theoretic agentâ€”an agent that can make rational decisions based on what it
believes and what it wants. Such an agent can make decisions in contexts in which uncertainty
and conflicting goals leave a logical agent with no way to decide: a goal-based agent has a
binary distinction between good (goal) and bad (non-goal) states, while a decision-theoretic
agent has a continuous measure of outcome quality.
Section 16.1 introduces the basic principle of decision theory: the maximization of
expected utility. Section 16.2 shows that the behavior of any rational agent can be captured
by supposing a utility function that is being maximized. Section 16.3 discusses the nature of
utility functions in more detail, and in particular their relation to individual quantities such as
money. Section 16.4 shows how to handle utility functions that depend on several quantities.
In Section 16.5, we describe the implementation of decision-making systems. In particular,
we introduce a formalism called a decision network (also known as an influence diagram)
that extends Bayesian networks by incorporating actions and utilities. The remainder of the
chapter discusses issues that arise in applications of decision theory to expert systems.

16.1

C OMBINING B ELIEFS AND D ESIRES UNDER U NCERTAINTY
Decision theory, in its simplest form, deals with choosing among actions based on the desirability of their immediate outcomes; that is, the environment is assumed to be episodic in the
sense defined on page 43. (This assumption is relaxed in Chapter 17.) In Chapter 3 we used
the notation R ESULT (s0 , a) for the state that is the deterministic outcome of taking action a
in state s0 . In this chapter we deal with nondeterministic partially observable environments.
Since the agent may not know the current state, we omit it and define R ESULT (a) as a random
variable whose values are the possible outcome states. The probability of outcome s , given
evidence observations e, is written
P (R ESULT (a) = s | a, e) ,
610

Section 16.2.

UTILITY FUNCTION
EXPECTED UTILITY

The Basis of Utility Theory

611

where the a on the right-hand side of the conditioning bar stands for the event that action a is
executed.1
The agentâ€™s preferences are captured by a utility function, U (s), which assigns a single
number to express the desirability of a state. The expected utility of an action given the evidence, EU (a|e), is just the average utility value of the outcomes, weighted by the probability
that the outcome occurs:

P (R ESULT (a) = s | a, e) U (s ) .
(16.1)
EU (a|e) =
s

MAXIMUM EXPECTED
UTILITY

The principle of maximum expected utility (MEU) says that a rational agent should choose
the action that maximizes the agentâ€™s expected utility:
action = argmax EU (a|e)
a

In a sense, the MEU principle could be seen as defining all of AI. All an intelligent agent has
to do is calculate the various quantities, maximize utility over its actions, and away it goes.
But this does not mean that the AI problem is solved by the definition!
The MEU principle formalizes the general notion that the agent should â€œdo the right
thing,â€ but goes only a small distance toward a full operationalization of that advice. Estimating the state of the world requires perception, learning, knowledge representation, and
inference. Computing P (R ESULT (a) | a, e) requires a complete causal model of the world
and, as we saw in Chapter 14, NP-hard inference in (very large) Bayesian networks. Computing the outcome utilities U (s ) often requires searching or planning, because an agent may
not know how good a state is until it knows where it can get to from that state. So, decision
theory is not a panacea that solves the AI problemâ€”but it does provide a useful framework.
The MEU principle has a clear relation to the idea of performance measures introduced
in Chapter 2. The basic idea is simple. Consider the environments that could lead to an
agent having a given percept history, and consider the different agents that we could design.
If an agent acts so as to maximize a utility function that correctly reflects the performance
measure, then the agent will achieve the highest possible performance score (averaged over
all the possible environments). This is the central justification for the MEU principle itself.
While the claim may seem tautological, it does in fact embody a very important transition
from a global, external criterion of rationalityâ€”the performance measure over environment
historiesâ€”to a local, internal criterion involving the maximization of a utility function applied
to the next state.

16.2

T HE BASIS OF U TILITY T HEORY
Intuitively, the principle of Maximum Expected Utility (MEU) seems like a reasonable way
to make decisions, but it is by no means obvious that it is the only rational way. After all,
why should maximizing the average utility be so special? Whatâ€™s wrong with an agent that
1

Classical decision theory leaves
P the current state S0 implicit, but we could make it explicit by writing
P (R ESULT(a) = s | a, e) = s P (R ESULT(s, a) = s | a)P (S0 = s | e).

612

Chapter

16.

Making Simple Decisions

maximizes the weighted sum of the cubes of the possible utilities, or tries to minimize the
worst possible loss? Could an agent act rationally just by expressing preferences between
states, without giving them numeric values? Finally, why should a utility function with the
required properties exist at all? We shall see.

16.2.1 Constraints on rational preferences
These questions can be answered by writing down some constraints on the preferences that a
rational agent should have and then showing that the MEU principle can be derived from the
constraints. We use the following notation to describe an agentâ€™s preferences:

LOTTERY

A'B

the agent prefers A over B.

Aâˆ¼B
A'
âˆ¼B

the agent is indifferent between A and B.
the agent prefers A over B or is indifferent between them.

Now the obvious question is, what sorts of things are A and B? They could be states of the
world, but more often than not there is uncertainty about what is really being offered. For
example, an airline passenger who is offered â€œthe pasta dish or the chickenâ€ does not know
what lurks beneath the tinfoil cover.2 The pasta could be delicious or congealed, the chicken
juicy or overcooked beyond recognition. We can think of the set of outcomes for each action
as a lotteryâ€”think of each action as a ticket. A lottery L with possible outcomes S1 , . . . , Sn
that occur with probabilities p1 , . . . , pn is written
L = [p1 , S1 ; p2 , S2 ; . . . pn , Sn ] .
In general, each outcome Si of a lottery can be either an atomic state or another lottery. The
primary issue for utility theory is to understand how preferences between complex lotteries
are related to preferences between the underlying states in those lotteries. To address this
issue we list six constraints that we require any reasonable preference relation to obey:
â€¢ Orderability: Given any two lotteries, a rational agent must either prefer one to the
other or else rate the two as equally preferable. That is, the agent cannot avoid deciding.
As we said on page 490, refusing to bet is like refusing to allow time to pass.

ORDERABILITY

Exactly one of (A ' B), (B ' A), or (A âˆ¼ B) holds.
â€¢ Transitivity: Given any three lotteries, if an agent prefers A to B and prefers B to C,
then the agent must prefer A to C.

TRANSITIVITY

(A ' B) âˆ§ (B ' C) â‡’ (A ' C) .
â€¢ Continuity: If some lottery B is between A and C in preference, then there is some
probability p for which the rational agent will be indifferent between getting B for sure
and the lottery that yields A with probability p and C with probability 1 âˆ’ p.

CONTINUITY

A ' B ' C â‡’ âˆƒ p [p, A; 1 âˆ’ p, C] âˆ¼ B .
â€¢ Substitutability: If an agent is indifferent between two lotteries A and B, then the
agent is indifferent between two more complex lotteries that are the same except that B

SUBSTITUTABILITY

2

We apologize to readers whose local airlines no longer offer food on long flights.

Section 16.2.

The Basis of Utility Theory

613

is substituted for A in one of them. This holds regardless of the probabilities and the
other outcome(s) in the lotteries.
A âˆ¼ B â‡’ [p, A; 1 âˆ’ p, C] âˆ¼ [p, B; 1 âˆ’ p, C] .
This also holds if we substitute ' for âˆ¼ in this axiom.
â€¢ Monotonicity: Suppose two lotteries have the same two possible outcomes, A and B.
If an agent prefers A to B, then the agent must prefer the lottery that has a higher
probability for A (and vice versa).

MONOTONICITY

A ' B â‡’ (p > q â‡” [p, A; 1 âˆ’ p, B] ' [q, A; 1 âˆ’ q, B]) .
â€¢ Decomposability: Compound lotteries can be reduced to simpler ones using the laws
of probability. This has been called the â€œno fun in gamblingâ€ rule because it says that
two consecutive lotteries can be compressed into a single equivalent lottery, as shown
in Figure 16.1(b).3

DECOMPOSABILITY

[p, A; 1 âˆ’ p, [q, B; 1 âˆ’ q, C]] âˆ¼ [p, A; (1 âˆ’ p)q, B; (1 âˆ’ p)(1 âˆ’ q), C] .
These constraints are known as the axioms of utility theory. Each axiom can be motivated
by showing that an agent that violates it will exhibit patently irrational behavior in some
situations. For example, we can motivate transitivity by making an agent with nontransitive
preferences give us all its money. Suppose that the agent has the nontransitive preferences
A ' B ' C ' A, where A, B, and C are goods that can be freely exchanged. If the agent
currently has A, then we could offer to trade C for A plus one cent. The agent prefers C,
and so would be willing to make this trade. We could then offer to trade B for C, extracting
another cent, and finally trade A for B. This brings us back where we started from, except
that the agent has given us three cents (Figure 16.1(a)). We can keep going around the cycle
until the agent has no money at all. Clearly, the agent has acted irrationally in this case.

16.2.2 Preferences lead to utility
Notice that the axioms of utility theory are really axioms about preferencesâ€”they say nothing
about a utility function. But in fact from the axioms of utility we can derive the following
consequences (for the proof, see von Neumann and Morgenstern, 1944):
â€¢ Existence of Utility Function: If an agentâ€™s preferences obey the axioms of utility, then
there exists a function U such that U (A) > U (B) if and only if A is preferred to B,
and U (A) = U (B) if and only if the agent is indifferent between A and B.
U (A) > U (B) â‡” A ' B
U (A) = U (B) â‡” A âˆ¼ B
â€¢ Expected Utility of a Lottery: The utility of a lottery is the sum of the probability of
each outcome times the utility of that outcome.

pi U (Si ) .
U ([p1 , S1 ; . . . ; pn , Sn ]) =
i
3

We can account for the enjoyment of gambling by encoding gambling events into the state description; for
example, â€œHave $10 and gambledâ€ could be preferred to â€œHave $10 and didnâ€™t gamble.â€

614

Chapter

16.

p

Making Simple Decisions
A

A
1Â¢

B

q

1Â¢

(1â€“p)
(1â€“q)
is equivalent to

B

p

C

(1â€“p)q

C
A
B

1Â¢
(1â€“p)(1â€“q)
(a)

C

(b)

Figure 16.1 (a) A cycle of exchanges showing that the nontransitive preferences A '
B ' C ' A result in irrational behavior. (b) The decomposability axiom.

In other words, once the probabilities and utilities of the possible outcome states are specified,
the utility of a compound lottery involving those states is completely determined. Because the
outcome of a nondeterministic action is a lottery, it follows that an agent can act rationallyâ€”
that is, consistently with its preferencesâ€”only by choosing an action that maximizes expected
utility according to Equation (16.1).
The preceding theorems establish that a utility function exists for any rational agent, but
they do not establish that it is unique. It is easy to see, in fact, that an agentâ€™s behavior would
not change if its utility function U (S) were transformed according to
U  (S) = aU (S) + b ,

VALUE FUNCTION
ORDINAL UTILITY
FUNCTION

(16.2)

where a and b are constants and a > 0; an affine transformation.4 This fact was noted in
Chapter 5 for two-player games of chance; here, we see that it is completely general.
As in game-playing, in a deterministic environment an agent just needs a preference
ranking on statesâ€”the numbers donâ€™t matter. This is called a value function or ordinal
utility function.
It is important to remember that the existence of a utility function that describes an
agentâ€™s preference behavior does not necessarily mean that the agent is explicitly maximizing
that utility function in its own deliberations. As we showed in Chapter 2, rational behavior can
be generated in any number of ways. By observing a rational agentâ€™s preferences, however,
an observer can construct the utility function that represents what the agent is actually trying
to achieve (even if the agent doesnâ€™t know it).
4

In this sense, utilities resemble temperatures: a temperature in Fahrenheit is 1.8 times the Celsius temperature
plus 32. You get the same results in either measurement system.

Section 16.3.

16.3

Utility Functions

615

U TILITY F UNCTIONS
Utility is a function that maps from lotteries to real numbers. We know there are some axioms
on utilities that all rational agents must obey. Is that all we can say about utility functions?
Strictly speaking, that is it: an agent can have any preferences it likes. For example, an agent
might prefer to have a prime number of dollars in its bank account; in which case, if it had $16
it would give away $3. This might be unusual, but we canâ€™t call it irrational. An agent might
prefer a dented 1973 Ford Pinto to a shiny new Mercedes. Preferences can also interact: for
example, the agent might prefer prime numbers of dollars only when it owns the Pinto, but
when it owns the Mercedes, it might prefer more dollars to fewer. Fortunately, the preferences
of real agents are usually more systematic, and thus easier to deal with.

16.3.1 Utility assessment and utility scales

PREFERENCE
ELICITATION

NORMALIZED
UTILITIES

STANDARD LOTTERY

If we want to build a decision-theoretic system that helps the agent make decisions or acts
on his or her behalf, we must first work out what the agentâ€™s utility function is. This process,
often called preference elicitation, involves presenting choices to the agent and using the
observed preferences to pin down the underlying utility function.
Equation (16.2) says that there is no absolute scale for utilities, but it is helpful, nonetheless, to establish some scale on which utilities can be recorded and compared for any particular problem. A scale can be established by fixing the utilities of any two particular outcomes,
just as we fix a temperature scale by fixing the freezing point and boiling point of water.
Typically, we fix the utility of a â€œbest possible prizeâ€ at U (S) = u and a â€œworst possible
catastropheâ€ at U (S) = uâŠ¥ . Normalized utilities use a scale with uâŠ¥ = 0 and u = 1.
Given a utility scale between u and uâŠ¥ , we can assess the utility of any particular
prize S by asking the agent to choose between S and a standard lottery [p, u ; (1 âˆ’ p), uâŠ¥ ].
The probability p is adjusted until the agent is indifferent between S and the standard lottery.
Assuming normalized utilities, the utility of S is given by p. Once this is done for each prize,
the utilities for all lotteries involving those prizes are determined.
In medical, transportation, and environmental decision problems, among others, peopleâ€™s lives are at stake. In such cases, uâŠ¥ is the value assigned to immediate death (or perhaps
many deaths). Although nobody feels comfortable with putting a value on human life, it is a
fact that tradeoffs are made all the time. Aircraft are given a complete overhaul at intervals
determined by trips and miles flown, rather than after every trip. Cars are manufactured in
a way that trades off costs against accident survival rates. Paradoxically, a refusal to â€œput a
monetary value on lifeâ€ means that life is often undervalued. Ross Shachter relates an experience with a government agency that commissioned a study on removing asbestos from
schools. The decision analysts performing the study assumed a particular dollar value for the
life of a school-age child, and argued that the rational choice under that assumption was to
remove the asbestos. The agency, morally outraged at the idea of setting the value of a life,
rejected the report out of hand. It then decided against asbestos removalâ€”implicitly asserting
a lower value for the life of a child than that assigned by the analysts.

616

MICROMORT

QALY

Chapter

16.

Making Simple Decisions

Some attempts have been made to find out the value that people place on their own
lives. One common â€œcurrencyâ€ used in medical and safety analysis is the micromort, a
one in a million chance of death. If you ask people how much they would pay to avoid a
riskâ€”for example, to avoid playing Russian roulette with a million-barreled revolverâ€”they
will respond with very large numbers, perhaps tens of thousands of dollars, but their actual
behavior reflects a much lower monetary value for a micromort. For example, driving in a car
for 230 miles incurs a risk of one micromort; over the life of your carâ€”say, 92,000 milesâ€”
thatâ€™s 400 micromorts. People appear to be willing to pay about $10,000 (at 2009 prices)
more for a safer car that halves the risk of death, or about $50 per micromort. A number
of studies have confirmed a figure in this range across many individuals and risk types. Of
course, this argument holds only for small risks. Most people wonâ€™t agree to kill themselves
for $50 million.
Another measure is the QALY, or quality-adjusted life year. Patients with a disability
are willing to accept a shorter life expectancy to be restored to full health. For example,
kidney patients on average are indifferent between living two years on a dialysis machine and
one year at full health.

16.3.2 The utility of money

MONOTONIC
PREFERENCE

EXPECTED
MONETARY VALUE

Utility theory has its roots in economics, and economics provides one obvious candidate
for a utility measure: money (or more specifically, an agentâ€™s total net assets). The almost
universal exchangeability of money for all kinds of goods and services suggests that money
plays a significant role in human utility functions.
It will usually be the case that an agent prefers more money to less, all other things being
equal. We say that the agent exhibits a monotonic preference for more money. This does
not mean that money behaves as a utility function, because it says nothing about preferences
between lotteries involving money.
Suppose you have triumphed over the other competitors in a television game show. The
host now offers you a choice: either you can take the $1,000,000 prize or you can gamble it
on the flip of a coin. If the coin comes up heads, you end up with nothing, but if it comes
up tails, you get $2,500,000. If youâ€™re like most people, you would decline the gamble and
pocket the million. Are you being irrational?
Assuming the coin is fair, the expected monetary value (EMV) of the gamble is 21 ($0)
+ 12 ($2,500,000) = $1,250,000, which is more than the original $1,000,000. But that does
not necessarily mean that accepting the gamble is a better decision. Suppose we use Sn to
denote the state of possessing total wealth $n, and that your current wealth is $k. Then the
expected utilities of the two actions of accepting and declining the gamble are
EU (Accept) =

1
1
2 U (Sk ) + 2 U (Sk+2,500,000 )

,

EU (Decline) = U (Sk+1,000,000 ) .
To determine what to do, we need to assign utilities to the outcome states. Utility is not
directly proportional to monetary value, because the utility for your first million is very high
(or so they say), whereas the utility for an additional million is smaller. Suppose you assign
a utility of 5 to your current financial status (Sk ), a 9 to the state Sk+2,500,000 , and an 8 to the

Section 16.3.

Utility Functions

617

U

U

o
o

o o

o

o

o

o

o
o

-150,000

$

$

o
o

800,000

o
o

o

(a)

(b)

Figure 16.2 The utility of money. (a) Empirical data for Mr. Beard over a limited range.
(b) A typical curve for the full range.

state Sk+1,000,000 . Then the rational action would be to decline, because the expected utility
of accepting is only 7 (less than the 8 for declining). On the other hand, a billionaire would
most likely have a utility function that is locally linear over the range of a few million more,
and thus would accept the gamble.
In a pioneering study of actual utility functions, Grayson (1960) found that the utility of
money was almost exactly proportional to the logarithm of the amount. (This idea was first
suggested by Bernoulli (1738); see Exercise 16.3.) One particular utility curve, for a certain
Mr. Beard, is shown in Figure 16.2(a). The data obtained for Mr. Beardâ€™s preferences are
consistent with a utility function
U (Sk+n ) = âˆ’263.31 + 22.09 log(n + 150, 000)
for the range between n = âˆ’$150, 000 and n = $800, 000.
We should not assume that this is the definitive utility function for monetary value, but
it is likely that most people have a utility function that is concave for positive wealth. Going
into debt is bad, but preferences between different levels of debt can display a reversal of
the concavity associated with positive wealth. For example, someone already $10,000,000 in
debt might well accept a gamble on a fair coin with a gain of $10,000,000 for heads and a
loss of $20,000,000 for tails.5 This yields the S-shaped curve shown in Figure 16.2(b).
If we restrict our attention to the positive part of the curves, where the slope is decreasing, then for any lottery L, the utility of being faced with that lottery is less than the utility of
being handed the expected monetary value of the lottery as a sure thing:
U (L) < U (SEMV (L) ) .
RISK-AVERSE

RISK-SEEKING

That is, agents with curves of this shape are risk-averse: they prefer a sure thing with a
payoff that is less than the expected monetary value of a gamble. On the other hand, in the
â€œdesperateâ€ region at large negative wealth in Figure 16.2(b), the behavior is risk-seeking.
5

Such behavior might be called desperate, but it is rational if one is already in a desperate situation.

618
CERTAINTY
EQUIVALENT

INSURANCE
PREMIUM

RISK-NEUTRAL

Chapter

16.

Making Simple Decisions

The value an agent will accept in lieu of a lottery is called the certainty equivalent of the
lottery. Studies have shown that most people will accept about $400 in lieu of a gamble that
gives $1000 half the time and $0 the other halfâ€”that is, the certainty equivalent of the lottery
is $400, while the EMV is $500. The difference between the EMV of a lottery and its certainty
equivalent is called the insurance premium. Risk aversion is the basis for the insurance
industry, because it means that insurance premiums are positive. People would rather pay a
small insurance premium than gamble the price of their house against the chance of a fire.
From the insurance companyâ€™s point of view, the price of the house is very small compared
with the firmâ€™s total reserves. This means that the insurerâ€™s utility curve is approximately
linear over such a small region, and the gamble costs the company almost nothing.
Notice that for small changes in wealth relative to the current wealth, almost any curve
will be approximately linear. An agent that has a linear curve is said to be risk-neutral. For
gambles with small sums, therefore, we expect risk neutrality. In a sense, this justifies the
simplified procedure that proposed small gambles to assess probabilities and to justify the
axioms of probability in Section 13.2.3.

16.3.3 Expected utility and post-decision disappointment
The rational way to choose the best action, aâˆ— , is to maximize expected utility:
aâˆ— = argmax EU (a|e) .
a

UNBIASED

If we have calculated the expected utility correctly according to our probability model, and if
the probability model correctly reflects the underlying stochastic processes that generate the
outcomes, then, on average, we will get the utility we expect if the whole process is repeated
many times.
In reality, however, our model usually oversimplifies the real situation, either because
we donâ€™t know enough (e.g., when making a complex investment decision) or because the
computation of the true expected utility is too difficult (e.g., when estimating the utility of
successor states of the root node in backgammon). In that case, we are really working with
! (a|e) of the true expected utility. We will assume, kindly perhaps, that the
estimates EU
! (a|e) âˆ’ EU (a|e))), is
estimates are unbiased, that is, the expected value of the error, E(EU
zero. In that case, it still seems reasonable to choose the action with the highest estimated
utility and to expect to receive that utility, on average, when the action is executed.
Unfortunately, the real outcome will usually be significantly worse than we estimated,
even though the estimate was unbiased! To see why, consider a decision problem in which
there are k choices, each of which has true estimated utility of 0. Suppose that the error in
each utility estimate has zero mean and standard deviation of 1, shown as the bold curve in
Figure 16.3. Now, as we actually start to generate the estimates, some of the errors will be
negative (pessimistic) and some will be positive (optimistic). Because we select the action
with the highest utility estimate, we are obviously favoring the overly optimistic estimates,
and that is the source of the bias. It is a straightforward matter to calculate the distribution
of the maximum of the k estimates (see Exercise 16.11) and hence quantify the extent of
our disappointment. The curve in Figure 16.3 for k = 3 has a mean around 0.85, so the
average disappointment will be about 85% of the standard deviation in the utility estimates.

Section 16.3.

Utility Functions

619

0.9
k=30

0.8
0.7

k=10

0.6
k=3

0.5
0.4
0.3
0.2
0.1
0
-5

-4

-3

-2

-1
0
1
Error in utility estimate

2

3

4

5

Figure 16.3 Plot of the error in each of k utility estimates and of the distribution of the
maximum of k estimates for k = 3, 10, and 30.

OPTIMIZERâ€™S CURSE

With more choices, extremely optimistic estimates are more likely to arise: for k = 30, the
disappointment will be around twice the standard deviation in the estimates.
This tendency for the estimated expected utility of the best choice to be too high is
called the optimizerâ€™s curse (Smith and Winkler, 2006). It afflicts even the most seasoned
decision analysts and statisticians. Serious manifestations include believing that an exciting
new drug that has cured 80% patients in a trial will cure 80% of patients (itâ€™s been chosen
from k = thousands of candidate drugs) or that a mutual fund advertised as having aboveaverage returns will continue to have them (itâ€™s been chosen to appear in the advertisement
out of k = dozens of funds in the companyâ€™s overall portfolio). It can even be the case that
what appears to be the best choice may not be, if the variance in the utility estimate is high:
a drug, selected from thousands tried, that has cured 9 of 10 patients is probably worse than
one that has cured 800 of 1000.
The optimizerâ€™s curse crops up everywhere because of the ubiquity of utility-maximizing
selection processes, so taking the utility estimates at face value is a bad idea. We can avoid the
! | EU ) of the error in the utility estimates.
curse by using an explicit probability model P(EU
Given this model and a prior P(EU ) on what we might reasonably expect the utilities to be,
we treat the utility estimate, once obtained, as evidence and compute the posterior distribution
for the true utility using Bayesâ€™ rule.

16.3.4 Human judgment and irrationality
NORMATIVE THEORY
DESCRIPTIVE
THEORY

Decision theory is a normative theory: it describes how a rational agent should act. A
descriptive theory, on the other hand, describes how actual agentsâ€”for example, humansâ€”
really do act. The application of economic theory would be greatly enhanced if the two
coincided, but there appears to be some experimental evidence to the contrary. The evidence
suggests that humans are â€œpredictably irrationalâ€ (Ariely, 2009).

620

Chapter

16.

Making Simple Decisions

The best-known problem is the Allais paradox (Allais, 1953). People are given a choice
between lotteries A and B and then between C and D, which have the following prizes:
A : 80% chance of $4000
B : 100% chance of $3000

CERTAINTY EFFECT

REGRET

Most people consistently prefer B over A (taking the sure thing), and C over D (taking the
higher EMV). The normative analysis disagrees! We can see this most easily if we use the
freedom implied by Equation (16.2) to set U ($0) = 0. In that case, then B ' A implies
that U ($3000) > 0.8 U ($4000), whereas C ' D implies exactly the reverse. In other
words, there is no utility function that is consistent with these choices. One explanation for
the apparently irrational preferences is the certainty effect (Kahneman and Tversky, 1979):
people are strongly attracted to gains that are certain. There are several reasons why this may
be so. First, people may prefer to reduce their computational burden; by choosing certain
outcomes, they donâ€™t have to compute with probabilities. But the effect persists even when
the computations involved are very easy ones. Second, people may distrust the legitimacy of
the stated probabilities. I trust that a coin flip is roughly 50/50 if I have control over the coin
and the flip, but I may distrust the result if the flip is done by someone with a vested interest
in the outcome.6 In the presence of distrust, it might be better to go for the sure thing.7 Third,
people may be accounting for their emotional state as well as their financial state. People
know they would experience regret if they gave up a certain reward (B) for an 80% chance at
a higher reward and then lost. In other words, if A is chosen, there is a 20% chance of getting
no money and feeling like a complete idiot, which is worse than just getting no money. So
perhaps people who choose B over A and C over D are not being irrational; they are just
saying that they are willing to give up $200 of EMV to avoid a 20% chance of feeling like an
idiot.
A related problem is the Ellsberg paradox. Here the prizes are fixed, but the probabilities
are underconstrained. Your payoff will depend on the color of a ball chosen from an urn. You
are told that the urn contains 1/3 red balls, and 2/3 either black or yellow balls, but you donâ€™t
know how many black and how many yellow. Again, you are asked whether you prefer lottery
A or B; and then C or D:
A : $100 for a red ball
B : $100 for a black ball

AMBIGUITY
AVERSION

C : 20% chance of $4000
D : 25% chance of $3000

C : $100 for a red or yellow ball
D : $100 for a black or yellow ball .

It should be clear that if you think there are more red than black balls then you should prefer
A over B and C over D; if you think there are fewer red than black you should prefer the
opposite. But it turns out that most people prefer A over B and also prefer D over C, even
though there is no state of the world for which this is rational. It seems that people have
ambiguity aversion: A gives you a 1/3 chance of winning, while B could be anywhere
between 0 and 2/3. Similarly, D gives you a 2/3 chance, while C could be anywhere between
1/3 and 3/3. Most people elect the known probability rather than the unknown unknowns.
6 For example, the mathematician/magician Persi Diaconis can make a coin flip come out the way he wants
every time (Landhuis, 2004).
7 Even the sure thing may not be certain. Despite cast-iron promises, we have not yet received that $27,000,000
from the Nigerian bank account of a previously unknown deceased relative.

Section 16.3.

FRAMING EFFECT

ANCHORING EFFECT

Utility Functions

621

Yet another problem is that the exact wording of a decision problem can have a big
impact on the agentâ€™s choices; this is called the framing effect. Experiments show that people
like a medical procedure that it is described as having a â€œ90% survival rateâ€ about twice as
much as one described as having a â€œ10% death rate,â€ even though these two statements mean
exactly the same thing. This discrepancy in judgment has been found in multiple experiments
and is about the same whether the subjects were patients in a clinic, statistically sophisticated
business school students, or experienced doctors.
People feel more comfortable making relative utility judgments rather than absolute
ones. I may have little idea how much I might enjoy the various wines offered by a restaurant.
The restaurant takes advantage of this by offering a $200 bottle that it knows nobody will buy,
but which serves to skew upward the customerâ€™s estimate of the value of all wines and make
the $55 bottle seem like a bargain. This is called the anchoring effect.
If human informants insist on contradictory preference judgments, there is nothing that
automated agents can do to be consistent with them. Fortunately, preference judgments made
by humans are often open to revision in the light of further consideration. Paradoxes like
the Allais paradox are greatly reduced (but not eliminated) if the choices are explained better. In work at the Harvard Business School on assessing the utility of money, Keeney and
Raiffa (1976, p. 210) found the following:
Subjects tend to be too risk-averse in the small and therefore . . . the fitted utility functions
exhibit unacceptably large risk premiums for lotteries with a large spread. . . . Most of the
subjects, however, can reconcile their inconsistencies and feel that they have learned an
important lesson about how they want to behave. As a consequence, some subjects cancel
their automobile collision insurance and take out more term insurance on their lives.

EVOLUTIONARY
PSYCHOLOGY

The evidence for human irrationality is also questioned by researchers in the field of evolutionary psychology, who point to the fact that our brainâ€™s decision-making mechanisms
did not evolve to solve word problems with probabilities and prizes stated as decimal numbers. Let us grant, for the sake of argument, that the brain has built-in neural mechanism
for computing with probabilities and utilities, or something functionally equivalent; if so, the
required inputs would be obtained through accumulated experience of outcomes and rewards
rather than through linguistic presentations of numerical values. It is far from obvious that we
can directly access the brainâ€™s built-in neural mechanisms by presenting decision problems in
linguistic/numerical form. The very fact that different wordings of the same decision problem elicit different choices suggests that the decision problem itself is not getting through.
Spurred by this observation, psychologists have tried presenting problems in uncertain reasoning and decision making in â€œevolutionarily appropriateâ€ forms; for example, instead of
saying â€œ90% survival rate,â€ the experimenter might show 100 stick-figure animations of the
operation, where the patient dies in 10 of them and survives in 90. (Boredom is a complicating factor in these experiments!) With decision problems posed in this way, people seem to
be much closer to rational behavior than previously suspected.

622

16.4

Chapter

16.

Making Simple Decisions

M ULTIATTRIBUTE U TILITY F UNCTIONS

MULTIATTRIBUTE
UTILITY THEORY

Decision making in the field of public policy involves high stakes, in both money and lives.
For example, in deciding what levels of harmful emissions to allow from a power plant, policy makers must weigh the prevention of death and disability against the benefit of the power
and the economic burden of mitigating the emissions. Siting a new airport requires consideration of the disruption caused by construction; the cost of land; the distance from centers
of population; the noise of flight operations; safety issues arising from local topography and
weather conditions; and so on. Problems like these, in which outcomes are characterized by
two or more attributes, are handled by multiattribute utility theory.
We will call the attributes X = X1 , . . . , Xn ; a complete vector of assignments will be
x = x1 , . . . , xn , where each xi is either a numeric value or a discrete value with an assumed
ordering on values. We will assume that higher values of an attribute correspond to higher
utilities, all other things being equal. For example, if we choose AbsenceOfNoise as an
attribute in the airport problem, then the greater its value, the better the solution.8 We begin by
examining cases in which decisions can be made without combining the attribute values into
a single utility value. Then we look at cases in which the utilities of attribute combinations
can be specified very concisely.

16.4.1 Dominance
STRICT DOMINANCE

STOCHASTIC
DOMINANCE

Suppose that airport site S1 costs less, generates less noise pollution, and is safer than site S2 .
One would not hesitate to reject S2 . We then say that there is strict dominance of S1 over
S2 . In general, if an option is of lower value on all attributes than some other option, it need
not be considered further. Strict dominance is often very useful in narrowing down the field
of choices to the real contenders, although it seldom yields a unique choice. Figure 16.4(a)
shows a schematic diagram for the two-attribute case.
That is fine for the deterministic case, in which the attribute values are known for sure.
What about the general case, where the outcomes are uncertain? A direct analog of strict
dominance can be constructed, where, despite the uncertainty, all possible concrete outcomes
for S1 strictly dominate all possible outcomes for S2 . (See Figure 16.4(b).) Of course, this
will probably occur even less often than in the deterministic case.
Fortunately, there is a more useful generalization called stochastic dominance, which
occurs very frequently in real problems. Stochastic dominance is easiest to understand in
the context of a single attribute. Suppose we believe that the cost of siting the airport at S1 is
uniformly distributed between $2.8 billion and $4.8 billion and that the cost at S2 is uniformly
distributed between $3 billion and $5.2 billion. Figure 16.5(a) shows these distributions, with
cost plotted as a negative value. Then, given only the information that utility decreases with
8 In some cases, it may be necessary to subdivide the range of values so that utility varies monotonically within
each range. For example, if the RoomTemperature attribute has a utility peak at 70â—¦ F, we would split it into two
attributes measuring the difference from the ideal, one colder and one hotter. Utility would then be monotonically
increasing in each attribute.

Multiattribute Utility Functions

623

X2

X2
This region
dominates A

C

B

B

C

A

A
D
X1

X1

(a)

(b)

Figure 16.4 Strict dominance. (a) Deterministic: Option A is strictly dominated by B but
not by C or D. (b) Uncertain: A is strictly dominated by B but not by C.
0.6

1.2

0.5

1

0.4

0.8

S2

0.3

Probability

Probability

Section 16.4.

S1

0.2
0.1

S2
0.6
S1
0.4
0.2

0

0
-6

-5.5

-5

-4.5 -4 -3.5 -3
Negative cost

(a)

-2.5

-2

-6

-5.5

-5

-4.5 -4 -3.5 -3
Negative cost

-2.5

-2

(b)

Figure 16.5 Stochastic dominance. (a) S1 stochastically dominates S2 on cost. (b) Cumulative distributions for the negative cost of S1 and S2 .

cost, we can say that S1 stochastically dominates S2 (i.e., S2 can be discarded). It is important
to note that this does not follow from comparing the expected costs. For example, if we knew
the cost of S1 to be exactly $3.8 billion, then we would be unable to make a decision without
additional information on the utility of money. (It might seem odd that more information on
the cost of S1 could make the agent less able to decide. The paradox is resolved by noting
that in the absence of exact cost information, the decision is easier to make but is more likely
to be wrong.)
The exact relationship between the attribute distributions needed to establish stochastic
dominance is best seen by examining the cumulative distributions, shown in Figure 16.5(b).
(See also Appendix A.) The cumulative distribution measures the probability that the cost is
less than or equal to any given amountâ€”that is, it integrates the original distribution. If the
cumulative distribution for S1 is always to the right of the cumulative distribution for S2 ,

624

Chapter

16.

Making Simple Decisions

then, stochastically speaking, S1 is cheaper than S2 . Formally, if two actions A1 and A2 lead
to probability distributions p1 (x) and p2 (x) on attribute X, then A1 stochastically dominates
A2 on X if
x
x


p1 (x ) dx â‰¤
p2 (x ) dx .
âˆ€x
âˆ’âˆž

QUALITATIVE
PROBABILISTIC
NETWORKS

âˆ’âˆž

The relevance of this definition to the selection of optimal decisions comes from the following
property: if A1 stochastically dominates A2 , then for any monotonically nondecreasing utility
function U (x), the expected utility of A1 is at least as high as the expected utility of A2 .
Hence, if an action is stochastically dominated by another action on all attributes, then it can
be discarded.
The stochastic dominance condition might seem rather technical and perhaps not so
easy to evaluate without extensive probability calculations. In fact, it can be decided very
easily in many cases. Suppose, for example, that the construction transportation cost depends
on the distance to the supplier. The cost itself is uncertain, but the greater the distance, the
greater the cost. If S1 is closer than S2 , then S1 will dominate S2 on cost. Although we
will not present them here, there exist algorithms for propagating this kind of qualitative
information among uncertain variables in qualitative probabilistic networks, enabling a
system to make rational decisions based on stochastic dominance, without using any numeric
values.

16.4.2 Preference structure and multiattribute utility

REPRESENTATION
THEOREM

Suppose we have n attributes, each of which has d distinct possible values. To specify the
complete utility function U (x1 , . . . , xn ), we need dn values in the worst case. Now, the worst
case corresponds to a situation in which the agentâ€™s preferences have no regularity at all. Multiattribute utility theory is based on the supposition that the preferences of typical agents have
much more structure than that. The basic approach is to identify regularities in the preference
behavior we would expect to see and to use what are called representation theorems to show
that an agent with a certain kind of preference structure has a utility function
U (x1 , . . . , xn ) = F [f1 (x1 ), . . . , fn (xn )] ,
where F is, we hope, a simple function such as addition. Notice the similarity to the use of
Bayesian networks to decompose the joint probability of several random variables.
Preferences without uncertainty

PREFERENCE
INDEPENDENCE

Let us begin with the deterministic case. Remember that for deterministic environments the
agent has a value function V (x1 , . . . , xn ); the aim is to represent this function concisely.
The basic regularity that arises in deterministic preference structures is called preference
independence. Two attributes X1 and X2 are preferentially independent of a third attribute
X3 if the preference between outcomes x1 , x2 , x3  and x1 , x2 , x3  does not depend on the
particular value x3 for attribute X3 .
Going back to the airport example, where we have (among other attributes) Noise,
Cost , and Deaths to consider, one may propose that Noise and Cost are preferentially inde-

Section 16.4.

MUTUAL
PREFERENTIAL
INDEPENDENCE

Multiattribute Utility Functions

625

pendent of Deaths. For example, if we prefer a state with 20,000 people residing in the flight
path and a construction cost of $4 billion over a state with 70,000 people residing in the flight
path and a cost of $3.7 billion when the safety level is 0.06 deaths per million passenger miles
in both cases, then we would have the same preference when the safety level is 0.12 or 0.03;
and the same independence would hold for preferences between any other pair of values for
Noise and Cost . It is also apparent that Cost and Deaths are preferentially independent of
Noise and that Noise and Deaths are preferentially independent of Cost. We say that the
set of attributes {Noise, Cost , Deaths} exhibits mutual preferential independence (MPI).
MPI says that, whereas each attribute may be important, it does not affect the way in which
one trades off the other attributes against each other.
Mutual preferential independence is something of a mouthful, but thanks to a remarkable theorem due to the economist GeÌrard Debreu (1960), we can derive from it a very simple
form for the agentâ€™s value function: If attributes X1 , . . . , Xn are mutually preferentially independent, then the agentâ€™s preference behavior can be described as maximizing the function

Vi (xi ) ,
V (x1 , . . . , xn ) =
i

where each Vi is a value function referring only to the attribute Xi . For example, it might
well be the case that the airport decision can be made using a value function
V (noise, cost , deaths ) = âˆ’noise Ã— 104 âˆ’ cost âˆ’ deaths Ã— 1012 .
ADDITIVE VALUE
FUNCTION

A value function of this type is called an additive value function. Additive functions are an
extremely natural way to describe an agentâ€™s preferences and are valid in many real-world
situations. For n attributes, assessing an additive value function requires assessing n separate
one-dimensional value functions rather than one n-dimensional function; typically, this represents an exponential reduction in the number of preference experiments that are needed. Even
when MPI does not strictly hold, as might be the case at extreme values of the attributes, an
additive value function might still provide a good approximation to the agentâ€™s preferences.
This is especially true when the violations of MPI occur in portions of the attribute ranges
that are unlikely to occur in practice.
To understand MPI better, it helps to look at cases where it doesnâ€™t hold. Suppose you
are at a medieval market, considering the purchase of some hunting dogs, some chickens,
and some wicker cages for the chickens. The hunting dogs are very valuable, but if you
donâ€™t have enough cages for the chickens, the dogs will eat the chickens; hence, the tradeoff
between dogs and chickens depends strongly on the number of cages, and MPI is violated.
The existence of these kinds of interactions among various attributes makes it much harder to
assess the overall value function.
Preferences with uncertainty
When uncertainty is present in the domain, we also need to consider the structure of preferences between lotteries and to understand the resulting properties of utility functions, rather
than just value functions. The mathematics of this problem can become quite complicated,
so we present just one of the main results to give a flavor of what can be done. The reader is
referred to Keeney and Raiffa (1976) for a thorough survey of the field.

626

Chapter

UTILITY
INDEPENDENCE

MUTUALLY UTILITY
INDEPENDENT

MULTIPLICATIVE
UTILITY FUNCTION

16.

Making Simple Decisions

The basic notion of utility independence extends preference independence to cover
lotteries: a set of attributes X is utility independent of a set of attributes Y if preferences between lotteries on the attributes in X are independent of the particular values of the attributes
in Y. A set of attributes is mutually utility independent (MUI) if each of its subsets is
utility-independent of the remaining attributes. Again, it seems reasonable to propose that
the airport attributes are MUI.
MUI implies that the agentâ€™s behavior can be described using a multiplicative utility
function (Keeney, 1974). The general form of a multiplicative utility function is best seen by
looking at the case for three attributes. For conciseness, we use Ui to mean Ui (xi ):
U = k1 U1 + k2 U2 + k3 U3 + k1 k2 U1 U2 + k2 k3 U2 U3 + k3 k1 U3 U1
+ k1 k2 k3 U1 U2 U3 .
Although this does not look very simple, it contains just three single-attribute utility functions
and three constants. In general, an n-attribute problem exhibiting MUI can be modeled using
n single-attribute utilities and n constants. Each of the single-attribute utility functions can
be developed independently of the other attributes, and this combination will be guaranteed
to generate the correct overall preferences. Additional assumptions are required to obtain a
purely additive utility function.

16.5

D ECISION N ETWORKS

INFLUENCE DIAGRAM
DECISION NETWORK

In this section, we look at a general mechanism for making rational decisions. The notation
is often called an influence diagram (Howard and Matheson, 1984), but we will use the
more descriptive term decision network. Decision networks combine Bayesian networks
with additional node types for actions and utilities. We use airport siting as an example.

16.5.1 Representing a decision problem with a decision network
In its most general form, a decision network represents information about the agentâ€™s current
state, its possible actions, the state that will result from the agentâ€™s action, and the utility of
that state. It therefore provides a substrate for implementing utility-based agents of the type
first introduced in Section 2.4.5. Figure 16.6 shows a decision network for the airport siting
problem. It illustrates the three types of nodes used:
CHANCE NODES

DECISION NODES

â€¢ Chance nodes (ovals) represent random variables, just as they do in Bayesian networks.
The agent could be uncertain about the construction cost, the level of air traffic and the
potential for litigation, and the Deaths, Noise, and total Cost variables, each of which
also depends on the site chosen. Each chance node has associated with it a conditional
distribution that is indexed by the state of the parent nodes. In decision networks, the
parent nodes can include decision nodes as well as chance nodes. Note that each of
the current-state chance nodes could be part of a large Bayesian network for assessing
construction costs, air traffic levels, or litigation potentials.
â€¢ Decision nodes (rectangles) represent points where the decision maker has a choice of

Section 16.5.

Decision Networks

627

Airport Site

Figure 16.6

Air Traffic

Deaths

Litigation

Noise

Construction

Cost

U

A simple decision network for the airport-siting problem.

actions. In this case, the AirportSite action can take on a different value for each site
under consideration. The choice influences the cost, safety, and noise that will result.
In this chapter, we assume that we are dealing with a single decision node. Chapter 17
deals with cases in which more than one decision must be made.
â€¢ Utility nodes (diamonds) represent the agentâ€™s utility function.9 The utility node has
as parents all variables describing the outcome that directly affect utility. Associated
with the utility node is a description of the agentâ€™s utility as a function of the parent
attributes. The description could be just a tabulation of the function, or it might be a
parameterized additive or linear function of the attribute values.

UTILITY NODES

ACTION-UTILITY
FUNCTION

A simplified form is also used in many cases. The notation remains identical, but the
chance nodes describing the outcome state are omitted. Instead, the utility node is connected
directly to the current-state nodes and the decision node. In this case, rather than representing
a utility function on outcome states, the utility node represents the expected utility associated
with each action, as defined in Equation (16.1) on page 611; that is, the node is associated
with an action-utility function (also known as a Q-function in reinforcement learning, as
described in Chapter 21). Figure 16.7 shows the action-utility representation of the airport
siting problem.
Notice that, because the Noise, Deaths, and Cost chance nodes in Figure 16.6 refer to
future states, they can never have their values set as evidence variables. Thus, the simplified
version that omits these nodes can be used whenever the more general form can be used.
Although the simplified form contains fewer nodes, the omission of an explicit description
of the outcome of the siting decision means that it is less flexible with respect to changes in
circumstances. For example, in Figure 16.6, a change in aircraft noise levels can be reflected
by a change in the conditional probability table associated with the Noise node, whereas a
change in the weight accorded to noise pollution in the utility function can be reflected by
9

These nodes are also called value nodes in the literature.

628

Chapter

16.

Making Simple Decisions

Airport Site

Air Traffic

Litigation

U

Construction
Figure 16.7 A simplified representation of the airport-siting problem. Chance nodes corresponding to outcome states have been factored out.

a change in the utility table. In the action-utility diagram, Figure 16.7, on the other hand,
all such changes have to be reflected by changes to the action-utility table. Essentially, the
action-utility formulation is a compiled version of the original formulation.

16.5.2 Evaluating decision networks
Actions are selected by evaluating the decision network for each possible setting of the decision node. Once the decision node is set, it behaves exactly like a chance node that has been
set as an evidence variable. The algorithm for evaluating decision networks is the following:
1. Set the evidence variables for the current state.
2. For each possible value of the decision node:
(a) Set the decision node to that value.
(b) Calculate the posterior probabilities for the parent nodes of the utility node, using
a standard probabilistic inference algorithm.
(c) Calculate the resulting utility for the action.
3. Return the action with the highest utility.
This is a straightforward extension of the Bayesian network algorithm and can be incorporated directly into the agent design given in Figure 13.1 on page 484. We will see in Chapter 17 that the possibility of executing several actions in sequence makes the problem much
more interesting.

16.6

T HE VALUE OF I NFORMATION
In the preceding analysis, we have assumed that all relevant information, or at least all available information, is provided to the agent before it makes its decision. In practice, this is

Section 16.6.

INFORMATION VALUE
THEORY

The Value of Information

629

hardly ever the case. One of the most important parts of decision making is knowing what
questions to ask. For example, a doctor cannot expect to be provided with the results of all
possible diagnostic tests and questions at the time a patient first enters the consulting room.10
Tests are often expensive and sometimes hazardous (both directly and because of associated
delays). Their importance depends on two factors: whether the test results would lead to a
significantly better treatment plan, and how likely the various test results are.
This section describes information value theory, which enables an agent to choose
what information to acquire. We assume that, prior to selecting a â€œrealâ€ action represented
by the decision node, the agent can acquire the value of any of the potentially observable
chance variables in the model. Thus, information value theory involves a simplified form
of sequential decision makingâ€”simplified because the observation actions affect only the
agentâ€™s belief state, not the external physical state. The value of any particular observation
must derive from the potential to affect the agentâ€™s eventual physical action; and this potential
can be estimated directly from the decision model itself.

16.6.1 A simple example
Suppose an oil company is hoping to buy one of n indistinguishable blocks of ocean-drilling
rights. Let us assume further that exactly one of the blocks contains oil worth C dollars, while
the others are worthless. The asking price of each block is C/n dollars. If the company is
risk-neutral, then it will be indifferent between buying a block and not buying one.
Now suppose that a seismologist offers the company the results of a survey of block
number 3, which indicates definitively whether the block contains oil. How much should
the company be willing to pay for the information? The way to answer this question is to
examine what the company would do if it had the information:
â€¢ With probability 1/n, the survey will indicate oil in block 3. In this case, the company
will buy block 3 for C/n dollars and make a profit of C âˆ’ C/n = (n âˆ’ 1)C/n dollars.
â€¢ With probability (nâˆ’1)/n, the survey will show that the block contains no oil, in which
case the company will buy a different block. Now the probability of finding oil in one
of the other blocks changes from 1/n to 1/(n âˆ’ 1), so the company makes an expected
profit of C/(n âˆ’ 1) âˆ’ C/n = C/n(n âˆ’ 1) dollars.
Now we can calculate the expected profit, given the survey information:
nâˆ’1
C
1 (n âˆ’ 1)C
Ã—
+
Ã—
= C/n .
n
n
n
n(n âˆ’ 1)
Therefore, the company should be willing to pay the seismologist up to C/n dollars for the
information: the information is worth as much as the block itself.
The value of information derives from the fact that with the information, oneâ€™s course
of action can be changed to suit the actual situation. One can discriminate according to the
situation, whereas without the information, one has to do whatâ€™s best on average over the
possible situations. In general, the value of a given piece of information is defined to be the
difference in expected value between best actions before and after information is obtained.
10

In the United States, the only question that is always asked beforehand is whether the patient has insurance.

630

Chapter

16.

Making Simple Decisions

16.6.2 A general formula for perfect information

VALUE OF PERFECT
INFORMATION

It is simple to derive a general mathematical formula for the value of information. We assume
that exact evidence can be obtained about the value of some random variable Ej (that is, we
learn Ej = ej ), so the phrase value of perfect information (VPI) is used.11
Let the agentâ€™s initial evidence be e. Then the value of the current best action Î± is
defined by

P (R ESULT (a) = s | a, e) U (s ) ,
EU (Î±|e) = max
a

s

and the value of the new best action (after the new evidence Ej = ej is obtained) will be

P (R ESULT (a) = s | a, e, ej ) U (s ) .
EU (Î±ej |e, ej ) = max
a

s

But Ej is a random variable whose value is currently unknown, so to determine the value of
discovering Ej , given current information e we must average over all possible values ejk that
we might discover for Ej , using our current beliefs about its value:


P (Ej = ejk |e) EU (Î±ejk |e, Ej = ejk ) âˆ’ EU (Î±|e) .
VPI e (Ej ) =
k

To get some intuition for this formula, consider the simple case where there are only two
actions, a1 and a2 , from which to choose. Their current expected utilities are U1 and U2 . The
information Ej = ejk will yield some new expected utilities U1 and U2 for the actions, but
before we obtain Ej , we will have some probability distributions over the possible values of
U1 and U2 (which we assume are independent).
Suppose that a1 and a2 represent two different routes through a mountain range in
winter. a1 is a nice, straight highway through a low pass, and a2 is a winding dirt road over
the top. Just given this information, a1 is clearly preferable, because it is quite possible that
a2 is blocked by avalanches, whereas it is unlikely that anything blocks a1 . U1 is therefore
clearly higher than U2 . It is possible to obtain satellite reports Ej on the actual state of each
road that would give new expectations, U1 and U2 , for the two crossings. The distributions
for these expectations are shown in Figure 16.8(a). Obviously, in this case, it is not worth the
expense of obtaining satellite reports, because it is unlikely that the information derived from
them will change the plan. With no change, information has no value.
Now suppose that we are choosing between two different winding dirt roads of slightly
different lengths and we are carrying a seriously injured passenger. Then, even when U1
and U2 are quite close, the distributions of U1 and U2 are very broad. There is a significant
possibility that the second route will turn out to be clear while the first is blocked, and in this
11 There is no loss of expressiveness in requiring perfect information. Suppose we wanted to model the case
in which we become somewhat more certain about a variable. We can do that by introducing another variable
about which we learn perfect information. For example, suppose we initially have broad uncertainty about the
variable Temperature . Then we gain the perfect knowledge Thermometer = 37; this gives us imperfect
information about the true Temperature , and the uncertainty due to measurement error is encoded in the sensor
model P(Thermometer | Temperature ). See Exercise 16.17 for another example.

Section 16.6.

The Value of Information

P(U | Ej)

631

P(U | Ej)

U2

U1
(a)

U

P(U | Ej)

U2 U1
(b)

U

U2 U1
(c)

U

Figure 16.8 Three generic cases for the value of information. In (a), a1 will almost certainly remain superior to a2 , so the information is not needed. In (b), the choice is unclear and
the information is crucial. In (c), the choice is unclear, but because it makes little difference,
the information is less valuable. (Note: The fact that U2 has a high peak in (c) means that its
expected value is known with higher certainty than U1 .)

case the difference in utilities will be very high. The VPI formula indicates that it might be
worthwhile getting the satellite reports. Such a situation is shown in Figure 16.8(b).
Finally, suppose that we are choosing between the two dirt roads in summertime, when
blockage by avalanches is unlikely. In this case, satellite reports might show one route to be
more scenic than the other because of flowering alpine meadows, or perhaps wetter because
of errant streams. It is therefore quite likely that we would change our plan if we had the
information. In this case, however, the difference in value between the two routes is still
likely to be very small, so we will not bother to obtain the reports. This situation is shown in
Figure 16.8(c).
In sum, information has value to the extent that it is likely to cause a change of plan
and to the extent that the new plan will be significantly better than the old plan.

16.6.3 Properties of the value of information
One might ask whether it is possible for information to be deleterious: can it actually have
negative expected value? Intuitively, one should expect this to be impossible. After all, one
could in the worst case just ignore the information and pretend that one has never received it.
This is confirmed by the following theorem, which applies to any decision-theoretic agent:
The expected value of information is nonnegative:
âˆ€ e, Ej VPI e (Ej ) â‰¥ 0 .
The theorem follows directly from the definition of VPI, and we leave the proof as an exercise
(Exercise 16.18). It is, of course, a theorem about expected value, not actual value. Additional
information can easily lead to a plan that turns out to be worse than the original plan if the
information happens to be misleading. For example, a medical test that gives a false positive
result may lead to unnecessary surgery; but that does not mean that the test shouldnâ€™t be done.

632

Chapter

16.

Making Simple Decisions

It is important to remember that VPI depends on the current state of information, which
is why it is subscripted. It can change as more information is acquired. For any given piece
of evidence Ej , the value of acquiring it can go down (e.g., if another variable strongly
constrains the posterior for Ej ) or up (e.g., if another variable provides a clue on which Ej
builds, enabling a new and better plan to be devised). Thus, VPI is not additive. That is,
VPI e (Ej , Ek ) = VPI e (Ej ) + VPI e (Ek )

(in general) .

VPI is, however, order independent. That is,
VPI e (Ej , Ek ) = VPI e (Ej ) + VPI e,ej (Ek ) = VPI e (Ek ) + VPI e,ek (Ej ) .
Order independence distinguishes sensing actions from ordinary actions and simplifies the
problem of calculating the value of a sequence of sensing actions.

16.6.4 Implementation of an information-gathering agent

MYOPIC

A sensible agent should ask questions in a reasonable order, should avoid asking questions
that are irrelevant, should take into account the importance of each piece of information in
relation to its cost, and should stop asking questions when that is appropriate. All of these
capabilities can be achieved by using the value of information as a guide.
Figure 16.9 shows the overall design of an agent that can gather information intelligently before acting. For now, we assume that with each observable evidence variable
Ej , there is an associated cost, Cost(Ej ), which reflects the cost of obtaining the evidence
through tests, consultants, questions, or whatever. The agent requests what appears to be the
most efficient observation in terms of utility gain per unit cost. We assume that the result of
the action Request (Ej ) is that the next percept provides the value of Ej . If no observation is
worth its cost, the agent selects a â€œrealâ€ action.
The agent algorithm we have described implements a form of information gathering
that is called myopic. This is because it uses the VPI formula shortsightedly, calculating the
value of information as if only a single evidence variable will be acquired. Myopic control
is based on the same heuristic idea as greedy search and often works well in practice. (For
example, it has been shown to outperform expert physicians in selecting diagnostic tests.)

function I NFORMATION -G ATHERING -AGENT( percept ) returns an action
persistent: D , a decision network
integrate percept into D
j â† the value that maximizes VPI (Ej ) / Cost (Ej )
if VPI (Ej ) > Cost (Ej )
return R EQUEST(Ej )
else return the best action from D
Figure 16.9 Design of a simple information-gathering agent. The agent works by repeatedly selecting the observation with the highest information value, until the cost of the next
observation is greater than its expected benefit.

Section 16.7.

Decision-Theoretic Expert Systems

633

However, if there is no single evidence variable that will help a lot, a myopic agent might
hastily take an action when it would have been better to request two or more variables first
and then take action. A better approach in this situation would be to construct a conditional
plan (as described in Section 11.3.2) that asks for variable values and takes different next
steps depending on the answer.
One final consideration is the effect a series of questions will have on a human respondent. People may respond better to a series of questions if they â€œmake sense,â€ so some expert
systems are built to take this into account, asking questions in an order that maximizes the
total utility of the system and human rather than an order that maximizes value of information.

16.7

D ECISION -T HEORETIC E XPERT S YSTEMS

DECISION ANALYSIS

DECISION MAKER
DECISION ANALYST

The field of decision analysis, which evolved in the 1950s and 1960s, studies the application
of decision theory to actual decision problems. It is used to help make rational decisions in
important domains where the stakes are high, such as business, government, law, military
strategy, medical diagnosis and public health, engineering design, and resource management.
The process involves a careful study of the possible actions and outcomes, as well as the
preferences placed on each outcome. It is traditional in decision analysis to talk about two
roles: the decision maker states preferences between outcomes, and the decision analyst
enumerates the possible actions and outcomes and elicits preferences from the decision maker
to determine the best course of action. Until the early 1980s, the main purpose of decision
analysis was to help humans make decisions that actually reflect their own preferences. As
more and more decision processes become automated, decision analysis is increasingly used
to ensure that the automated processes are behaving as desired.
Early expert system research concentrated on answering questions, rather than on making decisions. Those systems that did recommend actions rather than providing opinions on
matters of fact generally did so using condition-action rules, rather than with explicit representations of outcomes and preferences. The emergence of Bayesian networks in the late
1980s made it possible to build large-scale systems that generated sound probabilistic inferences from evidence. The addition of decision networks means that expert systems can be
developed that recommend optimal decisions, reflecting the preferences of the agent as well
as the available evidence.
A system that incorporates utilities can avoid one of the most common pitfalls associated with the consultation process: confusing likelihood and importance. A common strategy
in early medical expert systems, for example, was to rank possible diagnoses in order of likelihood and report the most likely. Unfortunately, this can be disastrous! For the majority of
patients in general practice, the two most likely diagnoses are usually â€œThereâ€™s nothing wrong
with youâ€ and â€œYou have a bad cold,â€ but if the third most likely diagnosis for a given patient
is lung cancer, thatâ€™s a serious matter. Obviously, a testing or treatment plan should depend
both on probabilities and utilities. Current medical expert systems can take into account the
value of information to recommend tests, and then describe a differential diagnosis.

634

AORTIC
COARCTATION

GOLD STANDARD

Chapter

16.

Making Simple Decisions

We now describe the knowledge engineering process for decision-theoretic expert systems. As an example we consider the problem of selecting a medical treatment for a kind of
congenital heart disease in children (see Lucas, 1996).
About 0.8% of children are born with a heart anomaly, the most common being aortic
coarctation (a constriction of the aorta). It can be treated with surgery, angioplasty (expanding the aorta with a balloon placed inside the artery), or medication. The problem is to decide
what treatment to use and when to do it: the younger the infant, the greater the risks of certain
treatments, but one mustnâ€™t wait too long. A decision-theoretic expert system for this problem
can be created by a team consisting of at least one domain expert (a pediatric cardiologist)
and one knowledge engineer. The process can be broken down into the following steps:
Create a causal model. Determine the possible symptoms, disorders, treatments, and
outcomes. Then draw arcs between them, indicating what disorders cause what symptoms,
and what treatments alleviate what disorders. Some of this will be well known to the domain
expert, and some will come from the literature. Often the model will match well with the
informal graphical descriptions given in medical textbooks.
Simplify to a qualitative decision model. Since we are using the model to make
treatment decisions and not for other purposes (such as determining the joint probability of
certain symptom/disorder combinations), we can often simplify by removing variables that
are not involved in treatment decisions. Sometimes variables will have to be split or joined
to match the expertâ€™s intuitions. For example, the original aortic coarctation model had a
Treatment variable with values surgery, angioplasty, and medication, and a separate variable
for Timing of the treatment. But the expert had a hard time thinking of these separately, so
they were combined, with Treatment taking on values such as surgery in 1 month. This gives
us the model of Figure 16.10.
Assign probabilities. Probabilities can come from patient databases, literature studies,
or the expertâ€™s subjective assessments. Note that a diagnostic system will reason from symptoms and other observations to the disease or other cause of the problems. Thus, in the early
years of building these systems, experts were asked for the probability of a cause given an
effect. In general they found this difficult to do, and were better able to assess the probability
of an effect given a cause. So modern systems usually assess causal knowledge and encode it
directly in the Bayesian network structure of the model, leaving the diagnostic reasoning to
the Bayesian network inference algorithms (Shachter and Heckerman, 1987).
Assign utilities. When there are a small number of possible outcomes, they can be
enumerated and evaluated individually using the methods of Section 16.3.1. We would create
a scale from best to worst outcome and give each a numeric value, for example 0 for death
and 1 for complete recovery. We would then place the other outcomes on this scale. This
can be done by the expert, but it is better if the patient (or in the case of infants, the patientâ€™s
parents) can be involved, because different people have different preferences. If there are exponentially many outcomes, we need some way to combine them using multiattribute utility
functions. For example, we may say that the costs of various complications are additive.
Verify and refine the model. To evaluate the system we need a set of correct (input,
output) pairs; a so-called gold standard to compare against. For medical expert systems
this usually means assembling the best available doctors, presenting them with a few cases,

Section 16.7.

Decision-Theoretic Expert Systems

635

Sex

Postcoarctectomy
Syndrome
Tachypnea

Tachycardia

Paradoxical
Hypertension

Failure
To Thrive
Dyspnea

Aortic
Aneurysm

Intercostal
Recession

Paraplegia
Heart
Failure

Age

Treatment

Intermediate
Result

Hepatomegaly

Late
Result
CVA

Aortic
Dissection

Pulmonary
Crepitations

Myocardial
Infarction

Cardiomegaly

U

Figure 16.10

SENSITIVITY
ANALYSIS

Influence diagram for aortic coarctation (courtesy of Peter Lucas).

and asking them for their diagnosis and recommended treatment plan. We then see how
well the system matches their recommendations. If it does poorly, we try to isolate the parts
that are going wrong and fix them. It can be useful to run the system â€œbackward.â€ Instead
of presenting the system with symptoms and asking for a diagnosis, we can present it with
a diagnosis such as â€œheart failure,â€ examine the predicted probability of symptoms such as
tachycardia, and compare with the medical literature.
Perform sensitivity analysis. This important step checks whether the best decision is
sensitive to small changes in the assigned probabilities and utilities by systematically varying
those parameters and running the evaluation again. If small changes lead to significantly
different decisions, then it could be worthwhile to spend more resources to collect better
data. If all variations lead to the same decision, then the agent will have more confidence that
it is the right decision. Sensitivity analysis is particularly important, because one of the main

636

Chapter

16.

Making Simple Decisions

criticisms of probabilistic approaches to expert systems is that it is too difficult to assess the
numerical probabilities required. Sensitivity analysis often reveals that many of the numbers
need be specified only very approximately. For example, we might be uncertain about the
conditional probability P (tachycardia | dyspnea), but if the optimal decision is reasonably
robust to small variations in the probability, then our ignorance is less of a concern.

16.8

S UMMARY
This chapter shows how to combine utility theory with probability to enable an agent to select
actions that will maximize its expected performance.
â€¢ Probability theory describes what an agent should believe on the basis of evidence,
utility theory describes what an agent wants, and decision theory puts the two together
to describe what an agent should do.
â€¢ We can use decision theory to build a system that makes decisions by considering all
possible actions and choosing the one that leads to the best expected outcome. Such a
system is known as a rational agent.
â€¢ Utility theory shows that an agent whose preferences between lotteries are consistent
with a set of simple axioms can be described as possessing a utility function; furthermore, the agent selects actions as if maximizing its expected utility.
â€¢ Multiattribute utility theory deals with utilities that depend on several distinct attributes of states. Stochastic dominance is a particularly useful technique for making
unambiguous decisions, even without precise utility values for attributes.
â€¢ Decision networks provide a simple formalism for expressing and solving decision
problems. They are a natural extension of Bayesian networks, containing decision and
utility nodes in addition to chance nodes.
â€¢ Sometimes, solving a problem involves finding more information before making a decision. The value of information is defined as the expected improvement in utility
compared with making a decision without the information.
â€¢ Expert systems that incorporate utility information have additional capabilities compared with pure inference systems. In addition to being able to make decisions, they
can use the value of information to decide which questions to ask, if any; they can recommend contingency plans; and they can calculate the sensitivity of their decisions to
small changes in probability and utility assessments.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

The book Lâ€™art de Penser, also known as the Port-Royal Logic (Arnauld, 1662) states:
To judge what one must do to obtain a good or avoid an evil, it is necessary to consider
not only the good and the evil in itself, but also the probability that it happens or does not
happen; and to view geometrically the proportion that all these things have together.

Bibliographical and Historical Notes

POST-DECISION
DISAPPOINTMENT

WINNERâ€™S CURSE

637

Modern texts talk of utility rather than good and evil, but this statement correctly notes that
one should multiply utility by probability (â€œview geometricallyâ€) to give expected utility,
and maximize that over all outcomes (â€œall these thingsâ€) to â€œjudge what one must do.â€ It
is remarkable how much this got right, 350 years ago, and only 8 years after Pascal and
Fermat showed how to use probability correctly. The Port-Royal Logic also marked the first
publication of Pascalâ€™s wager.
Daniel Bernoulli (1738), investigating the St. Petersburg paradox (see Exercise 16.3),
was the first to realize the importance of preference measurement for lotteries, writing â€œthe
value of an item must not be based on its price, but rather on the utility that it yieldsâ€ (italics his). Utilitarian philosopher Jeremy Bentham (1823) proposed the hedonic calculus for
weighing â€œpleasuresâ€ and â€œpains,â€ arguing that all decisions (not just monetary ones) could
be reduced to utility comparisons.
The derivation of numerical utilities from preferences was first carried out by Ramsey (1931); the axioms for preference in the present text are closer in form to those rediscovered in Theory of Games and Economic Behavior (von Neumann and Morgenstern, 1944).
A good presentation of these axioms, in the course of a discussion on risk preference, is given
by Howard (1977). Ramsey had derived subjective probabilities (not just utilities) from an
agentâ€™s preferences; Savage (1954) and Jeffrey (1983) carry out more recent constructions
of this kind. Von Winterfeldt and Edwards (1986) provide a modern perspective on decision
analysis and its relationship to human preference structures. The micromort utility measure
is discussed by Howard (1989). A 1994 survey by the Economist set the value of a life at
between $750,000 and $2.6 million. However, Richard Thaler (1992) found irrational framing effects on the price one is willing to pay to avoid a risk of death versus the price one is
willing to be paid to accept a risk. For a 1/1000 chance, a respondent wouldnâ€™t pay more
than $200 to remove the risk, but wouldnâ€™t accept $50,000 to take on the risk. How much are
people willing to pay for a QALY? When it comes down to a specific case of saving oneself
or a family member, the number is approximately â€œwhatever Iâ€™ve got.â€ But we can ask at a
societal level: suppose there is a vaccine that would yield X QALYs but costs Y dollars; is it
worth it? In this case people report a wide range of values from around $10,000 to $150,000
per QALY (Prades et al., 2008). QALYs are much more widely used in medical and social
policy decision making than are micromorts; see (Russell, 1990) for a typical example of an
argument for a major change in public health policy on grounds of increased expected utility
measured in QALYs.
The optimizerâ€™s curse was brought to the attention of decision analysts in a forceful
way by Smith and Winkler (2006), who pointed out that the financial benefits to the client
projected by analysts for their proposed course of action almost never materialized. They
trace this directly to the bias introduced by selecting an optimal action and show that a more
complete Bayesian analysis eliminates the problem. The same underlying concept has been
called post-decision disappointment by Harrison and March (1984) and was noted in the
context of analyzing capital investment projects by Brown (1974). The optimizerâ€™s curse is
also closely related to the winnerâ€™s curse (Capen et al., 1971; Thaler, 1992), which applies
to competitive bidding in auctions: whoever wins the auction is very likely to have overestimated the value of the object in question. Capen et al. quote a petroleum engineer on the

638

REGRESSION TO THE
MEAN

Chapter

16.

Making Simple Decisions

topic of bidding for oil-drilling rights: â€œIf one wins a tract against two or three others he may
feel fine about his good fortune. But how should he feel if he won against 50 others? Ill.â€
Finally, behind both curses is the general phenomenon of regression to the mean, whereby
individuals selected on the basis of exceptional characteristics previously exhibited will, with
high probability, become less exceptional in future.
The Allais paradox, due to Nobel Prize-winning economist Maurice Allais (1953) was
tested experimentally (Tversky and Kahneman, 1982; Conlisk, 1989) to show that people
are consistently inconsistent in their judgments. The Ellsberg paradox on ambiguity aversion was introduced in the Ph.D. thesis of Daniel Ellsberg (Ellsberg, 1962), who went on to
become a military analyst at the RAND Corporation and to leak documents known as The
Pentagon Papers, which contributed to the end of the Vietnam war and the resignation of
President Nixon. Fox and Tversky (1995) describe a further study of ambiguity aversion.
Mark Machina (2005) gives an overview of choice under uncertainty and how it can vary
from expected utility theory.
There has been a recent outpouring of more-or-less popular books on human irrationality. The best known is Predictably Irrational (Ariely, 2009); others include Sway (Brafman
and Brafman, 2009), Nudge (Thaler and Sunstein, 2009), Kluge (Marcus, 2009), How We
Decide (Lehrer, 2009) and On Being Certain (Burton, 2009). They complement the classic
(Kahneman et al., 1982) and the article that started it all (Kahneman and Tversky, 1979).
The field of evolutionary psychology (Buss, 2005), on the other hand, has run counter to this
literature, arguing that humans are quite rational in evolutionarily appropriate contexts. Its
adherents point out that irrationality is penalized by definition in an evolutionary context and
show that in some cases it is an artifact of the experimental setup (Cummins and Allen, 1998).
There has been a recent resurgence of interest in Bayesian models of cognition, overturning
decades of pessimism (Oaksford and Chater, 1998; Elio, 2002; Chater and Oaksford, 2008).
Keeney and Raiffa (1976) give a thorough introduction to multiattribute utility theory. They describe early computer implementations of methods for eliciting the necessary
parameters for a multiattribute utility function and include extensive accounts of real applications of the theory. In AI, the principal reference for MAUT is Wellmanâ€™s (1985) paper,
which includes a system called URP (Utility Reasoning Package) that can use a collection
of statements about preference independence and conditional independence to analyze the
structure of decision problems. The use of stochastic dominance together with qualitative
probability models was investigated extensively by Wellman (1988, 1990a). Wellman and
Doyle (1992) provide a preliminary sketch of how a complex set of utility-independence relationships might be used to provide a structured model of a utility function, in much the
same way that Bayesian networks provide a structured model of joint probability distributions. Bacchus and Grove (1995, 1996) and La Mura and Shoham (1999) give further results
along these lines.
Decision theory has been a standard tool in economics, finance, and management science since the 1950s. Until the 1980s, decision trees were the main tool used for representing
simple decision problems. Smith (1988) gives an overview of the methodology of decision analysis. Influence diagrams were introduced by Howard and Matheson (1984), based
on earlier work at SRI (Miller et al., 1976). Howard and Mathesonâ€™s method involved the

Bibliographical and Historical Notes

639

derivation of a decision tree from a decision network, but in general the tree is of exponential
size. Shachter (1986) developed a method for making decisions based directly on a decision
network, without the creation of an intermediate decision tree. This algorithm was also one
of the first to provide complete inference for multiply connected Bayesian networks. Zhang
et al. (1994) showed how to take advantage of conditional independence of information to reduce the size of trees in practice; they use the term decision network for networks that use this
approach (although others use it as a synonym for influence diagram). Nilsson and Lauritzen
(2000) link algorithms for decision networks to ongoing developments in clustering algorithms for Bayesian networks. Koller and Milch (2003) show how influence diagrams can be
used to solve games that involve gathering information by opposing players, and Detwarasiti
and Shachter (2005) show how influence diagrams can be used as an aid to decision making
for a team that shares goals but is unable to share all information perfectly. The collection
by Oliver and Smith (1990) has a number of useful articles on decision networks, as does the
1990 special issue of the journal Networks. Papers on decision networks and utility modeling
also appear regularly in the journals Management Science and Decision Analysis.
The theory of information value was explored first in the context of statistical experiments, where a quasi-utility (entropy reduction) was used (Lindley, 1956). The Russian control theorist Ruslan Stratonovich (1965) developed the more general theory presented here, in
which information has value by virtue of its ability to affect decisions. Stratonovichâ€™s work
was not known in the West, where Ron Howard (1966) pioneered the same idea. His paper
ends with the remark â€œIf information value theory and associated decision theoretic structures
do not in the future occupy a large part of the education of engineers, then the engineering
profession will find that its traditional role of managing scientific and economic resources for
the benefit of man has been forfeited to another profession.â€ To date, the implied revolution
in managerial methods has not occurred.
Recent work by Krause and Guestrin (2009) shows that computing the exact nonmyopic value of information is intractable even in polytree networks. There are other casesâ€”
more restricted than general value of informationâ€”in which the myopic algorithm does provide a provably good approximation to the optimal sequence of observations (Krause et al.,
2008). In some casesâ€”for example, looking for treasure buried in one of n placesâ€”ranking
experiments in order of success probability divided by cost gives an optimal solution (Kadane
and Simon, 1977).
Surprisingly few early AI researchers adopted decision-theoretic tools after the early
applications in medical decision making described in Chapter 13. One of the few exceptions
was Jerry Feldman, who applied decision theory to problems in vision (Feldman and Yakimovsky, 1974) and planning (Feldman and Sproull, 1977). After the resurgence of interest in
probabilistic methods in AI in the 1980s, decision-theoretic expert systems gained widespread
acceptance (Horvitz et al., 1988; Cowell et al., 2002). In fact, from 1991 onward, the cover
design of the journal Artificial Intelligence has depicted a decision network, although some
artistic license appears to have been taken with the direction of the arrows.

640

Chapter

16.

Making Simple Decisions

E XERCISES
16.1 (Adapted from David Heckerman.) This exercise concerns the Almanac Game, which
is used by decision analysts to calibrate numeric estimation. For each of the questions that
follow, give your best guess of the answer, that is, a number that you think is as likely to be
too high as it is to be too low. Also give your guess at a 25th percentile estimate, that is, a
number that you think has a 25% chance of being too high, and a 75% chance of being too
low. Do the same for the 75th percentile. (Thus, you should give three estimates in allâ€”low,
median, and highâ€”for each question.)
a.
b.
c.
d.
e.
f.
g.
h.
i.
j.

Number of passengers who flew between New York and Los Angeles in 1989.
Population of Warsaw in 1992.
Year in which Coronado discovered the Mississippi River.
Number of votes received by Jimmy Carter in the 1976 presidential election.
Age of the oldest living tree, as of 2002.
Height of the Hoover Dam in feet.
Number of eggs produced in Oregon in 1985.
Number of Buddhists in the world in 1992.
Number of deaths due to AIDS in the United States in 1981.
Number of U.S. patents granted in 1901.

The correct answers appear after the last exercise of this chapter. From the point of view of
decision analysis, the interesting thing is not how close your median guesses came to the real
answers, but rather how often the real answer came within your 25% and 75% bounds. If it
was about half the time, then your bounds are accurate. But if youâ€™re like most people, you
will be more sure of yourself than you should be, and fewer than half the answers will fall
within the bounds. With practice, you can calibrate yourself to give realistic bounds, and thus
be more useful in supplying information for decision making. Try this second set of questions
and see if there is any improvement:
a.
b.
c.
d.
e.
f.
g.
h.
i.
j.

Year of birth of Zsa Zsa Gabor.
Maximum distance from Mars to the sun in miles.
Value in dollars of exports of wheat from the United States in 1992.
Tons handled by the port of Honolulu in 1991.
Annual salary in dollars of the governor of California in 1993.
Population of San Diego in 1990.
Year in which Roger Williams founded Providence, Rhode Island.
Height of Mt. Kilimanjaro in feet.
Length of the Brooklyn Bridge in feet.
Number of deaths due to automobile accidents in the United States in 1992.

Exercises

641
16.2 Chris considers four used cars before buying the one with maximum expected utility.
Pat considers ten cars and does the same. All other things being equal, which one is more
likely to have the better car? Which is more likely to be disappointed with their carâ€™s quality?
By how much (in terms of standard deviations of expected quality)?
16.3 In 1713, Nicolas Bernoulli stated a puzzle, now called the St. Petersburg paradox,
which works as follows. You have the opportunity to play a game in which a fair coin is
tossed repeatedly until it comes up heads. If the first heads appears on the nth toss, you win
2n dollars.
a. Show that the expected monetary value of this game is infinite.
b. How much would you, personally, pay to play the game?
c. Nicolasâ€™s cousin Daniel Bernoulli resolved the apparent paradox in 1738 by suggesting
that the utility of money is measured on a logarithmic scale (i.e., U (Sn ) = a log2 n + b,
where Sn is the state of having $n). What is the expected utility of the game under this
assumption?
d. What is the maximum amount that it would be rational to pay to play the game, assuming that oneâ€™s initial wealth is $k ?
16.4 Write a computer program to automate the process in Exercise 16.9. Try your program out on several people of different net worth and political outlook. Comment on the
consistency of your results, both for an individual and across individuals.
16.5 The Surprise Candy Company makes candy in two flavors: 70% are strawberry flavor and 30% are anchovy flavor. Each new piece of candy starts out with a round shape;
as it moves along the production line, a machine randomly selects a certain percentage to
be trimmed into a square; then, each piece is wrapped in a wrapper whose color is chosen
randomly to be red or brown. 80% of the strawberry candies are round and 80% have a red
wrapper, while 90% of the anchovy candies are square and 90% have a brown wrapper. All
candies are sold individually in sealed, identical, black boxes.
Now you, the customer, have just bought a Surprise candy at the store but have not yet
opened the box. Consider the three Bayes nets in Figure 16.11.
Wrapper

Shape

Shape

Wrapper

Flavor

Flavor

(i)

(ii)

Figure 16.11

Flavor

Wrapper

Shape
(iii)

Three proposed Bayes nets for the Surprise Candy problem, Exercise 16.5.

a. Which network(s) can correctly represent P(F lavor, W rapper, Shape)?
b. Which network is the best representation for this problem?

642

Chapter

16.

Making Simple Decisions

c. Does network (i) assert that P(W rapper|Shape) = P(W rapper)?
d. What is the probability that your candy has a red wrapper?
e. In the box is a round candy with a red wrapper. What is the probability that its flavor is
strawberry?
f. A unwrapped strawberry candy is worth s on the open market and an unwrapped anchovy candy is worth a. Write an expression for the value of an unopened candy box.
g. A new law prohibits trading of unwrapped candies, but it is still legal to trade wrapped
candies (out of the box). Is an unopened candy box now worth more than less than, or
the same as before?
16.6 Prove that the judgments B ' A and C ' D in the Allais paradox (page 620) violate
the axiom of substitutability.
16.7 Consider the Allais paradox described on page 620: an agent who prefers B over
A (taking the sure thing), and C over D (taking the higher EMV) is not acting rationally,
according to utility theory. Do you think this indicates a problem for the agent, a problem for
the theory, or no problem at all? Explain.
16.8 Tickets to a lottery cost $1. There are two possible prizes: a $10 payoff with probability 1/50, and a $1,000,000 payoff with probability 1/2,000,000. What is the expected monetary value of a lottery ticket? When (if ever) is it rational to buy a ticket? Be preciseâ€”show an
equation involving utilities. You may assume current wealth of $k and that U (Sk ) = 0. You
may also assume that U (Sk+10 ) = 10 Ã— U (Sk+1 ), but you may not make any assumptions
about U (Sk+1,000,000 ). Sociological studies show that people with lower income buy a disproportionate number of lottery tickets. Do you think this is because they are worse decision
makers or because they have a different utility function? Consider the value of contemplating
the possibility of winning the lottery versus the value of contemplating becoming an action
hero while watching an adventure movie.
16.9 Assess your own utility for different incremental amounts of money by running a series
of preference tests between some definite amount M1 and a lottery [p, M2 ; (1âˆ’p), 0]. Choose
different values of M1 and M2 , and vary p until you are indifferent between the two choices.
Plot the resulting utility function.
16.10 How much is a micromort worth to you? Devise a protocol to determine this. Ask
questions based both on paying to avoid risk and being paid to accept risk.
16.11 Let continuous variables X1 , . . . , Xk be independently distributed according to the
same probability density function f (x). Prove that the density function for max{X1 , . . . , Xk }
is given by kf (x)(F (x))kâˆ’1 , where F is the cumulative distribution for f .
16.12 Economists often make use of an exponential utility function for money: U (x) =
âˆ’ex/R , where R is a positive constant representing an individualâ€™s risk tolerance. Risk tolerance reflects how likely an individual is to accept a lottery with a particular expected monetary
value (EMV) versus some certain payoff. As R (which is measured in the same units as x)
becomes larger, the individual becomes less risk-averse.

Exercises

643
a. Assume Mary has an exponential utility function with R = $500. Mary is given the
choice between receiving $500 with certainty (probability 1) or participating in a lottery which has a 60% probability of winning $5000 and a 40% probability of winning
nothing. Assuming Marry acts rationally, which option would she choose? Show how
you derived your answer.
b. Consider the choice between receiving $100 with certainty (probability 1) or participating in a lottery which has a 50% probability of winning $500 and a 50% probability of
winning nothing. Approximate the value of R (to 3 significant digits) in an exponential
utility function that would cause an individual to be indifferent to these two alternatives.
(You might find it helpful to write a short program to help you solve this problem.)
16.13

Repeat Exercise 16.16, using the action-utility representation shown in Figure 16.7.

16.14 For either of the airport-siting diagrams from Exercises 16.16 and 16.13, to which
conditional probability table entry is the utility most sensitive, given the available evidence?
16.15 Consider a student who has the choice to buy or not buy a textbook for a course. Weâ€™ll
model this as a decision problem with one Boolean decision node, B, indicating whether the
agent chooses to buy the book, and two Boolean chance nodes, M , indicating whether the
student has mastered the material in the book, and P , indicating whether the student passes
the course. Of course, there is also a utility node, U . A certain student, Sam, has an additive
utility function: 0 for not buying the book and -$100 for buying it; and $2000 for passing the
course and 0 for not passing. Samâ€™s conditional probability estimates are as follows:
P (p|b, m) = 0.9
P (m|b) = 0.9
P (p|b, Â¬m) = 0.5 P (m|Â¬b) = 0.7
P (p|Â¬b, m) = 0.8
P (p|Â¬b, Â¬m) = 0.3
You might think that P would be independent of B given M , But this course has an openbook finalâ€”so having the book helps.
a. Draw the decision network for this problem.
b. Compute the expected utility of buying the book and of not buying it.
c. What should Sam do?
16.16

This exercise completes the analysis of the airport-siting problem in Figure 16.6.

a. Provide reasonable variable domains, probabilities, and utilities for the network, assuming that there are three possible sites.
b. Solve the decision problem.
c. What happens if changes in technology mean that each aircraft generates half the noise?
d. What if noise avoidance becomes three times more important?
e. Calculate the VPI for AirTraï¬ƒc, Litigation, and Construction in your model.

644

Chapter

16.

Making Simple Decisions

16.17 (Adapted from Pearl (1988).) A used-car buyer can decide to carry out various tests
with various costs (e.g., kick the tires, take the car to a qualified mechanic) and then, depending on the outcome of the tests, decide which car to buy. We will assume that the buyer is
deciding whether to buy car c1 , that there is time to carry out at most one test, and that t1 is
the test of c1 and costs $50.
A car can be in good shape (quality q + ) or bad shape (quality q âˆ’ ), and the tests might
help indicate what shape the car is in. Car c1 costs $1,500, and its market value is $2,000 if it
is in good shape; if not, $700 in repairs will be needed to make it in good shape. The buyerâ€™s
estimate is that c1 has a 70% chance of being in good shape.
a. Draw the decision network that represents this problem.
b. Calculate the expected net gain from buying c1 , given no test.
c. Tests can be described by the probability that the car will pass or fail the test given that
the car is in good or bad shape. We have the following information:
P (pass(c1 , t1 )|q + (c1 )) = 0.8
P (pass(c1 , t1 )|q âˆ’ (c1 )) = 0.35
Use Bayesâ€™ theorem to calculate the probability that the car will pass (or fail) its test and
hence the probability that it is in good (or bad) shape given each possible test outcome.
d. Calculate the optimal decisions given either a pass or a fail, and their expected utilities.
e. Calculate the value of information of the test, and derive an optimal conditional plan
for the buyer.
16.18

SUBMODULARITY

Recall the definition of value of information in Section 16.6.

a. Prove that the value of information is nonnegative and order independent.
b. Explain why it is that some people would prefer not to get some informationâ€”for example, not wanting to know the sex of their baby when an ultrasound is done.
c. A function f on sets is submodular if, for any element x and any sets A and B such
that A âŠ† B, adding x to A gives a greater increase in f than adding x to B:
A âŠ† B â‡’ (f (A âˆª {x}) âˆ’ f (A)) â‰¥ (f (B âˆª {x}) âˆ’ f (B)) .
Submodularity captures the intuitive notion of diminishing returns. Is the value of information, viewed as a function f on sets of possible observations, submodular? Prove
this or find a counterexample.
The answers to Exercise 16.1 (where M stands for million): First set: 3M, 1.6M, 1541, 41M,
4768, 221, 649M, 295M, 132, 25,546. Second set: 1917, 155M, 4,500M, 11M, 120,000,
1.1M, 1636, 19,340, 1,595, 41,710.

17

MAKING COMPLEX
DECISIONS

In which we examine methods for deciding what to do today, given that we may
decide again tomorrow.

SEQUENTIAL
DECISION PROBLEM

17.1

In this chapter, we address the computational issues involved in making decisions in a stochastic environment. Whereas Chapter 16 was concerned with one-shot or episodic decision
problems, in which the utility of each actionâ€™s outcome was well known, we are concerned
here with sequential decision problems, in which the agentâ€™s utility depends on a sequence
of decisions. Sequential decision problems incorporate utilities, uncertainty, and sensing,
and include search and planning problems as special cases. Section 17.1 explains how sequential decision problems are defined, and Sections 17.2 and 17.3 explain how they can
be solved to produce optimal behavior that balances the risks and rewards of acting in an
uncertain environment. Section 17.4 extends these ideas to the case of partially observable
environments, and Section 17.4.3 develops a complete design for decision-theoretic agents in
partially observable environments, combining dynamic Bayesian networks from Chapter 15
with decision networks from Chapter 16.
The second part of the chapter covers environments with multiple agents. In such environments, the notion of optimal behavior is complicated by the interactions among the
agents. Section 17.5 introduces the main ideas of game theory, including the idea that rational agents might need to behave randomly. Section 17.6 looks at how multiagent systems
can be designed so that multiple agents can achieve a common goal.

S EQUENTIAL D ECISION P ROBLEMS
Suppose that an agent is situated in the 4 Ã— 3 environment shown in Figure 17.1(a). Beginning
in the start state, it must choose an action at each time step. The interaction with the environment terminates when the agent reaches one of the goal states, marked +1 or â€“1. Just as for
search problems, the actions available to the agent in each state are given by ACTIONS (s),
sometimes abbreviated to A(s); in the 4 Ã— 3 environment, the actions in every state are Up,
Down, Left, and Right. We assume for now that the environment is fully observable, so that
the agent always knows where it is.
645

646

Chapter

3

+1

2

â€“1

1

17.

Making Complex Decisions

0.8
0.1

0.1

START

1

2

3

(a)

4

(b)

Figure 17.1 (a) A simple 4 Ã— 3 environment that presents the agent with a sequential
decision problem. (b) Illustration of the transition model of the environment: the â€œintendedâ€
outcome occurs with probability 0.8, but with probability 0.2 the agent moves at right angles
to the intended direction. A collision with a wall results in no movement. The two terminal
states have reward +1 and â€“1, respectively, and all other states have a reward of â€“0.04.

REWARD

If the environment were deterministic, a solution would be easy: [Up, Up, Right, Right,
Right]. Unfortunately, the environment wonâ€™t always go along with this solution, because the
actions are unreliable. The particular model of stochastic motion that we adopt is illustrated
in Figure 17.1(b). Each action achieves the intended effect with probability 0.8, but the rest
of the time, the action moves the agent at right angles to the intended direction. Furthermore,
if the agent bumps into a wall, it stays in the same square. For example, from the start square
(1,1), the action Up moves the agent to (1,2) with probability 0.8, but with probability 0.1, it
moves right to (2,1), and with probability 0.1, it moves left, bumps into the wall, and stays in
(1,1). In such an environment, the sequence [Up, Up, Right, Right , Right] goes up around
the barrier and reaches the goal state at (4,3) with probability 0.85 = 0.32768. There is also a
small chance of accidentally reaching the goal by going the other way around with probability
0.14 Ã— 0.8, for a grand total of 0.32776. (See also Exercise 17.1.)
As in Chapter 3, the transition model (or just â€œmodel,â€ whenever no confusion can
arise) describes the outcome of each action in each state. Here, the outcome is stochastic,
so we write P (s | s, a) to denote the probability of reaching state s if action a is done in
state s. We will assume that transitions are Markovian in the sense of Chapter 15, that is, the
probability of reaching s from s depends only on s and not on the history of earlier states. For
now, you can think of P (s | s, a) as a big three-dimensional table containing probabilities.
Later, in Section 17.4.3, we will see that the transition model can be represented as a dynamic
Bayesian network, just as in Chapter 15.
To complete the definition of the task environment, we must specify the utility function
for the agent. Because the decision problem is sequential, the utility function will depend
on a sequence of statesâ€”an environment historyâ€”rather than on a single state. Later in
this section, we investigate how such utility functions can be specified in general; for now,
we simply stipulate that in each state s, the agent receives a reward R(s), which may be
positive or negative, but must be bounded. For our particular example, the reward is âˆ’0.04
in all states except the terminal states (which have rewards +1 and â€“1). The utility of an

Section 17.1.

MARKOV DECISION
PROCESS

POLICY

OPTIMAL POLICY

Sequential Decision Problems

647

environment history is just (for now) the sum of the rewards received. For example, if the
agent reaches the +1 state after 10 steps, its total utility will be 0.6. The negative reward of
â€“0.04 gives the agent an incentive to reach (4,3) quickly, so our environment is a stochastic
generalization of the search problems of Chapter 3. Another way of saying this is that the
agent does not enjoy living in this environment and so wants to leave as soon as possible.
To sum up: a sequential decision problem for a fully observable, stochastic environment
with a Markovian transition model and additive rewards is called a Markov decision process,
or MDP, and consists of a set of states (with an initial state s0 ); a set ACTIONS (s) of actions
in each state; a transition model P (s | s, a); and a reward function R(s).1
The next question is, what does a solution to the problem look like? We have seen that
any fixed action sequence wonâ€™t solve the problem, because the agent might end up in a state
other than the goal. Therefore, a solution must specify what the agent should do for any state
that the agent might reach. A solution of this kind is called a policy. It is traditional to denote
a policy by Ï€, and Ï€(s) is the action recommended by the policy Ï€ for state s. If the agent
has a complete policy, then no matter what the outcome of any action, the agent will always
know what to do next.
Each time a given policy is executed starting from the initial state, the stochastic nature
of the environment may lead to a different environment history. The quality of a policy is
therefore measured by the expected utility of the possible environment histories generated
by that policy. An optimal policy is a policy that yields the highest expected utility. We
use Ï€ âˆ— to denote an optimal policy. Given Ï€ âˆ— , the agent decides what to do by consulting
its current percept, which tells it the current state s, and then executing the action Ï€ âˆ— (s). A
policy represents the agent function explicitly and is therefore a description of a simple reflex
agent, computed from the information used for a utility-based agent.
An optimal policy for the world of Figure 17.1 is shown in Figure 17.2(a). Notice
that, because the cost of taking a step is fairly small compared with the penalty for ending
up in (4,2) by accident, the optimal policy for the state (3,1) is conservative. The policy
recommends taking the long way round, rather than taking the shortcut and thereby risking
entering (4,2).
The balance of risk and reward changes depending on the value of R(s) for the nonterminal states. Figure 17.2(b) shows optimal policies for four different ranges of R(s). When
R(s) â‰¤ âˆ’1.6284, life is so painful that the agent heads straight for the nearest exit, even if
the exit is worth â€“1. When âˆ’0.4278 â‰¤ R(s) â‰¤ âˆ’0.0850, life is quite unpleasant; the agent
takes the shortest route to the +1 state and is willing to risk falling into the â€“1 state by accident. In particular, the agent takes the shortcut from (3,1). When life is only slightly dreary
(âˆ’0.0221 < R(s) < 0), the optimal policy takes no risks at all. In (4,1) and (3,2), the agent
heads directly away from the â€“1 state so that it cannot fall in by accident, even though this
means banging its head against the wall quite a few times. Finally, if R(s) > 0, then life is
positively enjoyable and the agent avoids both exits. As long as the actions in (4,1), (3,2),
1 Some definitions of MDPs allow the reward to depend on the action and outcome too, so the reward function
is R(s, a, s ). This simplifies the description of some environments but does not change the problem in any
fundamental way, as shown in Exercise 17.4.

648

Chapter

3

+1

2

â€“1

17.

Making Complex Decisions

+1

+1

â€“1

â€“1

R(s) < â€“1.6284

â€“ 0.4278 < R(s) < â€“ 0.0850

+1

+1

â€“1

â€“1

1

1

2

3

4

R(s) > 0

â€“ 0.0221 < R(s) < 0
(a)

(b)

Figure 17.2 (a) An optimal policy for the stochastic environment with R(s) = âˆ’ 0.04 in
the nonterminal states. (b) Optimal policies for four different ranges of R(s).

and (3,3) are as shown, every policy is optimal, and the agent obtains infinite total reward because it never enters a terminal state. Surprisingly, it turns out that there are six other optimal
policies for various ranges of R(s); Exercise 17.5 asks you to find them.
The careful balancing of risk and reward is a characteristic of MDPs that does not
arise in deterministic search problems; moreover, it is a characteristic of many real-world
decision problems. For this reason, MDPs have been studied in several fields, including
AI, operations research, economics, and control theory. Dozens of algorithms have been
proposed for calculating optimal policies. In sections 17.2 and 17.3 we describe two of the
most important algorithm families. First, however, we must complete our investigation of
utilities and policies for sequential decision problems.

17.1.1 Utilities over time

FINITE HORIZON
INFINITE HORIZON

In the MDP example in Figure 17.1, the performance of the agent was measured by a sum of
rewards for the states visited. This choice of performance measure is not arbitrary, but it is
not the only possibility for the utility function on environment histories, which we write as
Uh ([s0 , s1 , . . . , sn ]). Our analysis draws on multiattribute utility theory (Section 16.4) and
is somewhat technical; the impatient reader may wish to skip to the next section.
The first question to answer is whether there is a finite horizon or an infinite horizon
for decision making. A finite horizon means that there is a fixed time N after which nothing
mattersâ€”the game is over, so to speak. Thus, Uh ([s0 , s1 , . . . , sN +k ]) = Uh ([s0 , s1 , . . . , sN ])
for all k > 0. For example, suppose an agent starts at (3,1) in the 4 Ã— 3 world of Figure 17.1,
and suppose that N = 3. Then, to have any chance of reaching the +1 state, the agent must
head directly for it, and the optimal action is to go Up. On the other hand, if N = 100,
then there is plenty of time to take the safe route by going Left. So, with a finite horizon,

Section 17.1.

NONSTATIONARY
POLICY

STATIONARY POLICY

STATIONARY
PREFERENCE

ADDITIVE REWARD

Sequential Decision Problems

649

the optimal action in a given state could change over time. We say that the optimal policy
for a finite horizon is nonstationary. With no fixed time limit, on the other hand, there is
no reason to behave differently in the same state at different times. Hence, the optimal action depends only on the current state, and the optimal policy is stationary. Policies for the
infinite-horizon case are therefore simpler than those for the finite-horizon case, and we deal
mainly with the infinite-horizon case in this chapter. (We will see later that for partially observable environments, the infinite-horizon case is not so simple.) Note that â€œinfinite horizonâ€
does not necessarily mean that all state sequences are infinite; it just means that there is no
fixed deadline. In particular, there can be finite state sequences in an infinite-horizon MDP
containing a terminal state.
The next question we must decide is how to calculate the utility of state sequences. In
the terminology of multiattribute utility theory, each state si can be viewed as an attribute of
the state sequence [s0 , s1 , s2 . . .]. To obtain a simple expression in terms of the attributes, we
will need to make some sort of preference-independence assumption. The most natural assumption is that the agentâ€™s preferences between state sequences are stationary. Stationarity
for preferences means the following: if two state sequences [s0 , s1 , s2 , . . .] and [s0 , s1 , s2 , . . .]
begin with the same state (i.e., s0 = s0 ), then the two sequences should be preference-ordered
the same way as the sequences [s1 , s2 , . . .] and [s1 , s2 , . . .]. In English, this means that if you
prefer one future to another starting tomorrow, then you should still prefer that future if it
were to start today instead. Stationarity is a fairly innocuous-looking assumption with very
strong consequences: it turns out that under stationarity there are just two coherent ways to
assign utilities to sequences:
1. Additive rewards: The utility of a state sequence is
Uh ([s0 , s1 , s2 , . . .]) = R(s0 ) + R(s1 ) + R(s2 ) + Â· Â· Â· .
The 4 Ã— 3 world in Figure 17.1 uses additive rewards. Notice that additivity was used
implicitly in our use of path cost functions in heuristic search algorithms (Chapter 3).

DISCOUNTED
REWARD

2. Discounted rewards: The utility of a state sequence is
Uh ([s0 , s1 , s2 , . . .]) = R(s0 ) + Î³R(s1 ) + Î³ 2 R(s2 ) + Â· Â· Â· ,

DISCOUNT FACTOR

where the discount factor Î³ is a number between 0 and 1. The discount factor describes
the preference of an agent for current rewards over future rewards. When Î³ is close
to 0, rewards in the distant future are viewed as insignificant. When Î³ is 1, discounted
rewards are exactly equivalent to additive rewards, so additive rewards are a special
case of discounted rewards. Discounting appears to be a good model of both animal
and human preferences over time. A discount factor of Î³ is equivalent to an interest rate
of (1/Î³) âˆ’ 1.
For reasons that will shortly become clear, we assume discounted rewards in the remainder
of the chapter, although sometimes we allow Î³ = 1.
Lurking beneath our choice of infinite horizons is a problem: if the environment does
not contain a terminal state, or if the agent never reaches one, then all environment histories
will be infinitely long, and utilities with additive, undiscounted rewards will generally be

650

Chapter

17.

Making Complex Decisions

infinite. While we can agree that+âˆž is better than âˆ’âˆž, comparing two state sequences with
+âˆž utility is more difficult. There are three solutions, two of which we have seen already:
1. With discounted rewards, the utility of an infinite sequence is finite. In fact, if Î³ < 1
and rewards are bounded by Â±Rmax , we have
âˆž
âˆž


t
Î³ R(st ) â‰¤
Î³ t Rmax = Rmax /(1 âˆ’ Î³) ,
(17.1)
Uh ([s0 , s1 , s2 , . . .]) =
t=0

PROPER POLICY

AVERAGE REWARD

t=0

using the standard formula for the sum of an infinite geometric series.
2. If the environment contains terminal states and if the agent is guaranteed to get to one
eventually, then we will never need to compare infinite sequences. A policy that is
guaranteed to reach a terminal state is called a proper policy. With proper policies, we
can use Î³ = 1 (i.e., additive rewards). The first three policies shown in Figure 17.2(b)
are proper, but the fourth is improper. It gains infinite total reward by staying away from
the terminal states when the reward for the nonterminal states is positive. The existence
of improper policies can cause the standard algorithms for solving MDPs to fail with
additive rewards, and so provides a good reason for using discounted rewards.
3. Infinite sequences can be compared in terms of the average reward obtained per time
step. Suppose that square (1,1) in the 4 Ã— 3 world has a reward of 0.1 while the other
nonterminal states have a reward of 0.01. Then a policy that does its best to stay in
(1,1) will have higher average reward than one that stays elsewhere. Average reward is
a useful criterion for some problems, but the analysis of average-reward algorithms is
beyond the scope of this book.
In sum, discounted rewards present the fewest difficulties in evaluating state sequences.

17.1.2 Optimal policies and the utilities of states
Having decided that the utility of a given state sequence is the sum of discounted rewards
obtained during the sequence, we can compare policies by comparing the expected utilities
obtained when executing them. We assume the agent is in some initial state s and define St
(a random variable) to be the state the agent reaches at time t when executing a particular
policy Ï€. (Obviously, S0 = s, the state the agent is in now.) The probability distribution over
state sequences S1 , S2 , . . . , is determined by the initial state s, the policy Ï€, and the transition
model for the environment.
The expected utility obtained by executing Ï€ starting in s is given by
"âˆž
#

Ï€
t
Î³ R(St ) ,
(17.2)
U (s) = E
t=0

where the expectation is with respect to the probability distribution over state sequences determined by s and Ï€. Now, out of all the policies the agent could choose to execute starting in
s, one (or more) will have higher expected utilities than all the others. Weâ€™ll use Ï€sâˆ— to denote
one of these policies:
Ï€sâˆ— = argmax U Ï€ (s) .
Ï€

(17.3)

Section 17.1.

Sequential Decision Problems

651

Remember that Ï€sâˆ— is a policy, so it recommends an action for every state; its connection
with s in particular is that itâ€™s an optimal policy when s is the starting state. A remarkable
consequence of using discounted utilities with infinite horizons is that the optimal policy is
independent of the starting state. (Of course, the action sequence wonâ€™t be independent;
remember that a policy is a function specifying an action for each state.) This fact seems
intuitively obvious: if policy Ï€aâˆ— is optimal starting in a and policy Ï€bâˆ— is optimal starting in b,
then, when they reach a third state c, thereâ€™s no good reason for them to disagree with each
other, or with Ï€câˆ— , about what to do next.2 So we can simply write Ï€ âˆ— for an optimal policy.
âˆ—
Given this definition, the true utility of a state is just U Ï€ (s)â€”that is, the expected
sum of discounted rewards if the agent executes an optimal policy. We write this as U (s),
matching the notation used in Chapter 16 for the utility of an outcome. Notice that U (s) and
R(s) are quite different quantities; R(s) is the â€œshort termâ€ reward for being in s, whereas
U (s) is the â€œlong termâ€ total reward from s onward. Figure 17.3 shows the utilities for the
4 Ã— 3 world. Notice that the utilities are higher for states closer to the +1 exit, because fewer
steps are required to reach the exit.

3

0.812

2

0.762

1

0.705

0.655

0.611

0.388

1

2

3

4

0.868

0.918

+1

0.660

â€“1

Figure 17.3 The utilities of the states in the 4 Ã— 3 world, calculated with Î³ = 1 and
R(s) = âˆ’ 0.04 for nonterminal states.

The utility function U (s) allows the agent to select actions by using the principle of
maximum expected utility from Chapter 16â€”that is, choose the action that maximizes the
expected utility of the subsequent state:
Ï€ âˆ— (s) = argmax
aâˆˆA(s)



P (s | s, a)U (s ) .

(17.4)

s

The next two sections describe algorithms for finding optimal policies.
2 Although this seems obvious, it does not hold for finite-horizon policies or for other ways of combining
rewards over time. The proof follows directly from the uniqueness of the utility function on states, as shown in
Section 17.2.

652

17.2

Chapter

17.

Making Complex Decisions

VALUE I TERATION

VALUE ITERATION

In this section, we present an algorithm, called value iteration, for calculating an optimal
policy. The basic idea is to calculate the utility of each state and then use the state utilities to
select an optimal action in each state.

17.2.1 The Bellman equation for utilities
Section 17.1.2 defined the utility of being in a state as the expected sum of discounted rewards
from that point onwards. From this, it follows that there is a direct relationship between the
utility of a state and the utility of its neighbors: the utility of a state is the immediate reward
for that state plus the expected discounted utility of the next state, assuming that the agent
chooses the optimal action. That is, the utility of a state is given by

P (s | s, a)U (s ) .
(17.5)
U (s) = R(s) + Î³ max
aâˆˆA(s)

BELLMAN EQUATION

s

This is called the Bellman equation, after Richard Bellman (1957). The utilities of the
statesâ€”defined by Equation (17.2) as the expected utility of subsequent state sequencesâ€”are
solutions of the set of Bellman equations. In fact, they are the unique solutions, as we show
in Section 17.2.3.
Let us look at one of the Bellman equations for the 4 Ã— 3 world. The equation for the
state (1,1) is
U (1, 1) = âˆ’0.04 + Î³ max[ 0.8U (1, 2) + 0.1U (2, 1) + 0.1U (1, 1),
0.9U (1, 1) + 0.1U (1, 2),
0.9U (1, 1) + 0.1U (2, 1),
0.8U (2, 1) + 0.1U (1, 2) + 0.1U (1, 1) ].

(Up)
(Left )
(Down)
(Right )

When we plug in the numbers from Figure 17.3, we find that Up is the best action.

17.2.2 The value iteration algorithm

BELLMAN UPDATE

The Bellman equation is the basis of the value iteration algorithm for solving MDPs. If there
are n possible states, then there are n Bellman equations, one for each state. The n equations
contain n unknownsâ€”the utilities of the states. So we would like to solve these simultaneous
equations to find the utilities. There is one problem: the equations are nonlinear, because the
â€œmaxâ€ operator is not a linear operator. Whereas systems of linear equations can be solved
quickly using linear algebra techniques, systems of nonlinear equations are more problematic.
One thing to try is an iterative approach. We start with arbitrary initial values for the utilities,
calculate the right-hand side of the equation, and plug it into the left-hand sideâ€”thereby
updating the utility of each state from the utilities of its neighbors. We repeat this until we
reach an equilibrium. Let Ui (s) be the utility value for state s at the ith iteration. The iteration
step, called a Bellman update, looks like this:

Ui+1 (s) â† R(s) + Î³ max
P (s | s, a)Ui (s ) ,
(17.6)
aâˆˆA(s)

s

Value Iteration

653

function VALUE -I TERATION(mdp, ) returns a utility function
inputs: mdp, an MDP with states S , actions A(s), transition model P (s | s, a),
rewards R(s), discount Î³
, the maximum error allowed in the utility of any state
local variables: U , U  , vectors of utilities for states in S , initially zero
Î´, the maximum change in the utility of any state in an iteration
repeat
U â† U ; Î´ â† 0
for each state s in S do

P (s | s, a) U [s ]
U  [s] â† R(s) + Î³ max
a âˆˆ A(s)



s

if |U [s] âˆ’ U [s]| > Î´ then Î´ â† |U  [s] âˆ’ U [s]|
until Î´ < (1 âˆ’ Î³)/Î³
return U
Figure 17.4 The value iteration algorithm for calculating utilities of states. The termination condition is from Equation (17.8).
1e+07

(4,3)
(3,3)

0.8
0.6

(1,1)
(3,1)

0.4

(4,1)

0.2
0

1e+06
Iterations required

1
Utility estimates

Section 17.2.

100000

c = 0.0001
c = 0.001
c = 0.01
c = 0.1

10000
1000
100
10

-0.2
1
0

5

10
15
20
25
Number of iterations

(a)

30

0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Discount factor Î³

(b)

Figure 17.5 (a) Graph showing the evolution of the utilities of selected states using value
iteration. (b) The number of value iterations k required to guarantee an error of at most
 = c Â· Rmax , for different values of c, as a function of the discount factor Î³.

where the update is assumed to be applied simultaneously to all the states at each iteration.
If we apply the Bellman update infinitely often, we are guaranteed to reach an equilibrium
(see Section 17.2.3), in which case the final utility values must be solutions to the Bellman
equations. In fact, they are also the unique solutions, and the corresponding policy (obtained
using Equation (17.4)) is optimal. The algorithm, called VALUE -I TERATION , is shown in
Figure 17.4.
We can apply value iteration to the 4 Ã— 3 world in Figure 17.1(a). Starting with initial
values of zero, the utilities evolve as shown in Figure 17.5(a). Notice how the states at differ-

654

Chapter

17.

Making Complex Decisions

ent distances from (4,3) accumulate negative reward until a path is found to (4,3), whereupon
the utilities start to increase. We can think of the value iteration algorithm as propagating
information through the state space by means of local updates.

17.2.3 Convergence of value iteration

CONTRACTION

We said that value iteration eventually converges to a unique set of solutions of the Bellman
equations. In this section, we explain why this happens. We introduce some useful mathematical ideas along the way, and we obtain some methods for assessing the error in the utility
function returned when the algorithm is terminated early; this is useful because it means that
we donâ€™t have to run forever. This section is quite technical.
The basic concept used in showing that value iteration converges is the notion of a contraction. Roughly speaking, a contraction is a function of one argument that, when applied to
two different inputs in turn, produces two output values that are â€œcloser together,â€ by at least
some constant factor, than the original inputs. For example, the function â€œdivide by twoâ€ is
a contraction, because, after we divide any two numbers by two, their difference is halved.
Notice that the â€œdivide by twoâ€ function has a fixed point, namely zero, that is unchanged by
the application of the function. From this example, we can discern two important properties
of contractions:
â€¢ A contraction has only one fixed point; if there were two fixed points they would not
get closer together when the function was applied, so it would not be a contraction.
â€¢ When the function is applied to any argument, the value must get closer to the fixed
point (because the fixed point does not move), so repeated application of a contraction
always reaches the fixed point in the limit.
Now, suppose we view the Bellman update (Equation (17.6)) as an operator B that is applied
simultaneously to update the utility of every state. Let Ui denote the vector of utilities for all
the states at the ith iteration. Then the Bellman update equation can be written as
Ui+1 â† B Ui .

MAX NORM

Next, we need a way to measure distances between utility vectors. We will use the max norm,
which measures the â€œlengthâ€ of a vector by the absolute value of its biggest component:
||U || = max |U (s)| .
s

With this definition, the â€œdistanceâ€ between two vectors, ||U âˆ’ U  ||, is the maximum difference between any two corresponding elements. The main result of this section is the
following: Let Ui and Ui be any two utility vectors. Then we have
||B Ui âˆ’ B Ui || â‰¤ Î³ ||Ui âˆ’ Ui || .

(17.7)

That is, the Bellman update is a contraction by a factor of Î³ on the space of utility vectors.
(Exercise 17.6 provides some guidance on proving this claim.) Hence, from the properties of
contractions in general, it follows that value iteration always converges to a unique solution
of the Bellman equations whenever Î³ < 1.

Section 17.2.

Value Iteration

655

We can also use the contraction property to analyze the rate of convergence to a solution. In particular, we can replace Ui in Equation (17.7) with the true utilities U , for which
B U = U . Then we obtain the inequality
||B Ui âˆ’ U || â‰¤ Î³ ||Ui âˆ’ U || .
So, if we view ||Ui âˆ’ U || as the error in the estimate Ui , we see that the error is reduced by a
factor of at least Î³ on each iteration. This means that value iteration converges exponentially
fast. We can calculate the number of iterations required to reach a specified error bound 
as follows: First, recall from Equation (17.1) that the utilities of all states are bounded by
Â±Rmax /(1 âˆ’ Î³). This means that the maximum initial error ||U0 âˆ’ U || â‰¤ 2Rmax /(1 âˆ’ Î³).
Suppose we run for N iterations to reach an error of at most . Then, because the error is
reduced by at least Î³ each time, we require Î³ N Â· 2Rmax /(1 âˆ’ Î³) â‰¤ . Taking logs, we find
N = (log(2Rmax /(1 âˆ’ Î³))/ log(1/Î³))
iterations suffice. Figure 17.5(b) shows how N varies with Î³, for different values of the ratio
/Rmax . The good news is that, because of the exponentially fast convergence, N does not
depend much on the ratio /Rmax . The bad news is that N grows rapidly as Î³ becomes close
to 1. We can get fast convergence if we make Î³ small, but this effectively gives the agent a
short horizon and could miss the long-term effects of the agentâ€™s actions.
The error bound in the preceding paragraph gives some idea of the factors influencing
the run time of the algorithm, but is sometimes overly conservative as a method of deciding
when to stop the iteration. For the latter purpose, we can use a bound relating the error
to the size of the Bellman update on any given iteration. From the contraction property
(Equation (17.7)), it can be shown that if the update is small (i.e., no stateâ€™s utility changes by
much), then the error, compared with the true utility function, also is small. More precisely,
if

POLICY LOSS

||Ui+1 âˆ’ Ui || < (1 âˆ’ Î³)/Î³

then

||Ui+1 âˆ’ U || <  .

(17.8)

This is the termination condition used in the VALUE -I TERATION algorithm of Figure 17.4.
So far, we have analyzed the error in the utility function returned by the value iteration
algorithm. What the agent really cares about, however, is how well it will do if it makes its
decisions on the basis of this utility function. Suppose that after i iterations of value iteration,
the agent has an estimate Ui of the true utility U and obtains the MEU policy Ï€i based on
one-step look-ahead using Ui (as in Equation (17.4)). Will the resulting behavior be nearly
as good as the optimal behavior? This is a crucial question for any real agent, and it turns out
that the answer is yes. U Ï€i (s) is the utility obtained if Ï€i is executed starting in s, and the
policy loss ||U Ï€i âˆ’ U || is the most the agent can lose by executing Ï€i instead of the optimal
policy Ï€ âˆ— . The policy loss of Ï€i is connected to the error in Ui by the following inequality:
if

||Ui âˆ’ U || < 

then

||U Ï€i âˆ’ U || < 2Î³/(1 âˆ’ Î³) .

(17.9)

In practice, it often occurs that Ï€i becomes optimal long before Ui has converged. Figure 17.6
shows how the maximum error in Ui and the policy loss approach zero as the value iteration
process proceeds for the 4 Ã— 3 environment with Î³ = 0.9. The policy Ï€i is optimal when i = 4,
even though the maximum error in Ui is still 0.46.
Now we have everything we need to use value iteration in practice. We know that
it converges to the correct utilities, we can bound the error in the utility estimates if we

656

Chapter

17.

Making Complex Decisions

stop after a finite number of iterations, and we can bound the policy loss that results from
executing the corresponding MEU policy. As a final note, all of the results in this section
depend on discounting with Î³ < 1. If Î³ = 1 and the environment contains terminal states,
then a similar set of convergence results and error bounds can be derived whenever certain
technical conditions are satisfied.

P OLICY I TERATION

POLICY ITERATION

POLICY EVALUATION

POLICY
IMPROVEMENT

In the previous section, we observed that it is possible to get an optimal policy even when
the utility function estimate is inaccurate. If one action is clearly better than all others, then
the exact magnitude of the utilities on the states involved need not be precise. This insight
suggests an alternative way to find optimal policies. The policy iteration algorithm alternates
the following two steps, beginning from some initial policy Ï€0 :
â€¢ Policy evaluation: given a policy Ï€i , calculate Ui = U Ï€i , the utility of each state if Ï€i
were to be executed.
â€¢ Policy improvement: Calculate a new MEU policy Ï€i+1 , using one-step look-ahead
based on Ui (as in Equation (17.4)).
The algorithm terminates when the policy improvement step yields no change in the utilities.
At this point, we know that the utility function Ui is a fixed point of the Bellman update, so
it is a solution to the Bellman equations, and Ï€i must be an optimal policy. Because there are
only finitely many policies for a finite state space, and each iteration can be shown to yield a
better policy, policy iteration must terminate. The algorithm is shown in Figure 17.7.
The policy improvement step is obviously straightforward, but how do we implement
the P OLICY-E VALUATION routine? It turns out that doing so is much simpler than solving
the standard Bellman equations (which is what value iteration does), because the action in
each state is fixed by the policy. At the ith iteration, the policy Ï€i specifies the action Ï€i (s) in
1
Max error/Policy loss

17.3

Max error
Policy loss

0.8
0.6
0.4
0.2
0
0

2

4
6
8
10
12
Number of iterations

14

Figure 17.6 The maximum error ||Ui âˆ’ U || of the utility estimates and the policy loss
||U Ï€i âˆ’ U ||, as a function of the number of iterations of value iteration.

Section 17.3.

Policy Iteration

657

state s. This means that we have a simplified version of the Bellman equation (17.5) relating
the utility of s (under Ï€i ) to the utilities of its neighbors:

P (s | s, Ï€i (s))Ui (s ) .
(17.10)
Ui (s) = R(s) + Î³
s

For example, suppose Ï€i is the policy shown in Figure 17.2(a). Then we have Ï€i (1, 1) = Up,
Ï€i (1, 2) = Up, and so on, and the simplified Bellman equations are
Ui (1, 1) = âˆ’0.04 + 0.8Ui (1, 2) + 0.1Ui (1, 1) + 0.1Ui (2, 1) ,
Ui (1, 2) = âˆ’0.04 + 0.8Ui (1, 3) + 0.2Ui (1, 2) ,
..
.
The important point is that these equations are linear, because the â€œmaxâ€ operator has been
removed. For n states, we have n linear equations with n unknowns, which can be solved
exactly in time O(n3 ) by standard linear algebra methods.
For small state spaces, policy evaluation using exact solution methods is often the most
efficient approach. For large state spaces, O(n3 ) time might be prohibitive. Fortunately, it
is not necessary to do exact policy evaluation. Instead, we can perform some number of
simplified value iteration steps (simplified because the policy is fixed) to give a reasonably
good approximation of the utilities. The simplified Bellman update for this process is

P (s | s, Ï€i (s))Ui (s ) ,
Ui+1 (s) â† R(s) + Î³
s
MODIFIED POLICY
ITERATION

and this is repeated k times to produce the next utility estimate. The resulting algorithm is
called modified policy iteration. It is often much more efficient than standard policy iteration
or value iteration.

function P OLICY-I TERATION(mdp) returns a policy
inputs: mdp, an MDP with states S , actions A(s), transition model P (s | s, a)
local variables: U , a vector of utilities for states in S , initially zero
Ï€, a policy vector indexed by state, initially random
repeat
U â† P OLICY-E VALUATION(Ï€, U , mdp)
unchanged ? â† true
for each state s
in S do

P (s | s, a) U [s ] >
P (s | s, Ï€[s]) U [s ] then do
if max
a âˆˆ A(s)
s
s

Ï€[s] â† argmax
P (s | s, a) U [s ]
a âˆˆ A(s)

s

unchanged ? â† false
until unchanged ?
return Ï€
Figure 17.7

The policy iteration algorithm for calculating an optimal policy.

658

Chapter

ASYNCHRONOUS
POLICY ITERATION

17.4

17.

Making Complex Decisions

The algorithms we have described so far require updating the utility or policy for all
states at once. It turns out that this is not strictly necessary. In fact, on each iteration, we
can pick any subset of states and apply either kind of updating (policy improvement or simplified value iteration) to that subset. This very general algorithm is called asynchronous
policy iteration. Given certain conditions on the initial policy and initial utility function,
asynchronous policy iteration is guaranteed to converge to an optimal policy. The freedom
to choose any states to work on means that we can design much more efficient heuristic
algorithmsâ€”for example, algorithms that concentrate on updating the values of states that
are likely to be reached by a good policy. This makes a lot of sense in real life: if one has no
intention of throwing oneself off a cliff, one should not spend time worrying about the exact
value of the resulting states.

PARTIALLY O BSERVABLE MDP S

PARTIALLY
OBSERVABLE MDP

The description of Markov decision processes in Section 17.1 assumed that the environment
was fully observable. With this assumption, the agent always knows which state it is in.
This, combined with the Markov assumption for the transition model, means that the optimal
policy depends only on the current state. When the environment is only partially observable,
the situation is, one might say, much less clear. The agent does not necessarily know which
state it is in, so it cannot execute the action Ï€(s) recommended for that state. Furthermore, the
utility of a state s and the optimal action in s depend not just on s, but also on how much the
agent knows when it is in s. For these reasons, partially observable MDPs (or POMDPsâ€”
pronounced â€œpom-dee-peesâ€) are usually viewed as much more difficult than ordinary MDPs.
We cannot avoid POMDPs, however, because the real world is one.

17.4.1 Definition of POMDPs
To get a handle on POMDPs, we must first define them properly. A POMDP has the same
elements as an MDPâ€”the transition model P (s | s, a), actions A(s), and reward function
R(s)â€”but, like the partially observable search problems of Section 4.4, it also has a sensor
model P (e | s). Here, as in Chapter 15, the sensor model specifies the probability of perceiving evidence e in state s.3 For example, we can convert the 4 Ã— 3 world of Figure 17.1 into
a POMDP by adding a noisy or partial sensor instead of assuming that the agent knows its
location exactly. Such a sensor might measure the number of adjacent walls, which happens
to be 2 in all the nonterminal squares except for those in the third column, where the value
is 1; a noisy version might give the wrong value with probability 0.1.
In Chapters 4 and 11, we studied nondeterministic and partially observable planning
problems and identified the belief stateâ€”the set of actual states the agent might be inâ€”as a
key concept for describing and calculating solutions. In POMDPs, the belief state b becomes a
probability distribution over all possible states, just as in Chapter 15. For example, the initial
3

As with the reward function for MDPs, the sensor model can also depend on the action and outcome state, but
again this change is not fundamental.

Section 17.4.

Partially Observable MDPs

659

belief state for the 4 Ã— 3 POMDP could be the uniform distribution over the nine nonterminal
states, i.e.,  19 , 19 , 19 , 19 , 19 , 19 , 19 , 19 , 19 , 0, 0. We write b(s) for the probability assigned to the
actual state s by belief state b. The agent can calculate its current belief state as the conditional
probability distribution over the actual states given the sequence of percepts and actions so
far. This is essentially the filtering task described in Chapter 15. The basic recursive filtering
equation (15.5 on page 572) shows how to calculate the new belief state from the previous
belief state and the new evidence. For POMDPs, we also have an action to consider, but the
result is essentially the same. If b(s) was the previous belief state, and the agent does action
a and then perceives evidence e, then the new belief state is given by

P (s | s, a)b(s) ,
b (s ) = Î± P (e | s )
s

where Î± is a normalizing constant that makes the belief state sum to 1. By analogy with the
update operator for filtering (page 572), we can write this as
b = F ORWARD(b, a, e) .

(17.11)

In the 4 Ã— 3 POMDP, suppose the agent moves Left and its sensor reports 1 adjacent wall; then
itâ€™s quite likely (although not guaranteed, because both the motion and the sensor are noisy)
that the agent is now in (3,1). Exercise 17.13 asks you to calculate the exact probability values
for the new belief state.
The fundamental insight required to understand POMDPs is this: the optimal action
depends only on the agentâ€™s current belief state. That is, the optimal policy can be described
by a mapping Ï€ âˆ— (b) from belief states to actions. It does not depend on the actual state the
agent is in. This is a good thing, because the agent does not know its actual state; all it knows
is the belief state. Hence, the decision cycle of a POMDP agent can be broken down into the
following three steps:
1. Given the current belief state b, execute the action a = Ï€ âˆ— (b).
2. Receive percept e.
3. Set the current belief state to F ORWARD(b, a, e) and repeat.
Now we can think of POMDPs as requiring a search in belief-state space, just like the methods for sensorless and contingency problems in Chapter 4. The main difference is that the
POMDP belief-state space is continuous, because a POMDP belief state is a probability distribution. For example, a belief state for the 4 Ã— 3 world is a point in an 11-dimensional
continuous space. An action changes the belief state, not just the physical state. Hence, the
action is evaluated at least in part according to the information the agent acquires as a result.
POMDPs therefore include the value of information (Section 16.6) as one component of the
decision problem.
Letâ€™s look more carefully at the outcome of actions. In particular, letâ€™s calculate the
probability that an agent in belief state b reaches belief state b after executing action a. Now,
if we knew the action and the subsequent percept, then Equation (17.11) would provide a
deterministic update to the belief state: b = F ORWARD (b, a, e). Of course, the subsequent
percept is not yet known, so the agent might arrive in one of several possible belief states b ,
depending on the percept that is received. The probability of perceiving e, given that a was

660

Chapter

17.

Making Complex Decisions

performed starting in belief state b, is given by summing over all the actual states s that the
agent might reach:

P (e|a, s , b)P (s |a, b)
P (e|a, b) =
s

=



P (e | s )P (s |a, b)

s

=



P (e | s )

s



P (s | s, a)b(s) .

s

Let us write the probability of reaching b from b, given action a, as P (b | b, a)). Then that
gives us

P (b |e, a, b)P (e|a, b)
P (b | b, a) = P (b |a, b) =
=


e

e


P (b |e, a, b)



P (e | s )

s



P (s | s, a)b(s) ,

(17.12)

s

where P (b |e, a, b) is 1 if b = F ORWARD (b, a, e) and 0 otherwise.
Equation (17.12) can be viewed as defining a transition model for the belief-state space.
We can also define a reward function for belief states (i.e., the expected reward for the actual
states the agent might be in):

b(s)R(s) .
Ï(b) =
s

P (b | b, a)

and Ï(b) define an observable MDP on the space of belief states. FurTogether,
thermore, it can be shown that an optimal policy for this MDP, Ï€ âˆ— (b), is also an optimal policy
for the original POMDP. In other words, solving a POMDP on a physical state space can be
reduced to solving an MDP on the corresponding belief-state space. This fact is perhaps less
surprising if we remember that the belief state is always observable to the agent, by definition.
Notice that, although we have reduced POMDPs to MDPs, the MDP we obtain has a
continuous (and usually high-dimensional) state space. None of the MDP algorithms described in Sections 17.2 and 17.3 applies directly to such MDPs. The next two subsections describe a value iteration algorithm designed specifically for POMDPs and an online
decision-making algorithm, similar to those developed for games in Chapter 5.

17.4.2 Value iteration for POMDPs
Section 17.2 described a value iteration algorithm that computed one utility value for each
state. With infinitely many belief states, we need to be more creative. Consider an optimal
policy Ï€ âˆ— and its application in a specific belief state b: the policy generates an action, then,
for each subsequent percept, the belief state is updated and a new action is generated, and so
on. For this specific b, therefore, the policy is exactly equivalent to a conditional plan, as defined in Chapter 4 for nondeterministic and partially observable problems. Instead of thinking
about policies, let us think about conditional plans and how the expected utility of executing
a fixed conditional plan varies with the initial belief state. We make two observations:

Section 17.4.

Partially Observable MDPs

661

1. Let the utility of executing a fixed conditional plan p starting in physical
state s be Î±p (s).

Then the expected utility of executing p in belief state b is just s b(s)Î±p (s), or b Â· Î±p
if we think of them both as vectors. Hence, the expected utility of a fixed conditional
plan varies linearly with b; that is, it corresponds to a hyperplane in belief space.
2. At any given belief state b, the optimal policy will choose to execute the conditional
plan with highest expected utility; and the expected utility of b under the optimal policy
is just the utility of that conditional plan:
âˆ—

U (b) = U Ï€ (b) = max b Â· Î±p .
p

Ï€âˆ—

If the optimal policy chooses to execute p starting at b, then it is reasonable to expect
that it might choose to execute p in belief states that are very close to b; in fact, if we
bound the depth of the conditional plans, then there are only finitely many such plans
and the continuous space of belief states will generally be divided into regions, each
corresponding to a particular conditional plan that is optimal in that region.
From these two observations, we see that the utility function U (b) on belief states, being the
maximum of a collection of hyperplanes, will be piecewise linear and convex.
To illustrate this, we use a simple two-state world. The states are labeled 0 and 1, with
R(0) = 0 and R(1) = 1. There are two actions: Stay stays put with probability 0.9 and Go
switches to the other state with probability 0.9. For now we will assume the discount factor
Î³ = 1. The sensor reports the correct state with probability 0.6. Obviously, the agent should
Stay when it thinks itâ€™s in state 1 and Go when it thinks itâ€™s in state 0.
The advantage of a two-state world is that the belief space can be viewed as onedimensional, because the two probabilities must sum to 1. In Figure 17.8(a), the x-axis
represents the belief state, defined by b(1), the probability of being in state 1. Now let us consider the one-step plans [Stay] and [Go], each of which receives the reward for the current
state followed by the (discounted) reward for the state reached after the action:
Î±[Stay] (0) = R(0) + Î³(0.9R(0) + 0.1R(1)) = 0.1
Î±[Stay] (1) = R(1) + Î³(0.9R(1) + 0.1R(0)) = 1.9
Î±[Go] (0) = R(0) + Î³(0.9R(1) + 0.1R(0)) = 0.9
Î±[Go] (1) = R(1) + Î³(0.9R(0) + 0.1R(1)) = 1.1
The hyperplanes (lines, in this case) for bÂ·Î±[Stay] and bÂ·Î±[Go] are shown in Figure 17.8(a) and
their maximum is shown in bold. The bold line therefore represents the utility function for
the finite-horizon problem that allows just one action, and in each â€œpieceâ€ of the piecewise
linear utility function the optimal action is the first action of the corresponding conditional
plan. In this case, the optimal one-step policy is to Stay when b(1) > 0.5 and Go otherwise.
Once we have utilities Î±p (s) for all the conditional plans p of depth 1 in each physical
state s, we can compute the utilities for conditional plans of depth 2 by considering each
possible first action, each possible subsequent percept, and then each way of choosing a
depth-1 plan to execute for each percept:
[Stay; if Percept = 0 then Stay else Stay]
[Stay; if Percept = 0 then Stay else Go] . . .

662

Chapter

3

3

2.5

2.5

Making Complex Decisions

2
Utility

Utility

2

17.

[Stay]

1.5
[Go]

1

1.5
1

0.5

0.5

0

0
0

0.2

0.4
0.6
0.8
Probability of state 1

1

0

0.2

1

(b)

3

7.5

2.5

7

2

6.5
Utility

Utility

(a)

0.4
0.6
0.8
Probability of state 1

1.5

6

1

5.5

0.5

5

0

4.5
0

0.2

0.4
0.6
0.8
Probability of state 1

1

0

(c)

0.2

0.4
0.6
0.8
Probability of state 1

1

(d)

Figure 17.8 (a) Utility of two one-step plans as a function of the initial belief state b(1)
for the two-state world, with the corresponding utility function shown in bold. (b) Utilities
for 8 distinct two-step plans. (c) Utilities for four undominated two-step plans. (d) Utility
function for optimal eight-step plans.

DOMINATED PLAN

There are eight distinct depth-2 plans in all, and their utilities are shown in Figure 17.8(b).
Notice that four of the plans, shown as dashed lines, are suboptimal across the entire belief
spaceâ€”we say these plans are dominated, and they need not be considered further. There
are four undominated plans, each of which is optimal in a specific region, as shown in Figure 17.8(c). The regions partition the belief-state space.
We repeat the process for depth 3, and so on. In general, let p be a depth-d conditional
plan whose initial action is a and whose depth-d âˆ’ 1 subplan for percept e is p.e; then



P (s | s, a)
P (e | s )Î±p.e (s ) .
(17.13)
Î±p (s) = R(s) + Î³
s

e

This recursion naturally gives us a value iteration algorithm, which is sketched in Figure 17.9.
The structure of the algorithm and its error analysis are similar to those of the basic value iteration algorithm in Figure 17.4 on page 653; the main difference is that instead of computing
one utility number for each state, POMDP-VALUE -I TERATION maintains a collection of

Section 17.4.

Partially Observable MDPs

663

function POMDP-VALUE -I TERATION(pomdp, ) returns a utility function
inputs: pomdp, a POMDP with states S , actions A(s), transition model P (s | s, a),
sensor model P (e | s), rewards R(s), discount Î³
, the maximum error allowed in the utility of any state
local variables: U , U  , sets of plans p with associated utility vectors Î±p
U  â† a set containing just the empty plan [ ], with Î±[ ] (s) = R(s)
repeat
U â†U
U  â† the set of all plans consisting of an action and, for each possible next percept,
a plan in U with utility vectors computed according to Equation (17.13)
U  â† R EMOVE -D OMINATED -P LANS(U  )
until M AX -D IFFERENCE(U , U  ) < (1 âˆ’ Î³)/Î³
return U
Figure 17.9 A high-level sketch of the value iteration algorithm for POMDPs. The
R EMOVE -D OMINATED -P LANS step and M AX -D IFFERENCE test are typically implemented
as linear programs.

undominated plans with their utility hyperplanes. The algorithmâ€™s complexity depends primarily on how many plans get generated. Given |A| actions and |E| possible observations, it
dâˆ’1
is easy to show that there are |A|O(|E| ) distinct depth-d plans. Even for the lowly two-state
world with d = 8, the exact number is 2255 . The elimination of dominated plans is essential
for reducing this doubly exponential growth: the number of undominated plans with d = 8 is
just 144. The utility function for these 144 plans is shown in Figure 17.8(d).
Notice that even though state 0 has lower utility than state 1, the intermediate belief
states have even lower utility because the agent lacks the information needed to choose a
good action. This is why information has value in the sense defined in Section 16.6 and
optimal policies in POMDPs often include information-gathering actions.
Given such a utility function, an executable policy can be extracted by looking at which
hyperplane is optimal at any given belief state b and executing the first action of the corresponding plan. In Figure 17.8(d), the corresponding optimal policy is still the same as for
depth-1 plans: Stay when b(1) > 0.5 and Go otherwise.
In practice, the value iteration algorithm in Figure 17.9 is hopelessly inefficient for
larger problemsâ€”even the 4 Ã— 3 POMDP is too hard. The main reason is that, given n conditional plans at level d, the algorithm constructs |A| Â· n|E| conditional plans at level d + 1
before eliminating the dominated ones. Since the 1970s, when this algorithm was developed,
there have been several advances including more efficient forms of value iteration and various
kinds of policy iteration algorithms. Some of these are discussed in the notes at the end of the
chapter. For general POMDPs, however, finding optimal policies is very difficult (PSPACEhard, in factâ€”i.e., very hard indeed). Problems with a few dozen states are often infeasible.
The next section describes a different, approximate method for solving POMDPs, one based
on look-ahead search.

664

Chapter

Atâ€“2

Atâ€“1
Xtâ€“1

At
Xt

Rtâ€“1
Etâ€“1

Making Complex Decisions

At+1
Xt+1

Rt
Et

17.

At+2
Xt+2

Ut+3

Rt+2

Rt+1
Et+1

Xt+3

Et+2

Et+3

Figure 17.10 The generic structure of a dynamic decision network. Variables with known
values are shaded. The current time is t and the agent must decide what to doâ€”that is, choose
a value for At . The network has been unrolled into the future for three steps and represents
future rewards, as well as the utility of the state at the look-ahead horizon.

17.4.3 Online agents for POMDPs
In this section, we outline a simple approach to agent design for partially observable, stochastic environments. The basic elements of the design are already familiar:

DYNAMIC DECISION
NETWORK

â€¢ The transition and sensor models are represented by a dynamic Bayesian network
(DBN), as described in Chapter 15.
â€¢ The dynamic Bayesian network is extended with decision and utility nodes, as used in
decision networks in Chapter 16. The resulting model is called a dynamic decision
network, or DDN.
â€¢ A filtering algorithm is used to incorporate each new percept and action and to update
the belief state representation.
â€¢ Decisions are made by projecting forward possible action sequences and choosing the
best one.
DBNs are factored representations in the terminology of Chapter 2; they typically have
an exponential complexity advantage over atomic representations and can model quite substantial real-world problems. The agent design is therefore a practical implementation of the
utility-based agent sketched in Chapter 2.
In the DBN, the single state St becomes a set of state variables Xt , and there may be
multiple evidence variables Et . We will use At to refer to the action at time t, so the transition
model becomes P(Xt+1 |Xt , At ) and the sensor model becomes P(Et |Xt ). We will use Rt to
refer to the reward received at time t and Ut to refer to the utility of the state at time t. (Both
of these are random variables.) With this notation, a dynamic decision network looks like the
one shown in Figure 17.10.
Dynamic decision networks can be used as inputs for any POMDP algorithm, including
those for value and policy iteration methods. In this section, we focus on look-ahead methods
that project action sequences forward from the current belief state in much the same way as do
the game-playing algorithms of Chapter 5. The network in Figure 17.10 has been projected
three steps into the future; the current and future decisions A and the future observations

Section 17.4.

Partially Observable MDPs

665

At in P(Xt | E1:t)
Et+1

...
...

At+1 in P(Xt+1 | E1:t+1)

...

...

...

...
...

...

...

Et+2

...
...

At+2 in P(Xt+2 | E1:t+2)

...

...

...
...

...

...

Et+3

...
...

...

U(Xt+3)

...

...

10

4

6

3

Figure 17.11 Part of the look-ahead solution of the DDN in Figure 17.10. Each decision
will be taken in the belief state indicated.

E and rewards R are all unknown. Notice that the network includes nodes for the rewards
for Xt+1 and Xt+2 , but the utility for Xt+3 . This is because the agent must maximize the
(discounted) sum of all future rewards, and U (Xt+3 ) represents the reward for Xt+3 and all
subsequent rewards. As in Chapter 5, we assume that U is available only in some approximate
form: if exact utility values were available, look-ahead beyond depth 1 would be unnecessary.
Figure 17.11 shows part of the search tree corresponding to the three-step look-ahead
DDN in Figure 17.10. Each of the triangular nodes is a belief state in which the agent makes
a decision At+i for i = 0, 1, 2, . . .. The round (chance) nodes correspond to choices by the
environment, namely, what evidence Et+i arrives. Notice that there are no chance nodes
corresponding to the action outcomes; this is because the belief-state update for an action is
deterministic regardless of the actual outcome.
The belief state at each triangular node can be computed by applying a filtering algorithm to the sequence of percepts and actions leading to it. In this way, the algorithm
takes into account the fact that, for decision At+i , the agent will have available percepts
Et+1 , . . . , Et+i , even though at time t it does not know what those percepts will be. In this
way, a decision-theoretic agent automatically takes into account the value of information and
will execute information-gathering actions where appropriate.
A decision can be extracted from the search tree by backing up the utility values from
the leaves, taking an average at the chance nodes and taking the maximum at the decision
nodes. This is similar to the E XPECTIMINIMAX algorithm for game trees with chance nodes,
except that (1) there can also be rewards at non-leaf states and (2) the decision nodes correspond to belief states rather than actual states. The time complexity of an exhaustive search
to depth d is O(|A|d Â· |E|d ), where |A| is the number of available actions and |E| is the number of possible percepts. (Notice that this is far less than the number of depth-d conditional

666

Chapter

17.

Making Complex Decisions

plans generated by value iteration.) For problems in which the discount factor Î³ is not too
close to 1, a shallow search is often good enough to give near-optimal decisions. It is also
possible to approximate the averaging step at the chance nodes, by sampling from the set of
possible percepts instead of summing over all possible percepts. There are various other ways
of finding good approximate solutions quickly, but we defer them to Chapter 21.
Decision-theoretic agents based on dynamic decision networks have a number of advantages compared with other, simpler agent designs presented in earlier chapters. In particular,
they handle partially observable, uncertain environments and can easily revise their â€œplansâ€ to
handle unexpected evidence. With appropriate sensor models, they can handle sensor failure
and can plan to gather information. They exhibit â€œgraceful degradationâ€ under time pressure
and in complex environments, using various approximation techniques. So what is missing?
One defect of our DDN-based algorithm is its reliance on forward search through state space,
rather than using the hierarchical and other advanced planning techniques described in Chapter 11. There have been attempts to extend these techniques into the probabilistic domain, but
so far they have proved to be inefficient. A second, related problem is the basically propositional nature of the DDN language. We would like to be able to extend some of the ideas for
first-order probabilistic languages to the problem of decision making. Current research has
shown that this extension is possible and has significant benefits, as discussed in the notes at
the end of the chapter.

17.5

GAME THEORY

D ECISIONS WITH M ULTIPLE AGENTS : G AME T HEORY
This chapter has concentrated on making decisions in uncertain environments. But what if
the uncertainty is due to other agents and the decisions they make? And what if the decisions
of those agents are in turn influenced by our decisions? We addressed this question once
before, when we studied games in Chapter 5. There, however, we were primarily concerned
with turn-taking games in fully observable environments, for which minimax search can be
used to find optimal moves. In this section we study the aspects of game theory that analyze
games with simultaneous moves and other sources of partial observability. (Game theorists
use the terms perfect information and imperfect information rather than fully and partially
observable.) Game theory can be used in at least two ways:
1. Agent design: Game theory can analyze the agentâ€™s decisions and compute the expected
utility for each decision (under the assumption that other agents are acting optimally
according to game theory). For example, in the game two-finger Morra, two players,
O and E, simultaneously display one or two fingers. Let the total number of fingers
be f . If f is odd, O collects f dollars from E; and if f is even, E collects f dollars
from O. Game theory can determine the best strategy against a rational player and the
expected return for each player. 4
4

Morra is a recreational version of an inspection game. In such games, an inspector chooses a day to inspect a
facility (such as a restaurant or a biological weapons plant), and the facility operator chooses a day to hide all the
nasty stuff. The inspector wins if the days are different, and the facility operator wins if they are the same.

Section 17.5.

Decisions with Multiple Agents: Game Theory

667

2. Mechanism design: When an environment is inhabited by many agents, it might be
possible to define the rules of the environment (i.e., the game that the agents must
play) so that the collective good of all agents is maximized when each agent adopts the
game-theoretic solution that maximizes its own utility. For example, game theory can
help design the protocols for a collection of Internet traffic routers so that each router
has an incentive to act in such a way that global throughput is maximized. Mechanism
design can also be used to construct intelligent multiagent systems that solve complex
problems in a distributed fashion.

17.5.1 Single-move games
We start by considering a restricted set of games: ones where all players take action simultaneously and the result of the game is based on this single set of actions. (Actually, it is not
crucial that the actions take place at exactly the same time; what matters is that no player has
knowledge of the other playersâ€™ choices.) The restriction to a single move (and the very use
of the word â€œgameâ€) might make this seem trivial, but in fact, game theory is serious business. It is used in decision-making situations including the auctioning of oil drilling rights
and wireless frequency spectrum rights, bankruptcy proceedings, product development and
pricing decisions, and national defenseâ€”situations involving billions of dollars and hundreds
of thousands of lives. A single-move game is defined by three components:
PLAYER

ACTION

PAYOFF FUNCTION

STRATEGIC FORM

â€¢ Players or agents who will be making decisions. Two-player games have received the
most attention, although n-player games for n > 2 are also common. We give players
capitalized names, like Alice and Bob or O and E.
â€¢ Actions that the players can choose. We will give actions lowercase names, like one or
testify. The players may or may not have the same set of actions available.
â€¢ A payoff function that gives the utility to each player for each combination of actions
by all the players. For single-move games the payoff function can be represented by a
matrix, a representation known as the strategic form (also called normal form). The
payoff matrix for two-finger Morra is as follows:
E: one
E: two

O: one
E = +2, O = âˆ’2
E = âˆ’3, O = +3

O: two
E = âˆ’3, O = +3
E = +4, O = âˆ’4

For example, the lower-right corner shows that when player O chooses action two and
E also chooses two, the payoff is +4 for E and âˆ’4 for O.
STRATEGY
PURE STRATEGY

MIXED STRATEGY

STRATEGY PROFILE
OUTCOME

Each player in a game must adopt and then execute a strategy (which is the name used in
game theory for a policy). A pure strategy is a deterministic policy; for a single-move game,
a pure strategy is just a single action. For many games an agent can do better with a mixed
strategy, which is a randomized policy that selects actions according to a probability distribution. The mixed strategy that chooses action a with probability p and action b otherwise
is written [p: a; (1 âˆ’ p): b]. For example, a mixed strategy for two-finger Morra might be
[0.5: one; 0.5: two]. A strategy profile is an assignment of a strategy to each player; given
the strategy profile, the gameâ€™s outcome is a numeric value for each player.

668
SOLUTION

PRISONERâ€™S
DILEMMA

Chapter

WEAK DOMINATION

PARETO OPTIMAL
PARETO DOMINATED

DOMINANT
STRATEGY
EQUILIBRIUM
EQUILIBRIUM

Making Complex Decisions

A solution to a game is a strategy profile in which each player adopts a rational strategy.
We will see that the most important issue in game theory is to define what â€œrationalâ€ means
when each agent chooses only part of the strategy profile that determines the outcome. It is
important to realize that outcomes are actual results of playing a game, while solutions are
theoretical constructs used to analyze a game. We will see that some games have a solution
only in mixed strategies. But that does not mean that a player must literally be adopting a
mixed strategy to be rational.
Consider the following story: Two alleged burglars, Alice and Bob, are caught redhanded near the scene of a burglary and are interrogated separately. A prosecutor offers each
a deal: if you testify against your partner as the leader of a burglary ring, youâ€™ll go free for
being the cooperative one, while your partner will serve 10 years in prison. However, if you
both testify against each other, youâ€™ll both get 5 years. Alice and Bob also know that if both
refuse to testify they will serve only 1 year each for the lesser charge of possessing stolen
property. Now Alice and Bob face the so-called prisonerâ€™s dilemma: should they testify
or refuse? Being rational agents, Alice and Bob each want to maximize their own expected
utility. Letâ€™s assume that Alice is callously unconcerned about her partnerâ€™s fate, so her utility
decreases in proportion to the number of years she will spend in prison, regardless of what
happens to Bob. Bob feels exactly the same way. To help reach a rational decision, they both
construct the following payoff matrix:
Bob:testify
Bob:refuse

DOMINANT
STRATEGY
STRONG
DOMINATION

17.

Alice:testify
A = âˆ’5, B = âˆ’5
A = 0, B = âˆ’10

Alice:refuse
A = âˆ’10, B = 0
A = âˆ’1, B = âˆ’1

Alice analyzes the payoff matrix as follows: â€œSuppose Bob testifies. Then I get 5 years if I
testify and 10 years if I donâ€™t, so in that case testifying is better. On the other hand, if Bob
refuses, then I get 0 years if I testify and 1 year if I refuse, so in that case as well testifying is
better. So in either case, itâ€™s better for me to testify, so thatâ€™s what I must do.â€
Alice has discovered that testify is a dominant strategy for the game. We say that a
strategy s for player p strongly dominates strategy s if the outcome for s is better for p than
the outcome for s , for every choice of strategies by the other player(s). Strategy s weakly
dominates s if s is better than s on at least one strategy profile and no worse on any other.
A dominant strategy is a strategy that dominates all others. It is irrational to play a dominated
strategy, and irrational not to play a dominant strategy if one exists. Being rational, Alice
chooses the dominant strategy. We need just a bit more terminology: we say that an outcome
is Pareto optimal5 if there is no other outcome that all players would prefer. An outcome is
Pareto dominated by another outcome if all players would prefer the other outcome.
If Alice is clever as well as rational, she will continue to reason as follows: Bobâ€™s
dominant strategy is also to testify. Therefore, he will testify and we will both get five years.
When each player has a dominant strategy, the combination of those strategies is called a
dominant strategy equilibrium. In general, a strategy profile forms an equilibrium if no
player can benefit by switching strategies, given that every other player sticks with the same
5

Pareto optimality is named after the economist Vilfredo Pareto (1848â€“1923).

Section 17.5.

NASH EQUILIBRIUM

Decisions with Multiple Agents: Game Theory

669

strategy. An equilibrium is essentially a local optimum in the space of policies; it is the top
of a peak that slopes downward along every dimension, where a dimension corresponds to a
playerâ€™s strategy choices.
The mathematician John Nash (1928â€“) proved that every game has at least one equilibrium. The general concept of equilibrium is now called Nash equilibrium in his honor.
Clearly, a dominant strategy equilibrium is a Nash equilibrium (Exercise 17.16), but some
games have Nash equilibria but no dominant strategies.
The dilemma in the prisonerâ€™s dilemma is that the equilibrium outcome is worse for
both players than the outcome they would get if they both refused to testify. In other words,
(testify, testify) is Pareto dominated by the (-1, -1) outcome of (refuse, refuse). Is there any
way for Alice and Bob to arrive at the (-1, -1) outcome? It is certainly an allowable option
for both of them to refuse to testify, but is is hard to see how rational agents can get there,
given the definition of the game. Either player contemplating playing refuse will realize that
he or she would do better by playing testify. That is the attractive power of an equilibrium
point. Game theorists agree that being a Nash equilibrium is a necessary condition for being
a solutionâ€”although they disagree whether it is a sufficient condition.
It is easy enough to get to the (refuse, refuse) solution if we modify the game. For
example, we could change to a repeated game in which the players know that they will meet
again. Or the agents might have moral beliefs that encourage cooperation and fairness. That
means they have a different utility function, necessitating a different payoff matrix, making
it a different game. We will see later that agents with limited computational powers, rather
than the ability to reason absolutely rationally, can reach non-equilibrium outcomes, as can an
agent that knows that the other agent has limited rationality. In each case, we are considering
a different game than the one described by the payoff matrix above.
Now letâ€™s look at a game that has no dominant strategy. Acme, a video game console
manufacturer, has to decide whether its next game machine will use Blu-ray discs or DVDs.
Meanwhile, the video game software producer Best needs to decide whether to produce its
next game on Blu-ray or DVD. The profits for both will be positive if they agree and negative
if they disagree, as shown in the following payoff matrix:
Best:bluray
Best:dvd

Acme:bluray
A = +9, B = +9
A = âˆ’3, B = âˆ’1

Acme:dvd
A = âˆ’4, B = âˆ’1
A = +5, B = +5

There is no dominant strategy equilibrium for this game, but there are two Nash equilibria:
(bluray, bluray) and (dvd, dvd). We know these are Nash equilibria because if either player
unilaterally moves to a different strategy, that player will be worse off. Now the agents have
a problem: there are multiple acceptable solutions, but if each agent aims for a different
solution, then both agents will suffer. How can they agree on a solution? One answer is
that both should choose the Pareto-optimal solution (bluray, bluray); that is, we can restrict
the definition of â€œsolutionâ€ to the unique Pareto-optimal Nash equilibrium provided that one
exists. Every game has at least one Pareto-optimal solution, but a game might have several,
or they might not be equilibrium points. For example, if (bluray, bluray) had payoff (5,
5), then there would be two equal Pareto-optimal equilibrium points. To choose between

670

COORDINATION
GAME

ZERO-SUM GAME

MAXIMIN

Chapter

17.

Making Complex Decisions

them the agents can either guess or communicate, which can be done either by establishing
a convention that orders the solutions before the game begins or by negotiating to reach a
mutually beneficial solution during the game (which would mean including communicative
actions as part of a sequential game). Communication thus arises in game theory for exactly
the same reasons that it arose in multiagent planning in Section 11.4. Games in which players
need to communicate like this are called coordination games.
A game can have more than one Nash equilibrium; how do we know that every game
must have at least one? Some games have no pure-strategy Nash equilibria. Consider, for
example, any pure-strategy profile for two-finger Morra (page 666). If the total number of
fingers is even, then O will want to switch; on the other hand (so to speak), if the total is odd,
then E will want to switch. Therefore, no pure strategy profile can be an equilibrium and we
must look to mixed strategies instead.
But which mixed strategy? In 1928, von Neumann developed a method for finding the
optimal mixed strategy for two-player, zero-sum gamesâ€”games in which the sum of the
payoffs is always zero.6 Clearly, Morra is such a game. For two-player, zero-sum games, we
know that the payoffs are equal and opposite, so we need consider the payoffs of only one
player, who will be the maximizer (just as in Chapter 5). For Morra, we pick the even player
E to be the maximizer, so we can define the payoff matrix by the values UE (e, o)â€”the payoff
to E if E does e and O does o. (For convenience we call player E â€œherâ€ and O â€œhim.â€) Von
Neumannâ€™s method is called the the maximin technique, and it works as follows:
â€¢ Suppose we change the rules as follows: first E picks her strategy and reveals it to
O. Then O picks his strategy, with knowledge of Eâ€™s strategy. Finally, we evaluate
the expected payoff of the game based on the chosen strategies. This gives us a turntaking game to which we can apply the standard minimax algorithm from Chapter 5.
Letâ€™s suppose this gives an outcome UE,O . Clearly, this game favors O, so the true
utility U of the original game (from Eâ€™s point of view) is at least UE,O . For example,
if we just look at pure strategies, the minimax game tree has a root value of âˆ’3 (see
Figure 17.12(a)), so we know that U â‰¥ âˆ’3.
â€¢ Now suppose we change the rules to force O to reveal his strategy first, followed by E.
Then the minimax value of this game is UO,E , and because this game favors E we know
that U is at most UO,E . With pure strategies, the value is +2 (see Figure 17.12(b)), so
we know U â‰¤ +2.
Combining these two arguments, we see that the true utility U of the solution to the original
game must satisfy
UE,O â‰¤ U â‰¤ UO,E

or in this case,

âˆ’3â‰¤U â‰¤2.

To pinpoint the value of U , we need to turn our analysis to mixed strategies. First, observe the
following: once the first player has revealed his or her strategy, the second player might as
well choose a pure strategy. The reason is simple: if the second player plays a mixed strategy,
[p: one; (1 âˆ’ p): two], its expected utility is a linear combination (p Â· uone + (1 âˆ’ p) Â· utwo ) of
6

or a constantâ€”see page 162.

Section 17.5.

Decisions with Multiple Agents: Game Theory

(a)

E

(b) O

-3
one

(c)

one

-3
two

one

two

one

two

one

two

2

-3

-3

4

2

-3

-3

4

(d) O
[q: one; (1 â€“ q): two]
E

O
one
2p â€“ 3(1 â€“ p)

two

one

3p + 4(1 â€“ p)

2q â€“ 3(1 â€“ q)

(f)

U
+4

0

two
3q + 4(1 â€“ q)

U
+4

+3

+1

4

one

[p: one; (1 â€“ p): two]

+2

two

2

E

E

(e)

2

two

-3

O

671

+3

two

+2

one

+1

1

p

0

â€“1

â€“1

â€“2

â€“2

â€“3

â€“3

two
one
1

q

Figure 17.12 (a) and (b): Minimax game trees for two-finger Morra if the players take
turns playing pure strategies. (c) and (d): Parameterized game trees where the first player
plays a mixed strategy. The payoffs depend on the probability parameter (p or q) in the
mixed strategy. (e) and (f): For any particular value of the probability parameter, the second
player will choose the â€œbetterâ€ of the two actions, so the value of the first playerâ€™s mixed
strategy is given by the heavy lines. The first player will choose the probability parameter for
the mixed strategy at the intersection point.

the utilities of the pure strategies, uone and utwo . This linear combination can never be better
than the better of uone and utwo , so the second player can just choose the better one.
With this observation in mind, the minimax trees can be thought of as having infinitely
many branches at the root, corresponding to the infinitely many mixed strategies the first

672

Chapter

17.

Making Complex Decisions

player can choose. Each of these leads to a node with two branches corresponding to the
pure strategies for the second player. We can depict these infinite trees finitely by having one
â€œparameterizedâ€ choice at the root:
â€¢ If E chooses first, the situation is as shown in Figure 17.12(c). E chooses the strategy
[p: one; (1âˆ’ p): two] at the root, and then O chooses a pure strategy (and hence a move)
given the value of p. If O chooses one, the expected payoff (to E) is 2pâˆ’3(1âˆ’p) = 5pâˆ’
3; if O chooses two, the expected payoff is âˆ’3p + 4(1 âˆ’ p) = 4 âˆ’ 7p. We can draw
these two payoffs as straight lines on a graph, where p ranges from 0 to 1 on the x-axis,
as shown in Figure 17.12(e). O, the minimizer, will always choose the lower of the two
lines, as shown by the heavy lines in the figure. Therefore, the best that E can do at the
root is to choose p to be at the intersection point, which is where
5p âˆ’ 3 = 4 âˆ’ 7p

â‡’

p = 7/12 .

The utility for E at this point is UE,O = âˆ’ 1/12.
â€¢ If O moves first, the situation is as shown in Figure 17.12(d). O chooses the strategy
[q: one; (1 âˆ’ q): two] at the root, and then E chooses a move given the value of q. The
payoffs are 2q âˆ’ 3(1âˆ’ q) = 5q âˆ’ 3 and âˆ’3q + 4(1âˆ’ q) = 4âˆ’ 7q.7 Again, Figure 17.12(f)
shows that the best O can do at the root is to choose the intersection point:
5q âˆ’ 3 = 4 âˆ’ 7q

â‡’

q = 7/12 .

The utility for E at this point is UO,E = âˆ’ 1/12.

MAXIMIN
EQUILIBRIUM

Now we know that the true utility of the original game lies between âˆ’1/12 and âˆ’1/12, that
is, it is exactly âˆ’1/12! (The moral is that it is better to be O than E if you are playing this
game.) Furthermore, the true utility is attained by the mixed strategy [7/12: one; 5/12: two],
which should be played by both players. This strategy is called the maximin equilibrium of
the game, and is a Nash equilibrium. Note that each component strategy in an equilibrium
mixed strategy has the same expected utility. In this case, both one and two have the same
expected utility, âˆ’1/12, as the mixed strategy itself.
Our result for two-finger Morra is an example of the general result by von Neumann:
every two-player zero-sum game has a maximin equilibrium when you allow mixed strategies.
Furthermore, every Nash equilibrium in a zero-sum game is a maximin for both players. A
player who adopts the maximin strategy has two guarantees: First, no other strategy can do
better against an opponent who plays well (although some other strategies might be better at
exploiting an opponent who makes irrational mistakes). Second, the player continues to do
just as well even if the strategy is revealed to the opponent.
The general algorithm for finding maximin equilibria in zero-sum games is somewhat
more involved than Figures 17.12(e) and (f) might suggest. When there are n possible actions,
a mixed strategy is a point in n-dimensional space and the lines become hyperplanes. Itâ€™s
also possible for some pure strategies for the second player to be dominated by others, so
that they are not optimal against any strategy for the first player. After removing all such
strategies (which might have to be done repeatedly), the optimal choice at the root is the
7

It is a coincidence that these equations are the same as those for p; the coincidence arises because
UE (one, two) = UE (two, one) = âˆ’ 3. This also explains why the optimal strategy is the same for both players.

Section 17.5.

Decisions with Multiple Agents: Game Theory

673

highest (or lowest) intersection point of the remaining hyperplanes. Finding this choice is
an example of a linear programming problem: maximizing an objective function subject to
linear constraints. Such problems can be solved by standard techniques in time polynomial
in the number of actions (and in the number of bits used to specify the reward function, if you
want to get technical).
The question remains, what should a rational agent actually do in playing a single game
of Morra? The rational agent will have derived the fact that [7/12: one; 5/12: two] is the
maximin equilibrium strategy, and will assume that this is mutual knowledge with a rational
opponent. The agent could use a 12-sided die or a random number generator to pick randomly
according to this mixed strategy, in which case the expected payoff would be -1/12 for E. Or
the agent could just decide to play one, or two. In either case, the expected payoff remains
-1/12 for E. Curiously, unilaterally choosing a particular action does not harm oneâ€™s expected
payoff, but allowing the other agent to know that one has made such a unilateral decision does
affect the expected payoff, because then the opponent can adjust his strategy accordingly.
Finding equilibria in non-zero-sum games is somewhat more complicated. The general
approach has two steps: (1) Enumerate all possible subsets of actions that might form mixed
strategies. For example, first try all strategy profiles where each player uses a single action,
then those where each player uses either one or two actions, and so on. This is exponential
in the number of actions, and so only applies to relatively small games. (2) For each strategy
profile enumerated in (1), check to see if it is an equilibrium. This is done by solving a set of
equations and inequalities that are similar to the ones used in the zero-sum case. For two players these equations are linear and can be solved with basic linear programming techniques,
but for three or more players they are nonlinear and may be very difficult to solve.

17.5.2 Repeated games
REPEATED GAME

So far we have looked only at games that last a single move. The simplest kind of multiplemove game is the repeated game, in which players face the same choice repeatedly, but each
time with knowledge of the history of all playersâ€™ previous choices. A strategy profile for a
repeated game specifies an action choice for each player at each time step for every possible
history of previous choices. As with MDPs, payoffs are additive over time.
Letâ€™s consider the repeated version of the prisonerâ€™s dilemma. Will Alice and Bob work
together and refuse to testify, knowing they will meet again? The answer depends on the
details of the engagement. For example, suppose Alice and Bob know that they must play
exactly 100 rounds of prisonerâ€™s dilemma. Then they both know that the 100th round will not
be a repeated gameâ€”that is, its outcome can have no effect on future roundsâ€”and therefore
they will both choose the dominant strategy, testify, in that round. But once the 100th round
is determined, the 99th round can have no effect on subsequent rounds, so it too will have
a dominant strategy equilibrium at (testify, testify). By induction, both players will choose
testify on every round, earning a total jail sentence of 500 years each.
We can get different solutions by changing the rules of the interaction. For example,
suppose that after each round there is a 99% chance that the players will meet again. Then
the expected number of rounds is still 100, but neither player knows for sure which round

674

PERPETUAL
PUNISHMENT

Chapter

17.

Making Complex Decisions

will be the last. Under these conditions, more cooperative behavior is possible. For example,
one equilibrium strategy is for each player to refuse unless the other player has ever played
testify. This strategy could be called perpetual punishment. Suppose both players have
adopted this strategy, and this is mutual knowledge. Then as long as neither player has played
testify, then at any point in time the expected future total payoff for each player is
âˆž

0.99t Â· (âˆ’1) = âˆ’100 .
t=0

A player who deviates from the strategy and chooses testify will gain a score of 0 rather than
âˆ’1 on the very next move, but from then on both players will play testify and the playerâ€™s
total expected future payoff becomes
âˆž

0.99t Â· (âˆ’5) = âˆ’495 .
0+
t=1

TIT-FOR-TAT

Therefore, at every step, there is no incentive to deviate from (refuse, refuse). Perpetual
punishment is the â€œmutually assured destructionâ€ strategy of the prisonerâ€™s dilemma: once
either player decides to testify, it ensures that both players suffer a great deal. But it works
as a deterrent only if the other player believes you have adopted this strategyâ€”or at least that
you might have adopted it.
Other strategies are more forgiving. The most famous, called tit-for-tat, calls for starting with refuse and then echoing the other playerâ€™s previous move on all subsequent moves.
So Alice would refuse as long as Bob refuses and would testify the move after Bob testified,
but would go back to refusing if Bob did. Although very simple, this strategy has proven to
be highly robust and effective against a wide variety of strategies.
We can also get different solutions by changing the agents, rather than changing the
rules of engagement. Suppose the agents are finite-state machines with n states and they
are playing a game with m > n total steps. The agents are thus incapable of representing
the number of remaining steps, and must treat it as an unknown. Therefore, they cannot do
the induction, and are free to arrive at the more favorable (refuse, refuse) equilibrium. In
this case, ignorance is blissâ€”or rather, having your opponent believe that you are ignorant is
bliss. Your success in these repeated games depends on the other playerâ€™s perception of you
as a bully or a simpleton, and not on your actual characteristics.

17.5.3 Sequential games
EXTENSIVE FORM

In the general case, a game consists of a sequence of turns that need not be all the same. Such
games are best represented by a game tree, which game theorists call the extensive form. The
tree includes all the same information we saw in Section 5.1: an initial state S0 , a function
P LAYER (s) that tells which player has the move, a function ACTIONS (s) enumerating the
possible actions, a function R ESULT (s, a) that defines the transition to a new state, and a
partial function U TILITY (s, p), which is defined only on terminal states, to give the payoff
for each player.
To represent stochastic games, such as backgammon, we add a distinguished player,
chance, that can take random actions. Chanceâ€™s â€œstrategyâ€ is part of the definition of the

Section 17.5.

INFORMATION SETS

Decisions with Multiple Agents: Game Theory

675

game, specified as a probability distribution over actions (the other players get to choose
their own strategy). To represent games with nondeterministic actions, such as billiards, we
break the action into two pieces: the playerâ€™s action itself has a deterministic result, and then
chance has a turn to react to the action in its own capricious way. To represent simultaneous
moves, as in the prisonerâ€™s dilemma or two-finger Morra, we impose an arbitrary order on the
players, but we have the option of asserting that the earlier playerâ€™s actions are not observable
to the subsequent players: e.g., Alice must choose refuse or testify first, then Bob chooses,
but Bob does not know what choice Alice made at that time (we can also represent the fact
that the move is revealed later). However, we assume the players always remember all their
own previous actions; this assumption is called perfect recall.
The key idea of extensive form that sets it apart from the game trees of Chapter 5 is
the representation of partial observability. We saw in Section 5.6 that a player in a partially
observable game such as Kriegspiel can create a game tree over the space of belief states.
With that tree, we saw that in some cases a player can find a sequence of moves (a strategy)
that leads to a forced checkmate regardless of what actual state we started in, and regardless of
what strategy the opponent uses. However, the techniques of Chapter 5 could not tell a player
what to do when there is no guaranteed checkmate. If the playerâ€™s best strategy depends
on the opponentâ€™s strategy and vice versa, then minimax (or alphaâ€“beta) by itself cannot
find a solution. The extensive form does allow us to find solutions because it represents the
belief states (game theorists call them information sets) of all players at once. From that
representation we can find equilibrium solutions, just as we did with normal-form games.
As a simple example of a sequential game, place two agents in the 4 Ã— 3 world of Figure 17.1 and have them move simultaneously until one agent reaches an exit square, and gets
the payoff for that square. If we specify that no movement occurs when the two agents try
to move into the same square simultaneously (a common problem at many traffic intersections), then certain pure strategies can get stuck forever. Thus, agents need a mixed strategy
to perform well in this game: randomly choose between moving ahead and staying put. This
is exactly what is done to resolve packet collisions in Ethernet networks.
Next weâ€™ll consider a very simple variant of poker. The deck has only four cards, two
aces and two kings. One card is dealt to each player. The first player then has the option
to raise the stakes of the game from 1 point to 2, or to check. If player 1 checks, the game
is over. If he raises, then player 2 has the option to call, accepting that the game is worth 2
points, or fold, conceding the 1 point. If the game does not end with a fold, then the payoff
depends on the cards: it is zero for both players if they have the same card; otherwise the
player with the king pays the stakes to the player with the ace.
The extensive-form tree for this game is shown in Figure 17.13. Nonterminal states are
shown as circles, with the player to move inside the circle; player 0 is chance. Each action is
depicted as an arrow with a label, corresponding to a raise, check, call, or fold, or, for chance,
the four possible deals (â€œAKâ€ means that player 1 gets an ace and player 2 a king). Terminal
states are rectangles labeled by their payoff to player 1 and player 2. Information sets are
shown as labeled dashed boxes; for example, I1,1 is the information set where it is player
1â€™s turn, and he knows he has an ace (but does not know what player 2 has). In information
set I2,1 , it is player 2â€™s turn and she knows that she has an ace and that player 1 has raised,

676

Chapter

1

r

17.

2

k

I1,1
1/6: AA
1
1/3: AK
0

0,0 !
r

1

2
+1,-1 !

r

1

+1,-1 !
c

c

0,0

f

r

+1,-1 !

I2,1
2

k

c
f

-2,+2
+1,-1 !

-1,+1 !

Figure 17.13

+2,-2
+1,-1 !

I2,2
2

0,0 !

I1,2

0,0 !

f

k

1/3: KA

c
f

I2,1

k

1/6: KK

Making Complex Decisions

Extensive form of a simplified version of poker.

but does not know what card player 1 has. (Due to the limits of two-dimensional paper, this
information set is shown as two boxes rather than one.)
One way to solve an extensive game is to convert it to a normal-form game. Recall that
the normal form is a matrix, each row of which is labeled with a pure strategy for player 1, and
each column by a pure strategy for player 2. In an extensive game a pure strategy for player
i corresponds to an action for each information set involving that player. So in Figure 17.13,
one pure strategy for player 1 is â€œraise when in I1,1 (that is, when I have an ace), and check
when in I1,2 (when I have a king).â€ In the payoff matrix below, this strategy is called rk.
Similarly, strategy cf for player 2 means â€œcall when I have an ace and fold when I have a
king.â€ Since this is a zero-sum game, the matrix below gives only the payoff for player 1;
player 2 always has the opposite payoff:
1:rr
1:kr
1:rk
1:kk

2:cc
0
-1/3
1/3
0

2:cf
-1/6
-1/6
0
0

2:ff
1
5/6
1/6
0

2:fc
7/6
2/3
1/2
0

This game is so simple that it has two pure-strategy equilibria, shown in bold: cf for player
2 and rk or kk for player 1. But in general we can solve extensive games by converting
to normal form and then finding a solution (usually a mixed strategy) using standard linear
programming methods. That works in theory. But if a player has I information sets and
a actions per set, then that player will have aI pure strategies. In other words, the size of
the normal-form matrix is exponential in the number of information sets, so in practice the

Section 17.5.

SEQUENCE FORM

ABSTRACTION

Decisions with Multiple Agents: Game Theory

677

approach works only for very small game trees, on the order of a dozen states. A game like
Texas holdâ€™em poker has about 1018 states, making this approach completely infeasible.
What are the alternatives? In Chapter 5 we saw how alphaâ€“beta search could handle
games of perfect information with huge game trees by generating the tree incrementally, by
pruning some branches, and by heuristically evaluating nonterminal nodes. But that approach
does not work well for games with imperfect information, for two reasons: first, it is harder
to prune, because we need to consider mixed strategies that combine multiple branches, not a
pure strategy that always chooses the best branch. Second, it is harder to heuristically evaluate
a nonterminal node, because we are dealing with information sets, not individual states.
Koller et al. (1996) come to the rescue with an alternative representation of extensive
games, called the sequence form, that is only linear in the size of the tree, rather than exponential. Rather than represent strategies, it represents paths through the tree; the number
of paths is equal to the number of terminal nodes. Standard linear programming methods
can again be applied to this representation. The resulting system can solve poker variants
with 25,000 states in a minute or two. This is an exponential speedup over the normal-form
approach, but still falls far short of handling full poker, with 1018 states.
If we canâ€™t handle 1018 states, perhaps we can simplify the problem by changing the
game to a simpler form. For example, if I hold an ace and am considering the possibility that
the next card will give me a pair of aces, then I donâ€™t care about the suit of the next card; any
suit will do equally well. This suggests forming an abstraction of the game, one in which
suits are ignored. The resulting game tree will be smaller by a factor of 4! = 24. Suppose I
can solve this smaller game; how will the solution to that game relate to the original game?
If no player is going for a flush (or bluffing so), then the suits donâ€™t matter to any player, and
the solution for the abstraction will also be a solution for the original game. However, if any
player is contemplating a flush, then the abstraction will be only an approximate solution (but
it is possible to compute bounds on the error).
There are many opportunities for abstraction. For example, at the point in a game where
each player has two cards, if I hold a pair of queens, then the other playersâ€™ hands could be
abstracted into three classes: better (only a pair of kings or a pair of aces), same (pair of
queens) or worse (everything else). However, this abstraction might be too coarse. A better
abstraction would divide worse into, say, medium pair (nines through jacks), low pair, and
no pair. These examples are abstractions of states; it is also possible to abstract actions. For
example, instead of having a bet action for each integer from 1 to 1000, we could restrict the
bets to 100 , 101 , 102 and 103 . Or we could cut out one of the rounds of betting altogether.
We can also abstract over chance nodes, by considering only a subset of the possible deals.
This is equivalent to the rollout technique used in Go programs. Putting all these abstractions
together, we can reduce the 1018 states of poker to 107 states, a size that can be solved with
current techniques.
Poker programs based on this approach can easily defeat novice and some experienced
human players, but are not yet at the level of master players. Part of the problem is that
the solution these programs approximateâ€”the equilibrium solutionâ€”is optimal only against
an opponent who also plays the equilibrium strategy. Against fallible human players it is
important to be able to exploit an opponentâ€™s deviation from the equilibrium strategy. As

678

COURNOT
COMPETITION

BAYESâ€“NASH
EQUILIBRIUM

Chapter

17.

Making Complex Decisions

Gautam Rao (aka â€œThe Countâ€), the worldâ€™s leading online poker player, said (Billings et al.,
2003), â€œYou have a very strong program. Once you add opponent modeling to it, it will kill
everyone.â€ However, good models of human fallability remain elusive.
In a sense, extensive game form is the one of the most complete representations we have
seen so far: it can handle partially observable, multiagent, stochastic, sequential, dynamic
environmentsâ€”most of the hard cases from the list of environment properties on page 42.
However, there are two limitations of game theory. First, it does not deal well with continuous
states and actions (although there have been some extensions to the continuous case; for
example, the theory of Cournot competition uses game theory to solve problems where two
companies choose prices for their products from a continuous space). Second, game theory
assumes the game is known. Parts of the game may be specified as unobservable to some of
the players, but it must be known what parts are unobservable. In cases in which the players
learn the unknown structure of the game over time, the model begins to break down. Letâ€™s
examine each source of uncertainty, and whether each can be represented in game theory.
Actions: There is no easy way to represent a game where the players have to discover
what actions are available. Consider the game between computer virus writers and security
experts. Part of the problem is anticipating what action the virus writers will try next.
Strategies: Game theory is very good at representing the idea that the other playersâ€™
strategies are initially unknownâ€”as long as we assume all agents are rational. The theory
itself does not say what to do when the other players are less than fully rational. The notion
of a Bayesâ€“Nash equilibrium partially addresses this point: it is an equilibrium with respect
to a playerâ€™s prior probability distribution over the other playersâ€™ strategiesâ€”in other words,
it expresses a playerâ€™s beliefs about the other playersâ€™ likely strategies.
Chance: If a game depends on the roll of a die, it is easy enough to model a chance node
with uniform distribution over the outcomes. But what if it is possible that the die is unfair?
We can represent that with another chance node, higher up in the tree, with two branches for
â€œdie is fairâ€ and â€œdie is unfair,â€ such that the corresponding nodes in each branch are in the
same information set (that is, the players donâ€™t know if the die is fair or not). And what if we
suspect the other opponent does know? Then we add another chance node, with one branch
representing the case where the opponent does know, and one where he doesnâ€™t.
Utilities: What if we donâ€™t know our opponentâ€™s utilities? Again, that can be modeled
with a chance node, such that the other agent knows its own utilities in each branch, but we
donâ€™t. But what if we donâ€™t know our own utilities? For example, how do I know if it is
rational to order the Chefâ€™s salad if I donâ€™t know how much I will like it? We can model that
with yet another chance node specifying an unobservable â€œintrinsic qualityâ€ of the salad.
Thus, we see that game theory is good at representing most sources of uncertaintyâ€”but
at the cost of doubling the size of the tree every time we add another node; a habit which
quickly leads to intractably large trees. Because of these and other problems, game theory
has been used primarily to analyze environments that are at equilibrium, rather than to control
agents within an environment. Next we shall see how it can help design environments.

Section 17.6.

17.6

Mechanism Design

679

M ECHANISM D ESIGN

MECHANISM DESIGN

MECHANISM
CENTER

In the previous section, we asked, â€œGiven a game, what is a rational strategy?â€ In this section, we ask, â€œGiven that agents pick rational strategies, what game should we design?â€ More
specifically, we would like to design a game whose solutions, consisting of each agent pursuing its own rational strategy, result in the maximization of some global utility function. This
problem is called mechanism design, or sometimes inverse game theory. Mechanism design is a staple of economics and political science. Capitalism 101 says that if everyone tries
to get rich, the total wealth of society will increase. But the examples we will discuss show
that proper mechanism design is necessary to keep the invisible hand on track. For collections
of agents, mechanism design allows us to construct smart systems out of a collection of more
limited systemsâ€”even uncooperative systemsâ€”in much the same way that teams of humans
can achieve goals beyond the reach of any individual.
Examples of mechanism design include auctioning off cheap airline tickets, routing
TCP packets between computers, deciding how medical interns will be assigned to hospitals,
and deciding how robotic soccer players will cooperate with their teammates. Mechanism
design became more than an academic subject in the 1990s when several nations, faced with
the problem of auctioning off licenses to broadcast in various frequency bands, lost hundreds
of millions of dollars in potential revenue as a result of poor mechanism design. Formally,
a mechanism consists of (1) a language for describing the set of allowable strategies that
agents may adopt, (2) a distinguished agent, called the center, that collects reports of strategy
choices from the agents in the game, and (3) an outcome rule, known to all agents, that the
center uses to determine the payoffs to each agent, given their strategy choices.

17.6.1 Auctions
AUCTION

ASCENDING-BID
ENGLISH AUCTION

Letâ€™s consider auctions first. An auction is a mechanism for selling some goods to members
of a pool of bidders. For simplicity, we concentrate on auctions with a single item for sale.
Each bidder i has a utility value vi for having the item. In some cases, each bidder has a
private value for the item. For example, the first item sold on eBay was a broken laser
pointer, which sold for $14.83 to a collector of broken laser pointers. Thus, we know that the
collector has vi â‰¥ $14.83, but most other people would have vj * $14.83. In other cases,
such as auctioning drilling rights for an oil tract, the item has a common valueâ€”the tract
will produce some amount of money, X, and all bidders value a dollar equallyâ€”but there is
uncertainty as to what the actual value of X is. Different bidders have different information,
and hence different estimates of the itemâ€™s true value. In either case, bidders end up with their
own vi . Given vi , each bidder gets a chance, at the appropriate time or times in the auction,
to make a bid bi . The highest bid, bmax wins the item, but the price paid need not be bmax ;
thatâ€™s part of the mechanism design.
The best-known auction mechanism is the ascending-bid,8 or English auction, in
which the center starts by asking for a minimum (or reserve) bid bmin . If some bidder is
8

The word â€œauctionâ€ comes from the Latin augere, to increase.

680

EFFICIENT

COLLUSION

STRATEGY-PROOF
TRUTH-REVEALING
REVELATION
PRINCIPLE

Chapter

17.

Making Complex Decisions

willing to pay that amount, the center then asks for bmin + d, for some increment d, and
continues up from there. The auction ends when nobody is willing to bid anymore; then the
last bidder wins the item, paying the price he bid.
How do we know if this is a good mechanism? One goal is to maximize expected
revenue for the seller. Another goal is to maximize a notion of global utility. These goals
overlap to some extent, because one aspect of maximizing global utility is to ensure that the
winner of the auction is the agent who values the item the most (and thus is willing to pay
the most). We say an auction is efficient if the goods go to the agent who values them most.
The ascending-bid auction is usually both efficient and revenue maximizing, but if the reserve
price is set too high, the bidder who values it most may not bid, and if the reserve is set too
low, the seller loses net revenue.
Probably the most important things that an auction mechanism can do is encourage a
sufficient number of bidders to enter the game and discourage them from engaging in collusion. Collusion is an unfair or illegal agreement by two or more bidders to manipulate prices.
It can happen in secret backroom deals or tacitly, within the rules of the mechanism.
For example, in 1999, Germany auctioned ten blocks of cell-phone spectrum with a
simultaneous auction (bids were taken on all ten blocks at the same time), using the rule that
any bid must be a minimum of a 10% raise over the previous bid on a block. There were only
two credible bidders, and the first, Mannesman, entered the bid of 20 million deutschmark
on blocks 1-5 and 18.18 million on blocks 6-10. Why 18.18M? One of T-Mobileâ€™s managers
said they â€œinterpreted Mannesmanâ€™s first bid as an offer.â€ Both parties could compute that
a 10% raise on 18.18M is 19.99M; thus Mannesmanâ€™s bid was interpreted as saying â€œwe
can each get half the blocks for 20M; letâ€™s not spoil it by bidding the prices up higher.â€
And in fact T-Mobile bid 20M on blocks 6-10 and that was the end of the bidding. The
German government got less than they expected, because the two competitors were able to
use the bidding mechanism to come to a tacit agreement on how not to compete. From
the governmentâ€™s point of view, a better result could have been obtained by any of these
changes to the mechanism: a higher reserve price; a sealed-bid first-price auction, so that
the competitors could not communicate through their bids; or incentives to bring in a third
bidder. Perhaps the 10% rule was an error in mechanism design, because it facilitated the
precise signaling from Mannesman to T-Mobile.
In general, both the seller and the global utility function benefit if there are more bidders, although global utility can suffer if you count the cost of wasted time of bidders that
have no chance of winning. One way to encourage more bidders is to make the mechanism
easier for them. After all, if it requires too much research or computation on the part of the
bidders, they may decide to take their money elsewhere. So it is desirable that the bidders
have a dominant strategy. Recall that â€œdominantâ€ means that the strategy works against all
other strategies, which in turn means that an agent can adopt it without regard for the other
strategies. An agent with a dominant strategy can just bid, without wasting time contemplating other agentsâ€™ possible strategies. A mechanism where agents have a dominant strategy
is called a strategy-proof mechanism. If, as is usually the case, that strategy involves the
bidders revealing their true value, vi , then it is called a truth-revealing, or truthful, auction;
the term incentive compatible is also used. The revelation principle states that any mecha-

Section 17.6.

SEALED-BID
AUCTION

SEALED-BID
SECOND-PRICE
AUCTION
VICKREY AUCTION

Mechanism Design

681

nism can be transformed into an equivalent truth-revealing mechanism, so part of mechanism
design is finding these equivalent mechanisms.
It turns out that the ascending-bid auction has most of the desirable properties. The
bidder with the highest value vi gets the goods at a price of bo + d, where bo is the highest
bid among all the other agents and d is the auctioneerâ€™s increment.9 Bidders have a simple
dominant strategy: keep bidding as long as the current cost is below your vi . The mechanism
is not quite truth-revealing, because the winning bidder reveals only that his vi â‰¥ bo + d; we
have a lower bound on vi but not an exact amount.
A disadvantage (from the point of view of the seller) of the ascending-bid auction is
that it can discourage competition. Suppose that in a bid for cell-phone spectrum there is
one advantaged company that everyone agrees would be able to leverage existing customers
and infrastructure, and thus can make a larger profit than anyone else. Potential competitors
can see that they have no chance in an ascending-bid auction, because the advantaged company can always bid higher. Thus, the competitors may not enter at all, and the advantaged
company ends up winning at the reserve price.
Another negative property of the English auction is its high communication costs. Either
the auction takes place in one room or all bidders have to have high-speed, secure communication lines; in either case they have to have the time available to go through several rounds of
bidding. An alternative mechanism, which requires much less communication, is the sealedbid auction. Each bidder makes a single bid and communicates it to the auctioneer, without
the other bidders seeing it. With this mechanism, there is no longer a simple dominant strategy. If your value is vi and you believe that the maximum of all the other agentsâ€™ bids will
be bo , then you should bid bo + , for some small , if that is less than vi . Thus, your bid
depends on your estimation of the other agentsâ€™ bids, requiring you to do more work. Also,
note that the agent with the highest vi might not win the auction. This is offset by the fact
that the auction is more competitive, reducing the bias toward an advantaged bidder.
A small change in the mechanism for sealed-bid auctions produces the sealed-bid
second-price auction, also known as a Vickrey auction.10 In such auctions, the winner pays
the price of the second-highest bid, bo , rather than paying his own bid. This simple modification completely eliminates the complex deliberations required for standard (or first-price)
sealed-bid auctions, because the dominant strategy is now simply to bid vi ; the mechanism is
truth-revealing. Note that the utility of agent i in terms of his bid bi , his value vi , and the best
bid among the other agents, bo , is

(vi âˆ’ bo ) if bi > bo
ui =
0 otherwise.
To see that bi = vi is a dominant strategy, note that when (vi âˆ’ bo ) is positive, any bid
that wins the auction is optimal, and bidding vi in particular wins the auction. On the other
hand, when (vi âˆ’ bo ) is negative, any bid that loses the auction is optimal, and bidding vi in
9 There is actually a small chance that the agent with highest v fails to get the goods, in the case in which
i
bo < vi < bo + d. The chance of this can be made arbitrarily small by decreasing the increment d.
10 Named after William Vickrey (1914â€“1996), who won the 1996 Nobel Prize in economics for this work and
died of a heart attack three days later.

682

REVENUE
EQUIVALENCE
THEOREM

Chapter

17.

Making Complex Decisions

particular loses the auction. So bidding vi is optimal for all possible values of bo , and in fact,
vi is the only bid that has this property. Because of its simplicity and the minimal computation
requirements for both seller and bidders, the Vickrey auction is widely used in constructing
distributed AI systems. Also, Internet search engines conduct over a billion auctions a day
to sell advertisements along with their search results, and online auction sites handle $100
billion a year in goods, all using variants of the Vickrey auction. Note that the expected value
to the seller is bo , which is the same expected return as the limit of the English auction as
the increment d goes to zero. This is actually a very general result: the revenue equivalence
theorem states that, with a few minor caveats, any auction mechanism where risk-neutral
bidders have values vi known only to themselves (but know a probability distribution from
which those values are sampled), will yield the same expected revenue. This principle means
that the various mechanisms are not competing on the basis of revenue generation, but rather
on other qualities.
Although the second-price auction is truth-revealing, it turns out that extending the idea
to multiple goods and using a next-price auction is not truth-revealing. Many Internet search
engines use a mechanism where they auction k slots for ads on a page. The highest bidder
wins the top spot, the second highest gets the second spot, and so on. Each winner pays the
price bid by the next-lower bidder, with the understanding that payment is made only if the
searcher actually clicks on the ad. The top slots are considered more valuable because they
are more likely to be noticed and clicked on. Imagine that three bidders, b1 , b2 and b3 , have
valuations for a click of v1 = 200, v2 = 180, and v3 = 100, and thatk = 2 slots are available,
where it is known that the top spot is clicked on 5% of the time and the bottom spot 2%. If
all bidders bid truthfully, then b1 wins the top slot and pays 180, and has an expected return
of (200 âˆ’ 180) Ã— 0.05 = 1. The second slot goes to b2 . But b1 can see that if she were to bid
anything in the range 101â€“179, she would concede the top slot to b2 , win the second slot, and
yield an expected return of (200 âˆ’ 100) Ã— .02 = 2. Thus, b1 can double her expected return by
bidding less than her true value in this case. In general, bidders in this multislot auction must
spend a lot of energy analyzing the bids of others to determine their best strategy; there is no
simple dominant strategy. Aggarwal et al. (2006) show that there is a unique truthful auction
mechanism for this multislot problem, in which the winner of slot j pays the full price for
slot j just for those additional clicks that are available at slot j and not at slot j + 1. The
winner pays the price for the lower slot for the remaining clicks. In our example, b1 would
bid 200 truthfully, and would pay 180 for the additional .05 âˆ’ .02 = .03 clicks in the top slot,
but would pay only the cost of the bottom slot, 100, for the remaining .02 clicks. Thus, the
total return to b1 would be (200 âˆ’ 180) Ã— .03 + (200 âˆ’ 100) Ã— .02 = 2.6.
Another example of where auctions can come into play within AI is when a collection
of agents are deciding whether to cooperate on a joint plan. Hunsberger and Grosz (2000)
show that this can be accomplished efficiently with an auction in which the agents bid for
roles in the joint plan.

Section 17.6.

Mechanism Design

683

17.6.2 Common goods

TRAGEDY OF THE
COMMONS

EXTERNALITIES

VICKREY-CLARKEGROVES
VCG

Now letâ€™s consider another type of game, in which countries set their policy for controlling
air pollution. Each country has a choice: they can reduce pollution at a cost of -10 points for
implementing the necessary changes, or they can continue to pollute, which gives them a net
utility of -5 (in added health costs, etc.) and also contributes -1 points to every other country
(because the air is shared across countries). Clearly, the dominant strategy for each country
is â€œcontinue to pollute,â€ but if there are 100 countries and each follows this policy, then each
country gets a total utility of -104, whereas if every country reduced pollution, they would
each have a utility of -10. This situation is called the tragedy of the commons: if nobody
has to pay for using a common resource, then it tends to be exploited in a way that leads to
a lower total utility for all agents. It is similar to the prisonerâ€™s dilemma: there is another
solution to the game that is better for all parties, but there appears to be no way for rational
agents to arrive at that solution.
The standard approach for dealing with the tragedy of the commons is to change the
mechanism to one that charges each agent for using the commons. More generally, we need
to ensure that all externalitiesâ€”effects on global utility that are not recognized in the individual agentsâ€™ transactionsâ€”are made explicit. Setting the prices correctly is the difficult
part. In the limit, this approach amounts to creating a mechanism in which each agent is
effectively required to maximize global utility, but can do so by making a local decision. For
this example, a carbon tax would be an example of a mechanism that charges for use of the
commons in a way that, if implemented well, maximizes global utility.
As a final example, consider the problem of allocating some common goods. Suppose a
city decides it wants to install some free wireless Internet transceivers. However, the number
of transceivers they can afford is less than the number of neighborhoods that want them. The
city wants to allocate the goods efficiently, to the neighborhoods
 that would value them the
most. That is, they want to maximize the global utility V = i vi . The problem is that if
they just ask each neighborhood council â€œhow much do you value this free gift?â€ they would
all have an incentive to lie, and report a high value. It turns out there is a mechanism, known
as the Vickrey-Clarke-Groves, or VCG, mechanism, that makes it a dominant strategy for
each agent to report its true utility and that achieves an efficient allocation of the goods. The
trick is that each agent pays a tax equivalent to the loss in global utility that occurs because
of the agentâ€™s presence in the game. The mechanism works like this:
1. The center asks each agent to report its value for receiving an item. Call this bi .
2. The center allocates the goods to a subset of the bidders. We call this subset A, and use
the notation bi (A) to mean the result to i under this allocation: bi if i is in A (that is, i
is a winner),
and 0 otherwise. The center chooses A to maximize total reported utility

B = i bi (A).
3. The center calculates (for each i) the
sum of the reported utilities for all the winners
except i. We use the notation Bâˆ’i = j=i bj (A). The center also computes (for each
i) the allocation that would maximize total global utility if i were not in the game; call
that sum Wâˆ’i .
4. Each agent i pays a tax equal to Wâˆ’i âˆ’ Bâˆ’i .

684

Chapter

17.

Making Complex Decisions

In this example, the VCG rule means that each winner would pay a tax equal to the highest
reported value among the losers. That is, if I report my value as 5, and that causes someone
with value 2 to miss out on an allocation, then I pay a tax of 2. All winners should be happy
because they pay a tax that is less than their value, and all losers are as happy as they can be,
because they value the goods less than the required tax.
Why is it that this mechanism is truth-revealing? First, consider the payoff to agent i,
which is the value of getting an item, minus the tax:
vi (A) âˆ’ (Wâˆ’i âˆ’ Bâˆ’i ) .

(17.14)

Here we distinguish the agentâ€™s true utility, vi , from his reported utility bi (but we are trying
to show that a dominant strategy is bi = vi ). Agent i knows that the center will maximize
global utility using the reported values,


bj (A) = bi (A) +
bj (A)
j

j=i

whereas agent i wants the center to maximize (17.14), which can be rewritten as

bj (A) âˆ’ Wâˆ’i .
vi (A) +
j=i

Since agent i cannot affect the value of Wâˆ’i (it depends only on the other agents), the only
way i can make the center optimize what i wants is to report the true utility, bi = vi .

17.7

S UMMARY
This chapter shows how to use knowledge about the world to make decisions even when the
outcomes of an action are uncertain and the rewards for acting might not be reaped until many
actions have passed. The main points are as follows:
â€¢ Sequential decision problems in uncertain environments, also called Markov decision
processes, or MDPs, are defined by a transition model specifying the probabilistic
outcomes of actions and a reward function specifying the reward in each state.
â€¢ The utility of a state sequence is the sum of all the rewards over the sequence, possibly
discounted over time. The solution of an MDP is a policy that associates a decision
with every state that the agent might reach. An optimal policy maximizes the utility of
the state sequences encountered when it is executed.
â€¢ The utility of a state is the expected utility of the state sequences encountered when
an optimal policy is executed, starting in that state. The value iteration algorithm for
solving MDPs works by iteratively solving the equations relating the utility of each state
to those of its neighbors.
â€¢ Policy iteration alternates between calculating the utilities of states under the current
policy and improving the current policy with respect to the current utilities.
â€¢ Partially observable MDPs, or POMDPs, are much more difficult to solve than are
MDPs. They can be solved by conversion to an MDP in the continuous space of belief

Bibliographical and Historical Notes

685

states; both value iteration and policy iteration algorithms have been devised. Optimal
behavior in POMDPs includes information gathering to reduce uncertainty and therefore make better decisions in the future.
â€¢ A decision-theoretic agent can be constructed for POMDP environments. The agent
uses a dynamic decision network to represent the transition and sensor models, to
update its belief state, and to project forward possible action sequences.
â€¢ Game theory describes rational behavior for agents in situations in which multiple
agents interact simultaneously. Solutions of games are Nash equilibriaâ€”strategy profiles in which no agent has an incentive to deviate from the specified strategy.
â€¢ Mechanism design can be used to set the rules by which agents will interact, in order
to maximize some global utility through the operation of individually rational agents.
Sometimes, mechanisms exist that achieve this goal without requiring each agent to
consider the choices made by other agents.
We shall return to the world of MDPs and POMDP in Chapter 21, when we study reinforcement learning methods that allow an agent to improve its behavior from experience in
sequential, uncertain environments.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Richard Bellman developed the ideas underlying the modern approach to sequential decision
problems while working at the RAND Corporation beginning in 1949. According to his autobiography (Bellman, 1984), he coined the exciting term â€œdynamic programmingâ€ to hide
from a research-phobic Secretary of Defense, Charles Wilson, the fact that his group was
doing mathematics. (This cannot be strictly true, because his first paper using the term (Bellman, 1952) appeared before Wilson became Secretary of Defense in 1953.) Bellmanâ€™s book,
Dynamic Programming (1957), gave the new field a solid foundation and introduced the basic
algorithmic approaches. Ron Howardâ€™s Ph.D. thesis (1960) introduced policy iteration and
the idea of average reward for solving infinite-horizon problems. Several additional results
were introduced by Bellman and Dreyfus (1962). Modified policy iteration is due to van
Nunen (1976) and Puterman and Shin (1978). Asynchronous policy iteration was analyzed
by Williams and Baird (1993), who also proved the policy loss bound in Equation (17.9). The
analysis of discounting in terms of stationary preferences is due to Koopmans (1972). The
texts by Bertsekas (1987), Puterman (1994), and Bertsekas and Tsitsiklis (1996) provide a
rigorous introduction to sequential decision problems. Papadimitriou and Tsitsiklis (1987)
describe results on the computational complexity of MDPs.
Seminal work by Sutton (1988) and Watkins (1989) on reinforcement learning methods
for solving MDPs played a significant role in introducing MDPs into the AI community, as
did the later survey by Barto et al. (1995). (Earlier work by Werbos (1977) contained many
similar ideas, but was not taken up to the same extent.) The connection between MDPs and
AI planning problems was made first by Sven Koenig (1991), who showed how probabilistic
S TRIPS operators provide a compact representation for transition models (see also Wellman,

686

FACTORED MDP

RELATIONAL MDP

Chapter

17.

Making Complex Decisions

1990b). Work by Dean et al. (1993) and Tash and Russell (1994) attempted to overcome
the combinatorics of large state spaces by using a limited search horizon and abstract states.
Heuristics based on the value of information can be used to select areas of the state space
where a local expansion of the horizon will yield a significant improvement in decision quality. Agents using this approach can tailor their effort to handle time pressure and generate
some interesting behaviors such as using familiar â€œbeaten pathsâ€ to find their way around the
state space quickly without having to recompute optimal decisions at each point.
As one might expect, AI researchers have pushed MDPs in the direction of more expressive representations that can accommodate much larger problems than the traditional
atomic representations based on transition matrices. The use of a dynamic Bayesian network
to represent transition models was an obvious idea, but work on factored MDPs (Boutilier
et al., 2000; Koller and Parr, 2000; Guestrin et al., 2003b) extends the idea to structured
representations of the value function with provable improvements in complexity. Relational
MDPs (Boutilier et al., 2001; Guestrin et al., 2003a) go one step further, using structured
representations to handle domains with many related objects.
The observation that a partially observable MDP can be transformed into a regular MDP
over belief states is due to Astrom (1965) and Aoki (1965). The first complete algorithm for
the exact solution of POMDPsâ€”essentially the value iteration algorithm presented in this
chapterâ€”was proposed by Edward Sondik (1971) in his Ph.D. thesis. (A later journal paper
by Smallwood and Sondik (1973) contains some errors, but is more accessible.) Lovejoy
(1991) surveyed the first twenty-five years of POMDP research, reaching somewhat pessimistic conclusions about the feasibility of solving large problems. The first significant
contribution within AI was the Witness algorithm (Cassandra et al., 1994; Kaelbling et al.,
1998), an improved version of POMDP value iteration. Other algorithms soon followed, including an approach due to Hansen (1998) that constructs a policy incrementally in the form
of a finite-state automaton. In this policy representation, the belief state corresponds directly
to a particular state in the automaton. More recent work in AI has focused on point-based
value iteration methods that, at each iteration, generate conditional plans and Î±-vectors for
a finite set of belief states rather than for the entire belief space. Lovejoy (1991) proposed
such an algorithm for a fixed grid of points, an approach taken also by Bonet (2002). An
influential paper by Pineau et al. (2003) suggested generating reachable points by simulating trajectories in a somewhat greedy fashion; Spaan and Vlassis (2005) observe that one
need generate plans for only a small, randomly selected subset of points to improve on the
plans from the previous iteration for all points in the set. Current point-based methodsâ€”
such as point-based policy iteration (Ji et al., 2007)â€”can generate near-optimal solutions for
POMDPs with thousands of states. Because POMDPs are PSPACE-hard (Papadimitriou and
Tsitsiklis, 1987), further progress may require taking advantage of various kinds of structure
within a factored representation.
The online approachâ€”using look-ahead search to select an action for the current belief
stateâ€”was first examined by Satia and Lave (1973). The use of sampling at chance nodes
was explored analytically by Kearns et al. (2000) and Ng and Jordan (2000). The basic
ideas for an agent architecture using dynamic decision networks were proposed by Dean
and Kanazawa (1989a). The book Planning and Control by Dean and Wellman (1991) goes

Bibliographical and Historical Notes

687

into much greater depth, making connections between DBN/DDN models and the classical
control literature on filtering. Tatman and Shachter (1990) showed how to apply dynamic
programming algorithms to DDN models. Russell (1998) explains various ways in which
such agents can be scaled up and identifies a number of open research issues.
The roots of game theory can be traced back to proposals made in the 17th century
by Christiaan Huygens and Gottfried Leibniz to study competitive and cooperative human
interactions scientifically and mathematically. Throughout the 19th century, several leading
economists created simple mathematical examples to analyze particular examples of competitive situations. The first formal results in game theory are due to Zermelo (1913) (who
had, the year before, suggested a form of minimax search for games, albeit an incorrect one).
Emile Borel (1921) introduced the notion of a mixed strategy. John von Neumann (1928)
proved that every two-person, zero-sum game has a maximin equilibrium in mixed strategies
and a well-defined value. Von Neumannâ€™s collaboration with the economist Oskar Morgenstern led to the publication in 1944 of the Theory of Games and Economic Behavior, the
defining book for game theory. Publication of the book was delayed by the wartime paper
shortage until a member of the Rockefeller family personally subsidized its publication.
In 1950, at the age of 21, John Nash published his ideas concerning equilibria in general
(non-zero-sum) games. His definition of an equilibrium solution, although originating in the
work of Cournot (1838), became known as Nash equilibrium. After a long delay because
of the schizophrenia he suffered from 1959 onward, Nash was awarded the Nobel Memorial
Prize in Economics (along with Reinhart Selten and John Harsanyi) in 1994. The Bayesâ€“Nash
equilibrium is described by Harsanyi (1967) and discussed by Kadane and Larkey (1982).
Some issues in the use of game theory for agent control are covered by Binmore (1982).
The prisonerâ€™s dilemma was invented as a classroom exercise by Albert W. Tucker in
1950 (based on an example by Merrill Flood and Melvin Dresher) and is covered extensively
by Axelrod (1985) and Poundstone (1993). Repeated games were introduced by Luce and
Raiffa (1957), and games of partial information in extensive form by Kuhn (1953). The first
practical algorithm for sequential, partial-information games was developed within AI by
Koller et al. (1996); the paper by Koller and Pfeffer (1997) provides a readable introduction
to the field and describe a working system for representing and solving sequential games.
The use of abstraction to reduce a game tree to a size that can be solved with Kollerâ€™s
technique is discussed by Billings et al. (2003). Bowling et al. (2008) show how to use
importance sampling to get a better estimate of the value of a strategy. Waugh et al. (2009)
show that the abstraction approach is vulnerable to making systematic errors in approximating
the equilibrium solution, meaning that the whole approach is on shaky ground: it works for
some games but not others. Korb et al. (1999) experiment with an opponent model in the
form of a Bayesian network. It plays five-card stud about as well as experienced humans.
(Zinkevich et al., 2008) show how an approach that minimizes regret can find approximate
equilibria for abstractions with 1012 states, 100 times more than previous methods.
Game theory and MDPs are combined in the theory of Markov games, also called
stochastic games (Littman, 1994; Hu and Wellman, 1998). Shapley (1953) actually described
the value iteration algorithm independently of Bellman, but his results were not widely appreciated, perhaps because they were presented in the context of Markov games. Evolu-

688

Chapter

17.

Making Complex Decisions

tionary game theory (Smith, 1982; Weibull, 1995) looks at strategy drift over time: if your
opponentâ€™s strategy is changing, how should you react? Textbooks on game theory from
an economics point of view include those by Myerson (1991), Fudenberg and Tirole (1991),
Osborne (2004), and Osborne and Rubinstein (1994); Mailath and Samuelson (2006) concentrate on repeated games. From an AI perspective we have Nisan et al. (2007), Leyton-Brown
and Shoham (2008), and Shoham and Leyton-Brown (2009).
The 2007 Nobel Memorial Prize in Economics went to Hurwicz, Maskin, and Myerson
â€œfor having laid the foundations of mechanism design theoryâ€ (Hurwicz, 1973). The tragedy
of the commons, a motivating problem for the field, was presented by Hardin (1968). The revelation principle is due to Myerson (1986), and the revenue equivalence theorem was developed independently by Myerson (1981) and Riley and Samuelson (1981). Two economists,
Milgrom (1997) and Klemperer (2002), write about the multibillion-dollar spectrum auctions
they were involved in.
Mechanism design is used in multiagent planning (Hunsberger and Grosz, 2000; Stone
et al., 2009) and scheduling (Rassenti et al., 1982). Varian (1995) gives a brief overview with
connections to the computer science literature, and Rosenschein and Zlotkin (1994) present a
book-length treatment with applications to distributed AI. Related work on distributed AI also
goes under other names, including collective intelligence (Tumer and Wolpert, 2000; Segaran,
2007) and market-based control (Clearwater, 1996). Since 2001 there has been an annual
Trading Agents Competition (TAC), in which agents try to make the best profit on a series
of auctions (Wellman et al., 2001; Arunachalam and Sadeh, 2005). Papers on computational
issues in auctions often appear in the ACM Conferences on Electronic Commerce.

E XERCISES
17.1 For the 4 Ã— 3 world shown in Figure 17.1, calculate which squares can be reached
from (1,1) by the action sequence [Up, Up, Right, Right , Right] and with what probabilities.
Explain how this computation is related to the prediction task (see Section 15.2.1) for a hidden
Markov model.
17.2 Select a specific member of the set of policies that are optimal for R(s) > 0 as shown
in Figure 17.2(b), and calculate the fraction of time the agent spends in each state, in the limit,
if the policy is executed forever. (Hint: Construct the state-to-state transition probability
matrix corresponding to the policy and see Exercise 15.2.)
17.3 Suppose that we define the utility of a state sequence to be the maximum reward obtained in any state in the sequence. Show that this utility function does not result in stationary
preferences between state sequences. Is it still possible to define a utility function on states
such that MEU decision making gives optimal behavior?
17.4 Sometimes MDPs are formulated with a reward function R(s, a) that depends on the
action taken or with a reward function R(s, a, s ) that also depends on the outcome state.
a. Write the Bellman equations for these formulations.

Exercises

689
b. Show how an MDP with reward function R(s, a, s ) can be transformed into a different
MDP with reward function R(s, a), such that optimal policies in the new MDP correspond exactly to optimal policies in the original MDP.
c. Now do the same to convert MDPs with R(s, a) into MDPs with R(s).
17.5 For the environment shown in Figure 17.1, find all the threshold values for R(s) such
that the optimal policy changes when the threshold is crossed. You will need a way to calculate the optimal policy and its value for fixed R(s). (Hint: Prove that the value of any fixed
policy varies linearly with R(s).)
17.6

Equation (17.7) on page 654 states that the Bellman operator is a contraction.

a. Show that, for any functions f and g,
| max f (a) âˆ’ max g(a)| â‰¤ max |f (a) âˆ’ g(a)| .
a

a

a

b. Write out an expression for |(B Ui âˆ’ B Ui )(s)| and then apply the result from (a) to
complete the proof that the Bellman operator is a contraction.
17.7 This exercise considers two-player MDPs that correspond to zero-sum, turn-taking
games like those in Chapter 5. Let the players be A and B, and let R(s) be the reward for
player A in state s. (The reward for B is always equal and opposite.)
a. Let UA (s) be the utility of state s when it is Aâ€™s turn to move in s, and let UB (s) be the
utility of state s when it is Bâ€™s turn to move in s. All rewards and utilities are calculated
from Aâ€™s point of view (just as in a minimax game tree). Write down Bellman equations
defining UA (s) and UB (s).
b. Explain how to do two-player value iteration with these equations, and define a suitable
termination criterion.
c. Consider the game described in Figure 5.17 on page 197. Draw the state space (rather
than the game tree), showing the moves by A as solid lines and moves by B as dashed
lines. Mark each state with R(s). You will find it helpful to arrange the states (sA , sB )
on a two-dimensional grid, using sA and sB as â€œcoordinates.â€
d. Now apply two-player value iteration to solve this game, and derive the optimal policy.
17.8 Consider the 3 Ã— 3 world shown in Figure 17.14(a). The transition model is the same
as in the 4 Ã— 3 Figure 17.1: 80% of the time the agent goes in the direction it selects; the rest
of the time it moves at right angles to the intended direction.
Implement value iteration for this world for each value of r below. Use discounted
rewards with a discount factor of 0.99. Show the policy obtained in each case. Explain
intuitively why the value of r leads to each policy.
a.
b.
c.
d.

r = 100
r = âˆ’3
r=0
r = +3

690

r

-1

+10

+50

-1

-1

-1

Start

-1

-1

-1

-50

-1

-1

Chapter

17.

Making Complex Decisions

-1

Â·Â·Â·

-1

-1

-1

-1

+1

+1

+1

+1

Â·Â·Â·
+1

+1

+1

(a)

Â·Â·Â·
(b)

Figure 17.14 (a) 3 Ã— 3 world for Exercise 17.8. The reward for each state is indicated.
The upper right square is a terminal state. (b) 101 Ã— 3 world for Exercise 17.9 (omitting 93
identical columns in the middle). The start state has reward 0.

17.9 Consider the 101 Ã— 3 world shown in Figure 17.14(b). In the start state the agent has
a choice of two deterministic actions, Up or Down, but in the other states the agent has one
deterministic action, Right. Assuming a discounted reward function, for what values of the
discount Î³ should the agent choose Up and for which Down? Compute the utility of each
action as a function of Î³. (Note that this simple example actually reflects many real-world
situations in which one must weigh the value of an immediate action versus the potential
continual long-term consequences, such as choosing to dump pollutants into a lake.)
17.10 Consider an undiscounted MDP having three states, (1, 2, 3), with rewards âˆ’1, âˆ’2,
0, respectively. State 3 is a terminal state. In states 1 and 2 there are two possible actions: a
and b. The transition model is as follows:
â€¢ In state 1, action a moves the agent to state 2 with probability 0.8 and makes the agent
stay put with probability 0.2.
â€¢ In state 2, action a moves the agent to state 1 with probability 0.8 and makes the agent
stay put with probability 0.2.
â€¢ In either state 1 or state 2, action b moves the agent to state 3 with probability 0.1 and
makes the agent stay put with probability 0.9.
Answer the following questions:
a. What can be determined qualitatively about the optimal policy in states 1 and 2?
b. Apply policy iteration, showing each step in full, to determine the optimal policy and
the values of states 1 and 2. Assume that the initial policy has action b in both states.
c. What happens to policy iteration if the initial policy has action a in both states? Does
discounting help? Does the optimal policy depend on the discount factor?
17.11

Consider the 4 Ã— 3 world shown in Figure 17.1.

a. Implement an environment simulator for this environment, such that the specific geography of the environment is easily altered. Some code for doing this is already in the
online code repository.

Exercises

691
b. Create an agent that uses policy iteration, and measure its performance in the environment simulator from various starting states. Perform several experiments from each
starting state, and compare the average total reward received per run with the utility of
the state, as determined by your algorithm.
c. Experiment with increasing the size of the environment. How does the run time for
policy iteration vary with the size of the environment?
17.12 How can the value determination algorithm be used to calculate the expected loss
experienced by an agent using a given set of utility estimates U and an estimated model P ,
compared with an agent using correct values?
17.13 Let the initial belief state b0 for the 4 Ã— 3 POMDP on page 658 be the uniform distribution over the nonterminal states, i.e.,  19 , 19 , 19 , 19 , 19 , 19 , 19 , 19 , 19 , 0, 0. Calculate the exact
belief state b1 after the agent moves Left and its sensor reports 1 adjacent wall. Also calculate
b2 assuming that the same thing happens again.
17.14 What is the time complexity of d steps of POMDP value iteration for a sensorless
environment?
17.15 Consider a version of the two-state POMDP on page 661 in which the sensor is
90% reliable in state 0 but provides no information in state 1 (that is, it reports 0 or 1 with
equal probability). Analyze, either qualitatively or quantitatively, the utility function and the
optimal policy for this problem.
17.16

Show that a dominant strategy equilibrium is a Nash equilibrium, but not vice versa.

17.17 In the childrenâ€™s game of rockâ€“paperâ€“scissors each player reveals at the same time
a choice of rock, paper, or scissors. Paper wraps rock, rock blunts scissors, and scissors cut
paper. In the extended version rockâ€“paperâ€“scissorsâ€“fireâ€“water, fire beats rock, paper, and
scissors; rock, paper, and scissors beat water; and water beats fire. Write out the payoff
matrix and find a mixed-strategy solution to this game.
17.18 The following payoff matrix, from Blinder (1983) by way of Bernstein (1996), shows
a game between politicians and the Federal Reserve.
Fed: contract
Pol: contract
F = 7, P = 1
Pol: do nothing F = 8, P = 2
Pol: expand
F = 3, P = 3

Fed: do nothing
F = 9, P = 4
F = 5, P = 5
F = 2, P = 7

Fed: expand
F = 6, P = 6
F = 4, P = 9
F = 1, P = 8

Politicians can expand or contract fiscal policy, while the Fed can expand or contract monetary policy. (And of course either side can choose to do nothing.) Each side also has preferences for who should do whatâ€”neither side wants to look like the bad guys. The payoffs
shown are simply the rank orderings: 9 for first choice through 1 for last choice. Find the
Nash equilibrium of the game in pure strategies. Is this a Pareto-optimal solution? You might
wish to analyze the policies of recent administrations in this light.

692

Chapter

17.

Making Complex Decisions

17.19 A Dutch auction is similar in an English auction, but rather than starting the bidding
at a low price and increasing, in a Dutch auction the seller starts at a high price and gradually
lowers the price until some buyer is willing to accept that price. (If multiple bidders accept
the price, one is arbitrarily chosen as the winner.) More formally, the seller begins with a
price p and gradually lowers p by increments of d until at least one buyer accepts the price.
Assuming all bidders act rationally, is it true that for arbitrarily small d, a Dutch auction will
always result in the bidder with the highest value for the item obtaining the item? If so, show
mathematically why. If not, explain how it may be possible for the bidder with highest value
for the item not to obtain it.
17.20 Imagine an auction mechanism that is just like an ascending-bid auction, except that
at the end, the winning bidder, the one who bid bmax , pays only bmax /2 rather than bmax .
Assuming all agents are rational, what is the expected revenue to the auctioneer for this
mechanism, compared with a standard ascending-bid auction?
17.21 Teams in the National Hockey League historically received 2 points for winning a
game and 0 for losing. If the game is tied, an overtime period is played; if nobody wins in
overtime, the game is a tie and each team gets 1 point. But league officials felt that teams
were playing too conservatively in overtime (to avoid a loss), and it would be more exciting
if overtime produced a winner. So in 1999 the officials experimented in mechanism design:
the rules were changed, giving a team that loses in overtime 1 point, not 0. It is still 2 points
for a win and 1 for a tie.
a. Was hockey a zero-sum game before the rule change? After?
b. Suppose that at a certain time t in a game, the home team has probability p of winning
in regulation time, probability 0.78 âˆ’ p of losing, and probability 0.22 of going into
overtime, where they have probability q of winning, .9 âˆ’ q of losing, and .1 of tying.
Give equations for the expected value for the home and visiting teams.
c. Imagine that it were legal and ethical for the two teams to enter into a pact where they
agree that they will skate to a tie in regulation time, and then both try in earnest to win
in overtime. Under what conditions, in terms of p and q, would it be rational for both
teams to agree to this pact?
d. Longley and Sankaran (2005) report that since the rule change, the percentage of games
with a winner in overtime went up 18.2%, as desired, but the percentage of overtime
games also went up 3.6%. What does that suggest about possible collusion or conservative play after the rule change?

18

LEARNING FROM
EXAMPLES

In which we describe agents that can improve their behavior through diligent
study of their own experiences.

LEARNING

18.1

An agent is learning if it improves its performance on future tasks after making observations
about the world. Learning can range from the trivial, as exhibited by jotting down a phone
number, to the profound, as exhibited by Albert Einstein, who inferred a new theory of the
universe. In this chapter we will concentrate on one class of learning problem, which seems
restricted but actually has vast applicability: from a collection of inputâ€“output pairs, learn a
function that predicts the output for new inputs.
Why would we want an agent to learn? If the design of the agent can be improved,
why wouldnâ€™t the designers just program in that improvement to begin with? There are three
main reasons. First, the designers cannot anticipate all possible situations that the agent
might find itself in. For example, a robot designed to navigate mazes must learn the layout
of each new maze it encounters. Second, the designers cannot anticipate all changes over
time; a program designed to predict tomorrowâ€™s stock market prices must learn to adapt when
conditions change from boom to bust. Third, sometimes human programmers have no idea
how to program a solution themselves. For example, most people are good at recognizing the
faces of family members, but even the best programmers are unable to program a computer
to accomplish that task, except by using learning algorithms. This chapter first gives an
overview of the various forms of learning, then describes one popular approach, decisiontree learning, in Section 18.3, followed by a theoretical analysis of learning in Sections 18.4
and 18.5. We look at various learning systems used in practice: linear models, nonlinear
models (in particular, neural networks), nonparametric models, and support vector machines.
Finally we show how ensembles of models can outperform a single model.

F ORMS OF L EARNING
Any component of an agent can be improved by learning from data. The improvements, and
the techniques used to make them, depend on four major factors:
â€¢ Which component is to be improved.
693

694

Chapter

18.

Learning from Examples

â€¢ What prior knowledge the agent already has.
â€¢ What representation is used for the data and the component.
â€¢ What feedback is available to learn from.
Components to be learned
Chapter 2 described several agent designs. The components of these agents include:
1. A direct mapping from conditions on the current state to actions.
2. A means to infer relevant properties of the world from the percept sequence.
3. Information about the way the world evolves and about the results of possible actions
the agent can take.
4. Utility information indicating the desirability of world states.
5. Action-value information indicating the desirability of actions.
6. Goals that describe classes of states whose achievement maximizes the agentâ€™s utility.
Each of these components can be learned. Consider, for example, an agent training to become
a taxi driver. Every time the instructor shouts â€œBrake!â€ the agent might learn a conditionâ€“
action rule for when to brake (component 1); the agent also learns every time the instructor
does not shout. By seeing many camera images that it is told contain buses, it can learn
to recognize them (2). By trying actions and observing the resultsâ€”for example, braking
hard on a wet roadâ€”it can learn the effects of its actions (3). Then, when it receives no tip
from passengers who have been thoroughly shaken up during the trip, it can learn a useful
component of its overall utility function (4).
Representation and prior knowledge

INDUCTIVE
LEARNING
DEDUCTIVE
LEARNING

We have seen several examples of representations for agent components: propositional and
first-order logical sentences for the components in a logical agent; Bayesian networks for
the inferential components of a decision-theoretic agent, and so on. Effective learning algorithms have been devised for all of these representations. This chapter (and most of current
machine learning research) covers inputs that form a factored representationâ€”a vector of
attribute valuesâ€”and outputs that can be either a continuous numerical value or a discrete
value. Chapter 19 covers functions and prior knowledge composed of first-order logic sentences, and Chapter 20 concentrates on Bayesian networks.
There is another way to look at the various types of learning. We say that learning
a (possibly incorrect) general function or rule from specific inputâ€“output pairs is called inductive learning. We will see in Chapter 19 that we can also do analytical or deductive
learning: going from a known general rule to a new rule that is logically entailed, but is
useful because it allows more efficient processing.
Feedback to learn from

UNSUPERVISED
LEARNING
CLUSTERING

There are three types of feedback that determine the three main types of learning:
In unsupervised learning the agent learns patterns in the input even though no explicit
feedback is supplied. The most common unsupervised learning task is clustering: detecting

Section 18.2.

REINFORCEMENT
LEARNING

SUPERVISED
LEARNING

SEMI-SUPERVISED
LEARNING

18.2

Supervised Learning

695

potentially useful clusters of input examples. For example, a taxi agent might gradually
develop a concept of â€œgood traffic daysâ€ and â€œbad traffic daysâ€ without ever being given
labeled examples of each by a teacher.
In reinforcement learning the agent learns from a series of reinforcementsâ€”rewards
or punishments. For example, the lack of a tip at the end of the journey gives the taxi agent an
indication that it did something wrong. The two points for a win at the end of a chess game
tells the agent it did something right. It is up to the agent to decide which of the actions prior
to the reinforcement were most responsible for it.
In supervised learning the agent observes some example inputâ€“output pairs and learns
a function that maps from input to output. In component 1 above, the inputs are percepts and
the output are provided by a teacher who says â€œBrake!â€ or â€œTurn left.â€ In component 2, the
inputs are camera images and the outputs again come from a teacher who says â€œthatâ€™s a bus.â€
In 3, the theory of braking is a function from states and braking actions to stopping distance
in feet. In this case the output value is available directly from the agentâ€™s percepts (after the
fact); the environment is the teacher.
In practice, these distinction are not always so crisp. In semi-supervised learning we
are given a few labeled examples and must make what we can of a large collection of unlabeled examples. Even the labels themselves may not be the oracular truths that we hope
for. Imagine that you are trying to build a system to guess a personâ€™s age from a photo. You
gather some labeled examples by snapping pictures of people and asking their age. Thatâ€™s
supervised learning. But in reality some of the people lied about their age. Itâ€™s not just
that there is random noise in the data; rather the inaccuracies are systematic, and to uncover
them is an unsupervised learning problem involving images, self-reported ages, and true (unknown) ages. Thus, both noise and lack of labels create a continuum between supervised and
unsupervised learning.

S UPERVISED L EARNING
The task of supervised learning is this:
Given a training set of N example inputâ€“output pairs

TRAINING SET

(x1 , y1 ), (x2 , y2 ), . . . (xN , yN ) ,
where each yj was generated by an unknown function y = f (x),
discover a function h that approximates the true function f .
HYPOTHESIS

TEST SET

Here x and y can be any value; they need not be numbers. The function h is a hypothesis.1
Learning is a search through the space of possible hypotheses for one that will perform well,
even on new examples beyond the training set. To measure the accuracy of a hypothesis we
give it a test set of examples that are distinct from the training set. We say a hypothesis
1

A note on notation: except where noted, we will use j to index the N examples; xj will always be the input and
yj the output. In cases where the input is specifically a vector of attribute values (beginning with Section 18.3),
we will use xj for the jth example and we will use i to index the n attributes of each example. The elements of
xj are written xj,1 , xj,2 , . . . , xj,n .

696

Chapter
f(x)

f(x)

Learning from Examples

f(x)

x
(a)

18.

f(x)

x
(b)

x
(c)

x
(d)

Figure 18.1 (a) Example (x, f (x)) pairs and a consistent, linear hypothesis. (b) A consistent, degree-7 polynomial hypothesis for the same data set. (c) A different data set, which
admits an exact degree-6 polynomial fit or an approximate linear fit. (d) A simple, exact
sinusoidal fit to the same data set.

GENERALIZATION

CLASSIFICATION

REGRESSION

HYPOTHESIS SPACE

CONSISTENT

OCKHAMâ€™S RAZOR

generalizes well if it correctly predicts the value of y for novel examples. Sometimes the
function f is stochasticâ€”it is not strictly a function of x, and what we have to learn is a
conditional probability distribution, P(Y | x).
When the output y is one of a finite set of values (such as sunny, cloudy or rainy),
the learning problem is called classification, and is called Boolean or binary classification
if there are only two values. When y is a number (such as tomorrowâ€™s temperature), the
learning problem is called regression. (Technically, solving a regression problem is finding
a conditional expectation or average value of y, because the probability that we have found
exactly the right real-valued number for y is 0.)
Figure 18.1 shows a familiar example: fitting a function of a single variable to some data
points. The examples are points in the (x, y) plane, where y = f (x). We donâ€™t know what f
is, but we will approximate it with a function h selected from a hypothesis space, H, which
for this example we will take to be the set of polynomials, such as x5 +3x2 +2. Figure 18.1(a)
shows some data with an exact fit by a straight line (the polynomial 0.4x + 3). The line is
called a consistent hypothesis because it agrees with all the data. Figure 18.1(b) shows a highdegree polynomial that is also consistent with the same data. This illustrates a fundamental
problem in inductive learning: how do we choose from among multiple consistent hypotheses?
One answer is to prefer the simplest hypothesis consistent with the data. This principle is
called Ockhamâ€™s razor, after the 14th-century English philosopher William of Ockham, who
used it to argue sharply against all sorts of complications. Defining simplicity is not easy, but
it seems clear that a degree-1 polynomial is simpler than a degree-7 polynomial, and thus (a)
should be preferred to (b). We will make this intuition more precise in Section 18.4.3.
Figure 18.1(c) shows a second data set. There is no consistent straight line for this
data set; in fact, it requires a degree-6 polynomial for an exact fit. There are just 7 data
points, so a polynomial with 7 parameters does not seem to be finding any pattern in the
data and we do not expect it to generalize well. A straight line that is not consistent with
any of the data points, but might generalize fairly well for unseen values of x, is also shown
in (c). In general, there is a tradeoff between complex hypotheses that fit the training data
well and simpler hypotheses that may generalize better. In Figure 18.1(d) we expand the

Section 18.3.

REALIZABLE

Learning Decision Trees

697

hypothesis space H to allow polynomials over both x and sin(x), and find that the data in
(c) can be fitted exactly by a simple function of the form ax + b + c sin(x). This shows the
importance of the choice of hypothesis space. We say that a learning problem is realizable if
the hypothesis space contains the true function. Unfortunately, we cannot always tell whether
a given learning problem is realizable, because the true function is not known.
In some cases, an analyst looking at a problem is willing to make more fine-grained
distinctions about the hypothesis space, to sayâ€”even before seeing any dataâ€”not just that a
hypothesis is possible or impossible, but rather how probable it is. Supervised learning can
be done by choosing the hypothesis hâˆ— that is most probable given the data:
hâˆ— = argmax P (h|data ) .
hâˆˆH

By Bayesâ€™ rule this is equivalent to
hâˆ— = argmax P (data|h) P (h) .
hâˆˆH

Then we can say that the prior probability P (h) is high for a degree-1 or -2 polynomial,
lower for a degree-7 polynomial, and especially low for degree-7 polynomials with large,
sharp spikes as in Figure 18.1(b). We allow unusual-looking functions when the data say we
really need them, but we discourage them by giving them a low prior probability.
Why not let H be the class of all Java programs, or Turing machines? After all, every
computable function can be represented by some Turing machine, and that is the best we
can do. One problem with this idea is that it does not take into account the computational
complexity of learning. There is a tradeoff between the expressiveness of a hypothesis space
and the complexity of finding a good hypothesis within that space. For example, fitting a
straight line to data is an easy computation; fitting high-degree polynomials is somewhat
harder; and fitting Turing machines is in general undecidable. A second reason to prefer
simple hypothesis spaces is that presumably we will want to use h after we have learned it,
and computing h(x) when h is a linear function is guaranteed to be fast, while computing
an arbitrary Turing machine program is not even guaranteed to terminate. For these reasons,
most work on learning has focused on simple representations.
We will see that the expressivenessâ€“complexity tradeoff is not as simple as it first seems:
it is often the case, as we saw with first-order logic in Chapter 8, that an expressive language
makes it possible for a simple hypothesis to fit the data, whereas restricting the expressiveness
of the language means that any consistent hypothesis must be very complex. For example,
the rules of chess can be written in a page or two of first-order logic, but require thousands of
pages when written in propositional logic.

18.3

L EARNING D ECISION T REES
Decision tree induction is one of the simplest and yet most successful forms of machine
learning. We first describe the representationâ€”the hypothesis spaceâ€”and then show how to
learn a good hypothesis.

698

Chapter

18.

Learning from Examples

18.3.1 The decision tree representation
DECISION TREE

POSITIVE
NEGATIVE

GOAL PREDICATE

A decision tree represents a function that takes as input a vector of attribute values and
returns a â€œdecisionâ€â€”a single output value. The input and output values can be discrete or
continuous. For now we will concentrate on problems where the inputs have discrete values
and the output has exactly two possible values; this is Boolean classification, where each
example input will be classified as true (a positive example) or false (a negative example).
A decision tree reaches its decision by performing a sequence of tests. Each internal
node in the tree corresponds to a test of the value of one of the input attributes, Ai , and
the branches from the node are labeled with the possible values of the attribute, Ai = vik .
Each leaf node in the tree specifies a value to be returned by the function. The decision tree
representation is natural for humans; indeed, many â€œHow Toâ€ manuals (e.g., for car repair)
are written entirely as a single decision tree stretching over hundreds of pages.
As an example, we will build a decision tree to decide whether to wait for a table at a
restaurant. The aim here is to learn a definition for the goal predicate WillWait. First we
list the attributes that we will consider as part of the input:
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

Alternate: whether there is a suitable alternative restaurant nearby.
Bar : whether the restaurant has a comfortable bar area to wait in.
Fri/Sat : true on Fridays and Saturdays.
Hungry: whether we are hungry.
Patrons: how many people are in the restaurant (values are None, Some, and Full ).
Price: the restaurantâ€™s price range ($, $$, $$$).
Raining : whether it is raining outside.
Reservation: whether we made a reservation.
Type: the kind of restaurant (French, Italian, Thai, or burger).
WaitEstimate: the wait estimated by the host (0â€“10 minutes, 10â€“30, 30â€“60, or >60).

Note that every variable has a small set of possible values; the value of WaitEstimate, for
example, is not an integer, rather it is one of the four discrete values 0â€“10, 10â€“30, 30â€“60, or
>60. The decision tree usually used by one of us (SR) for this domain is shown in Figure 18.2.
Notice that the tree ignores the Price and Type attributes. Examples are processed by the tree
starting at the root and following the appropriate branch until a leaf is reached. For instance,
an example with Patrons = Full and WaitEstimate = 0â€“10 will be classified as positive
(i.e., yes, we will wait for a table).

18.3.2 Expressiveness of decision trees
A Boolean decision tree is logically equivalent to the assertion that the goal attribute is true
if and only if the input attributes satisfy one of the paths leading to a leaf with value true.
Writing this out in propositional logic, we have
Goal â‡” (Path 1 âˆ¨ Path 2 âˆ¨ Â· Â· Â·) ,
where each Path is a conjunction of attribute-value tests required to follow that path. Thus,
the whole expression is equivalent to disjunctive normal form (see page 283), which means

Section 18.3.

Learning Decision Trees

699

that any function in propositional logic can be expressed as a decision tree. As an example,
the rightmost path in Figure 18.2 is
Path = (Patrons = Full âˆ§ WaitEstimate = 0â€“10) .
For a wide variety of problems, the decision tree format yields a nice, concise result. But
some functions cannot be represented concisely. For example, the majority function, which
returns true if and only if more than half of the inputs are true, requires an exponentially
large decision tree. In other words, decision trees are good for some kinds of functions and
bad for others. Is there any kind of representation that is efficient for all kinds of functions?
Unfortunately, the answer is no. We can show this in a general way. Consider the set of all
Boolean functions on n attributes. How many different functions are in this set? This is just
the number of different truth tables that we can write down, because the function is defined
by its truth table. A truth table over n attributes has 2n rows, one for each combination of
values of the attributes. We can consider the â€œanswerâ€ column of the table as a 2n -bit number
n
that defines the function. That means there are 22 different functions (and there will be more
than that number of trees, since more than one tree can compute the same function). This is
a scary number. For example, with just the ten Boolean attributes of our restaurant problem
there are 21024 or about 10308 different functions to choose from, and for 20 attributes there
are over 10300,000 . We will need some ingenious algorithms to find good hypotheses in such
a large space.

18.3.3 Inducing decision trees from examples
An example for a Boolean decision tree consists of an (x, y) pair, where x is a vector of values
for the input attributes, and y is a single Boolean output value. A training set of 12 examples

Patrons?
None

Some

No

Full

Yes

WaitEstimate?

>60

30-60

No

Alternate?
No

Yes

Bar?

No

Figure 18.2

Yes
Yes

Yes

0-10

Hungry?

Yes

Reservation?
No

No

10-30

No

Fri/Sat?
No

No

Yes

Yes

Yes

Yes

Yes

Alternate?
No

Yes

Yes

Raining?
No

No

A decision tree for deciding whether to wait for a table.

Yes

Yes

700

Chapter

Learning from Examples

Input Attributes

Example
x1
x2
x3
x4
x5
x6
x7
x8
x9
x10
x11
x12

18.

Alt

Bar

Fri

Hun

Pat

Yes
Yes
No
Yes
Yes
No
No
No
No
Yes
No
Yes

No
No
Yes
No
No
Yes
Yes
No
Yes
Yes
No
Yes

No
No
No
Yes
Yes
No
No
No
Yes
Yes
No
Yes

Yes
Yes
No
Yes
No
Yes
No
Yes
No
Yes
No
Yes

Some
Full
Some
Full
Full
Some
None
Some
Full
Full
None
Full

Figure 18.3

Goal

Price Rain Res

$$$
$
$
$
$$$
$$
$
$$
$
$$$
$
$

No
No
No
Yes
No
Yes
Yes
Yes
Yes
No
No
No

Yes
No
No
No
Yes
Yes
No
Yes
No
Yes
No
No

Type

Est

WillWait

French
Thai
Burger
Thai
French
Italian
Burger
Thai
Burger
Italian
Thai
Burger

0â€“10
30â€“60
0â€“10
10â€“30
>60
0â€“10
0â€“10
0â€“10
>60
10â€“30
0â€“10
30â€“60

y1 = Yes
y2 = No
y3 = Yes
y4 = Yes
y5 = No
y6 = Yes
y7 = No
y8 = Yes
y9 = No
y10 = No
y11 = No
y12 = Yes

Examples for the restaurant domain.

is shown in Figure 18.3. The positive examples are the ones in which the goal WillWait is
true (x1 , x3 , . . .); the negative examples are the ones in which it is false (x2 , x5 , . . .).
We want a tree that is consistent with the examples and is as small as possible. Unfortunately, no matter how we measure size, it is an intractable problem to find the smallest
n
consistent tree; there is no way to efficiently search through the 22 trees. With some simple
heuristics, however, we can find a good approximate solution: a small (but not smallest) consistent tree. The D ECISION -T REE -L EARNING algorithm adopts a greedy divide-and-conquer
strategy: always test the most important attribute first. This test divides the problem up into
smaller subproblems that can then be solved recursively. By â€œmost important attribute,â€ we
mean the one that makes the most difference to the classification of an example. That way, we
hope to get to the correct classification with a small number of tests, meaning that all paths in
the tree will be short and the tree as a whole will be shallow.
Figure 18.4(a) shows that Type is a poor attribute, because it leaves us with four possible
outcomes, each of which has the same number of positive as negative examples. On the other
hand, in (b) we see that Patrons is a fairly important attribute, because if the value is None or
Some, then we are left with example sets for which we can answer definitively (No and Yes,
respectively). If the value is Full , we are left with a mixed set of examples. In general, after
the first attribute test splits up the examples, each outcome is a new decision tree learning
problem in itself, with fewer examples and one less attribute. There are four cases to consider
for these recursive problems:
1. If the remaining examples are all positive (or all negative), then we are done: we can
answer Yes or No. Figure 18.4(b) shows examples of this happening in the None and
Some branches.
2. If there are some positive and some negative examples, then choose the best attribute to
split them. Figure 18.4(b) shows Hungry being used to split the remaining examples.
3. If there are no examples left, it means that no example has been observed for this com-

Section 18.3.

Learning Decision Trees

701

1

3

4

6

8 12

1

3

4

6

2

5

7

9 10 11

2

5

7

9 10 11

Type?
French

Italian

8 12

Patrons?
Thai

1

6

4

8

5

10

2 11

Burger

None

7

9

Some

1

3 12

3

Full

6

8

7 11
No

4 12
2

5

Yes

9 10

Hungry?
No

Yes

4 12
5

(a)

9

2 10

(b)

Figure 18.4 Splitting the examples by testing on attributes. At each node we show the
positive (light boxes) and negative (dark boxes) examples remaining. (a) Splitting on Type
brings us no nearer to distinguishing between positive and negative examples. (b) Splitting
on Patrons does a good job of separating positive and negative examples. After splitting on
Patrons, Hungry is a fairly good second test.

NOISE

bination of attribute values, and we return a default value calculated from the plurality
classification of all the examples that were used in constructing the nodeâ€™s parent. These
are passed along in the variable parent examples.
4. If there are no attributes left, but both positive and negative examples, it means that
these examples have exactly the same description, but different classifications. This can
happen because there is an error or noise in the data; because the domain is nondeterministic; or because we canâ€™t observe an attribute that would distinguish the examples.
The best we can do is return the plurality classification of the remaining examples.
The D ECISION -T REE -L EARNING algorithm is shown in Figure 18.5. Note that the set of
examples is crucial for constructing the tree, but nowhere do the examples appear in the tree
itself. A tree consists of just tests on attributes in the interior nodes, values of attributes on
the branches, and output values on the leaf nodes. The details of the I MPORTANCE function
are given in Section 18.3.4. The output of the learning algorithm on our sample training
set is shown in Figure 18.6. The tree is clearly different from the original tree shown in
Figure 18.2. One might conclude that the learning algorithm is not doing a very good job
of learning the correct function. This would be the wrong conclusion to draw, however. The
learning algorithm looks at the examples, not at the correct function, and in fact, its hypothesis
(see Figure 18.6) not only is consistent with all the examples, but is considerably simpler
than the original tree! The learning algorithm has no reason to include tests for Raining and
Reservation, because it can classify all the examples without them. It has also detected an
interesting and previously unsuspected pattern: the first author will wait for Thai food on
weekends. It is also bound to make some mistakes for cases where it has seen no examples.
For example, it has never seen a case where the wait is 0â€“10 minutes but the restaurant is full.

702

Chapter

18.

Learning from Examples

function D ECISION -T REE -L EARNING(examples, attributes, parent examples) returns
a tree
if examples is empty then return P LURALITY-VALUE(parent examples)
else if all examples have the same classification then return the classification
else if attributes is empty then return P LURALITY-VALUE(examples)
else
A â† argmaxa âˆˆ attributes I MPORTANCE(a, examples)
tree â† a new decision tree with root test A
for each value vk of A do
exs â† {e : e âˆˆ examples and e.A = vk }
subtree â† D ECISION -T REE -L EARNING(exs, attributes âˆ’ A, examples)
add a branch to tree with label (A = vk ) and subtree subtree
return tree
Figure 18.5 The decision-tree learning algorithm. The function I MPORTANCE is described in Section 18.3.4. The function P LURALITY-VALUE selects the most common output
value among a set of examples, breaking ties randomly.

Patrons?
None
No

Some

Hungry?

Yes

French
Yes

Figure 18.6

LEARNING CURVE

Full

No

Yes

No

Type?
Italian

Thai

Burger
Fri/Sat?

No
No

Yes

No

Yes

Yes

The decision tree induced from the 12-example training set.

In that case it says not to wait when Hungry is false, but I (SR) would certainly wait. With
more training examples the learning program could correct this mistake.
We note there is a danger of over-interpreting the tree that the algorithm selects. When
there are several variables of similar importance, the choice between them is somewhat arbitrary: with slightly different input examples, a different variable would be chosen to split on
first, and the whole tree would look completely different. The function computed by the tree
would still be similar, but the structure of the tree can vary widely.
We can evaluate the accuracy of a learning algorithm with a learning curve, as shown
in Figure 18.7. We have 100 examples at our disposal, which we split into a training set and

Section 18.3.

Learning Decision Trees

703

Proportion correct on test set

1
0.9
0.8
0.7
0.6
0.5
0.4
0

20

40
60
Training set size

80

100

Figure 18.7 A learning curve for the decision tree learning algorithm on 100 randomly
generated examples in the restaurant domain. Each data point is the average of 20 trials.

a test set. We learn a hypothesis h with the training set and measure its accuracy with the test
set. We do this starting with a training set of size 1 and increasing one at a time up to size
99. For each size we actually repeat the process of randomly splitting 20 times, and average
the results of the 20 trials. The curve shows that as the training set size grows, the accuracy
increases. (For this reason, learning curves are also called happy graphs.) In this graph we
reach 95% accuracy, and it looks like the curve might continue to increase with more data.

18.3.4 Choosing attribute tests

ENTROPY

The greedy search used in decision tree learning is designed to approximately minimize the
depth of the final tree. The idea is to pick the attribute that goes as far as possible toward
providing an exact classification of the examples. A perfect attribute divides the examples
into sets, each of which are all positive or all negative and thus will be leaves of the tree. The
Patrons attribute is not perfect, but it is fairly good. A really useless attribute, such as Type,
leaves the example sets with roughly the same proportion of positive and negative examples
as the original set.
All we need, then, is a formal measure of â€œfairly goodâ€ and â€œreally uselessâ€ and we can
implement the I MPORTANCE function of Figure 18.5. We will use the notion of information
gain, which is defined in terms of entropy, the fundamental quantity in information theory
(Shannon and Weaver, 1949).
Entropy is a measure of the uncertainty of a random variable; acquisition of information
corresponds to a reduction in entropy. A random variable with only one valueâ€”a coin that
always comes up headsâ€”has no uncertainty and thus its entropy is defined as zero; thus, we
gain no information by observing its value. A flip of a fair coin is equally likely to come up
heads or tails, 0 or 1, and we will soon show that this counts as â€œ1 bitâ€ of entropy. The roll
of a fair four-sided die has 2 bits of entropy, because it takes two bits to describe one of four
equally probable choices. Now consider an unfair coin that comes up heads 99% of the time.
Intuitively, this coin has less uncertainty than the fair coinâ€”if we guess heads weâ€™ll be wrong
only 1% of the timeâ€”so we would like it to have an entropy measure that is close to zero, but

704

Chapter

18.

Learning from Examples

positive. In general, the entropy of a random variable V with values vk , each with probability
P (vk ), is defined as


1
=âˆ’
P (vk ) log2
P (vk ) log2 P (vk ) .
Entropy: H(V ) =
P (vk )
k

k

We can check that the entropy of a fair coin flip is indeed 1 bit:
H(Fair ) = âˆ’(0.5 log 2 0.5 + 0.5 log2 0.5) = 1 .
If the coin is loaded to give 99% heads, we get
H(Loaded ) = âˆ’(0.99 log 2 0.99 + 0.01 log 2 0.01) â‰ˆ 0.08 bits.
It will help to define B(q) as the entropy of a Boolean random variable that is true with
probability q:
B(q) = âˆ’(q log2 q + (1 âˆ’ q) log2 (1 âˆ’ q)) .
Thus, H(Loaded ) = B(0.99) â‰ˆ 0.08. Now letâ€™s get back to decision tree learning. If a
training set contains p positive examples and n negative examples, then the entropy of the
goal attribute on the whole set is


p
.
H(Goal ) = B
p+n
The restaurant training set in Figure 18.3 has p = n = 6, so the corresponding entropy is
B(0.5) or exactly 1 bit. A test on a single attribute A might give us only part of this 1 bit. We
can measure exactly how much by looking at the entropy remaining after the attribute test.
An attribute A with d distinct values divides the training set E into subsets E1 , . . . , Ed .
Each subset Ek has pk positive examples and nk negative examples, so if we go along that
branch, we will need an additional B(pk /(pk + nk )) bits of information to answer the question. A randomly chosen example from the training set has the kth value for the attribute with
probability (pk + nk )/(p + n), so the expected entropy remaining after testing attribute A is
Remainder (A) =

d

p

k +nk

p+n

k
B( pkp+n
).
k

k=1
INFORMATION GAIN

The information gain from the attribute test on A is the expected reduction in entropy:
p
Gain(A) = B( p+n
) âˆ’ Remainder (A) .

In fact Gain(A) is just what we need to implement the I MPORTANCE function. Returning to
the attributes considered in Figure 18.4, we have
%
$2
4
6
B( 02 ) + 12
B( 44 ) + 12
B( 26 ) â‰ˆ 0.541 bits,
Gain(Patrons ) = 1 âˆ’ 12
%
$2
2
4
4
B( 12 ) + 12
B( 12 ) + 12
B( 24 ) + 12
B( 24 ) = 0 bits,
Gain(Type) = 1 âˆ’ 12
confirming our intuition that Patrons is a better attribute to split on. In fact, Patrons has
the maximum gain of any of the attributes and would be chosen by the decision-tree learning
algorithm as the root.

Section 18.3.

Learning Decision Trees

705

18.3.5 Generalization and overfitting

OVERFITTING

DECISION TREE
PRUNING

SIGNIFICANCE TEST
NULL HYPOTHESIS

On some problems, the D ECISION -T REE -L EARNING algorithm will generate a large tree
when there is actually no pattern to be found. Consider the problem of trying to predict
whether the roll of a die will come up as 6 or not. Suppose that experiments are carried out
with various dice and that the attributes describing each training example include the color
of the die, its weight, the time when the roll was done, and whether the experimenters had
their fingers crossed. If the dice are fair, the right thing to learn is a tree with a single node
that says â€œno,â€ But the D ECISION -T REE -L EARNING algorithm will seize on any pattern it
can find in the input. If it turns out that there are 2 rolls of a 7-gram blue die with fingers
crossed and they both come out 6, then the algorithm may construct a path that predicts 6 in
that case. This problem is called overfitting. A general phenomenon, overfitting occurs with
all types of learners, even when the target function is not at all random. In Figure 18.1(b) and
(c), we saw polynomial functions overfitting the data. Overfitting becomes more likely as the
hypothesis space and the number of input attributes grows, and less likely as we increase the
number of training examples.
For decision trees, a technique called decision tree pruning combats overfitting. Pruning works by eliminating nodes that are not clearly relevant. We start with a full tree, as
generated by D ECISION -T REE -L EARNING . We then look at a test node that has only leaf
nodes as descendants. If the test appears to be irrelevantâ€”detecting only noise in the dataâ€”
then we eliminate the test, replacing it with a leaf node. We repeat this process, considering
each test with only leaf descendants, until each one has either been pruned or accepted as is.
The question is, how do we detect that a node is testing an irrelevant attribute? Suppose
we are at a node consisting of p positive and n negative examples. If the attribute is irrelevant,
we would expect that it would split the examples into subsets that each have roughly the same
proportion of positive examples as the whole set, p/(p + n), and so the information gain will
be close to zero.2 Thus, the information gain is a good clue to irrelevance. Now the question
is, how large a gain should we require in order to split on a particular attribute?
We can answer this question by using a statistical significance test. Such a test begins
by assuming that there is no underlying pattern (the so-called null hypothesis). Then the actual data are analyzed to calculate the extent to which they deviate from a perfect absence of
pattern. If the degree of deviation is statistically unlikely (usually taken to mean a 5% probability or less), then that is considered to be good evidence for the presence of a significant
pattern in the data. The probabilities are calculated from standard distributions of the amount
of deviation one would expect to see in random sampling.
In this case, the null hypothesis is that the attribute is irrelevant and, hence, that the
information gain for an infinitely large sample would be zero. We need to calculate the
probability that, under the null hypothesis, a sample of size v = n + p would exhibit the
observed deviation from the expected distribution of positive and negative examples. We can
measure the deviation by comparing the actual numbers of positive and negative examples in
2

The gain will be strictly positive except for the unlikely case where all the proportions are exactly the same.
(See Exercise 18.5.)

706

Chapter

18.

Learning from Examples

each subset, pk and nk , with the expected numbers, pÌ‚k and nÌ‚k , assuming true irrelevance:
pk + n k
pk + n k
nÌ‚k = n Ã—
.
pÌ‚k = p Ã—
p+n
p+n
A convenient measure of the total deviation is given by
Î”=

d

(pk âˆ’ pÌ‚k )2
k=1

Ï‡2 PRUNING

EARLY STOPPING

pÌ‚k

+

(nk âˆ’ nÌ‚k )2
.
nÌ‚k

Under the null hypothesis, the value of Î” is distributed according to the Ï‡2 (chi-squared)
distribution with v âˆ’ 1 degrees of freedom. We can use a Ï‡2 table or a standard statistical
library routine to see if a particular Î” value confirms or rejects the null hypothesis. For
example, consider the restaurant type attribute, with four values and thus three degrees of
freedom. A value of Î” = 7.82 or more would reject the null hypothesis at the 5% level (and a
value of Î” = 11.35 or more would reject at the 1% level). Exercise 18.8 asks you to extend the
D ECISION -T REE -L EARNING algorithm to implement this form of pruning, which is known
as Ï‡2 pruning.
With pruning, noise in the examples can be tolerated. Errors in the exampleâ€™s label (e.g.,
an example (x, Yes) that should be (x, No)) give a linear increase in prediction error, whereas
errors in the descriptions of examples (e.g., Price = $ when it was actually Price = $$) have
an asymptotic effect that gets worse as the tree shrinks down to smaller sets. Pruned trees
perform significantly better than unpruned trees when the data contain a large amount of
noise. Also, the pruned trees are often much smaller and hence easier to understand.
One final warning: You might think that Ï‡2 pruning and information gain look similar,
so why not combine them using an approach called early stoppingâ€”have the decision tree
algorithm stop generating nodes when there is no good attribute to split on, rather than going
to all the trouble of generating nodes and then pruning them away. The problem with early
stopping is that it stops us from recognizing situations where there is no one good attribute,
but there are combinations of attributes that are informative. For example, consider the XOR
function of two binary attributes. If there are roughly equal number of examples for all four
combinations of input values, then neither attribute will be informative, yet the correct thing
to do is to split on one of the attributes (it doesnâ€™t matter which one), and then at the second
level we will get splits that are informative. Early stopping would miss this, but generateand-then-prune handles it correctly.

18.3.6 Broadening the applicability of decision trees
In order to extend decision tree induction to a wider variety of problems, a number of issues
must be addressed. We will briefly mention several, suggesting that a full understanding is
best obtained by doing the associated exercises:
â€¢ Missing data: In many domains, not all the attribute values will be known for every
example. The values might have gone unrecorded, or they might be too expensive to
obtain. This gives rise to two problems: First, given a complete decision tree, how
should one classify an example that is missing one of the test attributes? Second, how

Section 18.3.

Learning Decision Trees

707

should one modify the information-gain formula when some examples have unknown
values for the attribute? These questions are addressed in Exercise 18.9.

GAIN RATIO

SPLIT POINT

REGRESSION TREE

â€¢ Multivalued attributes: When an attribute has many possible values, the information
gain measure gives an inappropriate indication of the attributeâ€™s usefulness. In the extreme case, an attribute such as ExactTime has a different value for every example,
which means each subset of examples is a singleton with a unique classification, and
the information gain measure would have its highest value for this attribute. But choosing this split first is unlikely to yield the best tree. One solution is to use the gain ratio
(Exercise 18.10). Another possibility is to allow a Boolean test of the form A = vk , that
is, picking out just one of the possible values for an attribute, leaving the remaining
values to possibly be tested later in the tree.
â€¢ Continuous and integer-valued input attributes: Continuous or integer-valued attributes such as Height and Weight, have an infinite set of possible values. Rather than
generate infinitely many branches, decision-tree learning algorithms typically find the
split point that gives the highest information gain. For example, at a given node in
the tree, it might be the case that testing on Weight > 160 gives the most information. Efficient methods exist for finding good split points: start by sorting the values
of the attribute, and then consider only split points that are between two examples in
sorted order that have different classifications, while keeping track of the running totals
of positive and negative examples on each side of the split point. Splitting is the most
expensive part of real-world decision tree learning applications.
â€¢ Continuous-valued output attributes: If we are trying to predict a numerical output
value, such as the price of an apartment, then we need a regression tree rather than a
classification tree. A regression tree has at each leaf a linear function of some subset
of numerical attributes, rather than a single value. For example, the branch for twobedroom apartments might end with a linear function of square footage, number of
bathrooms, and average income for the neighborhood. The learning algorithm must
decide when to stop splitting and begin applying linear regression (see Section 18.6)
over the attributes.
A decision-tree learning system for real-world applications must be able to handle all of
these problems. Handling continuous-valued variables is especially important, because both
physical and financial processes provide numerical data. Several commercial packages have
been built that meet these criteria, and they have been used to develop thousands of fielded
systems. In many areas of industry and commerce, decision trees are usually the first method
tried when a classification method is to be extracted from a data set. One important property
of decision trees is that it is possible for a human to understand the reason for the output of the
learning algorithm. (Indeed, this is a legal requirement for financial decisions that are subject
to anti-discrimination laws.) This is a property not shared by some other representations,
such as neural networks.

708

18.4

Chapter

18.

Learning from Examples

E VALUATING AND C HOOSING THE B EST H YPOTHESIS

STATIONARITY
ASSUMPTION

We want to learn a hypothesis that fits the future data best. To make that precise we need
to define â€œfuture dataâ€ and â€œbest.â€ We make the stationarity assumption: that there is a
probability distribution over examples that remains stationary over time. Each example data
point (before we see it) is a random variable Ej whose observed value ej = (xj , yj ) is sampled
from that distribution, and is independent of the previous examples:
P(Ej |Ejâˆ’1 , Ejâˆ’2 , . . .) = P(Ej ) ,
and each example has an identical prior probability distribution:
P(Ej ) = P(Ejâˆ’1 ) = P(Ejâˆ’2 ) = Â· Â· Â· .

I.I.D.

ERROR RATE

HOLDOUT
CROSS-VALIDATION

K-FOLD
CROSS-VALIDATION

LEAVE-ONE-OUT
CROSS-VALIDATION
LOOCV
PEEKING

Examples that satisfy these assumptions are called independent and identically distributed or
i.i.d.. An i.i.d. assumption connects the past to the future; without some such connection, all
bets are offâ€”the future could be anything. (We will see later that learning can still occur if
there are slow changes in the distribution.)
The next step is to define â€œbest fit.â€ We define the error rate of a hypothesis as the
proportion of mistakes it makesâ€”the proportion of times that h(x) = y for an (x, y) example.
Now, just because a hypothesis h has a low error rate on the training set does not mean that
it will generalize well. A professor knows that an exam will not accurately evaluate students
if they have already seen the exam questions. Similarly, to get an accurate evaluation of a
hypothesis, we need to test it on a set of examples it has not seen yet. The simplest approach is
the one we have seen already: randomly split the available data into a training set from which
the learning algorithm produces h and a test set on which the accuracy of h is evaluated. This
method, sometimes called holdout cross-validation, has the disadvantage that it fails to use
all the available data; if we use half the data for the test set, then we are only training on half
the data, and we may get a poor hypothesis. On the other hand, if we reserve only 10% of
the data for the test set, then we may, by statistical chance, get a poor estimate of the actual
accuracy.
We can squeeze more out of the data and still get an accurate estimate using a technique
called k-fold cross-validation. The idea is that each example serves double dutyâ€”as training
data and test data. First we split the data into k equal subsets. We then perform k rounds of
learning; on each round 1/k of the data is held out as a test set and the remaining examples
are used as training data. The average test set score of the k rounds should then be a better
estimate than a single score. Popular values for k are 5 and 10â€”enough to give an estimate
that is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time.
The extreme is k = n, also known as leave-one-out cross-validation or LOOCV.
Despite the best efforts of statistical methodologists, users frequently invalidate their
results by inadvertently peeking at the test data. Peeking can happen like this: A learning
algorithm has various â€œknobsâ€ that can be twiddled to tune its behaviorâ€”for example, various
different criteria for choosing the next attribute in decision tree learning. The researcher
generates hypotheses for various different settings of the knobs, measures their error rates on
the test set, and reports the error rate of the best hypothesis. Alas, peeking has occurred! The

Section 18.4.

VALIDATION SET

Evaluating and Choosing the Best Hypothesis

709

reason is that the hypothesis was selected on the basis of its test set error rate, so information
about the test set has leaked into the learning algorithm.
Peeking is a consequence of using test-set performance to both choose a hypothesis and
evaluate it. The way to avoid this is to really hold the test set outâ€”lock it away until you
are completely done with learning and simply wish to obtain an independent evaluation of
the final hypothesis. (And then, if you donâ€™t like the results . . . you have to obtain, and lock
away, a completely new test set if you want to go back and find a better hypothesis.) If the
test set is locked away, but you still want to measure performance on unseen data as a way of
selecting a good hypothesis, then divide the available data (without the test set) into a training
set and a validation set. The next section shows how to use validation sets to find a good
tradeoff between hypothesis complexity and goodness of fit.

18.4.1 Model selection: Complexity versus goodness of fit

MODEL SELECTION

OPTIMIZATION

WRAPPER

In Figure 18.1 (page 696) we showed that higher-degree polynomials can fit the training data
better, but when the degree is too high they will overfit, and perform poorly on validation data.
Choosing the degree of the polynomial is an instance of the problem of model selection. You
can think of the task of finding the best hypothesis as two tasks: model selection defines the
hypothesis space and then optimization finds the best hypothesis within that space.
In this section we explain how to select among models that are parameterized by size.
For example, with polynomials we have size = 1 for linear functions, size = 2 for quadratics,
and so on. For decision trees, the size could be the number of nodes in the tree. In all cases
we want to find the value of the size parameter that best balances underfitting and overfitting
to give the best test set accuracy.
An algorithm to perform model selection and optimization is shown in Figure 18.8. It
is a wrapper that takes a learning algorithm as an argument (D ECISION -T REE -L EARNING ,
for example). The wrapper enumerates models according to a parameter, size. For each size,
it uses cross validation on Learner to compute the average error rate on the training and
test sets. We start with the smallest, simplest models (which probably underfit the data), and
iterate, considering more complex models at each step, until the models start to overfit. In
Figure 18.9 we see typical curves: the training set error decreases monotonically (although
there may in general be slight random variation), while the validation set error decreases at
first, and then increases when the model begins to overfit. The cross-validation procedure
picks the value of size with the lowest validation set error; the bottom of the U-shaped curve.
We then generate a hypothesis of that size, using all the data (without holding out any of it).
Finally, of course, we should evaluate the returned hypothesis on a separate test set.
This approach requires that the learning algorithm accept a parameter, size, and deliver
a hypothesis of that size. As we said, for decision tree learning, the size can be the number of
nodes. We can modify D ECISION -T REE -L EARNER so that it takes the number of nodes as
an input, builds the tree breadth-first rather than depth-first (but at each level it still chooses
the highest gain attribute first), and stops when it reaches the desired number of nodes.

710

Chapter

18.

Learning from Examples

function C ROSS -VALIDATION -W RAPPER (Learner , k , examples) returns a hypothesis
local variables: errT , an array, indexed by size, storing training-set error rates
errV , an array, indexed by size, storing validation-set error rates
for size = 1 to âˆž do
errT [size], errV [size] â† C ROSS -VALIDATION (Learner, size, k , examples)
if errT has converged then do
best size â† the value of size with minimum errV [size]
return Learner (best size, examples)
function C ROSS -VALIDATION(Learner , size, k , examples) returns two values:
average training set error rate, average validation set error rate
fold errT â† 0; fold errV â† 0
for fold = 1 to k do
training set , validation set â† PARTITION(examples, fold , k )
h â† Learner (size, training set )
fold errT â† fold errT + E RROR -R ATE (h, training set)
fold errV â† fold errV +E RROR -R ATE (h, validation set )
return fold errT /k , fold errV /k
Figure 18.8 An algorithm to select the model that has the lowest error rate on validation
data by building models of increasing complexity, and choosing the one with best empirical error rate on validation data. Here errT means error rate on the training data, and
errV means error rate on the validation data. Learner (size, examples) returns a hypothesis whose complexity is set by the parameter size, and which is trained on the examples.
PARTITION(examples, fold, k) splits examples into two subsets: a validation set of size N/k
and a training set with all the other examples. The split is different for each value of fold.

18.4.2 From error rates to loss

LOSS FUNCTION

So far, we have been trying to minimize error rate. This is clearly better than maximizing
error rate, but it is not the full story. Consider the problem of classifying email messages
as spam or non-spam. It is worse to classify non-spam as spam (and thus potentially miss
an important message) then to classify spam as non-spam (and thus suffer a few seconds of
annoyance). So a classifier with a 1% error rate, where almost all the errors were classifying
spam as non-spam, would be better than a classifier with only a 0.5% error rate, if most of
those errors were classifying non-spam as spam. We saw in Chapter 16 that decision-makers
should maximize expected utility, and utility is what learners should maximize as well. In
machine learning it is traditional to express utilities by means of a loss function. The loss
function L(x, y, yÌ‚) is defined as the amount of utility lost by predicting h(x) = yÌ‚ when the
correct answer is f (x) = y:
L(x, y, yÌ‚) = Utility(result of using y given an input x)
âˆ’ Utility(result of using yÌ‚ given an input x)

Section 18.4.

Evaluating and Choosing the Best Hypothesis

711

60
Validation Set Error
Training Set Error
50

Error rate

40

30

20

10

0
1

2

3

4

5
6
Tree size

7

8

9

10

Figure 18.9 Error rates on training data (lower, dashed line) and validation data (upper,
solid line) for different size decision trees. We stop when the training set error rate asymptotes, and then choose the tree with minimal error on the validation set; in this case the tree
of size 7 nodes.

This is the most general formulation of the loss function. Often a simplified version is used,
L(y, yÌ‚), that is independent of x. We will use the simplified version for the rest of this
chapter, which means we canâ€™t say that it is worse to misclassify a letter from Mom than it
is to misclassify a letter from our annoying cousin, but we can say it is 10 times worse to
classify non-spam as spam than vice-versa:
L(spam, nospam ) = 1,

L(nospam , spam) = 10 .

Note that L(y, y) is always zero; by definition there is no loss when you guess exactly right.
For functions with discrete outputs, we can enumerate a loss value for each possible misclassification, but we canâ€™t enumerate all the possibilities for real-valued data. If f (x) is
137.035999, we would be fairly happy with h(x) = 137.036, but just how happy should we
be? In general small errors are better than large ones; two functions that implement that idea
are the absolute value of the difference (called the L1 loss), and the square of the difference
(called the L2 loss). If we are content with the idea of minimizing error rate, we can use
the L0/1 loss function, which has a loss of 1 for an incorrect answer and is appropriate for
discrete-valued outputs:
Absolute value loss: L1 (y, yÌ‚) = |y âˆ’ yÌ‚|
Squared error loss: L2 (y, yÌ‚) = (y âˆ’ yÌ‚)2
0/1 loss:
L0/1 (y, yÌ‚) = 0 if y = yÌ‚, else 1

GENERALIZATION
LOSS

The learning agent can theoretically maximize its expected utility by choosing the hypothesis that minimizes expected loss over all inputâ€“output pairs it will see. It is meaningless
to talk about this expectation without defining a prior probability distribution, P(X, Y ) over
examples. Let E be the set of all possible inputâ€“output examples. Then the expected generalization loss for a hypothesis h (with respect to loss function L) is

712

Chapter
GenLoss L (h) =



18.

Learning from Examples

L(y, h(x)) P (x, y) ,

(x,y)âˆˆE

and the best hypothesis, hâˆ— , is the one with the minimum expected generalization loss:
hâˆ— = argmin GenLoss L (h) .
hâˆˆH

EMPIRICAL LOSS

Because P (x, y) is not known, the learning agent can only estimate generalization loss with
empirical loss on a set of examples, E:
1 
EmpLoss L,E (h) =
L(y, h(x)) .
N
(x,y)âˆˆE

The estimated best hypothesis hÌ‚âˆ— is then the one with minimum empirical loss:
hÌ‚âˆ— = argmin EmpLoss L,E (h) .
hâˆˆH

NOISE

SMALL-SCALE
LEARNING

LARGE-SCALE
LEARNING

There are four reasons why hÌ‚âˆ— may differ from the true function, f : unrealizability, variance,
noise, and computational complexity. First, f may not be realizableâ€”may not be in Hâ€”or
may be present in such a way that other hypotheses are preferred. Second, a learning algorithm will return different hypotheses for different sets of examples, even if those sets are
drawn from the same true function f , and those hypotheses will make different predictions
on new examples. The higher the variance among the predictions, the higher the probability
of significant error. Note that even when the problem is realizable, there will still be random
variance, but that variance decreases towards zero as the number of training examples increases. Third, f may be nondeterministic or noisyâ€”it may return different values for f (x)
each time x occurs. By definition, noise cannot be predicted; in many cases, it arises because
the observed labels y are the result of attributes of the environment not listed in x. And finally,
when H is complex, it can be computationally intractable to systematically search the whole
hypothesis space. The best we can do is a local search (hill climbing or greedy search) that
explores only part of the space. That gives us an approximation error. Combining the sources
of error, weâ€™re left with an estimation of an approximation of the true function f .
Traditional methods in statistics and the early years of machine learning concentrated
on small-scale learning, where the number of training examples ranged from dozens to the
low thousands. Here the generalization error mostly comes from the approximation error of
not having the true f in the hypothesis space, and from estimation error of not having enough
training examples to limit variance. In recent years there has been more emphasis on largescale learning, often with millions of examples. Here the generalization error is dominated
by limits of computation: there is enough data and a rich enough model that we could find an
h that is very close to the true f , but the computation to find it is too complex, so we settle
for a sub-optimal approximation.

18.4.3 Regularization
In Section 18.4.1, we saw how to do model selection with cross-validation on model size. An
alternative approach is to search for a hypothesis that directly minimizes the weighted sum of

Section 18.5.

The Theory of Learning

713

empirical loss and the complexity of the hypothesis, which we will call the total cost:
Cost (h) = EmpLoss(h) + Î» Complexity (h)
hÌ‚âˆ— = argmin Cost (h) .
hâˆˆH

REGULARIZATION

FEATURE SELECTION

MINIMUM
DESCRIPTION
LENGTH

18.5

Here Î» is a parameter, a positive number that serves as a conversion rate between loss and
hypothesis complexity (which after all are not measured on the same scale). This approach
combines loss and complexity into one metric, allowing us to find the best hypothesis all at
once. Unfortunately we still need to do a cross-validation search to find the hypothesis that
generalizes best, but this time it is with different values of Î» rather than size. We select the
value of Î» that gives us the best validation set score.
This process of explicitly penalizing complex hypotheses is called regularization (because it looks for a function that is more regular, or less complex). Note that the cost function
requires us to make two choices: the loss function and the complexity measure, which is
called a regularization function. The choice of regularization function depends on the hypothesis space. For example, a good regularization function for polynomials is the sum of
the squares of the coefficientsâ€”keeping the sum small would guide us away from the wiggly
polynomials in Figure 18.1(b) and (c). We will show an example of this type of regularization
in Section 18.6.
Another way to simplify models is to reduce the dimensions that the models work with.
A process of feature selection can be performed to discard attributes that appear to be irrelevant. Ï‡2 pruning is a kind of feature selection.
It is in fact possible to have the empirical loss and the complexity measured on the
same scale, without the conversion factor Î»: they can both be measured in bits. First encode
the hypothesis as a Turing machine program, and count the number of bits. Then count
the number of bits required to encode the data, where a correctly predicted example costs
zero bits and the cost of an incorrectly predicted example depends on how large the error is.
The minimum description length or MDL hypothesis minimizes the total number of bits
required. This works well in the limit, but for smaller problems there is a difficulty in that
the choice of encoding for the programâ€”for example, how best to encode a decision tree
as a bit stringâ€”affects the outcome. In Chapter 20 (page 805), we describe a probabilistic
interpretation of the MDL approach.

T HE T HEORY OF L EARNING
The main unanswered question in learning is this: How can we be sure that our learning
algorithm has produced a hypothesis that will predict the correct value for previously unseen
inputs? In formal terms, how do we know that the hypothesis h is close to the target function
f if we donâ€™t know what f is? These questions have been pondered for several centuries.
In more recent decades, other questions have emerged: how many examples do we need
to get a good h? What hypothesis space should we use? If the hypothesis space is very
complex, can we even find the best h, or do we have to settle for a local maximum in the

714

COMPUTATIONAL
LEARNING THEORY

PROBABLY
APPROXIMATELY
CORRECT
PAC LEARNING

Chapter

18.

Learning from Examples

space of hypotheses? How complex should h be? How do we avoid overfitting? This section
examines these questions.
Weâ€™ll start with the question of how many examples are needed for learning. We saw
from the learning curve for decision tree learning on the restaurant problem (Figure 18.7 on
page 703) that improves with more training data. Learning curves are useful, but they are
specific to a particular learning algorithm on a particular problem. Are there some more general principles governing the number of examples needed in general? Questions like this are
addressed by computational learning theory, which lies at the intersection of AI, statistics,
and theoretical computer science. The underlying principle is that any hypothesis that is seriously wrong will almost certainly be â€œfound outâ€ with high probability after a small number
of examples, because it will make an incorrect prediction. Thus, any hypothesis that is consistent with a sufficiently large set of training examples is unlikely to be seriously wrong: that is,
it must be probably approximately correct. Any learning algorithm that returns hypotheses
that are probably approximately correct is called a PAC learning algorithm; we can use this
approach to provide bounds on the performance of various learning algorithms.
PAC-learning theorems, like all theorems, are logical consequences of axioms. When
a theorem (as opposed to, say, a political pundit) states something about the future based on
the past, the axioms have to provide the â€œjuiceâ€ to make that connection. For PAC learning,
the juice is provided by the stationarity assumption introduced on page 708, which says that
future examples are going to be drawn from the same fixed distribution P(E) = P(X, Y )
as past examples. (Note that we do not have to know what distribution that is, just that it
doesnâ€™t change.) In addition, to keep things simple, we will assume that the true function f
is deterministic and is a member of the hypothesis class H that is being considered.
The simplest PAC theorems deal with Boolean functions, for which the 0/1 loss is appropriate. The error rate of a hypothesis h, defined informally earlier, is defined formally
here as the expected generalization error for examples drawn from the stationary distribution:

error(h) = GenLoss L0/1 (h) =
L0/1 (y, h(x)) P (x, y) .
x,y

-BALL

In other words, error(h) is the probability that h misclassifies a new example. This is the
same quantity being measured experimentally by the learning curves shown earlier.
A hypothesis h is called approximately correct if error(h) â‰¤ , where  is a small
constant. We will show that we can find an N such that, after seeing N examples, with high
probability, all consistent hypotheses will be approximately correct. One can think of an
approximately correct hypothesis as being â€œcloseâ€ to the true function in hypothesis space: it
lies inside what is called the -ball around the true function f . The hypothesis space outside
this ball is called Hbad .
We can calculate the probability that a â€œseriously wrongâ€ hypothesis hb âˆˆ Hbad is
consistent with the first N examples as follows. We know that error(hb ) > . Thus, the
probability that it agrees with a given example is at most 1 âˆ’ . Since the examples are
independent, the bound for N examples is
P (hb agrees with N examples) â‰¤ (1 âˆ’ )N .

Section 18.5.

The Theory of Learning

715

The probability that Hbad contains at least one consistent hypothesis is bounded by the sum
of the individual probabilities:
P (Hbad contains a consistent hypothesis) â‰¤ |Hbad |(1 âˆ’ )N â‰¤ |H|(1 âˆ’ )N ,
where we have used the fact that |Hbad | â‰¤ |H|. We would like to reduce the probability of
this event below some small number Î´:
|H|(1 âˆ’ )N â‰¤ Î´ .

SAMPLE
COMPLEXITY

Given that 1 âˆ’  â‰¤ eâˆ’ , we can achieve this if we allow the algorithm to see


1
1
ln + ln |H|
(18.1)
Nâ‰¥

Î´
examples. Thus, if a learning algorithm returns a hypothesis that is consistent with this many
examples, then with probability at least 1 âˆ’ Î´, it has error at most . In other words, it is
probably approximately correct. The number of required examples, as a function of  and Î´,
is called the sample complexity of the hypothesis space.
As we saw earlier, if H is the set of all Boolean functions on n attributes, then |H| =
n
22 . Thus, the sample complexity of the space grows as 2n . Because the number of possible
examples is also 2n , this suggests that PAC-learning in the class of all Boolean functions
requires seeing all, or nearly all, of the possible examples. A momentâ€™s thought reveals the
reason for this: H contains enough hypotheses to classify any given set of examples in all
possible ways. In particular, for any set of N examples, the set of hypotheses consistent with
those examples contains equal numbers of hypotheses that predict xN +1 to be positive and
hypotheses that predict xN +1 to be negative.
To obtain real generalization to unseen examples, then, it seems we need to restrict
the hypothesis space H in some way; but of course, if we do restrict the space, we might
eliminate the true function altogether. There are three ways to escape this dilemma. The first,
which we will cover in Chapter 19, is to bring prior knowledge to bear on the problem. The
second, which we introduced in Section 18.4.3, is to insist that the algorithm return not just
any consistent hypothesis, but preferably a simple one (as is done in decision tree learning). In
cases where finding simple consistent hypotheses is tractable, the sample complexity results
are generally better than for analyses based only on consistency. The third escape, which
we pursue next, is to focus on learnable subsets of the entire hypothesis space of Boolean
functions. This approach relies on the assumption that the restricted language contains a
hypothesis h that is close enough to the true function f ; the benefits are that the restricted
hypothesis space allows for effective generalization and is typically easier to search. We now
examine one such restricted language in more detail.

18.5.1 PAC learning example: Learning decision lists
DECISION LISTS

We now show how to apply PAC learning to a new hypothesis space: decision lists. A
decision list consists of a series of tests, each of which is a conjunction of literals. If a
test succeeds when applied to an example description, the decision list specifies the value
to be returned. If the test fails, processing continues with the next test in the list. Decision
lists resemble decision trees, but their overall structure is simpler: they branch only in one

716

Chapter

Patrons(x, Some)

No

Patrons(x, Full)

Yes

Learning from Examples

^ Fri/Sat(x)

No

No

Yes

Yes

Figure 18.10

18.

Yes

A decision list for the restaurant problem.

direction. In contrast, the individual tests are more complex. Figure 18.10 shows a decision
list that represents the following hypothesis:
WillWait â‡” (Patrons = Some) âˆ¨ (Patrons = Full âˆ§ Fri /Sat ) .

k-DL
k-DT

If we allow tests of arbitrary size, then decision lists can represent any Boolean function
(Exercise 18.14). On the other hand, if we restrict the size of each test to at most k literals,
then it is possible for the learning algorithm to generalize successfully from a small number
of examples. We call this language k-DL. The example in Figure 18.10 is in 2-DL. It is easy to
show (Exercise 18.14) that k-DL includes as a subset the language k-DT, the set of all decision
trees of depth at most k. It is important to remember that the particular language referred to
by k-DL depends on the attributes used to describe the examples. We will use the notation
k-DL(n) to denote a k-DL language using n Boolean attributes.
The first task is to show that k-DL is learnableâ€”that is, that any function in k-DL can
be approximated accurately after training on a reasonable number of examples. To do this,
we need to calculate the number of hypotheses in the language. Let the language of testsâ€”
conjunctions of at most k literals using n attributesâ€”be Conj (n, k). Because a decision list
is constructed of tests, and because each test can be attached to either a Yes or a No outcome
or can be absent from the decision list, there are at most 3|Conj (n,k)| distinct sets of component
tests. Each of these sets of tests can be in any order, so
|k-DL(n)| â‰¤ 3|Conj (n,k)| |Conj (n, k)|! .
The number of conjunctions of k literals from n attributes is given by

k 

2n
= O(nk ) .
|Conj (n, k)| =
i
i=0

Hence, after some work, we obtain
|k-DL(n)| = 2O(n

k

log2 (nk ))

.

We can plug this into Equation (18.1) to show that the number of examples needed for PAClearning a k-DL function is polynomial in n:


1
1
k
k
ln + O(n log2 (n )) .
Nâ‰¥

Î´
Therefore, any algorithm that returns a consistent decision list will PAC-learn a k-DL function
in a reasonable number of examples, for small k.
The next task is to find an efficient algorithm that returns a consistent decision list.
We will use a greedy algorithm called D ECISION -L IST-L EARNING that repeatedly finds a

Section 18.6.

Regression and Classification with Linear Models

717

function D ECISION -L IST-L EARNING(examples) returns a decision list, or failure
if examples is empty then return the trivial decision list No
t â† a test that matches a nonempty subset examples t of examples
such that the members of examples t are all positive or all negative
if there is no such t then return failure
if the examples in examples t are positive then o â† Yes else o â† No
return a decision list with initial test t and outcome o and remaining tests given by
D ECISION -L IST-L EARNING(examples âˆ’ examples t )
Figure 18.11

An algorithm for learning decision lists.

Proportion correct on test set

1
0.9
0.8
Decision tree
Decision list

0.7
0.6
0.5
0.4
0

20

40
60
Training set size

80

100

Figure 18.12 Learning curve for D ECISION -L IST-L EARNING algorithm on the restaurant
data. The curve for D ECISION -T REE -L EARNING is shown for comparison.

test that agrees exactly with some subset of the training set. Once it finds such a test, it
adds it to the decision list under construction and removes the corresponding examples. It
then constructs the remainder of the decision list, using just the remaining examples. This is
repeated until there are no examples left. The algorithm is shown in Figure 18.11.
This algorithm does not specify the method for selecting the next test to add to the
decision list. Although the formal results given earlier do not depend on the selection method,
it would seem reasonable to prefer small tests that match large sets of uniformly classified
examples, so that the overall decision list will be as compact as possible. The simplest strategy
is to find the smallest test t that matches any uniformly classified subset, regardless of the size
of the subset. Even this approach works quite well, as Figure 18.12 suggests.

18.6

R EGRESSION AND C LASSIFICATION WITH L INEAR M ODELS

LINEAR FUNCTION

Now it is time to move on from decision trees and lists to a different hypothesis space, one
that has been used for hundred of years: the class of linear functions of continuous-valued

718

Chapter

18.

Learning from Examples

House price in $1000

1000
900
800
700
600
500

Loss

400
w0

300
500

1000 1500 2000 2500 3000 3500
House size in square feet

w1

(a)

(b)

Figure 18.13 (a) Data points of price versus floor space of houses for sale in Berkeley,
CA, in July 2009, along with the linear function hypothesis
 that minimizes squared error
loss: y = 0.232x + 246. (b) Plot of the loss function j (w1 xj + w0 âˆ’ yj )2 for various
values of w0 , w1 . Note that the loss function is convex, with a single global minimum.

inputs. Weâ€™ll start with the simplest case: regression with a univariate linear function, otherwise known as â€œfitting a straight line.â€ Section 18.6.2 covers the multivariate case. Sections 18.6.3 and 18.6.4 show how to turn linear functions into classifiers by applying hard
and soft thresholds.

18.6.1 Univariate linear regression

WEIGHT

A univariate linear function (a straight line) with input x and output y has the form y = w1 x+
w0 , where w0 and w1 are real-valued coefficients to be learned. We use the letter w because
we think of the coefficients as weights; the value of y is changed by changing the relative
weight of one term or another. Weâ€™ll define w to be the vector [w0 , w1 ], and define
hw (x) = w1 x + w0 .

LINEAR REGRESSION

Figure 18.13(a) shows an example of a training set of n points in the x, y plane, each point
representing the size in square feet and the price of a house offered for sale. The task of
finding the hw that best fits these data is called linear regression. To fit a line to the data, all
we have to do is find the values of the weights [w0 , w1 ] that minimize the empirical loss. It is
traditional (going back to Gauss3 ) to use the squared loss function, L2 , summed over all the
training examples:
Loss(hw ) =

N

j=1

3

L2 (yj , hw (xj )) =

N


(yj âˆ’ hw (xj ))2 =

j=1

N


(yj âˆ’ (w1 xj + w0 ))2 .

j=1

Gauss showed that if the yj values have normally distributed noise, then the most likely values of w1 and w0
are obtained by minimizing the sum of the squares of the errors.

Section 18.6.

Regression and Classification with Linear Models

719


2
We would like to find wâˆ— = argminw Loss(hw ). The sum N
j = 1 (yj âˆ’ (w1 xj + w0 )) is
minimized when its partial derivatives with respect to w0 and w1 are zero:
N
N
âˆ‚ 
âˆ‚ 
(yj âˆ’ (w1 xj + w0 ))2 = 0 and
(yj âˆ’ (w1 xj + w0 ))2 = 0 .
âˆ‚w0
âˆ‚w1
j=1

These equations have a unique solution:





N ( xj yj ) âˆ’ ( xj )( yj )
 2

;
w
=
(
y
âˆ’
w
(
xj ))/N .
w1 =
0
j
1
N ( xj ) âˆ’ ( xj )2

WEIGHT SPACE

GRADIENT DESCENT

(18.2)

j=1

(18.3)

For the example in Figure 18.13(a), the solution is w1 = 0.232, w0 = 246, and the line with
those weights is shown as a dashed line in the figure.
Many forms of learning involve adjusting weights to minimize a loss, so it helps to
have a mental picture of whatâ€™s going on in weight spaceâ€”the space defined by all possible
settings of the weights. For univariate linear regression, the weight space defined by w0 and
w1 is two-dimensional, so we can graph the loss as a function of w0 and w1 in a 3D plot (see
Figure 18.13(b)). We see that the loss function is convex, as defined on page 133; this is true
for every linear regression problem with an L2 loss function, and implies that there are no
local minima. In some sense thatâ€™s the end of the story for linear models; if we need to fit
lines to data, we apply Equation (18.3).4
To go beyond linear models, we will need to face the fact that the equations defining
minimum loss (as in Equation (18.2)) will often have no closed-form solution. Instead, we
will face a general optimization search problem in a continuous weight space. As indicated
in Section 4.2 (page 129), such problems can be addressed by a hill-climbing algorithm that
follows the gradient of the function to be optimized. In this case, because we are trying to
minimize the loss, we will use gradient descent. We choose any starting point in weight
spaceâ€”here, a point in the (w0 , w1 ) planeâ€”and then move to a neighboring point that is
downhill, repeating until we converge on the minimum possible loss:
w â† any point in the parameter space
loop until convergence do
for each wi in w do
wi â† wi âˆ’ Î±

LEARNING RATE

âˆ‚
Loss(w)
âˆ‚wi

(18.4)

The parameter Î±, which we called the step size in Section 4.2, is usually called the learning
rate when we are trying to minimize loss in a learning problem. It can be a fixed constant, or
it can decay over time as the learning process proceeds.
For univariate regression, the loss function is a quadratic function, so the partial derivaâˆ‚ 2
x = 2x and
tive will be a linear function. (The only calculus you need to know is that âˆ‚x
âˆ‚
x
=
1.)
Letâ€™s
first
work
out
the
partial
derivativesâ€”the
slopesâ€”in
the
simplified
case of
âˆ‚x
4

With some caveats: the L2 loss function is appropriate when there is normally-distributed noise that is independent of x; all results rely on the stationarity assumption; etc.

720

Chapter

18.

Learning from Examples

only one training example, (x, y):
âˆ‚
âˆ‚
Loss(w) =
(y âˆ’ hw (x))2
âˆ‚wi
âˆ‚wi
âˆ‚
(y âˆ’ hw (x))
âˆ‚wi
âˆ‚
(y âˆ’ (w1 x + w0 )) ,
(18.5)
= 2(y âˆ’ hw (x)) Ã—
âˆ‚wi
applying this to both w0 and w1 we get:
âˆ‚
âˆ‚
Loss(w) = âˆ’2(y âˆ’ hw (x)) ;
Loss(w) = âˆ’2(y âˆ’ hw (x)) Ã— x
âˆ‚w0
âˆ‚w1
Then, plugging this back into Equation (18.4), and folding the 2 into the unspecified learning
rate Î±, we get the following learning rule for the weights:
= 2(y âˆ’ hw (x)) Ã—

w0 â† w0 + Î± (y âˆ’ hw (x)) ;

w1 â† w1 + Î± (y âˆ’ hw (x)) Ã— x

These updates make intuitive sense: if hw (x) > y, i.e., the output of the hypothesis is too
large, reduce w0 a bit, and reduce w1 if x was a positive input but increase w1 if x was a
negative input.
The preceding equations cover one training example. For N training examples, we want
to minimize the sum of the individual losses for each example. The derivative of a sum is the
sum of the derivatives, so we have:


(yj âˆ’ hw (xj )) ; w1 â† w1 + Î±
(yj âˆ’ hw (xj )) Ã— xj .
w0 â† w0 + Î±
j
BATCH GRADIENT
DESCENT

STOCHASTIC
GRADIENT DESCENT

j

These updates constitute the batch gradient descent learning rule for univariate linear regression. Convergence to the unique global minimum is guaranteed (as long as we pick Î±
small enough) but may be very slow: we have to cycle through all the training data for every
step, and there may be many steps.
There is another possibility, called stochastic gradient descent, where we consider
only a single training point at a time, taking a step after each one using Equation (18.5).
Stochastic gradient descent can be used in an online setting, where new data are coming in
one at a time, or offline, where we cycle through the same data as many times as is necessary, taking a step after considering each single example. It is often faster than batch gradient
descent. With a fixed learning rate Î±, however, it does not guarantee convergence; it can oscillate around the minimum without settling down. In some cases, as we see later, a schedule
of decreasing learning rates (as in simulated annealing) does guarantee convergence.

18.6.2 Multivariate linear regression
MULTIVARIATE
LINEAR REGRESSION

We can easily extend to multivariate linear regression problems, in which each example xj
is an n-element vector.5 Our hypothesis space is the set of functions of the form

wi xj,i .
hsw (xj ) = w0 + w1 xj,1 + Â· Â· Â· + wn xj,n = w0 +
i
5

The reader may wish to consult Appendix A for a brief summary of linear algebra.

Section 18.6.

Regression and Classification with Linear Models

721

The w0 term, the intercept, stands out as different from the others. We can fix that by inventing
a dummy input attribute, xj,0 , which is defined as always equal to 1. Then h is simply the
dot product of the weights and the input vector (or equivalently, the matrix product of the
transpose of the weights and the input vector):

wi xj,i .
hsw (xj ) = w Â· xj = w xj =
i

The best vector of weights, wâˆ— , minimizes squared-error loss over the examples:

L2 (yj , w Â· xj ) .
wâˆ— = argmin
w

j

Multivariate linear regression is actually not much more complicated than the univariate case
we just covered. Gradient descent will reach the (unique) minimum of the loss function; the
update equation for each weight wi is

xj,i (yj âˆ’ hw (xj )) .
(18.6)
wi â† wi + Î±
j

DATA MATRIX

It is also possible to solve analytically for the w that minimizes loss. Let y be the vector of
outputs for the training examples, and X be the data matrix, i.e., the matrix of inputs with
one n-dimensional example per row. Then the solution
wâˆ— = (X X)âˆ’1 X y
minimizes the squared error.
With univariate linear regression we didnâ€™t have to worry about overfitting. But with
multivariate linear regression in high-dimensional spaces it is possible that some dimension
that is actually irrelevant appears by chance to be useful, resulting in overfitting.
Thus, it is common to use regularization on multivariate linear functions to avoid overfitting. Recall that with regularization we minimize the total cost of a hypothesis, counting
both the empirical loss and the complexity of the hypothesis:
Cost (h) = EmpLoss(h) + Î» Complexity (h) .
For linear functions the complexity can be specified as a function of the weights. We can
consider a family of regularization functions:

|wi |q .
Complexity (hw ) = Lq (w) =
i

functions,6

SPARSE MODEL

with q = 1 we have L1 regularization, which minimizes the sum of
As with loss
the absolute values; with q = 2, L2 regularization minimizes the sum of squares. Which regularization function should you pick? That depends on the specific problem, but L1 regularization has an important advantage: it tends to produce a sparse model. That is, it often sets
many weights to zero, effectively declaring the corresponding attributes to be irrelevantâ€”just
as D ECISION -T REE -L EARNING does (although by a different mechanism). Hypotheses that
discard attributes can be easier for a human to understand, and may be less likely to overfit.
6

It is perhaps confusing that L1 and L2 are used for both loss functions and regularization functions. They need
not be used in pairs: you could use L2 loss with L1 regularization, or vice versa.

722

Chapter

18.

Learning from Examples

w2

w2

w*

w*

w1

w1

Figure 18.14 Why L1 regularization tends to produce a sparse model. (a) With L1 regularization (box), the minimal achievable loss (concentric contours) often occurs on an axis,
meaning a weight of zero. (b) With L2 regularization (circle), the minimal loss is likely to
occur anywhere on the circle, giving no preference to zero weights.

Figure 18.14 gives an intuitive explanation of why L1 regularization leads to weights of
zero, while L2 regularization does not. Note that minimizing Loss(w) + Î»Complexity (w)
is equivalent to minimizing Loss(w) subject to the constraint that Complexity (w) â‰¤ c, for
some constant c that is related to Î». Now, in Figure 18.14(a) the diamond-shaped box represents the set of points w in two-dimensional weight space that have L1 complexity less than
c; our solution will have to be somewhere inside this box. The concentric ovals represent
contours of the loss function, with the minimum loss at the center. We want to find the point
in the box that is closest to the minimum; you can see from the diagram that, for an arbitrary
position of the minimum and its contours, it will be common for the corner of the box to find
its way closest to the minimum, just because the corners are pointy. And of course the corners
are the points that have a value of zero in some dimension. In Figure 18.14(b), weâ€™ve done
the same for the L2 complexity measure, which represents a circle rather than a diamond.
Here you can see that, in general, there is no reason for the intersection to appear on one of
the axes; thus L2 regularization does not tend to produce zero weights. The result is that the
number of examples required to find a good h is linear in the number of irrelevant features for
L2 regularization, but only logarithmic with L1 regularization. Empirical evidence on many
problems supports this analysis.
Another way to look at it is that L1 regularization takes the dimensional axes seriously,
while L2 treats them as arbitrary. The L2 function is spherical, which makes it rotationally
invariant: Imagine a set of points in a plane, measured by their x and y coordinates. Now
imagine rotating the axes by 45o . Youâ€™d get a different set of (x , y  ) values representing
the same points. If you apply L2 regularization before and after rotating, you get exactly
the same point as the answer (although the point would be described with the new (x , y  )
coordinates). That is appropriate when the choice of axes really is arbitraryâ€”when it doesnâ€™t
matter whether your two dimensions are distances north and east; or distances north-east and

Section 18.6.

Regression and Classification with Linear Models

723

south-east. With L1 regularization youâ€™d get a different answer, because the L1 function is not
rotationally invariant. That is appropriate when the axes are not interchangeable; it doesnâ€™t
make sense to rotate â€œnumber of bathroomsâ€ 45o towards â€œlot size.â€

18.6.3 Linear classifiers with a hard threshold

7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5

x2

x2

Linear functions can be used to do classification as well as regression. For example, Figure 18.15(a) shows data points of two classes: earthquakes (which are of interest to seismologists) and underground explosions (which are of interest to arms control experts). Each point
is defined by two input values, x1 and x2 , that refer to body and surface wave magnitudes
computed from the seismic signal. Given these training data, the task of classification is to
learn a hypothesis h that will take new (x1 , x2 ) points and return either 0 for earthquakes or
1 for explosions.

4.5

5

5.5

6

6.5

7

7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
4.5

5

5.5

6

x1

x1

(a)

(b)

6.5

7

Figure 18.15 (a) Plot of two seismic data parameters, body wave magnitude x1 and surface wave magnitude x2 , for earthquakes (white circles) and nuclear explosions (black circles) occurring between 1982 and 1990 in Asia and the Middle East (Kebeasy et al., 1998).
Also shown is a decision boundary between the classes. (b) The same domain with more data
points. The earthquakes and explosions are no longer linearly separable.
DECISION
BOUNDARY

LINEAR SEPARATOR
LINEAR
SEPARABILITY

A decision boundary is a line (or a surface, in higher dimensions) that separates the
two classes. In Figure 18.15(a), the decision boundary is a straight line. A linear decision
boundary is called a linear separator and data that admit such a separator are called linearly
separable. The linear separator in this case is defined by
x2 = 1.7x1 âˆ’ 4.9

or

âˆ’ 4.9 + 1.7x1 âˆ’ x2 = 0 .

The explosions, which we want to classify with value 1, are to the right of this line with higher
values of x1 and lower values of x2 , so they are points for which âˆ’4.9 + 1.7x1 âˆ’ x2 > 0,
while earthquakes have âˆ’4.9 + 1.7x1 âˆ’ x2 < 0. Using the convention of a dummy input
x0 = 1, we can write the classification hypothesis as
hw (x) = 1 if w Â· x â‰¥ 0 and 0 otherwise.

724

THRESHOLD
FUNCTION

Chapter

18.

Learning from Examples

Alternatively, we can think of h as the result of passing the linear function w Â· x through a
threshold function:
hw (x) = Threshold (w Â· x) where Threshold (z) = 1 if z â‰¥ 0 and 0 otherwise.
The threshold function is shown in Figure 18.17(a).
Now that the hypothesis hw (x) has a well-defined mathematical form, we can think
about choosing the weights w to minimize the loss. In Sections 18.6.1 and 18.6.2, we did
this both in closed form (by setting the gradient to zero and solving for the weights) and
by gradient descent in weight space. Here, we cannot do either of those things because the
gradient is zero almost everywhere in weight space except at those points where w Â· x = 0,
and at those points the gradient is undefined.
There is, however, a simple weight update rule that converges to a solutionâ€”that is, a
linear separator that classifies the data perfectlyâ€“provided the data are linearly separable. For
a single example (x, y), we have
wi â† wi + Î± (y âˆ’ hw (x)) Ã— xi

PERCEPTRON
LEARNING RULE

(18.7)

which is essentially identical to the Equation (18.6), the update rule for linear regression! This
rule is called the perceptron learning rule, for reasons that will become clear in Section 18.7.
Because we are considering a 0/1 classification problem, however, the behavior is somewhat
different. Both the true value y and the hypothesis output hw (x) are either 0 or 1, so there are
three possibilities:
â€¢ If the output is correct, i.e., y = hw (x), then the weights are not changed.
â€¢ If y is 1 but hw (x) is 0, then wi is increased when the corresponding input xi is positive
and decreased when xi is negative. This makes sense, because we want to make w Â· x
bigger so that hw (x) outputs a 1.
â€¢ If y is 0 but hw (x) is 1, then wi is decreased when the corresponding input xi is positive
and increased when xi is negative. This makes sense, because we want to make w Â· x
smaller so that hw (x) outputs a 0.

TRAINING CURVE

Typically the learning rule is applied one example at a time, choosing examples at random
(as in stochastic gradient descent). Figure 18.16(a) shows a training curve for this learning
rule applied to the earthquake/explosion data shown in Figure 18.15(a). A training curve
measures the classifier performance on a fixed training set as the learning process proceeds
on that same training set. The curve shows the update rule converging to a zero-error linear
separator. The â€œconvergenceâ€ process isnâ€™t exactly pretty, but it always works. This particular
run takes 657 steps to converge, for a data set with 63 examples, so each example is presented
roughly 10 times on average. Typically, the variation across runs is very large.
We have said that the perceptron learning rule converges to a perfect linear separator
when the data points are linearly separable, but what if they are not? This situation is all
too common in the real world. For example, Figure 18.15(b) adds back in the data points
left out by Kebeasy et al. (1998) when they plotted the data shown in Figure 18.15(a). In
Figure 18.16(b), we show the perceptron learning rule failing to converge even after 10,000
steps: even though it hits the minimum-error solution (three errors) many times, the algorithm keeps changing the weights. In general, the perceptron rule may not converge to a

Regression and Classification with Linear Models

725

1

1

0.9

0.9

0.8
0.7
0.6
0.5
0.4

Proportion correct

1
0.9

Proportion correct

Proportion correct

Section 18.6.

0.8
0.7
0.6
0.5
0.4

0

100 200 300 400 500 600 700
Number of weight updates

0.8
0.7
0.6
0.5
0.4

0

20000 40000 60000 80000 100000
Number of weight updates

(a)

(b)

0

20000 40000 60000 80000 100000
Number of weight updates

(c)

Figure 18.16 (a) Plot of total training-set accuracy vs. number of iterations through the
training set for the perceptron learning rule, given the earthquake/explosion data in Figure 18.15(a). (b) The same plot for the noisy, non-separable data in Figure 18.15(b); note
the change in scale of the x-axis. (c) The same plot as in (b), with a learning rate schedule
Î±(t) = 1000/(1000 + t).

stable solution for fixed learning rate Î±, but if Î± decays as O(1/t) where t is the iteration
number, then the rule can be shown to converge to a minimum-error solution when examples
are presented in a random sequence. 7 It can also be shown that finding the minimum-error
solution is NP-hard, so one expects that many presentations of the examples will be required
for convergence to be achieved. Figure 18.16(b) shows the training process with a learning
rate schedule Î±(t) = 1000/(1000 + t): convergence is not perfect after 100,000 iterations,
but it is much better than the fixed-Î± case.

18.6.4 Linear classification with logistic regression
We have seen that passing the output of a linear function through the threshold function
creates a linear classifier; yet the hard nature of the threshold causes some problems: the
hypothesis hw (x) is not differentiable and is in fact a discontinuous function of its inputs and
its weights; this makes learning with the perceptron rule a very unpredictable adventure. Furthermore, the linear classifier always announces a completely confident prediction of 1 or 0,
even for examples that are very close to the boundary; in many situations, we really need
more gradated predictions.
All of these issues can be resolved to a large extent by softening the threshold functionâ€”
approximating the hard threshold with a continuous, differentiable function. In Chapter 14
(page 522), we saw two functions that look like soft thresholds: the integral of the standard
normal distribution (used for the probit model) and the logistic function (used for the logit
model). Although the two functions are very similar in shape, the logistic function
Logistic(z) =
7

1
1 + eâˆ’z

Technically, we require that
these conditions.

Pâˆž

t=1

Î±(t) = âˆž and

Pâˆž

t=1

Î±2 (t) < âˆž. The decay Î±(t) = O(1/t) satisfies

726

Chapter

18.

Learning from Examples

1

1

0.5

1
0.8
0.6
0.4
0.2
0

0.5

-2

0
x1

0

2

4

6

-2-4
20x
4
2
6
10 8

0
-8 -6 -4 -2 0 2 4 6 8

-6 -4 -2

(a)

0

2

4

6

(b)

(c)

Figure 18.17 (a) The hard threshold function Threshold (z) with 0/1 output. Note
that the function is nondifferentiable at z = 0. (b) The logistic function, Logistic(z) =
1
1+eâˆ’z , also known as the sigmoid function. (c) Plot of a logistic regression hypothesis
hw (x) = Logistic(w Â· x) for the data shown in Figure 18.15(b).

has more convenient mathematical properties. The function is shown in Figure 18.17(b).
With the logistic function replacing the threshold function, we now have
hw (x) = Logistic(w Â· x) =

LOGISTIC
REGRESSION

CHAIN RULE

1
.
1 + eâˆ’wÂ·x

An example of such a hypothesis for the two-input earthquake/explosion problem is shown in
Figure 18.17(c). Notice that the output, being a number between 0 and 1, can be interpreted
as a probability of belonging to the class labeled 1. The hypothesis forms a soft boundary
in the input space and gives a probability of 0.5 for any input at the center of the boundary
region, and approaches 0 or 1 as we move away from the boundary.
The process of fitting the weights of this model to minimize loss on a data set is called
logistic regression. There is no easy closed-form solution to find the optimal value of w with
this model, but the gradient descent computation is straightforward. Because our hypotheses
no longer output just 0 or 1, we will use the L2 loss function; also, to keep the formulas
readable, weâ€™ll use g to stand for the logistic function, with g its derivative.
For a single example (x, y), the derivation of the gradient is the same as for linear
regression (Equation (18.5)) up to the point where the actual form of h is inserted. (For this
derivation, we will need the chain rule: âˆ‚g(f (x))/âˆ‚x = g (f (x)) âˆ‚f (x)/âˆ‚x.) We have
âˆ‚
âˆ‚
Loss(w) =
(y âˆ’ hw (x))2
âˆ‚wi
âˆ‚wi
âˆ‚
(y âˆ’ hw (x))
âˆ‚wi
âˆ‚
wÂ·x
= âˆ’2(y âˆ’ hw (x)) Ã— g (w Â· x) Ã—
âˆ‚wi
= âˆ’2(y âˆ’ hw (x)) Ã— g (w Â· x) Ã— xi .
= 2(y âˆ’ hw (x)) Ã—

Artificial Neural Networks

0.9
0.8
0.7
0.6
0.5
0.4
0

1000 2000 3000 4000
Number of weight updates

5000

1

Squared error per example

1

727

Squared error per example

Squared error per example

Section 18.7.

0.9
0.8
0.7
0.6
0.5
0.4
0

20000 40000 60000 80000 100000
Number of weight updates

(a)

(b)

1
0.9
0.8
0.7
0.6
0.5
0.4
0

20000 40000 60000 80000 100000
Number of weight updates

(c)

Figure 18.18 Repeat of the experiments in Figure 18.16 using logistic regression and
squared error. The plot in (a) covers 5000 iterations rather than 1000, while (b) and (c) use
the same scale.

The derivative g of the logistic function satisfies g (z) = g(z)(1 âˆ’ g(z)), so we have
g (w Â· x) = g(w Â· x)(1 âˆ’ g(w Â· x)) = hw (x)(1 âˆ’ hw (x))
so the weight update for minimizing the loss is
wi â† wi + Î± (y âˆ’ hw (x)) Ã— hw (x)(1 âˆ’ hw (x)) Ã— xi .

(18.8)

Repeating the experiments of Figure 18.16 with logistic regression instead of the linear
threshold classifier, we obtain the results shown in Figure 18.18. In (a), the linearly separable case, logistic regression is somewhat slower to converge, but behaves much more
predictably. In (b) and (c), where the data are noisy and nonseparable, logistic regression
converges far more quickly and reliably. These advantages tend to carry over into real-world
applications and logistic regression has become one of the most popular classification techniques for problems in medicine, marketing and survey analysis, credit scoring, public health,
and other applications.

18.7

A RTIFICIAL N EURAL N ETWORKS

NEURAL NETWORK

We turn now to what seems to be a somewhat unrelated topic: the brain. In fact, as we
will see, the technical ideas we have discussed so far in this chapter turn out to be useful in
building mathematical models of the brainâ€™s activity; conversely, thinking about the brain has
helped in extending the scope of the technical ideas.
Chapter 1 touched briefly on the basic findings of neuroscienceâ€”in particular, the hypothesis that mental activity consists primarily of electrochemical activity in networks of
brain cells called neurons. (Figure 1.2 on page 11 showed a schematic diagram of a typical
neuron.) Inspired by this hypothesis, some of the earliest AI work aimed to create artificial
neural networks. (Other names for the field include connectionism, parallel distributed
processing, and neural computation.) Figure 18.19 shows a simple mathematical model
of the neuron devised by McCulloch and Pitts (1943). Roughly speaking, it â€œfiresâ€ when a
linear combination of its inputs exceeds some (hard or soft) thresholdâ€”that is, it implements

728

Chapter

Learning from Examples

Bias Weight

a0 = 1

ai

18.

aj = g(inj)

w0,j
wi,j

Input
Links

inj

g

Î£

Input
Activation
Function Function

aj

Output

Output
Links

Figure 
18.19 A simple mathematical model for a neuron. The unitâ€™s output activation is
n
aj = g( i = 0 wi,j ai ), where ai is the output activation of unit i and wi,j is the weight on the
link from unit i to this unit.

COMPUTATIONAL
NEUROSCIENCE

a linear classifier of the kind described in the preceding section. A neural network is just a
collection of units connected together; the properties of the network are determined by its
topology and the properties of the â€œneurons.â€
Since 1943, much more detailed and realistic models have been developed, both for
neurons and for larger systems in the brain, leading to the modern field of computational
neuroscience. On the other hand, researchers in AI and statistics became interested in the
more abstract properties of neural networks, such as their ability to perform distributed computation, to tolerate noisy inputs, and to learn. Although we understand now that other kinds
of systemsâ€”including Bayesian networksâ€”have these properties, neural networks remain
one of the most popular and effective forms of learning system and are worthy of study in
their own right.

18.7.1 Neural network structures
UNIT
LINK
ACTIVATION
WEIGHT

Neural networks are composed of nodes or units (see Figure 18.19) connected by directed
links. A link from unit i to unit j serves to propagate the activation ai from i to j.8 Each link
also has a numeric weight wi,j associated with it, which determines the strength and sign of
the connection. Just as in linear regression models, each unit has a dummy input a0 = 1 with
an associated weight w0,j . Each unit j first computes a weighted sum of its inputs:
in j =

n


wi,j ai .

i=0
ACTIVATION
FUNCTION

Then it applies an activation function g to this sum to derive the output:
 n

aj = g(in j ) = g
wi,j ai .

(18.9)

i=0
8 A note on notation: for this section, we are forced to suspend our usual conventions. Input attributes are still
indexed by i , so that an â€œexternalâ€ activation ai is given by input xi ; but index j will refer to internal units
rather than examples. Throughout this section, the mathematical derivations concern a single generic example x,
omitting the usual summations over examples to obtain results for the whole data set.

Section 18.7.

PERCEPTRON
SIGMOID
PERCEPTRON

FEED-FORWARD
NETWORK

RECURRENT
NETWORK

LAYERS

HIDDEN UNIT

Artificial Neural Networks

729

The activation function g is typically either a hard threshold (Figure 18.17(a)), in which case
the unit is called a perceptron, or a logistic function (Figure 18.17(b)), in which case the term
sigmoid perceptron is sometimes used. Both of these nonlinear activation function ensure
the important property that the entire network of units can represent a nonlinear function (see
Exercise 18.22). As mentioned in the discussion of logistic regression (page 725), the logistic
activation function has the added advantage of being differentiable.
Having decided on the mathematical model for individual â€œneurons,â€ the next task is
to connect them together to form a network. There are two fundamentally distinct ways to
do this. A feed-forward network has connections only in one directionâ€”that is, it forms a
directed acyclic graph. Every node receives input from â€œupstreamâ€ nodes and delivers output
to â€œdownstreamâ€ nodes; there are no loops. A feed-forward network represents a function of
its current input; thus, it has no internal state other than the weights themselves. A recurrent
network, on the other hand, feeds its outputs back into its own inputs. This means that
the activation levels of the network form a dynamical system that may reach a stable state or
exhibit oscillations or even chaotic behavior. Moreover, the response of the network to a given
input depends on its initial state, which may depend on previous inputs. Hence, recurrent
networks (unlike feed-forward networks) can support short-term memory. This makes them
more interesting as models of the brain, but also more difficult to understand. This section
will concentrate on feed-forward networks; some pointers for further reading on recurrent
networks are given at the end of the chapter.
Feed-forward networks are usually arranged in layers, such that each unit receives input
only from units in the immediately preceding layer. In the next two subsections, we will look
at single-layer networks, in which every unit connects directly from the networkâ€™s inputs to
its outputs, and multilayer networks, which have one or more layers of hidden units that are
not connected to the outputs of the network. So far in this chapter, we have considered only
learning problems with a single output variable y, but neural networks are often used in cases
where multiple outputs are appropriate. For example, if we want to train a network to add
two input bits, each a 0 or a 1, we will need one output for the sum bit and one for the carry
bit. Also, when the learning problem involves classification into more than two classesâ€”for
example, when learning to categorize images of handwritten digitsâ€”it is common to use one
output unit for each class.

18.7.2 Single-layer feed-forward neural networks (perceptrons)
PERCEPTRON
NETWORK

A network with all the inputs connected directly to the outputs is called a single-layer neural
network, or a perceptron network. Figure 18.20 shows a simple two-input, two-output
perceptron network. With such a network, we might hope to learn the two-bit adder function,
for example. Here are all the training data we will need:
x1
0
0
1
1

x2
0
1
0
1

y3 (carry)
0
0
0
1

y4 (sum)
0
1
1
0

730

Chapter

18.

Learning from Examples

The first thing to notice is that a perceptron network with m outputs is really m separate
networks, because each weight affects only one of the outputs. Thus, there will be m separate training processes. Furthermore, depending on the type of activation function used, the
training processes will be either the perceptron learning rule (Equation (18.7) on page 724)
or gradient descent rule for the logistic regression (Equation (18.8) on page 727).
If you try either method on the two-bit-adder data, something interesting happens. Unit
3 learns the carry function easily, but unit 4 completely fails to learn the sum function. No,
unit 4 is not defective! The problem is with the sum function itself. We saw in Section 18.6
that linear classifiers (whether hard or soft) can represent linear decision boundaries in the input space. This works fine for the carry function, which is a logical AND (see Figure 18.21(a)).
The sum function, however, is an XOR (exclusive OR) of the two inputs. As Figure 18.21(c)
illustrates, this function is not linearly separable so the perceptron cannot learn it.
The linearly separable functions constitute just a small fraction of all Boolean functions; Exercise 18.20 asks you to quantify this fraction. The inability of perceptrons to learn
even such simple functions as XOR was a significant setback to the nascent neural network

1

w1,3

1

3

w3,5

3

w1,4

w1,4

2

4

w 2,4

5

w3,6

w 2,3

w 2,3
2

w1,3

w 4,5
4

w 2,4

(a)

6

w 4,6

(b)

Figure 18.20 (a) A perceptron network with two inputs and two output units. (b) A neural
network with two inputs, one hidden layer of two units, and one output unit. Not shown are
the dummy inputs and their associated weights.

x1

x1

x1

1

1

1

?
0
0

1
(a) x1 and x2

x2

0
0

1
(b) x1 or x2

x2

0
0

1

x2

(c) x1 xor x2

Figure 18.21 Linear separability in threshold perceptrons. Black dots indicate a point in
the input space where the value of the function is 1, and white dots indicate a point where the
value is 0. The perceptron returns 1 on the region on the non-shaded side of the line. In (c),
no such line exists that correctly classifies the inputs.

Artificial Neural Networks

731
1

0.9
0.8
0.7
0.6

Perceptron
Decision tree

0.5
0.4

Proportion correct on test set

1
Proportion correct on test set

Section 18.7.

0.9
0.8
0.7
0.6
Perceptron
Decision tree

0.5
0.4

0

10 20 30 40 50 60 70 80 90 100
Training set size

0

10 20 30 40 50 60 70 80 90 100
Training set size

(a)

(b)

Figure 18.22 Comparing the performance of perceptrons and decision trees. (a) Perceptrons are better at learning the majority function of 11 inputs. (b) Decision trees are better at
learning the WillWait predicate in the restaurant example.

community in the 1960s. Perceptrons are far from useless, however. Section 18.6.4 noted
that logistic regression (i.e., training a sigmoid perceptron) is even today a very popular and
effective tool. Moreover, a perceptron can represent some quite â€œcomplexâ€ Boolean functions very compactly. For example, the majority function, which outputs a 1 only if more
than half of its n inputs are 1, can be represented by a perceptron with each wi = 1 and with
w0 = âˆ’n/2. A decision tree would need exponentially many nodes to represent this function.
Figure 18.22 shows the learning curve for a perceptron on two different problems. On
the left, we show the curve for learning the majority function with 11 Boolean inputs (i.e.,
the function outputs a 1 if 6 or more inputs are 1). As we would expect, the perceptron learns
the function quite quickly, because the majority function is linearly separable. On the other
hand, the decision-tree learner makes no progress, because the majority function is very hard
(although not impossible) to represent as a decision tree. On the right, we have the restaurant
example. The solution problem is easily represented as a decision tree, but is not linearly
separable. The best plane through the data correctly classifies only 65%.

18.7.3 Multilayer feed-forward neural networks
(McCulloch and Pitts, 1943) were well aware that a single threshold unit would not solve all
their problems. In fact, their paper proves that such a unit can represent the basic Boolean
functions AND, OR, and NOT and then goes on to argue that any desired functionality can be
obtained by connecting large numbers of units into (possibly recurrent) networks of arbitrary
depth. The problem was that nobody knew how to train such networks.
This turns out to be an easy problem if we think of a network the right way: as a
function hw (x) parameterized by the weights w. Consider the simple network shown in Figure 18.20(b), which has two input units, two hidden units, and two output unit. (In addition,
each unit has a dummy input fixed at 1.) Given an input vector x = (x1 , x2 ), the activations

732

Chapter

hW(x1, x2)
1
0.8
0.6
0.4
0.2
0
-4 -2
x1

0

2

4

-4

-2

0

4
2
x2

(a)

18.

hW(x1, x2)
1
0.8
0.6
0.4
0.2
0
-4 -2
x1

0

Learning from Examples

2

-4

4

-2

0

4
2
x2

(b)

Figure 18.23 (a) The result of combining two opposite-facing soft threshold functions to
produce a ridge. (b) The result of combining two ridges to produce a bump.

of the input units are set to (a1 , a2 ) = (x1 , x2 ). The output at unit 5 is given by
a5 = g(w0,5,+ w3,5 a3 + w4,5 a4 )
= g(w0,5,+ w3,5 g(w0,3 + w1,3 a1 + w2,3 a2 ) + w4,5 g(w0 4 + w1,4 a1 + w2,4 a2 ))
= g(w0,5,+ w3,5 g(w0,3 + w1,3 x1 + w2,3 x2 ) + w4,5 g(w0 4 + w1,4 x1 + w2,4 x2 )).

NONLINEAR
REGRESSION

Thus, we have the output expressed as a function of the inputs and the weights. A similar
expression holds for unit 6. As long as we can calculate the derivatives of such expressions
with respect to the weights, we can use the gradient-descent loss-minimization method to
train the network. Section 18.7.4 shows exactly how to do this. And because the function
represented by a network can be highly nonlinearâ€”composed, as it is, of nested nonlinear soft
threshold functionsâ€”we can see neural networks as a tool for doing nonlinear regression.
Before delving into learning rules, let us look at the ways in which networks generate
complicated functions. First, remember that each unit in a sigmoid network represents a soft
threshold in its input space, as shown in Figure 18.17(c) (page 726). With one hidden layer
and one output layer, as in Figure 18.20(b), each output unit computes a soft-thresholded
linear combination of several such functions. For example, by adding two opposite-facing
soft threshold functions and thresholding the result, we can obtain a â€œridgeâ€ function as shown
in Figure 18.23(a). Combining two such ridges at right angles to each other (i.e., combining
the outputs from four hidden units), we obtain a â€œbumpâ€ as shown in Figure 18.23(b).
With more hidden units, we can produce more bumps of different sizes in more places.
In fact, with a single, sufficiently large hidden layer, it is possible to represent any continuous
function of the inputs with arbitrary accuracy; with two layers, even discontinuous functions
can be represented.9 Unfortunately, for any particular network structure, it is harder to characterize exactly which functions can be represented and which ones cannot.
9

The proof is complex, but the main point is that the required number of hidden units grows exponentially with
the number of inputs. For example, 2n /n hidden units are needed to encode all Boolean functions of n inputs.

Section 18.7.

Artificial Neural Networks

733

18.7.4 Learning in multilayer networks
First, let us dispense with one minor complication arising in multilayer networks: interactions
among the learning problems when the network has multiple outputs. In such cases, we
should think of the network as implementing a vector function hw rather than a scalar function
hw ; for example, the network in Figure 18.20(b) returns a vector [a5 , a6 ]. Similarly, the
target output will be a vector y. Whereas a perceptron network decomposes into m separate
learning problems for an m-output problem, this decomposition fails in a multilayer network.
For example, both a5 and a6 in Figure 18.20(b) depend on all of the input-layer weights, so
updates to those weights will depend on errors in both a5 and a6 . Fortunately, this dependency
is very simple in the case of any loss function that is additive across the components of the
error vector y âˆ’ hw (x). For the L2 loss, we have, for any weight w,
 âˆ‚
âˆ‚
âˆ‚ 
âˆ‚
Loss(w) =
|y âˆ’ hw (x)|2 =
(yk âˆ’ ak )2 (18.10)
(yk âˆ’ ak )2 =
âˆ‚w
âˆ‚w
âˆ‚w
âˆ‚w
k

BACK-PROPAGATION

k

where the index k ranges over nodes in the output layer. Each term in the final summation
is just the gradient of the loss for the kth output, computed as if the other outputs did not
exist. Hence, we can decompose an m-output learning problem into m learning problems,
provided we remember to add up the gradient contributions from each of them when updating
the weights.
The major complication comes from the addition of hidden layers to the network.
Whereas the error y âˆ’ hw at the output layer is clear, the error at the hidden layers seems
mysterious because the training data do not say what value the hidden nodes should have.
Fortunately, it turns out that we can back-propagate the error from the output layer to the
hidden layers. The back-propagation process emerges directly from a derivation of the overall
error gradient. First, we will describe the process with an intuitive justification; then, we will
show the derivation.
At the output layer, the weight-update rule is identical to Equation (18.8). We have
multiple output units, so let Err k be the kth component of the error vector y âˆ’ hw . We will
also find it convenient to define a modified error Î”k = Err k Ã— g (in k ), so that the weightupdate rule becomes
wj,k â† wj,k + Î± Ã— aj Ã— Î”k .

(18.11)

To update the connections between the input units and the hidden units, we need to define a
quantity analogous to the error term for output nodes. Here is where we do the error backpropagation. The idea is that hidden node j is â€œresponsibleâ€ for some fraction of the error Î”k
in each of the output nodes to which it connects. Thus, the Î”k values are divided according
to the strength of the connection between the hidden node and the output node and are propagated back to provide the Î”j values for the hidden layer. The propagation rule for the Î”
values is the following:

wj,k Î”k .
(18.12)
Î”j = g (in j )
k

734

Chapter

18.

Learning from Examples

function BACK -P ROP -L EARNING (examples, network ) returns a neural network
inputs: examples, a set of examples, each with input vector x and output vector y
network , a multilayer network with L layers, weights wi,j , activation function g
local variables: Î”, a vector of errors, indexed by network node
repeat
for each weight wi,j in network do
wi,j â† a small random number
for each example (x, y) in examples do
/* Propagate the inputs forward to compute the outputs */
for each node i in the input layer do
ai â† xi
for  = 2 to L do
for each node
 j in layer  do
in j â† i wi,j ai
aj â† g(in j )
/* Propagate deltas backward from output layer to input layer */
for each node j in the output layer do
Î”[j] â† g  (in j ) Ã— (yj âˆ’ aj )
for  = L âˆ’ 1 to 1 do
for each node i in layer
  do
Î”[i] â† g  (in i ) j wi,j Î”[j]
/* Update every weight in network using deltas */
for each weight wi,j in network do
wi,j â† wi,j + Î± Ã— ai Ã— Î”[j]
until some stopping criterion is satisfied
return network
Figure 18.24

The back-propagation algorithm for learning in multilayer networks.

Now the weight-update rule for the weights between the inputs and the hidden layer is essentially identical to the update rule for the output layer:
wi,j â† wi,j + Î± Ã— ai Ã— Î”j .
The back-propagation process can be summarized as follows:
â€¢ Compute the Î” values for the output units, using the observed error.
â€¢ Starting with output layer, repeat the following for each layer in the network, until the
earliest hidden layer is reached:
â€“ Propagate the Î” values back to the previous layer.
â€“ Update the weights between the two layers.
The detailed algorithm is shown in Figure 18.24.
For the mathematically inclined, we will now derive the back-propagation equations
from first principles. The derivation is quite similar to the gradient calculation for logistic

Section 18.7.

Artificial Neural Networks

735

regression (leading up to Equation (18.8) on page 727), except that we have to use the chain
rule more than once.
Following Equation (18.10), we compute just the gradient for Loss k = (yk âˆ’ ak )2 at
the kth output. The gradient of this loss with respect to weights connecting the hidden layer
to the output layer will be zero except for weights wj,k that connect to the kth output unit.
For those weights, we have
âˆ‚ak
âˆ‚g(in k )
âˆ‚Loss k
= âˆ’2(yk âˆ’ ak )
= âˆ’2(yk âˆ’ ak )
âˆ‚wj,k
âˆ‚wj,k
âˆ‚wj,k
âŽ›
âŽž

âˆ‚in k
âˆ‚ âŽ
= âˆ’2(yk âˆ’ ak )g (in k )
= âˆ’2(yk âˆ’ ak )g (in k )
wj,k aj âŽ 
âˆ‚wj,k
âˆ‚wj,k
j



= âˆ’2(yk âˆ’ ak )g (in k )aj = âˆ’aj Î”k ,
with Î”k defined as before. To obtain the gradient with respect to the wi,j weights connecting
the input layer to the hidden Â¡layer, we have to expand out the activations aj and reapply the
chain rule. We will show the derivation in gory detail because it is interesting to see how the
derivative operator propagates back through the network:
âˆ‚ak
âˆ‚g(in k )
âˆ‚Loss k
= âˆ’2(yk âˆ’ ak )
= âˆ’2(yk âˆ’ ak )
âˆ‚wi,j
âˆ‚wi,j
âˆ‚wi,j
âŽ›
âŽž

âˆ‚in k
âˆ‚ âŽ
= âˆ’2(yk âˆ’ ak )g (in k )
= âˆ’2Î”k
wj,k aj âŽ 
âˆ‚wi,j
âˆ‚wi,j
j

âˆ‚aj
âˆ‚g(in j )
= âˆ’2Î”k wj,k
âˆ‚wi,j
âˆ‚wi,j
âˆ‚in
j
= âˆ’2Î”k wj,k g (in j )
âˆ‚wi,j


âˆ‚

= âˆ’2Î”k wj,k g (in j )
wi,j ai
âˆ‚wi,j

= âˆ’2Î”k wj,k

i

= âˆ’2Î”k wj,k g (in j )ai = âˆ’ai Î”j ,
where Î”j is defined as before. Thus, we obtain the update rules obtained earlier from intuitive
considerations. It is also clear that the process can be continued for networks with more than
one hidden layer, which justifies the general algorithm given in Figure 18.24.
Having made it through (or skipped over) all the mathematics, letâ€™s see how a singlehidden-layer network performs on the restaurant problem. First, we need to determine the
structure of the network. We have 10 attributes describing each example, so we will need
10 input units. Should we have one hidden layer or two? How many nodes in each layer?
Should they be fully connected? There is no good theory that will tell us the answer. (See the
next section.) As always, we can use cross-validation: try several different structures and see
which one works best. It turns out that a network with one hidden layer containing four nodes
is about right for this problem. In Figure 18.25, we show two curves. The first is a training
curve showing the mean squared error on a given training set of 100 restaurant examples

736

Chapter

Learning from Examples

1
Proportion correct on test set

Total error on training set

14

18.

12
10
8
6
4
2
0

0.9
0.8
0.7
0.6

Decision tree
Multilayer network

0.5
0.4

0

50

100 150 200 250 300 350 400
Number of epochs

(a)

0

10 20 30 40 50 60 70 80 90 100
Training set size

(b)

Figure 18.25 (a) Training curve showing the gradual reduction in error as weights are
modified over several epochs, for a given set of examples in the restaurant domain. (b)
Comparative learning curves showing that decision-tree learning does slightly better on the
restaurant problem than back-propagation in a multilayer network.

during the weight-updating process. This demonstrates that the network does indeed converge
to a perfect fit to the training data. The second curve is the standard learning curve for the
restaurant data. The neural network does learn well, although not quite as fast as decisiontree learning; this is perhaps not surprising, because the data were generated from a simple
decision tree in the first place.
Neural networks are capable of far more complex learning tasks of course, although it
must be said that a certain amount of twiddling is needed to get the network structure right
and to achieve convergence to something close to the global optimum in weight space. There
are literally tens of thousands of published applications of neural networks. Section 18.11.1
looks at one such application in more depth.

18.7.5 Learning neural network structures
So far, we have considered the problem of learning weights, given a fixed network structure;
just as with Bayesian networks, we also need to understand how to find the best network
structure. If we choose a network that is too big, it will be able to memorize all the examples
by forming a large lookup table, but will not necessarily generalize well to inputs that have
not been seen before.10 In other words, like all statistical models, neural networks are subject
to overfitting when there are too many parameters in the model. We saw this in Figure 18.1
(page 696), where the high-parameter models in (b) and (c) fit all the data, but might not
generalize as well as the low-parameter models in (a) and (d).
If we stick to fully connected networks, the only choices to be made concern the number
10 It has been observed that very large networks do generalize well as long as the weights are kept small. This
restriction keeps the activation values in the linear region of the sigmoid function g(x) where x is close to zero.
This, in turn, means that the network behaves like a linear function (Exercise 18.22) with far fewer parameters.

Section 18.8.

TILING

N ONPARAMETRIC M ODELS

PARAMETRIC MODEL

NONPARAMETRIC
MODEL

INSTANCE-BASED
LEARNING
TABLE LOOKUP

737

of hidden layers and their sizes. The usual approach is to try several and keep the best. The
cross-validation techniques of Chapter 18 are needed if we are to avoid peeking at the test
set. That is, we choose the network architecture that gives the highest prediction accuracy on
the validation sets.
If we want to consider networks that are not fully connected, then we need to find
some effective search method through the very large space of possible connection topologies.
The optimal brain damage algorithm begins with a fully connected network and removes
connections from it. After the network is trained for the first time, an information-theoretic
approach identifies an optimal selection of connections that can be dropped. The network
is then retrained, and if its performance has not decreased then the process is repeated. In
addition to removing connections, it is also possible to remove units that are not contributing
much to the result.
Several algorithms have been proposed for growing a larger network from a smaller one.
One, the tiling algorithm, resembles decision-list learning. The idea is to start with a single
unit that does its best to produce the correct output on as many of the training examples as
possible. Subsequent units are added to take care of the examples that the first unit got wrong.
The algorithm adds only as many units as are needed to cover all the examples.

OPTIMAL BRAIN
DAMAGE

18.8

Nonparametric Models

Linear regression and neural networks use the training data to estimate a fixed set of parameters w. That defines our hypothesis hw (x), and at that point we can throw away the training
data, because they are all summarized by w. A learning model that summarizes data with a
set of parameters of fixed size (independent of the number of training examples) is called a
parametric model.
No matter how much data you throw at a parametric model, it wonâ€™t change its mind
about how many parameters it needs. When data sets are small, it makes sense to have a strong
restriction on the allowable hypotheses, to avoid overfitting. But when there are thousands or
millions or billions of examples to learn from, it seems like a better idea to let the data speak
for themselves rather than forcing them to speak through a tiny vector of parameters. If the
data say that the correct answer is a very wiggly function, we shouldnâ€™t restrict ourselves to
linear or slightly wiggly functions.
A nonparametric model is one that cannot be characterized by a bounded set of parameters. For example, suppose that each hypothesis we generate simply retains within itself all
of the training examples and uses all of them to predict the next example. Such a hypothesis
family would be nonparametric because the effective number of parameters is unboundedâ€”
it grows with the number of examples. This approach is called instance-based learning or
memory-based learning. The simplest instance-based learning method is table lookup: take
all the training examples, put them in a lookup table, and then when asked for h(x), see if x is
in the table; if it is, return the corresponding y. The problem with this method is that it does
not generalize well: when x is not in the table all it can do is return some default value.

738

Chapter

x1

7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5

x1

4.5

5

5.5

6

6.5

x2

(k = 1)

7

18.

Learning from Examples

7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
4.5

5

5.5

6

6.5

7

x2

(k = 5)

Figure 18.26 (a) A k-nearest-neighbor model showing the extent of the explosion class for
the data in Figure 18.15, with k = 1. Overfitting is apparent. (b) With k = 5, the overfitting
problem goes away for this data set.

18.8.1 Nearest neighbor models
NEAREST
NEIGHBORS

MINKOWSKI
DISTANCE

We can improve on table lookup with a slight variation: given a query xq , find the k examples
that are nearest to xq . This is called k-nearest neighbors lookup. Weâ€™ll use the notation
NN (k, xq ) to denote the set of k nearest neighbors.
To do classification, first find NN (k, xq ), then take the plurality vote of the neighbors
(which is the majority vote in the case of binary classification). To avoid ties, k is always
chosen to be an odd number. To do regression, we can take the mean or median of the k
neighbors, or we can solve a linear regression problem on the neighbors.
In Figure 18.26, we show the decision boundary of k-nearest-neighbors classification
for k = 1 and 5 on the earthquake data set from Figure 18.15. Nonparametric methods are
still subject to underfitting and overfitting, just like parametric methods. In this case 1-nearest
neighbors is overfitting; it reacts too much to the black outlier in the upper right and the white
outlier at (5.4, 3.7). The 5-nearest-neighbors decision boundary is good; higher k would
underfit. As usual, cross-validation can be used to select the best value of k.
The very word â€œnearestâ€ implies a distance metric. How do we measure the distance
from a query point xq to an example point xj ? Typically, distances are measured with a
Minkowski distance or Lp norm, defined as

Lp (xj , xq ) = (
|xj,i âˆ’ xq,i |p )1/p .
i

HAMMING DISTANCE

With p = 2 this is Euclidean distance and with p = 1 it is Manhattan distance. With Boolean
attribute values, the number of attributes on which the two points differ is called the Hamming distance. Often p = 2 is used if the dimensions are measuring similar properties, such
as the width, height and depth of parts on a conveyor belt, and Manhattan distance is used if
they are dissimilar, such as age, weight, and gender of a patient. Note that if we use the raw
numbers from each dimension then the total distance will be affected by a change in scale
in any dimension. That is, if we change dimension i from measurements in centimeters to

Section 18.8.

NORMALIZATION

MAHALANOBIS
DISTANCE

CURSE OF
DIMENSIONALITY

Nonparametric Models

739

miles while keeping the other dimensions the same, weâ€™ll get different nearest neighbors. To
avoid this, it is common to apply normalization to the measurements in each dimension. One
simple approach is to compute the mean Î¼i and standard deviation Ïƒi of the values in each
dimension, and rescale them so that xj,i becomes (xj,i âˆ’ Î¼i )/Ïƒi . A more complex metric
known as the Mahalanobis distance takes into account the covariance between dimensions.
In low-dimensional spaces with plenty of data, nearest neighbors works very well: we
are likely to have enough nearby data points to get a good answer. But as the number of
dimensions rises we encounter a problem: the nearest neighbors in high-dimensional spaces
are usually not very near! Consider k-nearest-neighbors on a data set of N points uniformly
distributed throughout the interior of an n-dimensional unit hypercube. Weâ€™ll define the kneighborhood of a point as the smallest hypercube that contains the k-nearest neighbors. Let
 be the average side length of a neighborhood. Then the volume of the neighborhood (which
contains k points) is n and the volume of the full cube (which contains N points) is 1. So,
on average, n = k/N . Taking nth roots of both sides we get  = (k/N )1/n .
To be concrete, let k = 10 and N = 1, 000, 000. In two dimensions (n = 2; a unit
square), the average neighborhood has  = 0.003, a small fraction of the unit square, and
in 3 dimensions  is just 2% of the edge length of the unit cube. But by the time we get to 17
dimensions,  is half the edge length of the unit hypercube, and in 200 dimensions it is 94%.
This problem has been called the curse of dimensionality.
Another way to look at it: consider the points that fall within a thin shell making up the
outer 1% of the unit hypercube. These are outliers; in general it will be hard to find a good
value for them because we will be extrapolating rather than interpolating. In one dimension,
these outliers are only 2% of the points on the unit line (those points where x < .01 or
x > .99), but in 200 dimensions, over 98% of the points fall within this thin shellâ€”almost
all the points are outliers. You can see an example of a poor nearest-neighbors fit on outliers
if you look ahead to Figure 18.28(b).
The NN (k, xq ) function is conceptually trivial: given a set of N examples and a query
xq , iterate through the examples, measure the distance to xq from each one, and keep the best
k. If we are satisfied with an implementation that takes O(N ) execution time, then that is the
end of the story. But instance-based methods are designed for large data sets, so we would
like an algorithm with sublinear run time. Elementary analysis of algorithms tells us that
exact table lookup is O(N ) with a sequential table, O(log N ) with a binary tree, and O(1)
with a hash table. We will now see that binary trees and hash tables are also applicable for
finding nearest neighbors.

18.8.2 Finding nearest neighbors with k-d trees
K-D TREE

A balanced binary tree over data with an arbitrary number of dimensions is called a k-d tree,
for k-dimensional tree. (In our notation, the number of dimensions is n, so they would be
n-d trees. The construction of a k-d tree is similar to the construction of a one-dimensional
balanced binary tree. We start with a set of examples and at the root node we split them along
the ith dimension by testing whether xi â‰¤ m. We chose the value m to be the median of the
examples along the ith dimension; thus half the examples will be in the left branch of the tree

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
25

50 75 100 125 150 175 200
Number of dimensions

(a)

Proportion of points in exterior shell

Chapter

Edge length of neighborhood

740

18.

Learning from Examples

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
25

50 75 100 125 150 175 200
Number of dimensions

(b)

Figure 18.27 The curse of dimensionality: (a) The length of the average neighborhood for
10-nearest-neighbors in a unit hypercube with 1,000,000 points, as a function of the number
of dimensions. (b) The proportion of points that fall within a thin shell consisting of the
outer 1% of the hypercube, as a function of the number of dimensions. Sampled from 10,000
randomly distributed points.

and half in the right. We then recursively make a tree for the left and right sets of examples,
stopping when there are fewer than two examples left. To choose a dimension to split on at
each node of the tree, one can simply select dimension i mod n at level i of the tree. (Note
that we may need to split on any given dimension several times as we proceed down the tree.)
Another strategy is to split on the dimension that has the widest spread of values.
Exact lookup from a k-d tree is just like lookup from a binary tree (with the slight
complication that you need to pay attention to which dimension you are testing at each node).
But nearest neighbor lookup is more complicated. As we go down the branches, splitting
the examples in half, in some cases we can discard the other half of the examples. But not
always. Sometimes the point we are querying for falls very close to the dividing boundary.
The query point itself might be on the left hand side of the boundary, but one or more of
the k nearest neighbors might actually be on the right-hand side. We have to test for this
possibility by computing the distance of the query point to the dividing boundary, and then
searching both sides if we canâ€™t find k examples on the left that are closer than this distance.
Because of this problem, k-d trees are appropriate only when there are many more examples
than dimensions, preferably at least 2n examples. Thus, k-d trees work well with up to 10
dimensions with thousands of examples or up to 20 dimensions with millions of examples. If
we donâ€™t have enough examples, lookup is no faster than a linear scan of the entire data set.

18.8.3 Locality-sensitive hashing

LOCALITY-SENSITIVE
HASH

Hash tables have the potential to provide even faster lookup than binary trees. But how can
we find nearest neighbors using a hash table, when hash codes rely on an exact match? Hash
codes randomly distribute values among the bins, but we want to have near points grouped
together in the same bin; we want a locality-sensitive hash (LSH).

Section 18.8.

APPROXIMATE
NEAR-NEIGHBORS

Nonparametric Models

741

We canâ€™t use hashes to solve NN (k, xq ) exactly, but with a clever use of randomized
algorithms, we can find an approximate solution. First we define the approximate nearneighbors problem: given a data set of example points and a query point xq , find, with high
probability, an example point (or points) that is near xq . To be more precise, we require that
if there is a point xj that is within a radius r of xq , then with high probability the algorithm
will find a point xj  that is within distance c r of q. If there is no point within radius r then the
algorithm is allowed to report failure. The values of c and â€œhigh probabilityâ€ are parameters
of the algorithm.
To solve approximate near neighbors, we will need a hash function g(x) that has the
property that, for any two points xj and xj  , the probability that they have the same hash code
is small if their distance is more than c r, and is high if their distance is less than r. For
simplicity we will treat each point as a bit string. (Any features that are not Boolean can be
encoded into a set of Boolean features.)
The intuition we rely on is that if two points are close together in an n-dimensional
space, then they will necessarily be close when projected down onto a one-dimensional space
(a line). In fact, we can discretize the line into binsâ€”hash bucketsâ€”so that, with high probability, near points project down to exactly the same bin. Points that are far away from each
other will tend to project down into different bins for most projections, but there will always
be a few projections that coincidentally project far-apart points into the same bin. Thus, the
bin for point xq contains many (but not all) points that are near to xq , as well as some points
that are far away.
The trick of LSH is to create multiple random projections and combine them. A random
projection is just a random subset of the bit-string representation. We choose  different
random projections and create  hash tables, g1 (x), . . . , g (x). We then enter all the examples
into each hash table. Then when given a query point xq , we fetch the set of points in bin gk (q)
for each k, and union these sets together into a set of candidate points, C. Then we compute
the actual distance to xq for each of the points in C and return the k closest points. With high
probability, each of the points that are near to xq will show up in at least one of the bins, and
although some far-away points will show up as well, we can ignore those. With large realworld problems, such as finding the near neighbors in a data set of 13 million Web images
using 512 dimensions (Torralba et al., 2008), locality-sensitive hashing needs to examine only
a few thousand images out of 13 million to find nearest neighbors; a thousand-fold speedup
over exhaustive or k-d tree approaches.

18.8.4 Nonparametric regression
Now weâ€™ll look at nonparametric approaches to regression rather than classification. Figure 18.28 shows an example of some different models. In (a), we have perhaps the simplest
method of all, known informally as â€œconnect-the-dots,â€ and superciliously as â€œpiecewiselinear nonparametric regression.â€ This model creates a function h(x) that, when given a
query xq , solves the ordinary linear regression problem with just two points: the training
examples immediately to the left and right of xq . When noise is low, this trivial method is
actually not too bad, which is why it is a standard feature of charting software in spreadsheets.

742

Chapter

8

8

7

7

6

6

5

5

4

4

3

3

2

2

1

1

0

18.

Learning from Examples

0
0

2

4

6

8

10

12

14

0

2

4

6

(a)

8

10

12

14

10

12

14

(b)

8

8

7

7

6

6

5

5

4

4

3

3

2

2

1

1

0

0
0

2

4

6

8

(c)

10

12

14

0

2

4

6

8

(d)

Figure 18.28 Nonparametric regression models: (a) connect the dots, (b) 3-nearest neighbors average, (c) 3-nearest-neighbors linear regression, (d) locally weighted regression with
a quadratic kernel of width k = 10.

NEARESTNEIGHBORS
REGRESSION

LOCALLY WEIGHTED
REGRESSION

But when the data are noisy, the resulting function is spiky, and does not generalize well.
k-nearest-neighbors regression (Figure 18.28(b)) improves on connect-the-dots. Instead of using just the two examples to the left and right of a query point xq , we use the
k nearest neighbors (here 3). A larger value of k tends to smooth out the magnitude of
the spikes, although the resulting function has discontinuities.
 In (b), we have the k-nearestneighbors average: h(x) is the mean value of the k points, yj /k. Notice that at the outlying
points, near x = 0 and x = 14, the estimates are poor because all the evidence comes from one
side (the interior), and ignores the trend. In (c), we have k-nearest-neighbor linear regression,
which finds the best line through the k examples. This does a better job of capturing trends at
the outliers, but is still discontinuous. In both (b) and (c), weâ€™re left with the question of how
to choose a good value for k. The answer, as usual, is cross-validation.
Locally weighted regression (Figure 18.28(d)) gives us the advantages of nearest neighbors, without the discontinuities. To avoid discontinuities in h(x), we need to avoid disconti-

Section 18.8.

Nonparametric Models

743

1

0.5

0
-10

-5

0

5

10

Figure 18.29 A quadratic kernel, K(d) = max(0, 1 âˆ’ (2|x|/k)2 ), with kernel width
k = 10, centered on the query point x = 0.

KERNEL

nuities in the set of examples we use to estimate h(x). The idea of locally weighted regression
is that at each query point xq , the examples that are close to xq are weighted heavily, and the
examples that are farther away are weighted less heavily or not at all. The decrease in weight
over distance is always gradual, not sudden.
We decide how much to weight each example with a function known as a kernel. A
kernel function looks like a bump; in Figure 18.29 we see the specific kernel used to generate
Figure 18.28(d). We can see that the weight provided by this kernel is highest in the center
and reaches zero at a distance of Â±5. Can we choose just any function for a kernel? No. First,
note that we invoke a kernel function K with K(Distance(xj , xq )), where xq is a query point
that is a given distance from xj , and we want to know how much to weight that distance.
So K should be symmetric around 0 and have a maximum at 0. The area under the kernel
must remain bounded as we go to Â±âˆž. Other shapes, such as Gaussians, have been used for
kernels, but the latest research suggests that the choice of shape doesnâ€™t matter much. We
do have to be careful about the width of the kernel. Again, this is a parameter of the model
that is best chosen by cross-validation. Just as in choosing the k for nearest neighbors, if the
kernels are too wide weâ€™ll get underfitting and if they are too narrow weâ€™ll get overfitting. In
Figure 18.29(d), the value of k = 10 gives a smooth curve that looks about rightâ€”but maybe
it does not pay enough attention to the outlier at x = 6; a narrower kernel width would be
more responsive to individual points.
Doing locally weighted regression with kernels is now straightforward. For a given
query point xq we solve the following weighted regression problem using gradient descent:

K(Distance(xq , xj )) (yj âˆ’ w Â· xj )2 ,
wâˆ— = argmin
w

j

where Distance is any of the distance metrics discussed for nearest neighbors. Then the
answer is h(xq ) = wâˆ— Â· xq .
Note that we need to solve a new regression problem for every query pointâ€”thatâ€™s what
it means to be local. (In ordinary linear regression, we solved the regression problem once,
globally, and then used the same hw for any query point.) Mitigating against this extra work

744

Chapter

18.

Learning from Examples

is the fact that each regression problem will be easier to solve, because it involves only the
examples with nonzero weightâ€”the examples whose kernels overlap the query point. When
kernel widths are small, this may be just a few points.
Most nonparametric models have the advantage that it is easy to do leave-one-out crossvalidation without having to recompute everything. With a k-nearest-neighbors model, for
instance, when given a test example (x, y) we retrieve the k nearest neighbors once, compute
the per-example loss L(y, h(x)) from them, and record that as the leave-one-out result for
every example that is not one of the neighbors. Then we retrieve the k + 1 nearest neighbors
and record distinct results for leaving out each of the k neighbors. With N examples the
whole process is O(k), not O(kN ).

18.9

S UPPORT V ECTOR M ACHINES

SUPPORT VECTOR
MACHINE

The support vector machine or SVM framework is currently the most popular approach for
â€œoff-the-shelfâ€ supervised learning: if you donâ€™t have any specialized prior knowledge about
a domain, then the SVM is an excellent method to try first. There are three properties that
make SVMs attractive:
1. SVMs construct a maximum margin separatorâ€”a decision boundary with the largest
possible distance to example points. This helps them generalize well.
2. SVMs create a linear separating hyperplane, but they have the ability to embed the
data into a higher-dimensional space, using the so-called kernel trick. Often, data that
are not linearly separable in the original input space are easily separable in the higherdimensional space. The high-dimensional linear separator is actually nonlinear in the
original space. This means the hypothesis space is greatly expanded over methods that
use strictly linear representations.
3. SVMs are a nonparametric methodâ€”they retain training examples and potentially need
to store them all. On the other hand, in practice they often end up retaining only a
small fraction of the number of examplesâ€”sometimes as few as a small constant times
the number of dimensions. Thus SVMs combine the advantages of nonparametric and
parametric models: they have the flexibility to represent complex functions, but they
are resistant to overfitting.
You could say that SVMs are successful because of one key insight and one neat trick. We
will cover each in turn. In Figure 18.30(a), we have a binary classification problem with three
candidate decision boundaries, each a linear separator. Each of them is consistent with all
the examples, so from the point of view of 0/1 loss, each would be equally good. Logistic
regression would find some separating line; the exact location of the line depends on all the
example points. The key insight of SVMs is that some examples are more important than
others, and that paying attention to them can lead to better generalization.
Consider the lowest of the three separating lines in (a). It comes very close to 5 of the
black examples. Although it classifies all the examples correctly, and thus minimizes loss, it

Section 18.9.

Support Vector Machines

745

1

1

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
0

0.2

0.4

0.6

(a)

0.8

1

0

0.2

0.4

0.6

0.8

1

(b)

Figure 18.30 Support vector machine classification: (a) Two classes of points (black and
white circles) and three candidate linear separators. (b) The maximum margin separator
(heavy line), is at the midpoint of the margin (area between dashed lines). The support
vectors (points with large circles) are the examples closest to the separator.

MAXIMUM MARGIN
SEPARATOR
MARGIN

should make you nervous that so many examples are close to the line; it may be that other
black examples will turn out to fall on the other side of the line.
SVMs address this issue: Instead of minimizing expected empirical loss on the training
data, SVMs attempt to minimize expected generalization loss. We donâ€™t know where the
as-yet-unseen points may fall, but under the probabilistic assumption that they are drawn
from the same distribution as the previously seen examples, there are some arguments from
computational learning theory (Section 18.5) suggesting that we minimize generalization loss
by choosing the separator that is farthest away from the examples we have seen so far. We
call this separator, shown in Figure 18.30(b) the maximum margin separator. The margin
is the width of the area bounded by dashed lines in the figureâ€”twice the distance from the
separator to the nearest example point.
Now, how do we find this separator? Before showing the equations, some notation:
Traditionally SVMs use the convention that class labels are +1 and -1, instead of the +1 and
0 we have been using so far. Also, where we put the intercept into the weight vector w (and
a corresponding dummy 1 value into xj,0 ), SVMs do not do that; they keep the intercept
as a separate parameter, b. With that in mind, the separator is defined as the set of points
{x : w Â· x + b = 0}. We could search the space of w and b with gradient descent to find the
parameters that maximize the margin while correctly classifying all the examples.
However, it turns out there is another approach to solving this problem. We wonâ€™t
show the details, but will just say that there is an alternative representation called the dual

746

QUADRATIC
PROGRAMMING

Chapter

18.

Learning from Examples

representation, in which the optimal solution is found by solving

1
Î±j âˆ’
Î±j Î±k yj yk (xj Â· xk )
(18.13)
argmax
2
Î±
j
j,k

subject to the constraints Î±j â‰¥ 0 and j Î±j yj = 0. This is a quadratic programming
optimization problem, for which there are good softwarepackages. Once we have found the
vector Î± we can get back to w with the equation w = j Î±j xj , or we can stay in the dual
representation. There are three important properties of Equation (18.13). First, the expression
is convex; it has a single global maximum that can be found efficiently. Second, the data enter
the expression only in the form of dot products of pairs of points. This second property is also
true of the equation for the separator itself; once the optimal Î±j have been calculated, it is
âŽž
âŽ›

Î±j yj (x Â· xj ) âˆ’ bâŽ  .
(18.14)
h(x) = sign âŽ
j

SUPPORT VECTOR

A final important property is that the weights Î±j associated with each data point are zero except for the support vectorsâ€”the points closest to the separator. (They are called â€œsupportâ€
vectors because they â€œhold upâ€ the separating plane.) Because there are usually many fewer
support vectors than examples, SVMs gain some of the advantages of parametric models.
What if the examples are not linearly separable? Figure 18.31(a) shows an input space
defined by attributes x = (x1 , x2 ), with positive examples (y = + 1) inside a circular region
and negative examples (y = âˆ’1) outside. Clearly, there is no linear separator for this problem.
Now, suppose we re-express the input dataâ€”i.e., we map each input vector x to a new vector
of feature values, F (x). In particular, let us use the three features
âˆš
(18.15)
f2 = x22 ,
f3 = 2x1 x2 .
f1 = x21 ,
We will see shortly where these came from, but for now, just look at what happens. Figure 18.31(b) shows the data in the new, three-dimensional space defined by the three features;
the data are linearly separable in this space! This phenomenon is actually fairly general: if
data are mapped into a space of sufficiently high dimension, then they will almost always be
linearly separableâ€”if you look at a set of points from enough directions, youâ€™ll find a way to
make them line up. Here, we used only three dimensions;11 Exercise 18.16 asks you to show
that four dimensions suffice for linearly separating a circle anywhere in the plane (not just at
the origin), and five dimensions suffice to linearly separate any ellipse. In general (with some
special cases excepted) if we have N data points then they will always be separable in spaces
of N âˆ’ 1 dimensions or more (Exercise 18.25).
Now, we would not usually expect to find a linear separator in the input space x, but
we can find linear separators in the high-dimensional feature space F (x) simply by replacing
xj Â·xk in Equation (18.13) with F (xj )Â·F (xk ). This by itself is not remarkableâ€”replacing x by
F (x) in any learning algorithm has the required effectâ€”but the dot product has some special
properties. It turns out that F (xj ) Â· F (xk ) can often be computed without first computing F
11

The reader may notice that we could have used just f1 and f2 , but the 3D mapping illustrates the idea better.

Section 18.9.

Support Vector Machines

747

1.5
1

âˆš2x1x2
3
2
1
0
-1
-2
-3

x2

0.5
0
-0.5
-1

2.5
2
0

1.5

0.5

1

1
x21

-1.5
-1.5

-1

-0.5

0
x1

(a)

0.5

1

1.5

1.5

x22

0.5

2

(b)

Figure 18.31 (a) A two-dimensional training set with positive examples as black circles and negative examples as white circles. The true decision boundary, x21 + x22 â‰¤ 1,
is also shown.
(b) The same data after mapping into a three-dimensional input space
âˆš
(x21 , x22 , 2x1 x2 ). The circular decision boundary in (a) becomes a linear decision boundary
in three dimensions. Figure 18.30(b) gives a closeup of the separator in (b).

for each point. In our three-dimensional feature space defined by Equation (18.15), a little bit
of algebra shows that

KERNEL FUNCTION

MERCERâ€™S THEOREM

POLYNOMIAL
KERNEL

F (xj ) Â· F (xk ) = (xj Â· xk )2 .
âˆš
(Thatâ€™s why the 2 is in f3 .) The expression (xj Â· xk )2 is called a kernel function,12 and
is usually written as K(xj , xk ). The kernel function can be applied to pairs of input data to
evaluate dot products in some corresponding feature space. So, we can find linear separators
in the higher-dimensional feature space F (x) simply by replacing xj Â· xk in Equation (18.13)
with a kernel function K(xj , xk ). Thus, we can learn in the higher-dimensional space, but we
compute only kernel functions rather than the full list of features for each data point.
The next step is to see that thereâ€™s nothing special about the kernel K(xj , xk ) = (xj Â·xk )2 .
It corresponds to a particular higher-dimensional feature space, but other kernel functions
correspond to other feature spaces. A venerable result in mathematics, Mercerâ€™s theorem (1909), tells us that any â€œreasonableâ€13 kernel function corresponds to some feature
space. These feature spaces can be very large, even for innocuous-looking kernels. For example, the polynomial kernel, K(xj , xk ) = (1 + xj Â· xk )d , corresponds to a feature space
whose dimension is exponential in d.
12 This usage of â€œkernel functionâ€ is slightly different from the kernels in locally weighted regression. Some
SVM kernels are distance metrics, but not all are.
13 Here, â€œreasonableâ€ means that the matrix K
jk = K(xj , xk ) is positive definite.

748
KERNEL TRICK

SOFT MARGIN

KERNELIZATION

18.10

ENSEMBLE
LEARNING

Chapter

18.

Learning from Examples

This then is the clever kernel trick: Plugging these kernels into Equation (18.13),
optimal linear separators can be found efficiently in feature spaces with billions of (or, in
some cases, infinitely many) dimensions. The resulting linear separators, when mapped back
to the original input space, can correspond to arbitrarily wiggly, nonlinear decision boundaries between the positive and negative examples.
In the case of inherently noisy data, we may not want a linear separator in some highdimensional space. Rather, weâ€™d like a decision surface in a lower-dimensional space that
does not cleanly separate the classes, but reflects the reality of the noisy data. That is possible with the soft margin classifier, which allows examples to fall on the wrong side of the
decision boundary, but assigns them a penalty proportional to the distance required to move
them back on the correct side.
The kernel method can be applied not only with learning algorithms that find optimal
linear separators, but also with any other algorithm that can be reformulated to work only
with dot products of pairs of data points, as in Equations 18.13 and 18.14. Once this is
done, the dot product is replaced by a kernel function and we have a kernelized version
of the algorithm. This can be done easily for k-nearest-neighbors and perceptron learning
(Section 18.7.2), among others.

E NSEMBLE L EARNING
So far we have looked at learning methods in which a single hypothesis, chosen from a
hypothesis space, is used to make predictions. The idea of ensemble learning methods is
to select a collection, or ensemble, of hypotheses from the hypothesis space and combine
their predictions. For example, during cross-validation we might generate twenty different
decision trees, and have them vote on the best classification for a new example.
The motivation for ensemble learning is simple. Consider an ensemble of K = 5 hypotheses and suppose that we combine their predictions using simple majority voting. For the
ensemble to misclassify a new example, at least three of the five hypotheses have to misclassify it. The hope is that this is much less likely than a misclassification by a single hypothesis.
Suppose we assume that each hypothesis hk in the ensemble has an error of pâ€”that is, the
probability that a randomly chosen example is misclassified by hk is p. Furthermore, suppose
we assume that the errors made by each hypothesis are independent. In that case, if p is small,
then the probability of a large number of misclassifications occurring is minuscule. For example, a simple calculation (Exercise 18.18) shows that using an ensemble of five hypotheses
reduces an error rate of 1 in 10 down to an error rate of less than 1 in 100. Now, obviously
the assumption of independence is unreasonable, because hypotheses are likely to be misled
in the same way by any misleading aspects of the training data. But if the hypotheses are at
least a little bit different, thereby reducing the correlation between their errors, then ensemble
learning can be very useful.
Another way to think about the ensemble idea is as a generic way of enlarging the
hypothesis space. That is, think of the ensemble itself as a hypothesis and the new hypothesis

Section 18.10.

Ensemble Learning

749

â€“

â€“
â€“
â€“
â€“
â€“
â€“ â€“

â€“

â€“ â€“

â€“

â€“

â€“

â€“
â€“
â€“
â€“ â€“
â€“
â€“
â€“ â€“
â€“
â€“
â€“
â€“

+
+
+
++ +
+
+ +
+ ++ + +
â€“
â€“
â€“
â€“ â€“ â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“

Figure 18.32 Illustration of the increased expressive power obtained by ensemble learning. We take three linear threshold hypotheses, each of which classifies positively on the
unshaded side, and classify as positive any example classified positively by all three. The
resulting triangular region is a hypothesis not expressible in the original hypothesis space.

BOOSTING
WEIGHTED TRAINING
SET

WEAK LEARNING

space as the set of all possible ensembles constructable from hypotheses in the original space.
Figure 18.32 shows how this can result in a more expressive hypothesis space. If the original
hypothesis space allows for a simple and efficient learning algorithm, then the ensemble
method provides a way to learn a much more expressive class of hypotheses without incurring
much additional computational or algorithmic complexity.
The most widely used ensemble method is called boosting. To understand how it works,
we need first to explain the idea of a weighted training set. In such a training set, each
example has an associated weight wj â‰¥ 0. The higher the weight of an example, the higher
is the importance attached to it during the learning of a hypothesis. It is straightforward to
modify the learning algorithms we have seen so far to operate with weighted training sets.14
Boosting starts with wj = 1 for all the examples (i.e., a normal training set). From this
set, it generates the first hypothesis, h1 . This hypothesis will classify some of the training examples correctly and some incorrectly. We would like the next hypothesis to do better on the
misclassified examples, so we increase their weights while decreasing the weights of the correctly classified examples. From this new weighted training set, we generate hypothesis h2 .
The process continues in this way until we have generated K hypotheses, where K is an input
to the boosting algorithm. The final ensemble hypothesis is a weighted-majority combination
of all the K hypotheses, each weighted according to how well it performed on the training set.
Figure 18.33 shows how the algorithm works conceptually. There are many variants of the basic boosting idea, with different ways of adjusting the weights and combining the hypotheses.
One specific algorithm, called A DA B OOST , is shown in Figure 18.34. A DA B OOST has a very
important property: if the input learning algorithm L is a weak learning algorithmâ€”which
14

For learning algorithms in which this is not possible, one can instead create a replicated training set where
the jth example appears wj times, using randomization to handle fractional weights.

750

Chapter

h1 =

h2 =

h3 =

18.

Learning from Examples

h4 =

h
Figure 18.33 How the boosting algorithm works. Each shaded rectangle corresponds to
an example; the height of the rectangle corresponds to the weight. The checks and crosses
indicate whether the example was classified correctly by the current hypothesis. The size of
the decision tree indicates the weight of that hypothesis in the final ensemble.

DECISION STUMP

means that L always returns a hypothesis with accuracy on the training set that is slightly
better than random guessing (i.e., 50%+ for Boolean classification)â€”then A DA B OOST will
return a hypothesis that classifies the training data perfectly for large enough K. Thus, the
algorithm boosts the accuracy of the original learning algorithm on the training data. This
result holds no matter how inexpressive the original hypothesis space and no matter how
complex the function being learned.
Let us see how well boosting does on the restaurant data. We will choose as our original
hypothesis space the class of decision stumps, which are decision trees with just one test, at
the root. The lower curve in Figure 18.35(a) shows that unboosted decision stumps are not
very effective for this data set, reaching a prediction performance of only 81% on 100 training
examples. When boosting is applied (with K = 5), the performance is better, reaching 93%
after 100 examples.
An interesting thing happens as the ensemble size K increases. Figure 18.35(b) shows
the training set performance (on 100 examples) as a function of K. Notice that the error
reaches zero when K is 20; that is, a weighted-majority combination of 20 decision stumps
suffices to fit the 100 examples exactly. As more stumps are added to the ensemble, the error
remains at zero. The graph also shows that the test set performance continues to increase
long after the training set error has reached zero. At K = 20, the test performance is 0.95
(or 0.05 error), and the performance increases to 0.98 as late as K = 137, before gradually
dropping to 0.95.
This finding, which is quite robust across data sets and hypothesis spaces, came as quite
a surprise when it was first noticed. Ockhamâ€™s razor tells us not to make hypotheses more

Section 18.10.

Ensemble Learning

751

function A DA B OOST (examples, L, K ) returns a weighted-majority hypothesis
inputs: examples, set of N labeled examples (x1 , y1 ), . . . , (xN , yN )
L, a learning algorithm
K , the number of hypotheses in the ensemble
local variables: w, a vector of N example weights, initially 1/N
h, a vector of K hypotheses
z, a vector of K hypothesis weights
for k = 1 to K do
h[k ] â† L(examples, w)
error â† 0
for j = 1 to N do
if h[k ](xj ) = yj then error â† error + w[j]
for j = 1 to N do
if h[k ](xj ) = yj then w[j] â† w[j] Â· error /(1 âˆ’ error )
w â† N ORMALIZE(w)
z[k ] â† log (1 âˆ’ error )/error
return W EIGHTED -M AJORITY(h, z)

1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5

1
Training/test accuracy

Proportion correct on test set

Figure 18.34 The A DA B OOST variant of the boosting method for ensemble learning. The
algorithm generates hypotheses by successively reweighting the training examples. The function W EIGHTED -M AJORITY generates a hypothesis that returns the output value with the
highest vote from the hypotheses in h, with votes weighted by z.

Boosted decision stumps
Decision stump

0.95
0.9
0.85

Training error
Test error

0.8
0.75
0.7
0.65
0.6

0

20

40
60
Training set size

(a)

80

100

0

50
100
150
Number of hypotheses K

200

(b)

Figure 18.35 (a) Graph showing the performance of boosted decision stumps with K = 5
versus unboosted decision stumps on the restaurant data. (b) The proportion correct on the
training set and the test set as a function of K, the number of hypotheses in the ensemble.
Notice that the test set accuracy improves slightly even after the training accuracy reaches 1,
i.e., after the ensemble fits the data exactly.

752

Chapter

18.

Learning from Examples

complex than necessary, but the graph tells us that the predictions improve as the ensemble
hypothesis gets more complex! Various explanations have been proposed for this. One view
is that boosting approximates Bayesian learning (see Chapter 20), which can be shown to
be an optimal learning algorithm, and the approximation improves as more hypotheses are
added. Another possible explanation is that the addition of further hypotheses enables the
ensemble to be more definite in its distinction between positive and negative examples, which
helps it when it comes to classifying new examples.

18.10.1 Online Learning

ONLINE LEARNING

RANDOMIZED
WEIGHTED
MAJORITY
ALGORITHM

So far, everything we have done in this chapter has relied on the assumption that the data are
i.i.d. (independent and identically distributed). On the one hand, that is a sensible assumption:
if the future bears no resemblance to the past, then how can we predict anything? On the other
hand, it is too strong an assumption: it is rare that our inputs have captured all the information
that would make the future truly independent of the past.
In this section we examine what to do when the data are not i.i.d.; when they can change
over time. In this case, it matters when we make a prediction, so we will adopt the perspective
called online learning: an agent receives an input xj from nature, predicts the corresponding
yj , and then is told the correct answer. Then the process repeats with xj+1 , and so on. One
might think this task is hopelessâ€”if nature is adversarial, all the predictions may be wrong.
It turns out that there are some guarantees we can make.
Let us consider the situation where our input consists of predictions from a panel of
experts. For example, each day a set of K pundits predicts whether the stock market will go
up or down, and our task is to pool those predictions and make our own. One way to do this
is to keep track of how well each expert performs, and choose to believe them in proportion
to their past performance. This is called the randomized weighted majority algorithm. We
can described it more formally:
1.
2.
3.
4.
5.
6.

REGRET

Initialize a set of weights {w1 , . . . , wK } all to 1.
Receive the predictions {yÌ‚1 , . . . , yÌ‚K } from the experts.

Randomly choose an expert kâˆ— , in proportion to its weight: P (k) = wk /( k wk ).
Predict yÌ‚kâˆ— .
Receive the correct answer y.
For each expert k such that yÌ‚k = y, update wk â† Î²wk

Here Î² is a number, 0 < Î² < 1, that tells how much to penalize an expert for each mistake.
We measure the success of this algorithm in terms of regret, which is defined as the
number of additional mistakes we make compared to the expert who, in hindsight, had the
best prediction record. Let M âˆ— be the number of mistakes made by the best expert. Then the
number of mistakes, M , made by the random weighted majority algorithm, is bounded by15
M<
15

M âˆ— ln(1/Î²) + ln K
.
1âˆ’Î²

See (Blum, 1996) for the proof.

Section 18.11.

NO-REGRET
LEARNING

18.11

Practical Machine Learning

753

This bound holds for any sequence of examples, even ones chosen by adversaries trying to
do their worst. To be specific, when there are K = 10 experts, if we choose Î² = 1/2 then
our number of mistakes is bounded by 1.39M âˆ— + 4.6, and if Î² = 3/4 by 1.15M âˆ— + 9.2. In
general, if Î² is close to 1 then we are responsive to change over the long run; if the best expert
changes, we will pick up on it before too long. However, we pay a penalty at the beginning,
when we start with all experts trusted equally; we may accept the advice of the bad experts
for too long. When Î² is closer to 0, these two factors are reversed. Note that we can choose Î²
to get asymptotically close to M âˆ— in the long run; this is called no-regret learning (because
the average amount of regret per trial tends to 0 as the number of trials increases).
Online learning is helpful when the data may be changing rapidly over time. It is also
useful for applications that involve a large collection of data that is constantly growing, even
if changes are gradual. For example, with a database of millions of Web images, you wouldnâ€™t
want to train, say, a linear regression model on all the data, and then retrain from scratch every
time a new image is added. It would be more practical to have an online algorithm that allows
images to be added incrementally. For most learning algorithms based on minimizing loss,
there is an online version based on minimizing regret. It is a bonus that many of these online
algorithms come with guaranteed bounds on regret.
To some observers, it is surprising that there are such tight bounds on how well we can
do compared to a panel of experts. To others, the really surprising thing is that when panels
of human experts congregateâ€”predicting stock market prices, sports outcomes, or political
contestsâ€”the viewing public is so willing to listen to them pontificate and so unwilling to
quantify their error rates.

P RACTICAL M ACHINE L EARNING
We have introduced a wide range of machine learning techniques, each illustrated with simple
learning tasks. In this section, we consider two aspects of practical machine learning. The first
involves finding algorithms capable of learning to recognize handwritten digits and squeezing
every last drop of predictive performance out of them. The second involves anything butâ€”
pointing out that obtaining, cleaning, and representing the data can be at least as important as
algorithm engineering.

18.11.1 Case study: Handwritten digit recognition
Recognizing handwritten digits is an important problem with many applications, including
automated sorting of mail by postal code, automated reading of checks and tax returns, and
data entry for hand-held computers. It is an area where rapid progress has been made, in part
because of better learning algorithms and in part because of the availability of better training
sets. The United States National Institute of Science and Technology (NIST) has archived a
database of 60,000 labeled digits, each 20 Ã— 20 = 400 pixels with 8-bit grayscale values. It
has become one of the standard benchmark problems for comparing new learning algorithms.
Some example digits are shown in Figure 18.36.

754

Chapter

18.

Learning from Examples

Figure 18.36 Examples from the NIST database of handwritten digits. Top row: examples
of digits 0â€“9 that are easy to identify. Bottom row: more difficult examples of the same digits.

Many different learning approaches have been tried. One of the first, and probably the
simplest, is the 3-nearest-neighbor classifier, which also has the advantage of requiring no
training time. As a memory-based algorithm, however, it must store all 60,000 images, and
its run time performance is slow. It achieved a test error rate of 2.4%.
A single-hidden-layer neural network was designed for this problem with 400 input
units (one per pixel) and 10 output units (one per class). Using cross-validation, it was found
that roughly 300 hidden units gave the best performance. With full interconnections between
layers, there were a total of 123,300 weights. This network achieved a 1.6% error rate.
A series of specialized neural networks called LeNet were devised to take advantage
of the structure of the problemâ€”that the input consists of pixels in a twoâ€“dimensional array,
and that small changes in the position or slant of an image are unimportant. Each network
had an input layer of 32 Ã— 32 units, onto which the 20 Ã— 20 pixels were centered so that each
input unit is presented with a local neighborhood of pixels. This was followed by three layers
of hidden units. Each layer consisted of several planes of n Ã— n arrays, where n is smaller
than the previous layer so that the network is down-sampling the input, and where the weights
of every unit in a plane are constrained to be identical, so that the plane is acting as a feature
detector: it can pick out a feature such as a long vertical line or a short semi-circular arc. The
output layer had 10 units. Many versions of this architecture were tried; a representative one
had hidden layers with 768, 192, and 30 units, respectively. The training set was augmented
by applying affine transformations to the actual inputs: shifting, slightly rotating, and scaling
the images. (Of course, the transformations have to be small, or else a 6 will be transformed
into a 9!) The best error rate achieved by LeNet was 0.9%.
A boosted neural network combined three copies of the LeNet architecture, with the
second one trained on a mix of patterns that the first one got 50% wrong, and the third one
trained on patterns for which the first two disagreed. During testing, the three nets voted with
the majority ruling. The test error rate was 0.7%.
A support vector machine (see Section 18.9) with 25,000 support vectors achieved an
error rate of 1.1%. This is remarkable because the SVM technique, like the simple nearestneighbor approach, required almost no thought or iterated experimentation on the part of the
developer, yet it still came close to the performance of LeNet, which had had years of development. Indeed, the support vector machine makes no use of the structure of the problem,
and would perform just as well if the pixels were presented in a permuted order.

Section 18.11.
VIRTUAL SUPPORT
VECTOR MACHINE

Practical Machine Learning

755

A virtual support vector machine starts with a regular SVM and then improves it
with a technique that is designed to take advantage of the structure of the problem. Instead of
allowing products of all pixel pairs, this approach concentrates on kernels formed from pairs
of nearby pixels. It also augments the training set with transformations of the examples, just
as LeNet did. A virtual SVM achieved the best error rate recorded to date, 0.56%.
Shape matching is a technique from computer vision used to align corresponding parts
of two different images of objects (Belongie et al., 2002). The idea is to pick out a set
of points in each of the two images, and then compute, for each point in the first image,
which point in the second image it corresponds to. From this alignment, we then compute a
transformation between the images. The transformation gives us a measure of the distance
between the images. This distance measure is better motivated than just counting the number
of differing pixels, and it turns out that a 3â€“nearest neighbor algorithm using this distance
measure performs very well. Training on only 20,000 of the 60,000 digits, and using 100
sample points per image extracted from a Canny edge detector, a shape matching classifier
achieved 0.63% test error.
Humans are estimated to have an error rate of about 0.2% on this problem. This figure
is somewhat suspect because humans have not been tested as extensively as have machine
learning algorithms. On a similar data set of digits from the United States Postal Service,
human errors were at 2.5%.
The following figure summarizes the error rates, run time performance, memory requirements, and amount of training time for the seven algorithms we have discussed. It also
adds another measure, the percentage of digits that must be rejected to achieve 0.5% error.
For example, if the SVM is allowed to reject 1.8% of the inputsâ€”that is, pass them on for
someone else to make the final judgmentâ€”then its error rate on the remaining 98.2% of the
inputs is reduced from 1.1% to 0.5%.
The following table summarizes the error rate and some of the other characteristics of
the seven techniques we have discussed.
3
300
Boosted
Virtual Shape
NN Hidden LeNet LeNet SVM SVM Match
Error rate (pct.)
2.4
1.6
0.9
0.7
1.1
0.56 0.63
Run time (millisec/digit)
1000
10
30
50
2000 200
Memory requirements (Mbyte) 12
.49
.012
.21
11
Training time (days)
0
7
14
30
10
% rejected to reach 0.5% error 8.1
3.2
1.8
0.5
1.8

18.11.2 Case study: Word senses and house prices
In a textbook we need to deal with simple, toy data to get the ideas across: a small data set,
usually in two dimensions. But in practical applications of machine learning, the data set
is usually large, multidimensional, and messy. The data are not handed to the analyst in a
prepackaged set of (x, y) values; rather the analyst needs to go out and acquire the right data.
There is a task to be accomplished, and most of the engineering problem is deciding what
data are necessary to accomplish the task; a smaller part is choosing and implementing an

756

Chapter

18.

Learning from Examples

Proportion correct on test set

1
0.95
0.9
0.85
0.8
0.75
1

10
100
1000
Training set size (millions of words)

Figure 18.37 Learning curves for five learning algorithms on a common task. Note that
there appears to be more room for improvement in the horizontal direction (more training
data) than in the vertical direction (different machine learning algorithm). Adapted from
Banko and Brill (2001).

appropriate machine learning method to process the data. Figure 18.37 shows a typical realworld example, comparing five learning algorithms on the task of word-sense classification
(given a sentence such as â€œThe bank folded,â€ classify the word â€œbankâ€ as â€œmoney-bankâ€ or
â€œriver-bankâ€). The point is that machine learning researchers have focused mainly on the
vertical direction: Can I invent a new learning algorithm that performs better than previously
published algorithms on a standard training set of 1 million words? But the graph shows
there is more room for improvement in the horizontal direction: instead of inventing a new
algorithm, all I need to do is gather 10 million words of training data; even the worst algorithm
at 10 million words is performing better than the best algorithm at 1 million. As we gather
even more data, the curves continue to rise, dwarfing the differences between algorithms.
Consider another problem: the task of estimating the true value of houses that are for
sale. In Figure 18.13 we showed a toy version of this problem, doing linear regression of
house size to asking price. You probably noticed many limitations of this model. First, it is
measuring the wrong thing: we want to estimate the selling price of a house, not the asking
price. To solve this task weâ€™ll need data on actual sales. But that doesnâ€™t mean we should
throw away the data about asking priceâ€”we can use it as one of the input features. Besides
the size of the house, weâ€™ll need more information: the number of rooms, bedrooms and
bathrooms; whether the kitchen and bathrooms have been recently remodeled; the age of
the house; weâ€™ll also need information about the lot, and the neighborhood. But how do
we define neighborhood? By zip code? What if part of one zip code is on the â€œwrongâ€
side of the highway or train tracks, and the other part is desirable? What about the school
district? Should the name of the school district be a feature, or the average test scores? In
addition to deciding what features to include, we will have to deal with missing data; different
areas have different customs on what data are reported, and individual cases will always be
missing some data. If the data you want are not available, perhaps you can set up a social
networking site to encourage people to share and correct data. In the end, this process of

Section 18.12.

Summary

757

deciding what features to use, and how to use them, is just as important as choosing between
linear regression, decision trees, or some other form of learning.
That said, one does have to pick a method (or methods) for a problem. There is no
guaranteed way to pick the best method, but there are some rough guidelines. Decision
trees are good when there are a lot of discrete features and you believe that many of them
may be irrelevant. Nonparametric methods are good when you have a lot of data and no prior
knowledge, and when you donâ€™t want to worry too much about choosing just the right features
(as long as there are fewer than 20 or so). However, nonparametric methods usually give you
a function h that is more expensive to run. Support vector machines are often considered the
best method to try first, provided the data set is not too large.

18.12

S UMMARY
This chapter has concentrated on inductive learning of functions from examples. The main
points were as follows:
â€¢ Learning takes many forms, depending on the nature of the agent, the component to be
improved, and the available feedback.
â€¢ If the available feedback provides the correct answer for example inputs, then the learning problem is called supervised learning. The task is to learn a function y = h(x).
Learning a discrete-valued function is called classification; learning a continuous function is called regression.
â€¢ Inductive learning involves finding a hypothesis that agrees well with the examples.
Ockhamâ€™s razor suggests choosing the simplest consistent hypothesis. The difficulty
of this task depends on the chosen representation.
â€¢ Decision trees can represent all Boolean functions. The information-gain heuristic
provides an efficient method for finding a simple, consistent decision tree.
â€¢ The performance of a learning algorithm is measured by the learning curve, which
shows the prediction accuracy on the test set as a function of the training-set size.
â€¢ When there are multiple models to choose from, cross-validation can be used to select
a model that will generalize well.
â€¢ Sometimes not all errors are equal. A loss function tells us how bad each error is; the
goal is then to minimize loss over a validation set.
â€¢ Computational learning theory analyzes the sample complexity and computational
complexity of inductive learning. There is a tradeoff between the expressiveness of the
hypothesis language and the ease of learning.
â€¢ Linear regression is a widely used model. The optimal parameters of a linear regression model can be found by gradient descent search, or computed exactly.
â€¢ A linear classifier with a hard thresholdâ€”also known as a perceptronâ€”can be trained
by a simple weight update rule to fit data that are linearly separable. In other cases,
the rule fails to converge.

758

Chapter

18.

Learning from Examples

â€¢ Logistic regression replaces the perceptronâ€™s hard threshold with a soft threshold defined by a logistic function. Gradient descent works well even for noisy data that are
not linearly separable.
â€¢ Neural networks represent complex nonlinear functions with a network of linearthreshold units. termMultilayer feed-forward neural networks can represent any function, given enough units. The back-propagation algorithm implements a gradient descent in parameter space to minimize the output error.
â€¢ Nonparametric models use all the data to make each prediction, rather than trying to
summarize the data first with a few parameters. Examples include nearest neighbors
and locally weighted regression.
â€¢ Support vector machines find linear separators with maximum margin to improve
the generalization performance of the classifier. Kernel methods implicitly transform
the input data into a high-dimensional space where a linear separator may exist, even if
the original data are non-separable.
â€¢ Ensemble methods such as boosting often perform better than individual methods. In
online learning we can aggregate the opinions of experts to come arbitrarily close to the
best expertâ€™s performance, even when the distribution of the data is constantly shifting.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Chapter 1 outlined the history of philosophical investigations into inductive learning. William
of Ockham16 (1280â€“1349), the most influential philosopher of his century and a major contributor to medieval epistemology, logic, and metaphysics, is credited with a statement called
â€œOckhamâ€™s Razorâ€â€”in Latin, Entia non sunt multiplicanda praeter necessitatem, and in English, â€œEntities are not to be multiplied beyond necessity.â€ Unfortunately, this laudable piece
of advice is nowhere to be found in his writings in precisely these words (although he did
say â€œPluralitas non est ponenda sine necessitate,â€ or â€œplurality shouldnâ€™t be posited without
necessityâ€). A similar sentiment was expressed by Aristotle in 350 B . C . in Physics book I,
chapter VI: â€œFor the more limited, if adequate, is always preferable.â€
The first notable use of decision trees was in EPAM, the â€œElementary Perceiver And
Memorizerâ€ (Feigenbaum, 1961), which was a simulation of human concept learning. ID3
(Quinlan, 1979) added the crucial idea of choosing the attribute with maximum entropy; it is
the basis for the decision tree algorithm in this chapter. Information theory was developed by
Claude Shannon to aid in the study of communication (Shannon and Weaver, 1949). (Shannon also contributed one of the earliest examples of machine learning, a mechanical mouse
named Theseus that learned to navigate through a maze by trial and error.) The Ï‡2 method
of tree pruning was described by Quinlan (1986). C4.5, an industrial-strength decision tree
package, can be found in Quinlan (1993). An independent tradition of decision tree learning
exists in the statistical literature. Classification and Regression Trees (Breiman et al., 1984),
known as the â€œCART book,â€ is the principal reference.
16

The name is often misspelled as â€œOccam,â€ perhaps from the French rendering, â€œGuillaume dâ€™Occam.â€

Bibliographical and Historical Notes

KOLMOGOROV
COMPLEXITY

MINIMUM
DESCRIPTION
LENGTH

UNIFORM
CONVERGENCE
THEORY
VC DIMENSION

759

Cross-validation was first introduced by Larson (1931), and in a form close to what
we show by Stone (1974) and Golub et al. (1979). The regularization procedure is due to
Tikhonov (1963). Guyon and Elisseeff (2003) introduce a journal issue devoted to the problem of feature selection. Banko and Brill (2001) and Halevy et al. (2009) discuss the advantages of using large amounts of data. It was Robert Mercer, a speech researcher who said
in 1985 â€œThere is no data like more data.â€ (Lyman and Varian, 2003) estimate that about 5
exabytes (5 Ã— 1018 bytes) of data was produced in 2002, and that the rate of production is
doubling every 3 years.
Theoretical analysis of learning algorithms began with the work of Gold (1967) on
identification in the limit. This approach was motivated in part by models of scientific
discovery from the philosophy of science (Popper, 1962), but has been applied mainly to the
problem of learning grammars from example sentences (Osherson et al., 1986).
Whereas the identification-in-the-limit approach concentrates on eventual convergence,
the study of Kolmogorov complexity or algorithmic complexity, developed independently
by Solomonoff (1964, 2009) and Kolmogorov (1965), attempts to provide a formal definition
for the notion of simplicity used in Ockhamâ€™s razor. To escape the problem that simplicity
depends on the way in which information is represented, it is proposed that simplicity be
measured by the length of the shortest program for a universal Turing machine that correctly
reproduces the observed data. Although there are many possible universal Turing machines,
and hence many possible â€œshortestâ€ programs, these programs differ in length by at most a
constant that is independent of the amount of data. This beautiful insight, which essentially
shows that any initial representation bias will eventually be overcome by the data itself, is
marred only by the undecidability of computing the length of the shortest program. Approximate measures such as the minimum description length, or MDL (Rissanen, 1984, 2007)
can be used instead and have produced excellent results in practice. The text by Li and Vitanyi (1993) is the best source for Kolmogorov complexity.
The theory of PAC-learning was inaugurated by Leslie Valiant (1984). His work stressed
the importance of computational and sample complexity. With Michael Kearns (1990), Valiant
showed that several concept classes cannot be PAC-learned tractably, even though sufficient
information is available in the examples. Some positive results were obtained for classes such
as decision lists (Rivest, 1987).
An independent tradition of sample-complexity analysis has existed in statistics, beginning with the work on uniform convergence theory (Vapnik and Chervonenkis, 1971). The
so-called VC dimension provides a measure roughly analogous to, but more general than, the
ln |H| measure obtained from PAC analysis. The VC dimension can be applied to continuous
function classes, to which standard PAC analysis does not apply. PAC-learning theory and
VC theory were first connected by the â€œfour Germansâ€ (none of whom actually is German):
Blumer, Ehrenfeucht, Haussler, and Warmuth (1989).
Linear regression with squared error loss goes back to Legendre (1805) and Gauss
(1809), who were both working on predicting orbits around the sun. The modern use of
multivariate regression for machine learning is covered in texts such as Bishop (2007). Ng
(2004) analyzed the differences between L1 and L2 regularization.

760

BAGGING

Chapter

18.

Learning from Examples

The term logistic function comes from Pierre-FrancÌ§ois Verhulst (1804â€“1849), a statistician who used the curve to model population growth with limited resources, a more realistic model than the unconstrained geometric growth proposed by Thomas Malthus. Verhulst
called it the courbe logistique, because of its relation to the logarithmic curve. The term regression is due to Francis Galton, nineteenth century statistician, cousin of Charles Darwin,
and initiator of the fields of meteorology, fingerprint analysis, and statistical correlation, who
used it in the sense of regression to the mean. The term curse of dimensionality comes from
Richard Bellman (1961).
Logistic regression can be solved with gradient descent, or with the Newton-Raphson
method (Newton, 1671; Raphson, 1690). A variant of the Newton method called L-BFGS is
sometimes used for large-dimensional problems; the L stands for â€œlimited memory,â€ meaning
that it avoids creating the full matrices all at once, and instead creates parts of them on the
fly. BFGS are authorsâ€™ initials (Byrd et al., 1995).
Nearest-neighbors models date back at least to Fix and Hodges (1951) and have been a
standard tool in statistics and pattern recognition ever since. Within AI, they were popularized
by Stanfill and Waltz (1986), who investigated methods for adapting the distance metric to the
data. Hastie and Tibshirani (1996) developed a way to localize the metric to each point in the
space, depending on the distribution of data around that point. Gionis et al. (1999) introduced
locality-sensitive hashing, which has revolutionized the retrieval of similar objects in highdimensional spaces, particularly in computer vision. Andoni and Indyk (2006) provide a
recent survey of LSH and related methods.
The ideas behind kernel machines come from Aizerman et al. (1964) (who also introduced the kernel trick), but the full development of the theory is due to Vapnik and his
colleagues (Boser et al., 1992). SVMs were made practical with the introduction of the
soft-margin classifier for handling noisy data in a paper that won the 2008 ACM Theory
and Practice Award (Cortes and Vapnik, 1995), and of the Sequential Minimal Optimization
(SMO) algorithm for efficiently solving SVM problems using quadratic programming (Platt,
1999). SVMs have proven to be very popular and effective for tasks such as text categorization (Joachims, 2001), computational genomics (Cristianini and Hahn, 2007), and natural language processing, such as the handwritten digit recognition of DeCoste and SchoÌˆlkopf (2002).
As part of this process, many new kernels have been designed that work with strings, trees,
and other nonnumerical data types. A related technique that also uses the kernel trick to implicitly represent an exponential feature space is the voted perceptron (Freund and Schapire,
1999; Collins and Duffy, 2002). Textbooks on SVMs include Cristianini and Shawe-Taylor
(2000) and SchoÌˆlkopf and Smola (2002). A friendlier exposition appears in the AI Magazine
article by Cristianini and SchoÌˆlkopf (2002). Bengio and LeCun (2007) show some of the
limitations of SVMs and other local, nonparametric methods for learning functions that have
a global structure but do not have local smoothness.
Ensemble learning is an increasingly popular technique for improving the performance
of learning algorithms. Bagging (Breiman, 1996), the first effective method, combines hypotheses learned from multiple bootstrap data sets, each generated by subsampling the original data set. The boosting method described in this chapter originated with theoretical work
by Schapire (1990). The A DA B OOST algorithm was developed by Freund and Schapire

Bibliographical and Historical Notes

761

(1996) and analyzed theoretically by Schapire (2003). Friedman et al. (2000) explain boosting from a statisticianâ€™s viewpoint. Online learning is covered in a survey by Blum (1996)
and a book by Cesa-Bianchi and Lugosi (2006). Dredze et al. (2008) introduce the idea of
confidence-weighted online learning for classification: in addition to keeping a weight for
each parameter, they also maintain a measure of confidence, so that a new example can have
a large effect on features that were rarely seen before (and thus had low confidence) and a
small effect on common features that have already been well-estimated.
The literature on neural networks is rather too large (approximately 150,000 papers to
date) to cover in detail. Cowan and Sharp (1988b, 1988a) survey the early history, beginning
with the work of McCulloch and Pitts (1943). (As mentioned in Chapter 1, John McCarthy
has pointed to the work of Nicolas Rashevsky (1936, 1938) as the earliest mathematical model
of neural learning.) Norbert Wiener, a pioneer of cybernetics and control theory (Wiener,
1948), worked with McCulloch and Pitts and influenced a number of young researchers including Marvin Minsky, who may have been the first to develop a working neural network in
hardware in 1951 (see Minsky and Papert, 1988, pp. ixâ€“x). Turing (1948) wrote a research
report titled Intelligent Machinery that begins with the sentence â€œI propose to investigate the
question as to whether it is possible for machinery to show intelligent behaviourâ€ and goes on
to describe a recurrent neural network architecture he called â€œB-type unorganized machinesâ€
and an approach to training them. Unfortunately, the report went unpublished until 1969, and
was all but ignored until recently.
Frank Rosenblatt (1957) invented the modern â€œperceptronâ€ and proved the perceptron convergence theorem (1960), although it had been foreshadowed by purely mathematical work outside the context of neural networks (Agmon, 1954; Motzkin and Schoenberg,
1954). Some early work was also done on multilayer networks, including Gamba perceptrons (Gamba et al., 1961) and madalines (Widrow, 1962). Learning Machines (Nilsson,
1965) covers much of this early work and more. The subsequent demise of early perceptron
research efforts was hastened (or, the authors later claimed, merely explained) by the book
Perceptrons (Minsky and Papert, 1969), which lamented the fieldâ€™s lack of mathematical
rigor. The book pointed out that single-layer perceptrons could represent only linearly separable concepts and noted the lack of effective learning algorithms for multilayer networks.
The papers in (Hinton and Anderson, 1981), based on a conference in San Diego in
1979, can be regarded as marking a renaissance of connectionism. The two-volume â€œPDPâ€
(Parallel Distributed Processing) anthology (Rumelhart et al., 1986a) and a short article in
Nature (Rumelhart et al., 1986b) attracted a great deal of attentionâ€”indeed, the number of
papers on â€œneural networksâ€ multiplied by a factor of 200 between 1980â€“84 and 1990â€“94.
The analysis of neural networks using the physical theory of magnetic spin glasses (Amit
et al., 1985) tightened the links between statistical mechanics and neural network theoryâ€”
providing not only useful mathematical insights but also respectability. The back-propagation
technique had been invented quite early (Bryson and Ho, 1969) but it was rediscovered several
times (Werbos, 1974; Parker, 1985).
The probabilistic interpretation of neural networks has several sources, including Baum
and Wilczek (1988) and Bridle (1990). The role of the sigmoid function is discussed by
Jordan (1995). Bayesian parameter learning for neural networks was proposed by MacKay

762

RADIAL BASIS
FUNCTION

HOPFIELD NETWORK

ASSOCIATIVE
MEMORY

Chapter

18.

Learning from Examples

(1992) and is explored further by Neal (1996). The capacity of neural networks to represent
functions was investigated by Cybenko (1988, 1989), who showed that two hidden layers are
enough to represent any function and a single layer is enough to represent any continuous
function. The â€œoptimal brain damageâ€ method for removing useless connections is by LeCun
et al. (1989), and Sietsma and Dow (1988) show how to remove useless units. The tiling
algorithm for growing larger structures is due to MeÌzard and Nadal (1989). LeCun et al.
(1995) survey a number of algorithms for handwritten digit recognition. Improved error rates
since then were reported by Belongie et al. (2002) for shape matching and DeCoste and
SchoÌˆlkopf (2002) for virtual support vectors. At the time of writing, the best test error rate
reported is 0.39% by Ranzato et al. (2007) using a convolutional neural network.
The complexity of neural network learning has been investigated by researchers in computational learning theory. Early computational results were obtained by Judd (1990), who
showed that the general problem of finding a set of weights consistent with a set of examples
is NP-complete, even under very restrictive assumptions. Some of the first sample complexity
results were obtained by Baum and Haussler (1989), who showed that the number of examples required for effective learning grows as roughly W log W , where W is the number of
weights.17 Since then, a much more sophisticated theory has been developed (Anthony and
Bartlett, 1999), including the important result that the representational capacity of a network
depends on the size of the weights as well as on their number, a result that should not be
surprising in the light of our discussion of regularization.
The most popular kind of neural network that we did not cover is the radial basis
function, or RBF, network. A radial basis function combines a weighted collection of kernels
(usually Gaussians, of course) to do function approximation. RBF networks can be trained in
two phases: first, an unsupervised clustering approach is used to train the parameters of the
Gaussiansâ€”the means and variancesâ€”are trained, as in Section 20.3.1. In the second phase,
the relative weights of the Gaussians are determined. This is a system of linear equations,
which we know how to solve directly. Thus, both phases of RBF training have a nice benefit:
the first phase is unsupervised, and thus does not require labeled training data, and the second
phase, although supervised, is efficient. See Bishop (1995) for more details.
Recurrent networks, in which units are linked in cycles, were mentioned in the chapter but not explored in depth. Hopfield networks (Hopfield, 1982) are probably the bestunderstood class of recurrent networks. They use bidirectional connections with symmetric
weights (i.e., wi,j = wj,i ), all of the units are both input and output units, the activation
function g is the sign function, and the activation levels can only be Â±1. A Hopfield network
functions as an associative memory: after the network trains on a set of examples, a new
stimulus will cause it to settle into an activation pattern corresponding to the example in the
training set that most closely resembles the new stimulus. For example, if the training set consists of a set of photographs, and the new stimulus is a small piece of one of the photographs,
then the network activation levels will reproduce the photograph from which the piece was
taken. Notice that the original photographs are not stored separately in the network; each
17

This approximately confirmed â€œUncle Bernieâ€™s rule.â€ The rule was named after Bernie Widrow, who recommended using roughly ten times as many examples as weights.

Exercises

BOLTZMANN
MACHINE

763
weight is a partial encoding of all the photographs. One of the most interesting theoretical
results is that Hopfield networks can reliably store up to 0.138N training examples, where N
is the number of units in the network.
Boltzmann machines (Hinton and Sejnowski, 1983, 1986) also use symmetric weights,
but include hidden units. In addition, they use a stochastic activation function, such that
the probability of the output being 1 is some function of the total weighted input. Boltzmann machines therefore undergo state transitions that resemble a simulated annealing search
(see Chapter 4) for the configuration that best approximates the training set. It turns out that
Boltzmann machines are very closely related to a special case of Bayesian networks evaluated
with a stochastic simulation algorithm. (See Section 14.5.)
For neural nets, Bishop (1995), Ripley (1996), and Haykin (2008) are the leading texts.
The field of computational neuroscience is covered by Dayan and Abbott (2001).
The approach taken in this chapter was influenced by the excellent course notes of David
Cohn, Tom Mitchell, Andrew Moore, and Andrew Ng. There are several top-notch textbooks
in Machine Learning (Mitchell, 1997; Bishop, 2007) and in the closely allied and overlapping
fields of pattern recognition (Ripley, 1996; Duda et al., 2001), statistics (Wasserman, 2004;
Hastie et al., 2001), data mining (Hand et al., 2001; Witten and Frank, 2005), computational
learning theory (Kearns and Vazirani, 1994; Vapnik, 1998) and information theory (Shannon
and Weaver, 1949; MacKay, 2002; Cover and Thomas, 2006). Other books concentrate on
implementations (Segaran, 2007; Marsland, 2009) and comparisons of algorithms (Michie
et al., 1994). Current research in machine learning is published in the annual proceedings
of the International Conference on Machine Learning (ICML) and the conference on Neural
Information Processing Systems (NIPS), in Machine Learning and the Journal of Machine
Learning Research, and in mainstream AI journals.

E XERCISES
18.1 Consider the problem faced by an infant learning to speak and understand a language.
Explain how this process fits into the general learning model. Describe the percepts and
actions of the infant, and the types of learning the infant must do. Describe the subfunctions
the infant is trying to learn in terms of inputs and outputs, and available example data.
18.2 Repeat Exercise 18.1 for the case of learning to play tennis (or some other sport with
which you are familiar). Is this supervised learning or reinforcement learning?
18.3 Suppose we generate a training set from a decision tree and then apply decision-tree
learning to that training set. Is it the case that the learning algorithm will eventually return
the correct tree as the training-set size goes to infinity? Why or why not?
18.4 In the recursive construction of decision trees, it sometimes happens that a mixed set
of positive and negative examples remains at a leaf node, even after all the attributes have
been used. Suppose that we have p positive examples and n negative examples.

764

CLASS PROBABILITY

Chapter

18.

Learning from Examples

a. Show that the solution used by D ECISION -T REE -L EARNING , which picks the majority
classification, minimizes the absolute error over the set of examples at the leaf.
b. Show that the class probability p/(p + n) minimizes the sum of squared errors.
18.5 Suppose that an attribute splits the set of examples E into subsets Ek and that each
subset has pk positive examples and nk negative examples. Show that the attribute has strictly
positive information gain unless the ratio pk /(pk + nk ) is the same for all k.
18.6 Consider the following data set comprised of three binary input attributes (A1 , A2 , and
A3 ) and one binary output:
Example
x1
x2
x3
x4
x5

A1
1
1
0
1
1

A2
0
0
1
1
1

A3 Output y
0
0
1
0
0
0
1
1
0
1

Use the algorithm in Figure 18.5 (page 702) to learn a decision tree for these data. Show the
computations made to determine the attribute to split at each node.
18.7 A decision graph is a generalization of a decision tree that allows nodes (i.e., attributes
used for splits) to have multiple parents, rather than just a single parent. The resulting graph
must still be acyclic. Now, consider the XOR function of three binary input attributes, which
produces the value 1 if and only if an odd number of the three input attributes has value 1.
a. Draw a minimal-sized decision tree for the three-input XOR function.
b. Draw a minimal-sized decision graph for the three-input XOR function.
18.8

This exercise considers Ï‡2 pruning of decision trees (Section 18.3.5).

a. Create a data set with two input attributes, such that the information gain at the root of
the tree for both attributes is zero, but there is a decision tree of depth 2 that is consistent
with all the data. What would Ï‡2 pruning do on this data set if applied bottom up? If
applied top down?
b. Modify D ECISION -T REE -L EARNING to include Ï‡2 -pruning. You might wish to consult Quinlan (1986) or Kearns and Mansour (1998) for details.
18.9 The standard D ECISION -T REE -L EARNING algorithm described in the chapter does
not handle cases in which some examples have missing attribute values.
a. First, we need to find a way to classify such examples, given a decision tree that includes
tests on the attributes for which values can be missing. Suppose that an example x has
a missing value for attribute A and that the decision tree tests for A at a node that x
reaches. One way to handle this case is to pretend that the example has all possible
values for the attribute, but to weight each value according to its frequency among all
of the examples that reach that node in the decision tree. The classification algorithm
should follow all branches at any node for which a value is missing and should multiply

Exercises

765
the weights along each path. Write a modified classification algorithm for decision trees
that has this behavior.
b. Now modify the information-gain calculation so that in any given collection of examples C at a given node in the tree during the construction process, the examples with
missing values for any of the remaining attributes are given â€œas-ifâ€ values according to
the frequencies of those values in the set C.
18.10 In Section 18.3.6, we noted that attributes with many different possible values can
cause problems with the gain measure. Such attributes tend to split the examples into numerous small classes or even singleton classes, thereby appearing to be highly relevant according
to the gain measure. The gain-ratio criterion selects attributes according to the ratio between
their gain and their intrinsic information contentâ€”that is, the amount of information contained in the answer to the question, â€œWhat is the value of this attribute?â€ The gain-ratio criterion therefore tries to measure how efficiently an attribute provides information on the correct
classification of an example. Write a mathematical expression for the information content of
an attribute, and implement the gain ratio criterion in D ECISION -T REE -L EARNING .
18.11 Suppose you are running a learning experiment on a new algorithm for Boolean classification. You have a data set consisting of 100 positive and 100 negative examples. You
plan to use leave-one-out cross-validation and compare your algorithm to a baseline function,
a simple majority classifier. (A majority classifier is given a set of training data and then
always outputs the class that is in the majority in the training set, regardless of the input.)
You expect the majority classifier to score about 50% on leave-one-out cross-validation, but
to your surprise, it scores zero every time. Can you explain why?
18.12 Construct a decision list to classify the data below. Select tests to be as small as
possible (in terms of attributes), breaking ties among tests with the same number of attributes
by selecting the one that classifies the greatest number of examples correctly. If multiple tests
have the same number of attributes and classify the same number of examples, then break the
tie using attributes with lower index numbers (e.g., select A1 over A2 ).
Example
x1
x2
x3
x4
x5
x6
x7
x8

A1
1
1
0
0
1
0
0
0

A2
0
0
1
1
1
1
0
0

A3
0
1
0
1
0
0
1
1

A4
0
1
0
0
1
1
1
0

y
1
1
1
0
1
0
1
0

18.13 Prove that a decision list can represent the same function as a decision tree while
using at most as many rules as there are leaves in the decision tree for that function. Give an
example of a function represented by a decision list using strictly fewer rules than the number
of leaves in a minimal-sized decision tree for that same function.

766

Chapter
18.14

18.

Learning from Examples

This exercise concerns the expressiveness of decision lists (Section 18.5).

a. Show that decision lists can represent any Boolean function, if the size of the tests is
not limited.
b. Show that if the tests can contain at most k literals each, then decision lists can represent
any function that can be represented by a decision tree of depth k.
18.15 Suppose a 7-nearest-neighbors regression search returns {7, 6, 8, 4, 7, 11, 100} as the
7 nearest y values for a given x value. What is the value of yÌ‚ that minimizes the L1 loss
function on this data? There is a common name in statistics for this value as a function of the
y values; what is it? Answer the same two questions for the L2 loss function.
18.16 Figure 18.31 showed how a circle at the origin can be linearly separated by mapping
from the features (x1 , x2 ) to the two dimensions (x21 , x22 ). But what if the circle is not located
at the origin? What if it is an ellipse, not a circle? The general equation for a circle (and
hence the decision boundary) is (x1 âˆ’ a)2 + (x2 âˆ’ b)2 âˆ’ r 2 = 0, and the general equation for
an ellipse is c(x1 âˆ’ a)2 + d(x2 âˆ’ b)2 âˆ’ 1 = 0.
a. Expand out the equation for the circle and show what the weights wi would be for the
decision boundary in the four-dimensional feature space (x1 , x2 , x21 , x22 ). Explain why
this means that any circle is linearly separable in this space.
b. Do the same for ellipses in the five-dimensional feature space (x1 , x2 , x21 , x22 , x1 x2 ).
18.17 Construct a support vector machine that computes the XOR function. Use values of
+1 and â€“1 (instead of 1 and 0) for both inputs and outputs, so that an example looks like
([âˆ’1, 1], 1) or ([âˆ’1, âˆ’1], âˆ’1). Map the input [x1 , x2 ] into a space consisting of x1 and x1 x2 .
Draw the four input points in this space, and the maximal margin separator. What is the
margin? Now draw the separating line back in the original Euclidean input space.
18.18 Consider an ensemble learning algorithm that uses simple majority voting among
K learned hypotheses. Suppose that each hypothesis has error  and that the errors made
by each hypothesis are independent of the othersâ€™. Calculate a formula for the error of the
ensemble algorithm in terms of K and , and evaluate it for the cases where K = 5, 10, and
20 and  = 0.1, 0.2, and 0.4. If the independence assumption is removed, is it possible for the
ensemble error to be worse than ?
18.19 Construct by hand a neural network that computes the XOR function of two inputs.
Make sure to specify what sort of units you are using.
n

18.20 Recall from Chapter 18 that there are 22 distinct Boolean functions of n inputs. How
many of these are representable by a threshold perceptron?
18.21 Section 18.6.4 (page 725) noted that the output of the logistic function could be interpreted as a probability p assigned by the model to the proposition that f (x) = 1; the probability that f (x) = 0 is therefore 1 âˆ’ p. Write down the probability p as a function of x
and calculate the derivative of log p with respect to each weight wi . Repeat the process for
log(1 âˆ’ p). These calculations give a learning rule for minimizing the negative-log-likelihood

Exercises

767
loss function for a probabilistic hypothesis. Comment on any resemblance to other learning
rules in the chapter.
18.22 Suppose you had a neural network with linear activation functions. That is, for each
unit the output is some constant c times the weighted sum of the inputs.
a. Assume that the network has one hidden layer. For a given assignment to the weights
w, write down equations for the value of the units in the output layer as a function of
w and the input layer x, without any explicit mention of the output of the hidden layer.
Show that there is a network with no hidden units that computes the same function.
b. Repeat the calculation in part (a), but this time do it for a network with any number of
hidden layers.
c. Suppose a network with one hidden layer and linear activation functions has n input
and output nodes and h hidden nodes. What effect does the transformation in part (a)
to a network with no hidden layers have on the total number of weights? Discuss in
particular the case h * n.
18.23 Suppose that a training set contains only a single example, repeated 100 times. In
80 of the 100 cases, the single output value is 1; in the other 20, it is 0. What will a backpropagation network predict for this example, assuming that it has been trained and reaches
a global optimum? (Hint: to find the global optimum, differentiate the error function and set
it to zero.)
18.24 The neural network whose learning performance is measured in Figure 18.25 has four
hidden nodes. This number was chosen somewhat arbitrarily. Use a cross-validation method
to find the best number of hidden nodes.
18.25 Consider the problem of separating N data points into positive and negative examples
using a linear separator. Clearly, this can always be done for N = 2 points on a line of
dimension d = 1, regardless of how the points are labeled or where they are located (unless
the points are in the same place).
a. Show that it can always be done for N = 3 points on a plane of dimension d = 2, unless
they are collinear.
b. Show that it cannot always be done for N = 4 points on a plane of dimension d = 2.
c. Show that it can always be done for N = 4 points in a space of dimension d = 3, unless
they are coplanar.
d. Show that it cannot always be done for N = 5 points in a space of dimension d = 3.
e. The ambitious student may wish to prove that N points in general position (but not
N + 1) are linearly separable in a space of dimension N âˆ’ 1.

19

KNOWLEDGE IN
LEARNING

In which we examine the problem of learning when you know something already.

PRIOR KNOWLEDGE

19.1

In all of the approaches to learning described in the previous chapter, the idea is to construct
a function that has the inputâ€“output behavior observed in the data. In each case, the learning
methods can be understood as searching a hypothesis space to find a suitable function, starting
from only a very basic assumption about the form of the function, such as â€œsecond-degree
polynomialâ€ or â€œdecision treeâ€ and perhaps a preference for simpler hypotheses. Doing this
amounts to saying that before you can learn something new, you must first forget (almost)
everything you know. In this chapter, we study learning methods that can take advantage
of prior knowledge about the world. In most cases, the prior knowledge is represented
as general first-order logical theories; thus for the first time we bring together the work on
knowledge representation and learning.

A L OGICAL F ORMULATION OF L EARNING
Chapter 18 defined pure inductive learning as a process of finding a hypothesis that agrees
with the observed examples. Here, we specialize this definition to the case where the hypothesis is represented by a set of logical sentences. Example descriptions and classifications will
also be logical sentences, and a new example can be classified by inferring a classification
sentence from the hypothesis and the example description. This approach allows for incremental construction of hypotheses, one sentence at a time. It also allows for prior knowledge,
because sentences that are already known can assist in the classification of new examples.
The logical formulation of learning may seem like a lot of extra work at first, but it turns out
to clarify many of the issues in learning. It enables us to go well beyond the simple learning
methods of Chapter 18 by using the full power of logical inference in the service of learning.

19.1.1 Examples and hypotheses
Recall from Chapter 18 the restaurant learning problem: learning a rule for deciding whether
to wait for a table. Examples were described by attributes such as Alternate, Bar , Fri/Sat ,
768

Section 19.1.

A Logical Formulation of Learning

769

and so on. In a logical setting, an example is described by a logical sentence; the attributes
become unary predicates. Let us generically call the ith example Xi . For instance, the first
example from Figure 18.3 (page 700) is described by the sentences
Alternate(X1 ) âˆ§ Â¬Bar (X1 ) âˆ§ Â¬Fri/Sat (X1 ) âˆ§ Hungry(X1 ) âˆ§ . . .
We will use the notation Di (Xi ) to refer to the description of Xi , where Di can be any logical
expression taking a single argument. The classification of the example is given by a literal
using the goal predicate, in this case
WillWait(X1 )

or

Â¬WillWait(X1 ) .

The complete training set can thus be expressed as the conjunction of all the example descriptions and goal literals.
The aim of inductive learning in general is to find a hypothesis that classifies the examples well and generalizes well to new examples. Here we are concerned with hypotheses
expressed in logic; each hypothesis hj will have the form
âˆ€ x Goal (x) â‡” Cj (x) ,
where Cj (x) is a candidate definitionâ€”some expression involving the attribute predicates.
For example, a decision tree can be interpreted as a logical expression of this form. Thus, the
tree in Figure 18.6 (page 702) expresses the following logical definition (which we will call
hr for future reference):
âˆ€ r WillWait(r) â‡” Patrons (r, Some)
âˆ¨ Patrons (r, Full ) âˆ§ Hungry(r) âˆ§ Type(r, French)
âˆ¨ Patrons (r, Full ) âˆ§ Hungry(r) âˆ§ Type(r, Thai )
âˆ§ Fri /Sat (r)
âˆ¨ Patrons (r, Full ) âˆ§ Hungry(r) âˆ§ Type(r, Burger ) .
EXTENSION

(19.1)

Each hypothesis predicts that a certain set of examplesâ€”namely, those that satisfy its candidate definitionâ€”will be examples of the goal predicate. This set is called the extension of
the predicate. Two hypotheses with different extensions are therefore logically inconsistent
with each other, because they disagree on their predictions for at least one example. If they
have the same extension, they are logically equivalent.
The hypothesis space H is the set of all hypotheses {h1 , . . . , hn } that the learning algorithm is designed to entertain. For example, the D ECISION -T REE -L EARNING algorithm can
entertain any decision tree hypothesis defined in terms of the attributes provided; its hypothesis space therefore consists of all these decision trees. Presumably, the learning algorithm
believes that one of the hypotheses is correct; that is, it believes the sentence
h1 âˆ¨ h2 âˆ¨ h3 âˆ¨ . . . âˆ¨ hn .

(19.2)

As the examples arrive, hypotheses that are not consistent with the examples can be ruled
out. Let us examine this notion of consistency more carefully. Obviously, if hypothesis hj is
consistent with the entire training set, it has to be consistent with each example in the training
set. What would it mean for it to be inconsistent with an example? There are two possible
ways that this can happen:

770

Chapter 19.

Knowledge in Learning

â€¢ An example can be a false negative for the hypothesis, if the hypothesis says it should
be negative but in fact it is positive. For instance, the new example X13 described by
Patrons(X13 , Full ) âˆ§ Â¬Hungry(X13 ) âˆ§ . . . âˆ§ WillWait(X13 )
would be a false negative for the hypothesis hr given earlier. From hr and the example
description, we can deduce both WillWait(X13 ), which is what the example says,
and Â¬WillWait(X13 ), which is what the hypothesis predicts. The hypothesis and the
example are therefore logically inconsistent.
â€¢ An example can be a false positive for the hypothesis, if the hypothesis says it should
be positive but in fact it is negative.1

FALSE NEGATIVE

FALSE POSITIVE

If an example is a false positive or false negative for a hypothesis, then the example and the
hypothesis are logically inconsistent with each other. Assuming that the example is a correct
observation of fact, then the hypothesis can be ruled out. Logically, this is exactly analogous
to the resolution rule of inference (see Chapter 9), where the disjunction of hypotheses corresponds to a clause and the example corresponds to a literal that resolves against one of the
literals in the clause. An ordinary logical inference system therefore could, in principle, learn
from the example by eliminating one or more hypotheses. Suppose, for example, that the
example is denoted by the sentence I1 , and the hypothesis space is h1 âˆ¨ h2 âˆ¨ h3 âˆ¨ h4 . Then if
I1 is inconsistent with h2 and h3 , the logical inference system can deduce the new hypothesis
space h1 âˆ¨ h4 .
We therefore can characterize inductive learning in a logical setting as a process of
gradually eliminating hypotheses that are inconsistent with the examples, narrowing down
the possibilities. Because the hypothesis space is usually vast (or even infinite in the case of
first-order logic), we do not recommend trying to build a learning system using resolutionbased theorem proving and a complete enumeration of the hypothesis space. Instead, we will
describe two approaches that find logically consistent hypotheses with much less effort.

19.1.2 Current-best-hypothesis search
CURRENT-BESTHYPOTHESIS

GENERALIZATION

The idea behind current-best-hypothesis search is to maintain a single hypothesis, and to
adjust it as new examples arrive in order to maintain consistency. The basic algorithm was
described by John Stuart Mill (1843), and may well have appeared even earlier.
Suppose we have some hypothesis such as hr , of which we have grown quite fond.
As long as each new example is consistent, we need do nothing. Then along comes a false
negative example, X13 . What do we do? Figure 19.1(a) shows hr schematically as a region:
everything inside the rectangle is part of the extension of hr . The examples that have actually
been seen so far are shown as â€œ+â€ or â€œâ€“â€, and we see that hr correctly categorizes all the
examples as positive or negative examples of WillWait. In Figure 19.1(b), a new example
(circled) is a false negative: the hypothesis says it should be negative but it is actually positive.
The extension of the hypothesis must be increased to include it. This is called generalization;
one possible generalization is shown in Figure 19.1(c). Then in Figure 19.1(d), we see a false
positive: the hypothesis says the new example (circled) should be positive, but it actually is
1

The terms â€œfalse positiveâ€ and â€œfalse negativeâ€ are used in medicine to describe erroneous results from lab
tests. A result is a false positive if it indicates that the patient has the disease when in fact no disease is present.

Section 19.1.

A Logical Formulation of Learning

â€“

â€“

â€“

+
+

â€“

â€“

â€“

â€“
+
+

â€“

+
+

â€“

+

â€“

â€“

â€“ â€“

â€“

â€“

â€“

+
+

+
+

â€“

+

â€“

(c)

â€“

â€“
â€“

+
+

â€“
++

â€“
â€“

â€“

â€“

+
+

+

++

â€“
â€“

(b)

â€“

+

++

â€“
â€“

(a)

â€“

+
+

â€“

â€“

â€“

â€“

+

++
â€“

â€“

771

â€“

â€“

+

+

â€“
++

+
â€“

(d)

â€“

+
+

â€“

â€“

â€“

â€“

+

â€“

â€“

+

+
â€“
â€“

(e)

Figure 19.1 (a) A consistent hypothesis. (b) A false negative. (c) The hypothesis is generalized. (d) A false positive. (e) The hypothesis is specialized.

function C URRENT-B EST-L EARNING(examples, h) returns a hypothesis or fail
if examples is empty then
return h
e â† F IRST (examples)
if e is consistent with h then
return C URRENT-B EST-L EARNING(R EST(examples), h)
else if e is a false positive for h then
for each h  in specializations of h consistent with examples seen so far do
h  â† C URRENT-B EST-L EARNING(R EST(examples), h  )
if h  = fail then return h 
else if e is a false negative for h then
for each h  in generalizations of h consistent with examples seen so far do
h  â† C URRENT-B EST-L EARNING(R EST(examples), h  )
if h  = fail then return h 
return fail
Figure 19.2 The current-best-hypothesis learning algorithm. It searches for a consistent hypothesis that fits all the examples and backtracks when no consistent specialization/generalization can be found. To start the algorithm, any hypothesis can be passed in;
it will be specialized or gneralized as needed.

SPECIALIZATION

negative. The extension of the hypothesis must be decreased to exclude the example. This is
called specialization; in Figure 19.1(e) we see one possible specialization of the hypothesis.
The â€œmore general thanâ€ and â€œmore specific thanâ€ relations between hypotheses provide the
logical structure on the hypothesis space that makes efficient search possible.
We can now specify the C URRENT-B EST-L EARNING algorithm, shown in Figure 19.2.
Notice that each time we consider generalizing or specializing the hypothesis, we must check
for consistency with the other examples, because an arbitrary increase/decrease in the extension might include/exclude previously seen negative/positive examples.

772

Chapter 19.

Knowledge in Learning

We have defined generalization and specialization as operations that change the extension of a hypothesis. Now we need to determine exactly how they can be implemented as
syntactic operations that change the candidate definition associated with the hypothesis, so
that a program can carry them out. This is done by first noting that generalization and specialization are also logical relationships between hypotheses. If hypothesis h1 , with definition
C1 , is a generalization of hypothesis h2 with definition C2 , then we must have
âˆ€ x C2 (x) â‡’ C1 (x) .

DROPPING
CONDITIONS

Therefore in order to construct a generalization of h2 , we simply need to find a definition C1 that is logically implied by C2 . This is easily done. For example, if C2 (x) is
Alternate(x) âˆ§ Patrons(x, Some), then one possible generalization is given by C1 (x) â‰¡
Patrons(x, Some ). This is called dropping conditions. Intuitively, it generates a weaker
definition and therefore allows a larger set of positive examples. There are a number of other
generalization operations, depending on the language being operated on. Similarly, we can
specialize a hypothesis by adding extra conditions to its candidate definition or by removing
disjuncts from a disjunctive definition. Let us see how this works on the restaurant example,
using the data in Figure 18.3.
â€¢ The first example, X1 , is positive. The attribute Alternate(X1 ) is true, so let the initial
hypothesis be
h1 : âˆ€ x WillWait(x) â‡” Alternate(x) .
â€¢ The second example, X2 , is negative. h1 predicts it to be positive, so it is a false positive.
Therefore, we need to specialize h1 . This can be done by adding an extra condition that
will rule out X2 , while continuing to classify X1 as positive. One possibility is
h2 : âˆ€ x WillWait(x) â‡” Alternate(x) âˆ§ Patrons (x, Some) .
â€¢ The third example, X3 , is positive. h2 predicts it to be negative, so it is a false negative.
Therefore, we need to generalize h2 . We drop the Alternate condition, yielding
h3 : âˆ€ x WillWait(x) â‡” Patrons (x, Some) .
â€¢ The fourth example, X4 , is positive. h3 predicts it to be negative, so it is a false negative.
We therefore need to generalize h3 . We cannot drop the Patrons condition, because
that would yield an all-inclusive hypothesis that would be inconsistent with X2 . One
possibility is to add a disjunct:
h4 : âˆ€ x WillWait(x) â‡” Patrons (x, Some)
âˆ¨ (Patrons (x, Full ) âˆ§ Fri/Sat (x)) .
Already, the hypothesis is starting to look reasonable. Obviously, there are other possibilities
consistent with the first four examples; here are two of them:
h4 : âˆ€ x WillWait(x) â‡” Â¬WaitEstimate(x, 30-60) .
h4 : âˆ€ x WillWait(x) â‡” Patrons(x, Some)
âˆ¨ (Patrons(x, Full ) âˆ§ WaitEstimate(x, 10-30)) .
The C URRENT-B EST-L EARNING algorithm is described nondeterministically, because at any
point, there may be several possible specializations or generalizations that can be applied. The

Section 19.1.

A Logical Formulation of Learning

773

function V ERSION -S PACE -L EARNING(examples) returns a version space
local variables: V , the version space: the set of all hypotheses
V â† the set of all hypotheses
for each example e in examples do
if V is not empty then V â† V ERSION -S PACE -U PDATE (V , e)
return V
function V ERSION -S PACE -U PDATE(V , e) returns an updated version space
V â† {h âˆˆ V : h is consistent with e}
Figure 19.3 The version space learning algorithm. It finds a subset of V that is consistent
with all the examples.

choices that are made will not necessarily lead to the simplest hypothesis, and may lead to an
unrecoverable situation where no simple modification of the hypothesis is consistent with all
of the data. In such cases, the program must backtrack to a previous choice point.
The C URRENT-B EST-L EARNING algorithm and its variants have been used in many
machine learning systems, starting with Patrick Winstonâ€™s (1970) â€œarch-learningâ€ program.
With a large number of examples and a large space, however, some difficulties arise:
1. Checking all the previous examples over again for each modification is very expensive.
2. The search process may involve a great deal of backtracking. As we saw in Chapter 18,
hypothesis space can be a doubly exponentially large place.

19.1.3 Least-commitment search
Backtracking arises because the current-best-hypothesis approach has to choose a particular
hypothesis as its best guess even though it does not have enough data yet to be sure of the
choice. What we can do instead is to keep around all and only those hypotheses that are
consistent with all the data so far. Each new example will either have no effect or will get
rid of some of the hypotheses. Recall that the original hypothesis space can be viewed as a
disjunctive sentence
h1 âˆ¨ h2 âˆ¨ h3 . . . âˆ¨ hn .

VERSION SPACE
CANDIDATE
ELIMINATION

As various hypotheses are found to be inconsistent with the examples, this disjunction shrinks,
retaining only those hypotheses not ruled out. Assuming that the original hypothesis space
does in fact contain the right answer, the reduced disjunction must still contain the right answer because only incorrect hypotheses have been removed. The set of hypotheses remaining
is called the version space, and the learning algorithm (sketched in Figure 19.3) is called the
version space learning algorithm (also the candidate elimination algorithm).
One important property of this approach is that it is incremental: one never has to
go back and reexamine the old examples. All remaining hypotheses are guaranteed to be
consistent with them already. But there is an obvious problem. We already said that the

774

Chapter 19.

Knowledge in Learning

This region all inconsistent
G1

G2

G3

Gm

...

More general

More specific
S1

S2

...

Sn

This region all inconsistent
Figure 19.4

BOUNDARY SET
G-SET
S-SET

The version space contains all hypotheses consistent with the examples.

hypothesis space is enormous, so how can we possibly write down this enormous disjunction?
The following simple analogy is very helpful. How do you represent all the real numbers between 1 and 2? After all, there are an infinite number of them! The answer is to use
an interval representation that just specifies the boundaries of the set: [1,2]. It works because
we have an ordering on the real numbers.
We also have an ordering on the hypothesis space, namely, generalization/specialization.
This is a partial ordering, which means that each boundary will not be a point but rather a
set of hypotheses called a boundary set. The great thing is that we can represent the entire
version space using just two boundary sets: a most general boundary (the G-set) and a most
specific boundary (the S-set). Everything in between is guaranteed to be consistent with the
examples. Before we prove this, let us recap:
â€¢ The current version space is the set of hypotheses consistent with all the examples so
far. It is represented by the S-set and G-set, each of which is a set of hypotheses.
â€¢ Every member of the S-set is consistent with all observations so far, and there are no
consistent hypotheses that are more specific.
â€¢ Every member of the G-set is consistent with all observations so far, and there are no
consistent hypotheses that are more general.
We want the initial version space (before any examples have been seen) to represent all possible hypotheses. We do this by setting the G-set to contain True (the hypothesis that contains
everything), and the S-set to contain False (the hypothesis whose extension is empty).
Figure 19.4 shows the general structure of the boundary-set representation of the version
space. To show that the representation is sufficient, we need the following two properties:

Section 19.1.

A Logical Formulation of Learning

775

1. Every consistent hypothesis (other than those in the boundary sets) is more specific than
some member of the G-set, and more general than some member of the S-set. (That is,
there are no â€œstragglersâ€ left outside.) This follows directly from the definitions of S
and G. If there were a straggler h, then it would have to be no more specific than any
member of G, in which case it belongs in G; or no more general than any member of
S, in which case it belongs in S.
2. Every hypothesis more specific than some member of the G-set and more general than
some member of the S-set is a consistent hypothesis. (That is, there are no â€œholesâ€ between the boundaries.) Any h between S and G must reject all the negative examples
rejected by each member of G (because it is more specific), and must accept all the positive examples accepted by any member of S (because it is more general). Thus, h must
agree with all the examples, and therefore cannot be inconsistent. Figure 19.5 shows
the situation: there are no known examples outside S but inside G, so any hypothesis
in the gap must be consistent.
We have therefore shown that if S and G are maintained according to their definitions, then
they provide a satisfactory representation of the version space. The only remaining problem
is how to update S and G for a new example (the job of the V ERSION -S PACE -U PDATE
function). This may appear rather complicated at first, but from the definitions and with the
help of Figure 19.4, it is not too hard to reconstruct the algorithm.

â€“

â€“
â€“

â€“
â€“

â€“

â€“

â€“

G1
â€“

+
+
S
+
+
+ 1
+
+
+
+
+
â€“

â€“

G2 â€“
â€“

â€“

Figure 19.5 The extensions of the members of G and S. No known examples lie in
between the two sets of boundaries.

We need to worry about the members Si and Gi of the S- and G-sets. For each one, the
new example may be a false positive or a false negative.
1. False positive for Si : This means Si is too general, but there are no consistent specializations of Si (by definition), so we throw it out of the S-set.
2. False negative for Si : This means Si is too specific, so we replace it by all its immediate
generalizations, provided they are more specific than some member of G.
3. False positive for Gi : This means Gi is too general, so we replace it by all its immediate
specializations, provided they are more general than some member of S.

776

Chapter 19.

Knowledge in Learning

4. False negative for Gi : This means Gi is too specific, but there are no consistent generalizations of Gi (by definition) so we throw it out of the G-set.
We continue these operations for each new example until one of three things happens:
1. We have exactly one hypothesis left in the version space, in which case we return it as
the unique hypothesis.
2. The version space collapsesâ€”either S or G becomes empty, indicating that there are
no consistent hypotheses for the training set. This is the same case as the failure of the
simple version of the decision tree algorithm.
3. We run out of examples and have several hypotheses remaining in the version space.
This means the version space represents a disjunction of hypotheses. For any new
example, if all the disjuncts agree, then we can return their classification of the example.
If they disagree, one possibility is to take the majority vote.
We leave as an exercise the application of the V ERSION -S PACE -L EARNING algorithm to the
restaurant data.
There are two principal drawbacks to the version-space approach:
â€¢ If the domain contains noise or insufficient attributes for exact classification, the version
space will always collapse.
â€¢ If we allow unlimited disjunction in the hypothesis space, the S-set will always contain
a single most-specific hypothesis, namely, the disjunction of the descriptions of the
positive examples seen to date. Similarly, the G-set will contain just the negation of the
disjunction of the descriptions of the negative examples.
â€¢ For some hypothesis spaces, the number of elements in the S-set or G-set may grow
exponentially in the number of attributes, even though efficient learning algorithms exist
for those hypothesis spaces.

GENERALIZATION
HIERARCHY

To date, no completely successful solution has been found for the problem of noise. The
problem of disjunction can be addressed by allowing only limited forms of disjunction or by
including a generalization hierarchy of more general predicates. For example, instead of
using the disjunction WaitEstimate(x, 30-60) âˆ¨ WaitEstimate(x, >60), we might use the
single literal LongWait (x). The set of generalization and specialization operations can be
easily extended to handle this.
The pure version space algorithm was first applied in the Meta-D ENDRAL system,
which was designed to learn rules for predicting how molecules would break into pieces in
a mass spectrometer (Buchanan and Mitchell, 1978). Meta-D ENDRAL was able to generate
rules that were sufficiently novel to warrant publication in a journal of analytical chemistryâ€”
the first real scientific knowledge generated by a computer program. It was also used in the
elegant L EX system (Mitchell et al., 1983), which was able to learn to solve symbolic integration problems by studying its own successes and failures. Although version space methods
are probably not practical in most real-world learning problems, mainly because of noise,
they provide a good deal of insight into the logical structure of hypothesis space.

Section 19.2.

Knowledge in Learning

777

Prior
knowledge

Observations

Knowledge-based
inductive learning

Hypotheses

Predictions

Figure 19.6 A cumulative learning process uses, and adds to, its stock of background
knowledge over time.

19.2

K NOWLEDGE IN L EARNING
The preceding section described the simplest setting for inductive learning. To understand the
role of prior knowledge, we need to talk about the logical relationships among hypotheses,
example descriptions, and classifications. Let Descriptions denote the conjunction of all the
example descriptions in the training set, and let Classiï¬cations denote the conjunction of all
the example classifications. Then a Hypothesis that â€œexplains the observationsâ€ must satisfy
the following property (recall that |= means â€œlogically entailsâ€):
Hypothesis âˆ§ Descriptions |= Classiï¬cations .

ENTAILMENT
CONSTRAINT

(19.3)

We call this kind of relationship an entailment constraint, in which Hypothesis is the â€œunknown.â€ Pure inductive learning means solving this constraint, where Hypothesis is drawn
from some predefined hypothesis space. For example, if we consider a decision tree as a
logical formula (see Equation (19.1) on page 769), then a decision tree that is consistent with
all the examples will satisfy Equation (19.3). If we place no restrictions on the logical form
of the hypothesis, of course, then Hypothesis = Classiï¬cations also satisfies the constraint.
Ockhamâ€™s razor tells us to prefer small, consistent hypotheses, so we try to do better than
simply memorizing the examples.
This simple knowledge-free picture of inductive learning persisted until the early 1980s.
The modern approach is to design agents that already know something and are trying to learn
some more. This may not sound like a terrifically deep insight, but it makes quite a difference
to the way we design agents. It might also have some relevance to our theories about how
science itself works. The general idea is shown schematically in Figure 19.6.
An autonomous learning agent that uses background knowledge must somehow obtain
the background knowledge in the first place, in order for it to be used in the new learning
episodes. This method must itself be a learning process. The agentâ€™s life history will therefore be characterized by cumulative, or incremental, development. Presumably, the agent
could start out with nothing, performing inductions in vacuo like a good little pure induction program. But once it has eaten from the Tree of Knowledge, it can no longer pursue
such naive speculations and should use its background knowledge to learn more and more
effectively. The question is then how to actually do this.

778

Chapter 19.

Knowledge in Learning

19.2.1 Some simple examples
Let us consider some commonsense examples of learning with background knowledge. Many
apparently rational cases of inferential behavior in the face of observations clearly do not
follow the simple principles of pure induction.
â€¢ Sometimes one leaps to general conclusions after only one observation. Gary Larson
once drew a cartoon in which a bespectacled caveman, Zog, is roasting his lizard on
the end of a pointed stick. He is watched by an amazed crowd of his less intellectual
contemporaries, who have been using their bare hands to hold their victuals over the fire.
This enlightening experience is enough to convince the watchers of a general principle
of painless cooking.
â€¢ Or consider the case of the traveler to Brazil meeting her first Brazilian. On hearing him
speak Portuguese, she immediately concludes that Brazilians speak Portuguese, yet on
discovering that his name is Fernando, she does not conclude that all Brazilians are
called Fernando. Similar examples appear in science. For example, when a freshman
physics student measures the density and conductance of a sample of copper at a particular temperature, she is quite confident in generalizing those values to all pieces of
copper. Yet when she measures its mass, she does not even consider the hypothesis that
all pieces of copper have that mass. On the other hand, it would be quite reasonable to
make such a generalization over all pennies.
â€¢ Finally, consider the case of a pharmacologically ignorant but diagnostically sophisticated medical student observing a consulting session between a patient and an expert
internist. After a series of questions and answers, the expert tells the patient to take a
course of a particular antibiotic. The medical student infers the general rule that that
particular antibiotic is effective for a particular type of infection.
These are all cases in which the use of background knowledge allows much faster learning
than one might expect from a pure induction program.

19.2.2 Some general schemes

EXPLANATIONBASED
LEARNING

In each of the preceding examples, one can appeal to prior knowledge to try to justify the
generalizations chosen. We will now look at what kinds of entailment constraints are operating in each case. The constraints will involve the Background knowledge, in addition to the
Hypothesis and the observed Descriptions and Classiï¬cations .
In the case of lizard toasting, the cavemen generalize by explaining the success of the
pointed stick: it supports the lizard while keeping the hand away from the fire. From this
explanation, they can infer a general rule: that any long, rigid, sharp object can be used to toast
small, soft-bodied edibles. This kind of generalization process has been called explanationbased learning, or EBL. Notice that the general rule follows logically from the background
knowledge possessed by the cavemen. Hence, the entailment constraints satisfied by EBL are
the following:
Hypothesis âˆ§ Descriptions |= Classiï¬cations
Background |= Hypothesis .

Section 19.2.

RELEVANCE

Knowledge in Learning

779

Because EBL uses Equation (19.3), it was initially thought to be a way to learn from examples. But because it requires that the background knowledge be sufficient to explain the
Hypothesis, which in turn explains the observations, the agent does not actually learn anything factually new from the example. The agent could have derived the example from what
it already knew, although that might have required an unreasonable amount of computation.
EBL is now viewed as a method for converting first-principles theories into useful, specialpurpose knowledge. We describe algorithms for EBL in Section 19.3.
The situation of our traveler in Brazil is quite different, for she cannot necessarily explain why Fernando speaks the way he does, unless she knows her papal bulls. Moreover,
the same generalization would be forthcoming from a traveler entirely ignorant of colonial
history. The relevant prior knowledge in this case is that, within any given country, most
people tend to speak the same language; on the other hand, Fernando is not assumed to be
the name of all Brazilians because this kind of regularity does not hold for names. Similarly,
the freshman physics student also would be hard put to explain the particular values that she
discovers for the conductance and density of copper. She does know, however, that the material of which an object is composed and its temperature together determine its conductance.
In each case, the prior knowledge Background concerns the relevance of a set of features to
the goal predicate. This knowledge, together with the observations, allows the agent to infer
a new, general rule that explains the observations:
Hypothesis âˆ§ Descriptions |= Classiï¬cations ,

RELEVANCE-BASED
LEARNING

(19.4)
Background âˆ§ Descriptions âˆ§ Classiï¬cations |= Hypothesis .
We call this kind of generalization relevance-based learning, or RBL (although the name is
not standard). Notice that whereas RBL does make use of the content of the observations, it
does not produce hypotheses that go beyond the logical content of the background knowledge
and the observations. It is a deductive form of learning and cannot by itself account for the
creation of new knowledge starting from scratch.
In the case of the medical student watching the expert, we assume that the studentâ€™s
prior knowledge is sufficient to infer the patientâ€™s disease D from the symptoms. This is
not, however, enough to explain the fact that the doctor prescribes a particular medicine M .
The student needs to propose another rule, namely, that M generally is effective against D.
Given this rule and the studentâ€™s prior knowledge, the student can now explain why the expert
prescribes M in this particular case. We can generalize this example to come up with the
entailment constraint
Background âˆ§ Hypothesis âˆ§ Descriptions |= Classiï¬cations .

KNOWLEDGE-BASED
INDUCTIVE
LEARNING
INDUCTIVE LOGIC
PROGRAMMING

(19.5)

That is, the background knowledge and the new hypothesis combine to explain the examples.
As with pure inductive learning, the learning algorithm should propose hypotheses that are as
simple as possible, consistent with this constraint. Algorithms that satisfy constraint (19.5)
are called knowledge-based inductive learning, or KBIL, algorithms.
KBIL algorithms, which are described in detail in Section 19.5, have been studied
mainly in the field of inductive logic programming, or ILP. In ILP systems, prior knowledge plays two key roles in reducing the complexity of learning:

780

Chapter 19.

Knowledge in Learning

1. Because any hypothesis generated must be consistent with the prior knowledge as well
as with the new observations, the effective hypothesis space size is reduced to include
only those theories that are consistent with what is already known.
2. For any given set of observations, the size of the hypothesis required to construct an
explanation for the observations can be much reduced, because the prior knowledge
will be available to help out the new rules in explaining the observations. The smaller
the hypothesis, the easier it is to find.
In addition to allowing the use of prior knowledge in induction, ILP systems can formulate
hypotheses in general first-order logic, rather than in the restricted attribute-based language
of Chapter 18. This means that they can learn in environments that cannot be understood by
simpler systems.

19.3

MEMOIZATION

E XPLANATION -BASED L EARNING
Explanation-based learning is a method for extracting general rules from individual observations. As an example, consider the problem of differentiating and simplifying algebraic
expressions (Exercise 9.17). If we differentiate an expression such as X 2 with respect to
X, we obtain 2X. (We use a capital letter for the arithmetic unknown X, to distinguish it
from the logical variable x.) In a logical reasoning system, the goal might be expressed as
A SK (Derivative(X 2 , X) = d, KB), with solution d = 2X.
Anyone who knows differential calculus can see this solution â€œby inspectionâ€ as a result
of practice in solving such problems. A student encountering such problems for the first time,
or a program with no experience, will have a much more difficult job. Application of the
standard rules of differentiation eventually yields the expression 1 Ã— (2 Ã— (X (2âˆ’1) )), and
eventually this simplifies to 2X. In the authorsâ€™ logic programming implementation, this
takes 136 proof steps, of which 99 are on dead-end branches in the proof. After such an
experience, we would like the program to solve the same problem much more quickly the
next time it arises.
The technique of memoization has long been used in computer science to speed up
programs by saving the results of computation. The basic idea of memo functions is to
accumulate a database of inputâ€“output pairs; when the function is called, it first checks the
database to see whether it can avoid solving the problem from scratch. Explanation-based
learning takes this a good deal further, by creating general rules that cover an entire class
of cases. In the case of differentiation, memoization would remember that the derivative of
X 2 with respect to X is 2X, but would leave the agent to calculate the derivative of Z 2 with
respect to Z from scratch. We would like to be able to extract the general rule that for any
arithmetic unknown u, the derivative of u2 with respect to u is 2u. (An even more general
rule for un can also be produced, but the current example suffices to make the point.) In
logical terms, this is expressed by the rule
ArithmeticUnknown(u) â‡’ Derivative(u2 , u) = 2u .

Section 19.3.

Explanation-Based Learning

781

If the knowledge base contains such a rule, then any new case that is an instance of this rule
can be solved immediately.
This is, of course, merely a trivial example of a very general phenomenon. Once something is understood, it can be generalized and reused in other circumstances. It becomes an
â€œobviousâ€ step and can then be used as a building block in solving problems still more complex. Alfred North Whitehead (1911), co-author with Bertrand Russell of Principia Mathematica, wrote â€œCivilization advances by extending the number of important operations that
we can do without thinking about them,â€ perhaps himself applying EBL to his understanding
of events such as Zogâ€™s discovery. If you have understood the basic idea of the differentiation example, then your brain is already busily trying to extract the general principles of
explanation-based learning from it. Notice that you hadnâ€™t already invented EBL before you
saw the example. Like the cavemen watching Zog, you (and we) needed an example before
we could generate the basic principles. This is because explaining why something is a good
idea is much easier than coming up with the idea in the first place.

19.3.1 Extracting general rules from examples
The basic idea behind EBL is first to construct an explanation of the observation using prior
knowledge, and then to establish a definition of the class of cases for which the same explanation structure can be used. This definition provides the basis for a rule covering all of the
cases in the class. The â€œexplanationâ€ can be a logical proof, but more generally it can be any
reasoning or problem-solving process whose steps are well defined. The key is to be able to
identify the necessary conditions for those same steps to apply to another case.
We will use for our reasoning system the simple backward-chaining theorem prover
described in Chapter 9. The proof tree for Derivative(X 2 , X) = 2X is too large to use as an
example, so we will use a simpler problem to illustrate the generalization method. Suppose
our problem is to simplify 1 Ã— (0 + X). The knowledge base includes the following rules:
Rewrite(u, v) âˆ§ Simplify(v, w) â‡’ Simplify(u, w) .
Primitive(u) â‡’ Simplify(u, u) .
ArithmeticUnknown(u) â‡’ Primitive(u) .
Number (u) â‡’ Primitive(u) .
Rewrite(1 Ã— u, u) .
Rewrite(0 + u, u) .
..
.
The proof that the answer is X is shown in the top half of Figure 19.7. The EBL method
actually constructs two proof trees simultaneously. The second proof tree uses a variabilized
goal in which the constants from the original goal are replaced by variables. As the original
proof proceeds, the variabilized proof proceeds in step, using exactly the same rule applications. This could cause some of the variables to become instantiated. For example, in order
to use the rule Rewrite(1 Ã— u, u), the variable x in the subgoal Rewrite(x Ã— (y + z), v) must
be bound to 1. Similarly, y must be bound to 0 in the subgoal Rewrite(y + z, v  ) in order to
use the rule Rewrite(0 + u, u). Once we have the generalized proof tree, we take the leaves

782

Chapter 19.

Knowledge in Learning

Simplify(1 Ã— (0+X),w)
Rewrite(1 Ã— (0+X),v)

Simplify(0+X,w)

Yes, {v / 0+X}
Rewrite(0+X,v')

Simplify(X,w)

Yes, {v' / X}

{w / X}
Primitive(X)

ArithmeticUnknown(X)
Yes, { }

Simplify(x Ã— (y+z),w)

Rewrite(x Ã— (y+z),v)

Simplify(y+z,w)

Yes, {x / 1, v / y+z}
Rewrite(y+z,v')

Simplify(z,w)
{w / z}

Yes, {y / 0, v'/ z}

Primitive(z)

ArithmeticUnknown(z)
Yes, { }

Figure 19.7 Proof trees for the simplification problem. The first tree shows the proof for
the original problem instance, from which we can derive
ArithmeticUnknown(z) â‡’ Simplify (1 Ã— (0 + z), z) .
The second tree shows the proof for a problem instance with all constants replaced by variables, from which we can derive a variety of other rules.

(with the necessary bindings) and form a general rule for the goal predicate:
Rewrite(1 Ã— (0 + z), 0 + z) âˆ§ Rewrite(0 + z, z) âˆ§ ArithmeticUnknown(z)
â‡’ Simplify(1 Ã— (0 + z), z) .
Notice that the first two conditions on the left-hand side are true regardless of the value of z.
We can therefore drop them from the rule, yielding
ArithmeticUnknown(z) â‡’ Simplify(1 Ã— (0 + z), z) .
In general, conditions can be dropped from the final rule if they impose no constraints on the
variables on the right-hand side of the rule, because the resulting rule will still be true and
will be more efficient. Notice that we cannot drop the condition ArithmeticUnknown(z),
because not all possible values of z are arithmetic unknowns. Values other than arithmetic
unknowns might require different forms of simplification: for example, if z were 2 Ã— 3, then
the correct simplification of 1 Ã— (0 + (2 Ã— 3)) would be 6 and not 2 Ã— 3.
To recap, the basic EBL process works as follows:
1. Given an example, construct a proof that the goal predicate applies to the example using
the available background knowledge.

Section 19.3.

Explanation-Based Learning

783

2. In parallel, construct a generalized proof tree for the variabilized goal using the same
inference steps as in the original proof.
3. Construct a new rule whose left-hand side consists of the leaves of the proof tree and
whose right-hand side is the variabilized goal (after applying the necessary bindings
from the generalized proof).
4. Drop any conditions from the left-hand side that are true regardless of the values of the
variables in the goal.

19.3.2 Improving efficiency
The generalized proof tree in Figure 19.7 actually yields more than one generalized rule. For
example, if we terminate, or prune, the growth of the right-hand branch in the proof tree
when it reaches the Primitive step, we get the rule
Primitive(z) â‡’ Simplify(1 Ã— (0 + z), z) .
This rule is as valid as, but more general than, the rule using ArithmeticUnknown, because
it covers cases where z is a number. We can extract a still more general rule by pruning after
the step Simplify(y + z, w), yielding the rule
Simplify(y + z, w) â‡’ Simplify(1 Ã— (y + z), w) .
In general, a rule can be extracted from any partial subtree of the generalized proof tree. Now
we have a problem: which of these rules do we choose?
The choice of which rule to generate comes down to the question of efficiency. There
are three factors involved in the analysis of efficiency gains from EBL:
1. Adding large numbers of rules can slow down the reasoning process, because the inference mechanism must still check those rules even in cases where they do not yield a
solution. In other words, it increases the branching factor in the search space.
2. To compensate for the slowdown in reasoning, the derived rules must offer significant
increases in speed for the cases that they do cover. These increases come about mainly
because the derived rules avoid dead ends that would otherwise be taken, but also because they shorten the proof itself.
3. Derived rules should be as general as possible, so that they apply to the largest possible
set of cases.
OPERATIONALITY

A common approach to ensuring that derived rules are efficient is to insist on the operationality of each subgoal in the rule. A subgoal is operational if it is â€œeasyâ€ to solve. For example,
the subgoal Primitive(z) is easy to solve, requiring at most two steps, whereas the subgoal
Simplify(y + z, w) could lead to an arbitrary amount of inference, depending on the values
of y and z. If a test for operationality is carried out at each step in the construction of the
generalized proof, then we can prune the rest of a branch as soon as an operational subgoal is
found, keeping just the operational subgoal as a conjunct of the new rule.
Unfortunately, there is usually a tradeoff between operationality and generality. More
specific subgoals are generally easier to solve but cover fewer cases. Also, operationality
is a matter of degree: one or two steps is definitely operational, but what about 10 or 100?

784

Chapter 19.

Knowledge in Learning

Finally, the cost of solving a given subgoal depends on what other rules are available in the
knowledge base. It can go up or down as more rules are added. Thus, EBL systems really
face a very complex optimization problem in trying to maximize the efficiency of a given
initial knowledge base. It is sometimes possible to derive a mathematical model of the effect
on overall efficiency of adding a given rule and to use this model to select the best rule to
add. The analysis can become very complicated, however, especially when recursive rules
are involved. One promising approach is to address the problem of efficiency empirically,
simply by adding several rules and seeing which ones are useful and actually speed things up.
Empirical analysis of efficiency is actually at the heart of EBL. What we have been
calling loosely the â€œefficiency of a given knowledge baseâ€ is actually the average-case complexity on a distribution of problems. By generalizing from past example problems, EBL
makes the knowledge base more efficient for the kind of problems that it is reasonable to
expect. This works as long as the distribution of past examples is roughly the same as for
future examplesâ€”the same assumption used for PAC-learning in Section 18.5. If the EBL
system is carefully engineered, it is possible to obtain significant speedups. For example, a
very large Prolog-based natural language system designed for speech-to-speech translation
between Swedish and English was able to achieve real-time performance only by the application of EBL to the parsing process (Samuelsson and Rayner, 1991).

19.4

L EARNING U SING R ELEVANCE I NFORMATION
Our traveler in Brazil seems to be able to make a confident generalization concerning the language spoken by other Brazilians. The inference is sanctioned by her background knowledge,
namely, that people in a given country (usually) speak the same language. We can express
this in first-order logic as follows:2
Nationality (x, n) âˆ§ Nationality(y, n) âˆ§ Language(x, l) â‡’ Language(y, l) . (19.6)
(Literal translation: â€œIf x and y have the same nationality n and x speaks language l, then y
also speaks it.â€) It is not difficult to show that, from this sentence and the observation that
Nationality (Fernando, Brazil ) âˆ§ Language(Fernando, Portuguese) ,
the following conclusion is entailed (see Exercise 19.1):
Nationality (x, Brazil ) â‡’ Language(x, Portuguese) .

FUNCTIONAL
DEPENDENCY
DETERMINATION

Sentences such as (19.6) express a strict form of relevance: given nationality, language
is fully determined. (Put another way: language is a function of nationality.) These sentences
are called functional dependencies or determinations. They occur so commonly in certain
kinds of applications (e.g., defining database designs) that a special syntax is used to write
them. We adopt the notation of Davies (1985):
Nationality (x, n) ' Language(x, l) .
2

We assume for the sake of simplicity that a person speaks only one language. Clearly, the rule would have to
be amended for countries such as Switzerland and India.

Section 19.4.

Learning Using Relevance Information

785

As usual, this is simply a syntactic sugaring, but it makes it clear that the determination is
really a relationship between the predicates: nationality determines language. The relevant
properties determining conductance and density can be expressed similarly:
Material (x, m) âˆ§ Temperature (x, t) ' Conductance(x, Ï) ;
Material (x, m) âˆ§ Temperature (x, t) ' Density(x, d) .
The corresponding generalizations follow logically from the determinations and observations.

19.4.1 Determining the hypothesis space
Although the determinations sanction general conclusions concerning all Brazilians, or all
pieces of copper at a given temperature, they cannot, of course, yield a general predictive
theory for all nationalities, or for all temperatures and materials, from a single example.
Their main effect can be seen as limiting the space of hypotheses that the learning agent need
consider. In predicting conductance, for example, one need consider only material and temperature and can ignore mass, ownership, day of the week, the current president, and so on.
Hypotheses can certainly include terms that are in turn determined by material and temperature, such as molecular structure, thermal energy, or free-electron density. Determinations
specify a sufficient basis vocabulary from which to construct hypotheses concerning the target
predicate. This statement can be proven by showing that a given determination is logically
equivalent to a statement that the correct definition of the target predicate is one of the set of
all definitions expressible using the predicates on the left-hand side of the determination.
Intuitively, it is clear that a reduction in the hypothesis space size should make it easier to learn the target predicate. Using the basic results of computational learning theory
(Section 18.5), we can quantify the possible gains. First, recall that for Boolean functions,
log(|H|) examples are required to converge to a reasonable hypothesis, where |H| is the
size of the hypothesis space. If the learner has n Boolean features with which to construct
n
hypotheses, then, in the absence of further restrictions, |H| = O(22 ), so the number of examples is O(2n ). If the determination contains d predicates in the left-hand side, the learner
will require only O(2d ) examples, a reduction of O(2nâˆ’d ).

19.4.2 Learning and using relevance information
As we stated in the introduction to this chapter, prior knowledge is useful in learning; but
it too has to be learned. In order to provide a complete story of relevance-based learning,
we must therefore provide a learning algorithm for determinations. The learning algorithm
we now present is based on a straightforward attempt to find the simplest determination consistent with the observations. A determination P ' Q says that if any examples match on
P , then they must also match on Q. A determination is therefore consistent with a set of
examples if every pair that matches on the predicates on the left-hand side also matches on
the goal predicate. For example, suppose we have the following examples of conductance
measurements on material samples:

786

Chapter 19.

Knowledge in Learning

function M INIMAL -C ONSISTENT-D ET(E , A) returns a set of attributes
inputs: E , a set of examples
A, a set of attributes, of size n
for i = 0 to n do
for each subset Ai of A of size i do
if C ONSISTENT-D ET ?(Ai , E ) then return Ai
function C ONSISTENT-D ET ?(A, E ) returns a truth value
inputs: A, a set of attributes
E , a set of examples
local variables: H , a hash table
for each example e in E do
if some example in H has the same values as e for the attributes A
but a different classification then return false
store the class of e in H , indexed by the values for attributes A of the example e
return true
Figure 19.8

An algorithm for finding a minimal consistent determination.

Sample Mass Temperature Material Size Conductance
S1
S1
S2
S3
S3
S4

12
12
24
12
12
24

26
100
26
26
100
26

Copper
Copper
Copper
Lead
Lead
Lead

3
3
6
2
2
4

0.59
0.57
0.59
0.05
0.04
0.05

The minimal consistent determination is Material âˆ§ Temperature ' Conductance. There
is a nonminimal but consistent determination, namely, Mass âˆ§ Size âˆ§ Temperature '
Conductance. This is consistent with the examples because mass and size determine density
and, in our data set, we do not have two different materials with the same density. As usual,
we would need a larger sample set in order to eliminate a nearly correct hypothesis.
There are several possible algorithms for finding minimal consistent determinations.
The most obvious approach is to conduct a search through the space of determinations, checking all determinations with one predicate, two predicates, and so on, until a consistent determination is found. We will assume a simple attribute-based representation, like that used for
decision tree learning in Chapter 18. A determination d will be represented by the set of
attributes on the left-hand side, because the target predicate is assumed to be fixed. The basic
algorithm is outlined in Figure 19.8.
The time complexity of this algorithm depends on the size of the smallest consistent
determination. Suppose this determination has p attributes out of the n total attributes.
Then
 
the algorithm will not find it until searching the subsets of A of size p. There are np = O(np )

Section 19.4.

Learning Using Relevance Information

787

Proportion correct on test set

1
0.9

RBDTL
DTL

0.8
0.7
0.6
0.5
0.4
0

20

40

60
80 100
Training set size

120

140

Figure 19.9 A performance comparison between D ECISION -T REE -L EARNING and
RBDTL on randomly generated data for a target function that depends on only 5 of 16
attributes.

DECLARATIVE BIAS

such subsets; hence the algorithm is exponential in the size of the minimal determination. It
turns out that the problem is NP-complete, so we cannot expect to do better in the general
case. In most domains, however, there will be sufficient local structure (see Chapter 14 for a
definition of locally structured domains) that p will be small.
Given an algorithm for learning determinations, a learning agent has a way to construct
a minimal hypothesis within which to learn the target predicate. For example, we can combine
M INIMAL-C ONSISTENT-D ET with the D ECISION -T REE -L EARNING algorithm. This yields
a relevance-based decision-tree learning algorithm RBDTL that first identifies a minimal
set of relevant attributes and then passes this set to the decision tree algorithm for learning.
Unlike D ECISION -T REE -L EARNING , RBDTL simultaneously learns and uses relevance information in order to minimize its hypothesis space. We expect that RBDTL will learn faster
than D ECISION -T REE -L EARNING , and this is in fact the case. Figure 19.9 shows the learning
performance for the two algorithms on randomly generated data for a function that depends
on only 5 of 16 attributes. Obviously, in cases where all the available attributes are relevant,
RBDTL will show no advantage.
This section has only scratched the surface of the field of declarative bias, which aims
to understand how prior knowledge can be used to identify the appropriate hypothesis space
within which to search for the correct target definition. There are many unanswered questions:
â€¢ How can the algorithms be extended to handle noise?
â€¢ Can we handle continuous-valued variables?
â€¢ How can other kinds of prior knowledge be used, besides determinations?
â€¢ How can the algorithms be generalized to cover any first-order theory, rather than just
an attribute-based representation?
Some of these questions are addressed in the next section.

788

19.5

Chapter 19.

Knowledge in Learning

I NDUCTIVE L OGIC P ROGRAMMING
Inductive logic programming (ILP) combines inductive methods with the power of first-order
representations, concentrating in particular on the representation of hypotheses as logic programs.3 It has gained popularity for three reasons. First, ILP offers a rigorous approach to
the general knowledge-based inductive learning problem. Second, it offers complete algorithms for inducing general, first-order theories from examples, which can therefore learn
successfully in domains where attribute-based algorithms are hard to apply. An example is
in learning how protein structures fold (Figure 19.10). The three-dimensional configuration
of a protein molecule cannot be represented reasonably by a set of attributes, because the
configuration inherently refers to relationships between objects, not to attributes of a single
object. First-order logic is an appropriate language for describing the relationships. Third,
inductive logic programming produces hypotheses that are (relatively) easy for humans to
read. For example, the English translation in Figure 19.10 can be scrutinized and criticized
by working biologists. This means that inductive logic programming systems can participate
in the scientific cycle of experimentation, hypothesis generation, debate, and refutation. Such
participation would not be possible for systems that generate â€œblack-boxâ€ classifiers, such as
neural networks.

19.5.1 An example
Recall from Equation (19.5) that the general knowledge-based induction problem is to â€œsolveâ€
the entailment constraint
Background âˆ§ Hypothesis âˆ§ Descriptions |= Classiï¬cations
for the unknown Hypothesis, given the Background knowledge and examples described by
Descriptions and Classiï¬cations . To illustrate this, we will use the problem of learning
family relationships from examples. The descriptions will consist of an extended family
tree, described in terms of Mother , Father , and Married relations and Male and Female
properties. As an example, we will use the family tree from Exercise 8.14, shown here in
Figure 19.11. The corresponding descriptions are as follows:
Father (Philip, Charles )
Mother (Mum, Margaret )
Married (Diana, Charles)
Male(Philip)
Female(Beatrice)

Father (Philip, Anne)
Mother (Mum, Elizabeth)
Married (Elizabeth, Philip)
Male(Charles)
Female(Margaret )

...
...
...
...
...

The sentences in Classiï¬cations depend on the target concept being learned. We might want
to learn Grandparent , BrotherInLaw , or Ancestor , for example. For Grandparent , the
3

It might be appropriate at this point for the reader to refer to Chapter 7 for some of the underlying concepts,
including Horn clauses, conjunctive normal form, unification, and resolution.

Section 19.5.

Inductive Logic Programming

789

complete set of Classiï¬cations contains 20 Ã— 20 = 400 conjuncts of the form
Grandparent (Mum, Charles) Grandparent (Elizabeth, Beatrice) . . .
Â¬Grandparent (Mum, Harry) Â¬Grandparent (Spencer , Peter )
...
We could of course learn from a subset of this complete set.
The object of an inductive learning program is to come up with a set of sentences for
the Hypothesis such that the entailment constraint is satisfied. Suppose, for the moment, that
the agent has no background knowledge: Background is empty. Then one possible solution

H:5[111-113]

H:1[19-37]

H:6[79-88]

H:3[71-84]

H:4[61-64]

H:5[66-70]

H:2[26-33]

H:1[8-17]

E:2[96-98]
E:1[57-59]
H:4[93-108]
H:2[41-64]

H:7[99-106]

2mhr - Four-helical up-and-down bundle

H:3[40-50]

1omd - EF-Hand

(a)

(b)

Figure 19.10 (a) and (b) show positive and negative examples, respectively, of the
â€œfour-helical up-and-down bundleâ€ concept in the domain of protein folding. Each
example structure is coded into a logical expression of about 100 conjuncts such as
TotalLength(D2mhr , 118)âˆ§NumberHelices(D2mhr , 6)âˆ§. . .. From these descriptions and
from classifications such as Fold (F OUR -H ELICAL -U P -A ND -D OWN -B UNDLE, D2mhr ),
the ILP system P ROGOL (Muggleton, 1995) learned the following rule:
Fold (F OUR -H ELICAL -U P -A ND -D OWN -B UNDLE , p) â‡
Helix (p, h1 ) âˆ§ Length(h1 , H IGH ) âˆ§ Position (p, h1 , n)
âˆ§ (1 â‰¤ n â‰¤ 3) âˆ§ Adjacent (p, h1 , h2 ) âˆ§ Helix (p, h2 ) .
This kind of rule could not be learned, or even represented, by an attribute-based mechanism
such as we saw in previous chapters. The rule can be translated into English as â€œ Protein p
has fold class â€œFour-helical up-and-down-bundleâ€ if it contains a long helix h1 at a secondary
structure position between 1 and 3 and h1 is next to a second helix.â€

790

Chapter 19.

Knowledge in Learning

for Hypothesis is the following:
Grandparent (x, y)

â‡”
âˆ¨
âˆ¨
âˆ¨

[âˆƒ z
[âˆƒ z
[âˆƒ z
[âˆƒ z

Mother (x, z) âˆ§ Mother (z, y)]
Mother (x, z) âˆ§ Father (z, y)]
Father (x, z) âˆ§ Mother (z, y)]
Father (x, z) âˆ§ Father (z, y)] .

Notice that an attribute-based learning algorithm, such as D ECISION -T REE -L EARNING , will
get nowhere in solving this problem. In order to express Grandparent as an attribute (i.e., a
unary predicate), we would need to make pairs of people into objects:
Grandparent (Mum, Charles ) . . .
Then we get stuck in trying to represent the example descriptions. The only possible attributes
are horrible things such as
FirstElementIsMotherOfElizabeth (Mum, Charles ) .
The definition of Grandparent in terms of these attributes simply becomes a large disjunction of specific cases that does not generalize to new examples at all. Attribute-based learning
algorithms are incapable of learning relational predicates. Thus, one of the principal advantages of ILP algorithms is their applicability to a much wider range of problems, including
relational problems.
The reader will certainly have noticed that a little bit of background knowledge would
help in the representation of the Grandparent definition. For example, if Background included the sentence
Parent (x, y) â‡” [Mother (x, y) âˆ¨ Father (x, y)] ,
then the definition of Grandparent would be reduced to
Grandparent (x, y) â‡” [âˆƒ z Parent(x, z) âˆ§ Parent (z, y)] .
This shows how background knowledge can dramatically reduce the size of hypotheses required to explain the observations.
It is also possible for ILP algorithms to create new predicates in order to facilitate the
expression of explanatory hypotheses. Given the example data shown earlier, it is entirely
reasonable for the ILP program to propose an additional predicate, which we would call
George

Spencer

Kydd

Diana

Charles

William Harry

Figure 19.11

Elizabeth

Anne

Peter

A typical family tree.

Mark

Mum

Philip

Andrew

Margaret

Sarah

Zara Beatrice Eugenie

Edward

Louise

Sophie

James

Section 19.5.

CONSTRUCTIVE
INDUCTION

Inductive Logic Programming

791

â€œParent ,â€ in order to simplify the definitions of the target predicates. Algorithms that can
generate new predicates are called constructive induction algorithms. Clearly, constructive
induction is a necessary part of the picture of cumulative learning. It has been one of the
hardest problems in machine learning, but some ILP techniques provide effective mechanisms
for achieving it.
In the rest of this chapter, we will study the two principal approaches to ILP. The first
uses a generalization of decision tree methods, and the second uses techniques based on
inverting a resolution proof.

19.5.2 Top-down inductive learning methods
The first approach to ILP works by starting with a very general rule and gradually specializing
it so that it fits the data. This is essentially what happens in decision-tree learning, where a
decision tree is gradually grown until it is consistent with the observations. To do ILP we
use first-order literals instead of attributes, and the hypothesis is a set of clauses instead of a
decision tree. This section describes F OIL (Quinlan, 1990), one of the first ILP programs.
Suppose we are trying to learn a definition of the Grandfather (x, y) predicate, using
the same family data as before. As with decision-tree learning, we can divide the examples
into positive and negative examples. Positive examples are
George, Anne, Philip, Peter , Spencer , Harry , . . .
and negative examples are
George, Elizabeth, Harry, Zara, Charles, Philip, . . .
Notice that each example is a pair of objects, because Grandfather is a binary predicate. In
all, there are 12 positive examples in the family tree and 388 negative examples (all the other
pairs of people).
F OIL constructs a set of clauses, each with Grandfather (x, y) as the head. The clauses
must classify the 12 positive examples as instances of the Grandfather (x, y) relationship,
while ruling out the 388 negative examples. The clauses are Horn clauses, with the extension
that negated literals are allowed in the body of a clause and are interpreted using negation as
failure, as in Prolog. The initial clause has an empty body:
â‡’ Grandfather (x, y) .
This clause classifies every example as positive, so it needs to be specialized. We do this by
adding literals one at a time to the left-hand side. Here are three potential additions:
Father (x, y) â‡’ Grandfather (x, y) .
Parent (x, z) â‡’ Grandfather (x, y) .
Father (x, z) â‡’ Grandfather (x, y) .
(Notice that we are assuming that a clause defining Parent is already part of the background
knowledge.) The first of these three clauses incorrectly classifies all of the 12 positive examples as negative and can thus be ignored. The second and third agree with all of the positive
examples, but the second is incorrect on a larger fraction of the negative examplesâ€”twice as
many, because it allows mothers as well as fathers. Hence, we prefer the third clause.

792

Chapter 19.

Knowledge in Learning

Now we need to specialize this clause further, to rule out the cases in which x is the
father of some z, but z is not a parent of y. Adding the single literal Parent (z, y) gives
Father (x, z) âˆ§ Parent (z, y) â‡’ Grandfather (x, y) ,
which correctly classifies all the examples. F OIL will find and choose this literal, thereby
solving the learning task. In general, the solution is a set of Horn clauses, each of which
implies the target predicate. For example, if we didnâ€™t have the Parent predicate in our
vocabulary, then the solution might be
Father (x, z) âˆ§ Father (z, y) â‡’ Grandfather (x, y)
Father (x, z) âˆ§ Mother (z, y) â‡’ Grandfather (x, y) .
Note that each of these clauses covers some of the positive examples, that together they cover
all the positive examples, and that N EW-C LAUSE is designed in such a way that no clause
will incorrectly cover a negative example. In general F OIL will have to search through many
unsuccessful clauses before finding a correct solution.
This example is a very simple illustration of how F OIL operates. A sketch of the complete algorithm is shown in Figure 19.12. Essentially, the algorithm repeatedly constructs a
clause, literal by literal, until it agrees with some subset of the positive examples and none of
the negative examples. Then the positive examples covered by the clause are removed from
the training set, and the process continues until no positive examples remain. The two main
subroutines to be explained are N EW-L ITERALS , which constructs all possible new literals to
add to the clause, and C HOOSE -L ITERAL , which selects a literal to add.
N EW-L ITERALS takes a clause and constructs all possible â€œusefulâ€ literals that could
be added to the clause. Let us use as an example the clause
Father (x, z) â‡’ Grandfather (x, y) .
There are three kinds of literals that can be added:
1. Literals using predicates: the literal can be negated or unnegated, any existing predicate
(including the goal predicate) can be used, and the arguments must all be variables. Any
variable can be used for any argument of the predicate, with one restriction: each literal
must include at least one variable from an earlier literal or from the head of the clause.
Literals such as Mother (z, u), Married (z, z), Â¬Male(y), and Grandfather (v, x) are
allowed, whereas Married (u, v) is not. Notice that the use of the predicate from the
head of the clause allows F OIL to learn recursive definitions.
2. Equality and inequality literals: these relate variables already appearing in the clause.
For example, we might add z = x. These literals can also include user-specified constants. For learning arithmetic we might use 0 and 1, and for learning list functions we
might use the empty list [ ].
3. Arithmetic comparisons: when dealing with functions of continuous variables, literals
such as x > y and y â‰¤ z can be added. As in decision-tree learning, a constant
threshold value can be chosen to maximize the discriminatory power of the test.
The resulting branching factor in this search space is very large (see Exercise 19.6), but F OIL
can also use type information to reduce it. For example, if the domain included numbers as

Section 19.5.

Inductive Logic Programming

793

function F OIL(examples, target) returns a set of Horn clauses
inputs: examples, set of examples
target , a literal for the goal predicate
local variables: clauses, set of clauses, initially empty
while examples contains positive examples do
clause â† N EW-C LAUSE(examples, target )
remove positive examples covered by clause from examples
add clause to clauses
return clauses
function N EW-C LAUSE(examples, target ) returns a Horn clause
local variables: clause, a clause with target as head and an empty body
l , a literal to be added to the clause
extended examples, a set of examples with values for new variables
extended examples â† examples
while extended examples contains negative examples do
l â† C HOOSE -L ITERAL(N EW-L ITERALS(clause), extended examples)
append l to the body of clause
extended examples â† set of examples created by applying E XTEND -E XAMPLE
to each example in extended examples
return clause
function E XTEND -E XAMPLE(example, literal ) returns a set of examples
if example satisfies literal
then return the set of examples created by extending example with
each possible constant value for each new variable in literal
else return the empty set
Figure 19.12 Sketch of the F OIL algorithm for learning sets of first-order Horn clauses
from examples. N EW-L ITERALS and C HOOSE -L ITERAL are explained in the text.

well as people, type restrictions would prevent N EW-L ITERALS from generating literals such
as Parent (x, n), where x is a person and n is a number.
C HOOSE -L ITERAL uses a heuristic somewhat similar to information gain (see page 704)
to decide which literal to add. The exact details are not important here, and a number of
different variations have been tried. One interesting additional feature of F OIL is the use of
Ockhamâ€™s razor to eliminate some hypotheses. If a clause becomes longer (according to some
metric) than the total length of the positive examples that the clause explains, that clause is
not considered as a potential hypothesis. This technique provides a way to avoid overcomplex
clauses that fit noise in the data.
F OIL and its relatives have been used to learn a wide variety of definitions. One of the
most impressive demonstrations (Quinlan and Cameron-Jones, 1993) involved solving a long
sequence of exercises on list-processing functions from Bratkoâ€™s (1986) Prolog textbook. In

794

Chapter 19.

Knowledge in Learning

each case, the program was able to learn a correct definition of the function from a small set
of examples, using the previously learned functions as background knowledge.

19.5.3 Inductive learning with inverse deduction
INVERSE
RESOLUTION

The second major approach to ILP involves inverting the normal deductive proof process.
Inverse resolution is based on the observation that if the example Classiï¬cations follow
from Background âˆ§ Hypothesis âˆ§ Descriptions, then one must be able to prove this fact by
resolution (because resolution is complete). If we can â€œrun the proof backward,â€ then we can
find a Hypothesis such that the proof goes through. The key, then, is to find a way to invert
the resolution process.
We will show a backward proof process for inverse resolution that consists of individual
backward steps. Recall that an ordinary resolution step takes two clauses C1 and C2 and
resolves them to produce the resolvent C. An inverse resolution step takes a resolvent C
and produces two clauses C1 and C2 , such that C is the result of resolving C1 and C2 .
Alternatively, it may take a resolvent C and clause C1 and produce a clause C2 such that C
is the result of resolving C1 and C2 .
The early steps in an inverse resolution process are shown in Figure 19.13, where we
focus on the positive example Grandparent (George, Anne). The process begins at the end
of the proof (shown at the bottom of the figure). We take the resolvent C to be empty
clause (i.e. a contradiction) and C2 to be Â¬Grandparent (George, Anne), which is the negation of the goal example. The first inverse step takes C and C2 and generates the clause
Grandparent (George, Anne) for C1 . The next step takes this clause as C and the clause
Parent (Elizabeth, Anne) as C2 , and generates the clause
Â¬Parent(Elizabeth, y) âˆ¨ Grandparent (George, y)
as C1 . The final step treats this clause as the resolvent. With Parent(George, Elizabeth) as
C2 , one possible clause C1 is the hypothesis
Parent (x, z) âˆ§ Parent (z, y) â‡’ Grandparent (x, y) .
Now we have a resolution proof that the hypothesis, descriptions, and background knowledge
entail the classification Grandparent (George, Anne).
Clearly, inverse resolution involves a search. Each inverse resolution step is nondeterministic, because for any C, there can be many or even an infinite number of clauses
C1 and C2 that resolve to C. For example, instead of choosing Â¬Parent(Elizabeth, y) âˆ¨
Grandparent (George, y) for C1 in the last step of Figure 19.13, the inverse resolution step
might have chosen any of the following sentences:
Â¬Parent(Elizabeth, Anne) âˆ¨ Grandparent (George, Anne) .
Â¬Parent(z, Anne) âˆ¨ Grandparent (George, Anne) .
Â¬Parent(z, y) âˆ¨ Grandparent (George, y) .
..
.
(See Exercises 19.4 and 19.5.) Furthermore, the clauses that participate in each step can be
chosen from the Background knowledge, from the example Descriptions, from the negated

Section 19.5.

Inductive Logic Programming

795

Classiï¬cations , or from hypothesized clauses that have already been generated in the inverse
resolution tree. The large number of possibilities means a large branching factor (and therefore an inefficient search) without additional controls. A number of approaches to taming the
search have been tried in implemented ILP systems:

INVERSE
ENTAILMENT

1. Redundant choices can be eliminatedâ€”for example, by generating only the most specific hypotheses possible and by requiring that all the hypothesized clauses be consistent
with each other, and with the observations. This last criterion would rule out the clause
Â¬Parent(z, y) âˆ¨ Grandparent (George, y), listed before.
2. The proof strategy can be restricted. For example, we saw in Chapter 9 that linear
resolution is a complete, restricted strategy. Linear resolution produces proof trees that
have a linear branching structureâ€”the whole tree follows one line, with only single
clauses branching off that line (as in Figure 19.13).
3. The representation language can be restricted, for example by eliminating function symbols or by allowing only Horn clauses. For instance, P ROGOL operates with Horn
clauses using inverse entailment. The idea is to change the entailment constraint
Background âˆ§ Hypothesis âˆ§ Descriptions |= Classiï¬cations
to the logically equivalent form
Background âˆ§ Descriptions âˆ§ Â¬Classiï¬cations |= Â¬Hypothesis.
From this, one can use a process similar to the normal Prolog Horn-clause deduction,
with negation-as-failure to derive Hypothesis. Because it is restricted to Horn clauses,
this is an incomplete method, but it can be more efficient than full resolution. It is also
possible to apply complete inference with inverse entailment (Inoue, 2001).
4. Inference can be done with model checking rather than theorem proving. The P ROGOL
system (Muggleton, 1995) uses a form of model checking to limit the search. That

Â¬ Parent(z,y)

>

>

Â¬ Parent(x,z)

Grandparent(x,y)

Parent(George,Elizabeth)

{x/George, z/Elizabeth}
>

Â¬ Parent(Elizabeth,y)

Grandparent(George,y)

Parent(Elizabeth,Anne)

{y/Anne}
Grandparent(George,Anne)

Â¬ Grandparent(George,Anne)

Figure 19.13 Early steps in an inverse resolution process. The shaded clauses are
generated by inverse resolution steps from the clause to the right and the clause below.
The unshaded clauses are from the Descriptions and Classiï¬cations (including negated
Classiï¬cations).

796

Chapter 19.

Knowledge in Learning

is, like answer set programming, it generates possible values for logical variables, and
checks for consistency.
5. Inference can be done with ground propositional clauses rather than in first-order logic.
The L INUS system (Lavrauc and Duzeroski, 1994) works by translating first-order theories into propositional logic, solving them with a propositional learning system, and
then translating back. Working with propositional formulas can be more efficient on
some problems, as we saw with SATP LAN in Chapter 10.

19.5.4 Making discoveries with inductive logic programming
An inverse resolution procedure that inverts a complete resolution strategy is, in principle, a
complete algorithm for learning first-order theories. That is, if some unknown Hypothesis
generates a set of examples, then an inverse resolution procedure can generate Hypothesis
from the examples. This observation suggests an interesting possibility: Suppose that the
available examples include a variety of trajectories of falling bodies. Would an inverse resolution program be theoretically capable of inferring the law of gravity? The answer is clearly
yes, because the law of gravity allows one to explain the examples, given suitable background
mathematics. Similarly, one can imagine that electromagnetism, quantum mechanics, and the
theory of relativity are also within the scope of ILP programs. Of course, they are also within
the scope of a monkey with a typewriter; we still need better heuristics and new ways to
structure the search space.
One thing that inverse resolution systems will do for you is invent new predicates. This
ability is often seen as somewhat magical, because computers are often thought of as â€œmerely
working with what they are given.â€ In fact, new predicates fall directly out of the inverse
resolution step. The simplest case arises in hypothesizing two new clauses C1 and C2 , given
a clause C. The resolution of C1 and C2 eliminates a literal that the two clauses share; hence,
it is quite possible that the eliminated literal contained a predicate that does not appear in C.
Thus, when working backward, one possibility is to generate a new predicate from which to
reconstruct the missing literal.
Figure 19.14 shows an example in which the new predicate P is generated in the process
of learning a definition for Ancestor . Once generated, P can be used in later inverse resolution steps. For example, a later step might hypothesize that Mother (x, y) â‡’ P (x, y). Thus,
the new predicate P has its meaning constrained by the generation of hypotheses that involve
it. Another example might lead to the constraint Father (x, y) â‡’ P (x, y). In other words,
the predicate P is what we usually think of as the Parent relationship. As we mentioned
earlier, the invention of new predicates can significantly reduce the size of the definition of
the goal predicate. Hence, by including the ability to invent new predicates, inverse resolution
systems can often solve learning problems that are infeasible with other techniques.
Some of the deepest revolutions in science come from the invention of new predicates
and functionsâ€”for example, Galileoâ€™s invention of acceleration or Jouleâ€™s invention of thermal energy. Once these terms are available, the discovery of new laws becomes (relatively)
easy. The difficult part lies in realizing that some new entity, with a specific relationship
to existing entities, will allow an entire body of observations to be explained with a much

Section 19.6.

Summary

797

Â¬ P(George,y)

P(x,y)

>

>

Â¬ Father(x,y)

Ancestor(George,y)

{x/George}

Figure 19.14

>

Father(George,y)

Ancestor(George,y)

An inverse resolution step that generates a new predicate P .

simpler and more elegant theory than previously existed.
As yet, ILP systems have not made discoveries on the level of Galileo or Joule, but their
discoveries have been deemed publishable in the scientific literature. For example, in the
Journal of Molecular Biology, Turcotte et al. (2001) describe the automated discovery of rules
for protein folding by the ILP program P ROGOL. Many of the rules discovered by P ROGOL
could have been derived from known principles, but most had not been previously published
as part of a standard biological database. (See Figure 19.10 for an example.). In related
work, Srinivasan et al. (1994) dealt with the problem of discovering molecular-structurebased rules for the mutagenicity of nitroaromatic compounds. These compounds are found in
automobile exhaust fumes. For 80% of the compounds in a standard database, it is possible to
identify four important features, and linear regression on these features outperforms ILP. For
the remaining 20%, the features alone are not predictive, and ILP identifies relationships that
allow it to outperform linear regression, neural nets, and decision trees. Most impressively,
King et al. (2009) endowed a robot with the ability to perform molecular biology experiments
and extended ILP techniques to include experiment design, thereby creating an autonomous
scientist that actually discovered new knowledge about the functional genomics of yeast. For
all these examples it appears that the ability both to represent relations and to use background
knowledge contribute to ILPâ€™s high performance. The fact that the rules found by ILP can be
interpreted by humans contributes to the acceptance of these techniques in biology journals
rather than just computer science journals.
ILP has made contributions to other sciences besides biology. One of the most important is natural language processing, where ILP has been used to extract complex relational
information from text. These results are summarized in Chapter 23.

19.6

S UMMARY
This chapter has investigated various ways in which prior knowledge can help an agent to
learn from new experiences. Because much prior knowledge is expressed in terms of relational models rather than attribute-based models, we have also covered systems that allow
learning of relational models. The important points are:
â€¢ The use of prior knowledge in learning leads to a picture of cumulative learning, in
which learning agents improve their learning ability as they acquire more knowledge.
â€¢ Prior knowledge helps learning by eliminating otherwise consistent hypotheses and by

798

Chapter 19.

â€¢
â€¢

â€¢

â€¢
â€¢

â€¢
â€¢

Knowledge in Learning

â€œfilling inâ€ the explanation of examples, thereby allowing for shorter hypotheses. These
contributions often result in faster learning from fewer examples.
Understanding the different logical roles played by prior knowledge, as expressed by
entailment constraints, helps to define a variety of learning techniques.
Explanation-based learning (EBL) extracts general rules from single examples by explaining the examples and generalizing the explanation. It provides a deductive method
for turning first-principles knowledge into useful, efficient, special-purpose expertise.
Relevance-based learning (RBL) uses prior knowledge in the form of determinations
to identify the relevant attributes, thereby generating a reduced hypothesis space and
speeding up learning. RBL also allows deductive generalizations from single examples.
Knowledge-based inductive learning (KBIL) finds inductive hypotheses that explain
sets of observations with the help of background knowledge.
Inductive logic programming (ILP) techniques perform KBIL on knowledge that is
expressed in first-order logic. ILP methods can learn relational knowledge that is not
expressible in attribute-based systems.
ILP can be done with a top-down approach of refining a very general rule or through a
bottom-up approach of inverting the deductive process.
ILP methods naturally generate new predicates with which concise new theories can be
expressed and show promise as general-purpose scientific theory formation systems.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Although the use of prior knowledge in learning would seem to be a natural topic for philosophers of science, little formal work was done until quite recently. Fact, Fiction, and Forecast,
by the philosopher Nelson Goodman (1954), refuted the earlier supposition that induction
was simply a matter of seeing enough examples of some universally quantified proposition
and then adopting it as a hypothesis. Consider, for example, the hypothesis â€œAll emeralds are
grue,â€ where grue means â€œgreen if observed before time t, but blue if observed thereafter.â€
At any time up to t, we might have observed millions of instances confirming the rule that
emeralds are grue, and no disconfirming instances, and yet we are unwilling to adopt the rule.
This can be explained only by appeal to the role of relevant prior knowledge in the induction
process. Goodman proposes a variety of different kinds of prior knowledge that might be useful, including a version of determinations called overhypotheses. Unfortunately, Goodmanâ€™s
ideas were never pursued in machine learning.
The current-best-hypothesis approach is an old idea in philosophy (Mill, 1843). Early
work in cognitive psychology also suggested that it is a natural form of concept learning in
humans (Bruner et al., 1957). In AI, the approach is most closely associated with the work
of Patrick Winston, whose Ph.D. thesis (Winston, 1970) addressed the problem of learning
descriptions of complex objects. The version space method (Mitchell, 1977, 1982) takes
a different approach, maintaining the set of all consistent hypotheses and eliminating those
found to be inconsistent with new examples. The approach was used in the Meta-D ENDRAL

Bibliographical and Historical Notes

ANALOGICAL
REASONING

799

expert system for chemistry (Buchanan and Mitchell, 1978), and later in Mitchellâ€™s (1983)
L EX system, which learns to solve calculus problems. A third influential thread was formed
by the work of Michalski and colleagues on the AQ series of algorithms, which learned sets
of logical rules (Michalski, 1969; Michalski et al., 1986).
EBL had its roots in the techniques used by the S TRIPS planner (Fikes et al., 1972).
When a plan was constructed, a generalized version of it was saved in a plan library and
used in later planning as a macro-operator. Similar ideas appeared in Andersonâ€™s ACT*
architecture, under the heading of knowledge compilation (Anderson, 1983), and in the
S OAR architecture, as chunking (Laird et al., 1986). Schema acquisition (DeJong, 1981),
analytical generalization (Mitchell, 1982), and constraint-based generalization (Minton,
1984) were immediate precursors of the rapid growth of interest in EBL stimulated by the
papers of Mitchell et al. (1986) and DeJong and Mooney (1986). Hirsh (1987) introduced
the EBL algorithm described in the text, showing how it could be incorporated directly into a
logic programming system. Van Harmelen and Bundy (1988) explain EBL as a variant of the
partial evaluation method used in program analysis systems (Jones et al., 1993).
Initial enthusiasm for EBL was tempered by Mintonâ€™s finding (1988) that, without extensive extra work, EBL could easily slow down a program significantly. Formal probabilistic
analysis of the expected payoff of EBL can be found in Greiner (1989) and Subramanian and
Feldman (1990). An excellent survey of early work on EBL appears in Dietterich (1990).
Instead of using examples as foci for generalization, one can use them directly to solve
new problems, in a process known as analogical reasoning. This form of reasoning ranges
from a form of plausible reasoning based on degree of similarity (Gentner, 1983), through
a form of deductive inference based on determinations but requiring the participation of the
example (Davies and Russell, 1987), to a form of â€œlazyâ€ EBL that tailors the direction of
generalization of the old example to fit the needs of the new problem. This latter form of
analogical reasoning is found most commonly in case-based reasoning (Kolodner, 1993)
and derivational analogy (Veloso and Carbonell, 1993).
Relevance information in the form of functional dependencies was first developed in
the database community, where it is used to structure large sets of attributes into manageable subsets. Functional dependencies were used for analogical reasoning by Carbonell
and Collins (1973) and rediscovered and given a full logical analysis by Davies and Russell (Davies, 1985; Davies and Russell, 1987). Their role as prior knowledge in inductive
learning was explored by Russell and Grosof (1987). The equivalence of determinations to
a restricted-vocabulary hypothesis space was proved in Russell (1988). Learning algorithms
for determinations and the improved performance obtained by RBDTL were first shown in
the F OCUS algorithm, due to Almuallim and Dietterich (1991). Tadepalli (1993) describes a
very ingenious algorithm for learning with determinations that shows large improvements in
learning speed.
The idea that inductive learning can be performed by inverse deduction can be traced
to W. S. Jevons (1874), who wrote, â€œThe study both of Formal Logic and of the Theory of
Probabilities has led me to adopt the opinion that there is no such thing as a distinct method
of induction as contrasted with deduction, but that induction is simply an inverse employment of deduction.â€ Computational investigations began with the remarkable Ph.D. thesis by

800

DISCOVERY SYSTEM

Chapter 19.

Knowledge in Learning

Gordon Plotkin (1971) at Edinburgh. Although Plotkin developed many of the theorems and
methods that are in current use in ILP, he was discouraged by some undecidability results for
certain subproblems in induction. MIS (Shapiro, 1981) reintroduced the problem of learning
logic programs, but was seen mainly as a contribution to the theory of automated debugging. Work on rule induction, such as the ID3 (Quinlan, 1986) and CN2 (Clark and Niblett,
1989) systems, led to F OIL (Quinlan, 1990), which for the first time allowed practical induction of relational rules. The field of relational learning was reinvigorated by Muggleton and
Buntine (1988), whose C IGOL program incorporated a slightly incomplete version of inverse
resolution and was capable of generating new predicates. The inverse resolution method also
appears in (Russell, 1986), with a simple algorithm given in a footnote. The next major system was G OLEM (Muggleton and Feng, 1990), which uses a covering algorithm based on
Plotkinâ€™s concept of relative least general generalization. I TOU (Rouveirol and Puget, 1989)
and C LINT (De Raedt, 1992) were other systems of that era. More recently, P ROGOL (Muggleton, 1995) has taken a hybrid (top-down and bottom-up) approach to inverse entailment
and has been applied to a number of practical problems, particularly in biology and natural
language processing. Muggleton (2000) describes an extension of P ROGOL to handle uncertainty in the form of stochastic logic programs.
A formal analysis of ILP methods appears in Muggleton (1991), a large collection of
papers in Muggleton (1992), and a collection of techniques and applications in the book
by Lavrauc and Duzeroski (1994). Page and Srinivasan (2002) give a more recent overview of
the fieldâ€™s history and challenges for the future. Early complexity results by Haussler (1989)
suggested that learning first-order sentences was intractible. However, with better understanding of the importance of syntactic restrictions on clauses, positive results have been obtained
even for clauses with recursion (Duzeroski et al., 1992). Learnability results for ILP are
surveyed by Kietz and Duzeroski (1994) and Cohen and Page (1995).
Although ILP now seems to be the dominant approach to constructive induction, it has
not been the only approach taken. So-called discovery systems aim to model the process
of scientific discovery of new concepts, usually by a direct search in the space of concept
definitions. Doug Lenatâ€™s Automated Mathematician, or AM (Davis and Lenat, 1982), used
discovery heuristics expressed as expert system rules to guide its search for concepts and
conjectures in elementary number theory. Unlike most systems designed for mathematical
reasoning, AM lacked a concept of proof and could only make conjectures. It rediscovered
Goldbachâ€™s conjecture and the Unique Prime Factorization theorem. AMâ€™s architecture was
generalized in the E URISKO system (Lenat, 1983) by adding a mechanism capable of rewriting the systemâ€™s own discovery heuristics. E URISKO was applied in a number of areas other
than mathematical discovery, although with less success than AM. The methodology of AM
and E URISKO has been controversial (Ritchie and Hanna, 1984; Lenat and Brown, 1984).
Another class of discovery systems aims to operate with real scientific data to find new
laws. The systems DALTON , G LAUBER , and S TAHL (Langley et al., 1987) are rule-based
systems that look for quantitative relationships in experimental data from physical systems;
in each case, the system has been able to recapitulate a well-known discovery from the history of science. Discovery systems based on probabilistic techniquesâ€”especially clustering
algorithms that discover new categoriesâ€”are discussed in Chapter 20.

Exercises

801

E XERCISES
19.1 Show, by translating into conjunctive normal form and applying resolution, that the
conclusion drawn on page 784 concerning Brazilians is sound.
19.2 For each of the following determinations, write down the logical representation and
explain why the determination is true (if it is):
a.
b.
c.
d.
19.3

Design and denomination determine the mass of a coin.
For a given program, input determines output.
Climate, food intake, exercise, and metabolism determine weight gain and loss.
Baldness is determined by the baldness (or lack thereof) of oneâ€™s maternal grandfather.
Would a probabilistic version of determinations be useful? Suggest a definition.

19.4 Fill in the missing values for the clauses C1 or C2 (or both) in the following sets of
clauses, given that C is the resolvent of C1 and C2 :
a. C = True â‡’ P (A, B), C1 = P (x, y) â‡’ Q(x, y), C2 =??.
b. C = True â‡’ P (A, B), C1 =??, C2 =??.
c. C = P (x, y) â‡’ P (x, f (y)), C1 =??, C2 =??.
If there is more than one possible solution, provide one example of each different kind.
19.5 Suppose one writes a logic program that carries out a resolution inference step. That
is, let Resolve(c1 , c2 , c) succeed if c is the result of resolving c1 and c2 . Normally, Resolve
would be used as part of a theorem prover by calling it with c1 and c2 instantiated to particular clauses, thereby generating the resolvent c. Now suppose instead that we call it with
c instantiated and c1 and c2 uninstantiated. Will this succeed in generating the appropriate
results of an inverse resolution step? Would you need any special modifications to the logic
programming system for this to work?
19.6 Suppose that F OIL is considering adding a literal to a clause using a binary predicate
P and that previous literals (including the head of the clause) contain five different variables.
a. How many functionally different literals can be generated? Two literals are functionally
identical if they differ only in the names of the new variables that they contain.
b. Can you find a general formula for the number of different literals with a predicate of
arity r when there are n variables previously used?
c. Why does F OIL not allow literals that contain no previously used variables?
19.7 Using the data from the family tree in Figure 19.11, or a subset thereof, apply the F OIL
algorithm to learn a definition for the Ancestor predicate.

20

LEARNING
PROBABILISTIC MODELS

In which we view learning as a form of uncertain reasoning from observations.

Chapter 13 pointed out the prevalence of uncertainty in real environments. Agents can handle
uncertainty by using the methods of probability and decision theory, but first they must learn
their probabilistic theories of the world from experience. This chapter explains how they
can do that, by formulating the learning task itself as a process of probabilistic inference
(Section 20.1). We will see that a Bayesian view of learning is extremely powerful, providing
general solutions to the problems of noise, overfitting, and optimal prediction. It also takes
into account the fact that a less-than-omniscient agent can never be certain about which theory
of the world is correct, yet must still make decisions by using some theory of the world.
We describe methods for learning probability modelsâ€”primarily Bayesian networksâ€”
in Sections 20.2 and 20.3. Some of the material in this chapter is fairly mathematical, although the general lessons can be understood without plunging into the details. It may benefit
the reader to review Chapters 13 and 14 and peek at Appendix A.

20.1

S TATISTICAL L EARNING
The key concepts in this chapter, just as in Chapter 18, are data and hypotheses. Here, the
data are evidenceâ€”that is, instantiations of some or all of the random variables describing the
domain. The hypotheses in this chapter are probabilistic theories of how the domain works,
including logical theories as a special case.
Consider a simple example. Our favorite Surprise candy comes in two flavors: cherry
(yum) and lime (ugh). The manufacturer has a peculiar sense of humor and wraps each piece
of candy in the same opaque wrapper, regardless of flavor. The candy is sold in very large
bags, of which there are known to be five kindsâ€”again, indistinguishable from the outside:
h1 : 100% cherry,
h2 : 75% cherry + 25% lime,
h3 : 50% cherry + 50% lime,
h4 : 25% cherry + 75% lime,
h5 : 100% lime .
802

Section 20.1.

BAYESIAN LEARNING

Statistical Learning

803

Given a new bag of candy, the random variable H (for hypothesis) denotes the type of the
bag, with possible values h1 through h5 . H is not directly observable, of course. As the
pieces of candy are opened and inspected, data are revealedâ€”D1 , D2 , . . ., DN , where each
Di is a random variable with possible values cherry and lime. The basic task faced by the
agent is to predict the flavor of the next piece of candy.1 Despite its apparent triviality, this
scenario serves to introduce many of the major issues. The agent really does need to infer a
theory of its world, albeit a very simple one.
Bayesian learning simply calculates the probability of each hypothesis, given the data,
and makes predictions on that basis. That is, the predictions are made by using all the hypotheses, weighted by their probabilities, rather than by using just a single â€œbestâ€ hypothesis.
In this way, learning is reduced to probabilistic inference. Let D represent all the data, with
observed value d; then the probability of each hypothesis is obtained by Bayesâ€™ rule:
P (hi | d) = Î±P (d | hi )P (hi ) .

(20.1)

Now, suppose we want to make a prediction about an unknown quantity X. Then we have


P(X | d, hi )P(hi | d) =
P(X | hi )P (hi | d) ,
(20.2)
P(X | d) =
i

HYPOTHESIS PRIOR
LIKELIHOOD

i

where we have assumed that each hypothesis determines a probability distribution over X.
This equation shows that predictions are weighted averages over the predictions of the individual hypotheses. The hypotheses themselves are essentially â€œintermediariesâ€ between the
raw data and the predictions. The key quantities in the Bayesian approach are the hypothesis
prior, P (hi ), and the likelihood of the data under each hypothesis, P (d | hi ).
For our candy example, we will assume for the time being that the prior distribution
over h1 , . . . , h5 is given by 0.1, 0.2, 0.4, 0.2, 0.1, as advertised by the manufacturer. The
likelihood of the data is calculated under the assumption that the observations are i.i.d. (see
page 708), so that

P (dj | hi ) .
(20.3)
P (d | hi ) =
j

For example, suppose the bag is really an all-lime bag (h5 ) and the first 10 candies are all
lime; then P (d | h3 ) is 0.510 , because half the candies in an h3 bag are lime.2 Figure 20.1(a)
shows how the posterior probabilities of the five hypotheses change as the sequence of 10
lime candies is observed. Notice that the probabilities start out at their prior values, so h3
is initially the most likely choice and remains so after 1 lime candy is unwrapped. After 2
lime candies are unwrapped, h4 is most likely; after 3 or more, h5 (the dreaded all-lime bag)
is the most likely. After 10 in a row, we are fairly certain of our fate. Figure 20.1(b) shows
the predicted probability that the next candy is lime, based on Equation (20.2). As we would
expect, it increases monotonically toward 1.
1 Statistically sophisticated readers will recognize this scenario as a variant of the urn-and-ball setup. We find
urns and balls less compelling than candy; furthermore, candy lends itself to other tasks, such as deciding whether
to trade the bag with a friendâ€”see Exercise 20.2.
2 We stated earlier that the bags of candy are very large; otherwise, the i.i.d. assumption fails to hold. Technically,
it is more correct (but less hygienic) to rewrap each candy after inspection and return it to the bag.

1

Probability that next candy is lime

Chapter 20.

Posterior probability of hypothesis

804

P(h1 | d)
P(h2 | d)
P(h3 | d)
P(h4 | d)
P(h5 | d)

0.8
0.6
0.4
0.2
0
0

2
4
6
8
Number of observations in d

(a)

10

Learning Probabilistic Models

1
0.9
0.8
0.7
0.6
0.5
0.4
0

2
4
6
8
Number of observations in d

10

(b)

Figure 20.1 (a) Posterior probabilities P (hi | d1 , . . . , dN ) from Equation (20.1). The
number of observations N ranges from 1 to 10, and each observation is of a lime candy.
(b) Bayesian prediction P (dN +1 = lime | d1 , . . . , dN ) from Equation (20.2).

MAXIMUM A
POSTERIORI

The example shows that the Bayesian prediction eventually agrees with the true hypothesis. This is characteristic of Bayesian learning. For any fixed prior that does not rule
out the true hypothesis, the posterior probability of any false hypothesis will, under certain
technical conditions, eventually vanish. This happens simply because the probability of generating â€œuncharacteristicâ€ data indefinitely is vanishingly small. (This point is analogous to
one made in the discussion of PAC learning in Chapter 18.) More important, the Bayesian
prediction is optimal, whether the data set be small or large. Given the hypothesis prior, any
other prediction is expected to be correct less often.
The optimality of Bayesian learning comes at a price, of course. For real learning
problems, the hypothesis space is usually very large or infinite, as we saw in Chapter 18. In
some cases, the summation in Equation (20.2) (or integration, in the continuous case) can be
carried out tractably, but in most cases we must resort to approximate or simplified methods.
A very common approximationâ€”one that is usually adopted in scienceâ€”is to make predictions based on a single most probable hypothesisâ€”that is, an hi that maximizes P (hi | d).
This is often called a maximum a posteriori or MAP (pronounced â€œem-ay-peeâ€) hypothesis.
Predictions made according to an MAP hypothesis hMAP are approximately Bayesian to the
extent that P(X | d) â‰ˆ P(X | hMAP ). In our candy example, hMAP = h5 after three lime candies in a row, so the MAP learner then predicts that the fourth candy is lime with probability
1.0â€”a much more dangerous prediction than the Bayesian prediction of 0.8 shown in Figure 20.1(b). As more data arrive, the MAP and Bayesian predictions become closer, because
the competitors to the MAP hypothesis become less and less probable.
Although our example doesnâ€™t show it, finding MAP hypotheses is often much easier
than Bayesian learning, because it requires solving an optimization problem instead of a large
summation (or integration) problem. We will see examples of this later in the chapter.

Section 20.1.

Statistical Learning

805

In both Bayesian learning and MAP learning, the hypothesis prior P (hi ) plays an important role. We saw in Chapter 18 that overfitting can occur when the hypothesis space
is too expressive, so that it contains many hypotheses that fit the data set well. Rather than
placing an arbitrary limit on the hypotheses to be considered, Bayesian and MAP learning
methods use the prior to penalize complexity. Typically, more complex hypotheses have a
lower prior probabilityâ€”in part because there are usually many more complex hypotheses
than simple hypotheses. On the other hand, more complex hypotheses have a greater capacity to fit the data. (In the extreme case, a lookup table can reproduce the data exactly with
probability 1.) Hence, the hypothesis prior embodies a tradeoff between the complexity of a
hypothesis and its degree of fit to the data.
We can see the effect of this tradeoff most clearly in the logical case, where H contains
only deterministic hypotheses. In that case, P (d | hi ) is 1 if hi is consistent and 0 otherwise.
Looking at Equation (20.1), we see that hMAP will then be the simplest logical theory that
is consistent with the data. Therefore, maximum a posteriori learning provides a natural
embodiment of Ockhamâ€™s razor.
Another insight into the tradeoff between complexity and degree of fit is obtained by
taking the logarithm of Equation (20.1). Choosing hMAP to maximize P (d | hi )P (hi ) is
equivalent to minimizing
âˆ’ log2 P (d | hi ) âˆ’ log2 P (hi ) .

MAXIMUMLIKELIHOOD

Using the connection between information encoding and probability that we introduced in
Chapter 18.3.4, we see that the âˆ’ log2 P (hi ) term equals the number of bits required to specify the hypothesis hi . Furthermore, âˆ’ log2 P (d | hi ) is the additional number of bits required
to specify the data, given the hypothesis. (To see this, consider that no bits are required
if the hypothesis predicts the data exactlyâ€”as with h5 and the string of lime candiesâ€”and
log2 1 = 0.) Hence, MAP learning is choosing the hypothesis that provides maximum compression of the data. The same task is addressed more directly by the minimum description
length, or MDL, learning method. Whereas MAP learning expresses simplicity by assigning
higher probabilities to simpler hypotheses, MDL expresses it directly by counting the bits in
a binary encoding of the hypotheses and data.
A final simplification is provided by assuming a uniform prior over the space of hypotheses. In that case, MAP learning reduces to choosing an hi that maximizes P (d | hi ).
This is called a maximum-likelihood (ML) hypothesis, hML . Maximum-likelihood learning
is very common in statistics, a discipline in which many researchers distrust the subjective
nature of hypothesis priors. It is a reasonable approach when there is no reason to prefer one
hypothesis over another a prioriâ€”for example, when all hypotheses are equally complex. It
provides a good approximation to Bayesian and MAP learning when the data set is large,
because the data swamps the prior distribution over hypotheses, but it has problems (as we
shall see) with small data sets.

806

20.2

Chapter 20.

Learning Probabilistic Models

L EARNING WITH C OMPLETE DATA

DENSITY ESTIMATION

COMPLETE DATA

PARAMETER
LEARNING

The general task of learning a probability model, given data that are assumed to be generated
from that model, is called density estimation. (The term applied originally to probability
density functions for continuous variables, but is used now for discrete distributions too.)
This section covers the simplest case, where we have complete data. Data are complete when each data point contains values for every variable in the probability model being
learned. We focus on parameter learningâ€”finding the numerical parameters for a probability model whose structure is fixed. For example, we might be interested in learning the
conditional probabilities in a Bayesian network with a given structure. We will also look
briefly at the problem of learning structure and at nonparametric density estimation.

20.2.1 Maximum-likelihood parameter learning: Discrete models
Suppose we buy a bag of lime and cherry candy from a new manufacturer whose limeâ€“cherry
proportions are completely unknown; the fraction could be anywhere between 0 and 1. In
that case, we have a continuum of hypotheses. The parameter in this case, which we call
Î¸, is the proportion of cherry candies, and the hypothesis is hÎ¸ . (The proportion of limes is
just 1 âˆ’ Î¸.) If we assume that all proportions are equally likely a priori, then a maximumlikelihood approach is reasonable. If we model the situation with a Bayesian network, we
need just one random variable, Flavor (the flavor of a randomly chosen candy from the bag).
It has values cherry and lime, where the probability of cherry is Î¸ (see Figure 20.2(a)). Now
suppose we unwrap N candies, of which c are cherries and  = N âˆ’ c are limes. According
to Equation (20.3), the likelihood of this particular data set is
P (d | hÎ¸ ) =

N


P (dj | hÎ¸ ) = Î¸ c Â· (1 âˆ’ Î¸) .

j =1

LOG LIKELIHOOD

The maximum-likelihood hypothesis is given by the value of Î¸ that maximizes this expression. The same value is obtained by maximizing the log likelihood,
L(d | hÎ¸ ) = log P (d | hÎ¸ ) =

N


log P (dj | hÎ¸ ) = c log Î¸ +  log(1 âˆ’ Î¸) .

j =1

(By taking logarithms, we reduce the product to a sum over the data, which is usually easier
to maximize.) To find the maximum-likelihood value of Î¸, we differentiate L with respect to
Î¸ and set the resulting expression to zero:
c

c
c
dL(d | hÎ¸ )
= âˆ’
=0
â‡’ Î¸=
=
.
dÎ¸
Î¸ 1âˆ’Î¸
c+
N
In English, then, the maximum-likelihood hypothesis hML asserts that the actual proportion
of cherries in the bag is equal to the observed proportion in the candies unwrapped so far!
It appears that we have done a lot of work to discover the obvious. In fact, though,
we have laid out one standard method for maximum-likelihood parameter learning, a method
with broad applicability:

Section 20.2.

Learning with Complete Data

807
P(F=cherry)

Î¸
P(F=cherry)

Flavor

Î¸

F

Flavor

P(W=red | F)

cherry

Î¸1

lime

Î¸2

Wrapper
(a)

(b)

Figure 20.2 (a) Bayesian network model for the case of candies with an unknown proportion of cherries and limes. (b) Model for the case where the wrapper color depends (probabilistically) on the candy flavor.

1. Write down an expression for the likelihood of the data as a function of the parameter(s).
2. Write down the derivative of the log likelihood with respect to each parameter.
3. Find the parameter values such that the derivatives are zero.
The trickiest step is usually the last. In our example, it was trivial, but we will see that in
many cases we need to resort to iterative solution algorithms or other numerical optimization
techniques, as described in Chapter 4. The example also illustrates a significant problem
with maximum-likelihood learning in general: when the data set is small enough that some
events have not yet been observedâ€”for instance, no cherry candiesâ€”the maximum-likelihood
hypothesis assigns zero probability to those events. Various tricks are used to avoid this
problem, such as initializing the counts for each event to 1 instead of 0.
Let us look at another example. Suppose this new candy manufacturer wants to give a
little hint to the consumer and uses candy wrappers colored red and green. The Wrapper for
each candy is selected probabilistically, according to some unknown conditional distribution,
depending on the flavor. The corresponding probability model is shown in Figure 20.2(b).
Notice that it has three parameters: Î¸, Î¸1 , and Î¸2 . With these parameters, the likelihood of
seeing, say, a cherry candy in a green wrapper can be obtained from the standard semantics
for Bayesian networks (page 513):
P (Flavor = cherry, Wrapper = green | hÎ¸,Î¸1 ,Î¸2 )
= P (Flavor = cherry | hÎ¸,Î¸1 ,Î¸2 )P (Wrapper = green | Flavor = cherry, hÎ¸,Î¸1 ,Î¸2 )
= Î¸ Â· (1 âˆ’ Î¸1 ) .
Now we unwrap N candies, of which c are cherries and  are limes. The wrapper counts are
as follows: rc of the cherries have red wrappers and gc have green, while r of the limes have
red and g have green. The likelihood of the data is given by
P (d | hÎ¸,Î¸1 ,Î¸2 ) = Î¸ c (1 âˆ’ Î¸) Â· Î¸1rc (1 âˆ’ Î¸1 )gc Â· Î¸2r (1 âˆ’ Î¸2 )g .

808

Chapter 20.

Learning Probabilistic Models

This looks pretty horrible, but taking logarithms helps:
L = [c log Î¸ +  log(1 âˆ’ Î¸)] + [rc log Î¸1 + gc log(1 âˆ’ Î¸1 )] + [r log Î¸2 + g log(1 âˆ’ Î¸2 )] .

The benefit of taking logs is clear: the log likelihood is the sum of three terms, each of which
contains a single parameter. When we take derivatives with respect to each parameter and set
them to zero, we get three independent equations, each containing just one parameter:
âˆ‚L
âˆ‚Î¸
âˆ‚L
âˆ‚Î¸1
âˆ‚L
âˆ‚Î¸2

=
=
=

c

Î¸ âˆ’ 1âˆ’Î¸ = 0
gc
rc
Î¸1 âˆ’ 1âˆ’Î¸1 = 0
g
r
Î¸2 âˆ’ 1âˆ’Î¸2 = 0

â‡’
â‡’
â‡’

c
Î¸ = c+
c
Î¸1 = rcr+g
c
r
Î¸2 = r +g .

The solution for Î¸ is the same as before. The solution for Î¸1 , the probability that a cherry
candy has a red wrapper, is the observed fraction of cherry candies with red wrappers, and
similarly for Î¸2 .
These results are very comforting, and it is easy to see that they can be extended to any
Bayesian network whose conditional probabilities are represented as tables. The most important point is that, with complete data, the maximum-likelihood parameter learning problem
for a Bayesian network decomposes into separate learning problems, one for each parameter.
(See Exercise 20.6 for the nontabulated case, where each parameter affects several conditional
probabilities.) The second point is that the parameter values for a variable, given its parents,
are just the observed frequencies of the variable values for each setting of the parent values.
As before, we must be careful to avoid zeroes when the data set is small.

20.2.2 Naive Bayes models
Probably the most common Bayesian network model used in machine learning is the naive
Bayes model first introduced on page 499. In this model, the â€œclassâ€ variable C (which is to
be predicted) is the root and the â€œattributeâ€ variables Xi are the leaves. The model is â€œnaiveâ€
because it assumes that the attributes are conditionally independent of each other, given the
class. (The model in Figure 20.2(b) is a naive Bayes model with class Flavor and just one
attribute, Wrapper.) Assuming Boolean variables, the parameters are
Î¸ = P (C = true), Î¸i1 = P (Xi = true | C = true), Î¸i2 = P (Xi = true | C = false).
The maximum-likelihood parameter values are found in exactly the same way as for Figure 20.2(b). Once the model has been trained in this way, it can be used to classify new examples for which the class variable C is unobserved. With observed attribute values x1 , . . . , xn ,
the probability of each class is given by

P(xi | C) .
P(C | x1 , . . . , xn ) = Î± P(C)
i

A deterministic prediction can be obtained by choosing the most likely class. Figure 20.3
shows the learning curve for this method when it is applied to the restaurant problem from
Chapter 18. The method learns fairly well but not as well as decision-tree learning; this is
presumably because the true hypothesisâ€”which is a decision treeâ€”is not representable exactly using a naive Bayes model. Naive Bayes learning turns out to do surprisingly well in a
wide range of applications; the boosted version (Exercise 20.4) is one of the most effective

Learning with Complete Data

809

1
Proportion correct on test set

Section 20.2.

0.9
0.8
0.7
0.6
Decision tree
Naive Bayes

0.5
0.4
0

20

40
60
Training set size

80

100

Figure 20.3 The learning curve for naive Bayes learning applied to the restaurant problem
from Chapter 18; the learning curve for decision-tree learning is shown for comparison.

general-purpose learning algorithms. Naive Bayes learning scales well to very large problems: with n Boolean attributes, there are just 2n + 1 parameters, and no search is required
to find hML , the maximum-likelihood naive Bayes hypothesis. Finally, naive Bayes learning
systems have no difficulty with noisy or missing data and can give probabilistic predictions
when appropriate.

20.2.3 Maximum-likelihood parameter learning: Continuous models
Continuous probability models such as the linear Gaussian model were introduced in Section 14.3. Because continuous variables are ubiquitous in real-world applications, it is important to know how to learn the parameters of continuous models from data. The principles for
maximum-likelihood learning are identical in the continuous and discrete cases.
Let us begin with a very simple case: learning the parameters of a Gaussian density
function on a single variable. That is, the data are generated as follows:
(xâˆ’Î¼)2
1
P (x) = âˆš
eâˆ’ 2Ïƒ2 .
2Ï€Ïƒ
The parameters of this model are the mean Î¼ and the standard deviation Ïƒ. (Notice that the
normalizing â€œconstantâ€ depends on Ïƒ, so we cannot ignore it.) Let the observed values be
x1 , . . . , xN . Then the log likelihood is
N


N
(x âˆ’Î¼)2

âˆš
(xj âˆ’ Î¼)2
1
âˆ’ j 2
e 2Ïƒ
log âˆš
= N (âˆ’ log 2Ï€ âˆ’ log Ïƒ) âˆ’
.
L=
2Ïƒ 2
2Ï€Ïƒ
j=1
j =1

Setting the derivatives to zero as usual, we obtain
âˆ‚L
1 N
j=1 (xj âˆ’ Î¼) = 0
âˆ‚Î¼ = âˆ’ Ïƒ2
N
1 N
âˆ‚L
2
j=1 (xj âˆ’ Î¼) = 0
âˆ‚Ïƒ = âˆ’ Ïƒ + Ïƒ3

P

â‡’

Î¼=

â‡’

Ïƒ=

j

xj

	NP

2
j (xj âˆ’Î¼)

N

(20.4)
.

That is, the maximum-likelihood value of the mean is the sample average and the maximumlikelihood value of the standard deviation is the square root of the sample variance. Again,
these are comforting results that confirm â€œcommonsenseâ€ practice.

810

Chapter 20.

Learning Probabilistic Models

1
0.8
0.6
y

P(y |x)
4
3.5
3
2.5
2
1.5
1
0.5
0
0 0.2

0.4

0.4 0.6
0.8
x

1

0

1
0.8
0.6
0.4 y
0.2

(a)

0.2
0
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
x

1

(b)

Figure 20.4 (a) A linear Gaussian model described as y = Î¸1 x + Î¸2 plus Gaussian noise
with fixed variance. (b) A set of 50 data points generated from this model.

Now consider a linear Gaussian model with one continuous parent X and a continuous
child Y . As explained on page 520, Y has a Gaussian distribution whose mean depends
linearly on the value of X and whose standard deviation is fixed. To learn the conditional
distribution P (Y | X), we can maximize the conditional likelihood
(yâˆ’(Î¸1 x+Î¸2 ))2
1
2Ïƒ 2
eâˆ’
.
(20.5)
2Ï€Ïƒ
Here, the parameters are Î¸1 , Î¸2 , and Ïƒ. The data are a collection of (xj , yj ) pairs, as illustrated
in Figure 20.4. Using the usual methods (Exercise 20.5), we can find the maximum-likelihood
values of the parameters. The point here is different. If we consider just the parameters Î¸1
and Î¸2 that define the linear relationship between x and y, it becomes clear that maximizing
the log likelihood with respect to these parameters is the same as minimizing the numerator
(y âˆ’ (Î¸1 x + Î¸2 ))2 in the exponent of Equation (20.5). This is the L2 loss, the squared error between the actual value y and the prediction Î¸1 x + Î¸2 . This is the quantity minimized
by the standard linear regression procedure described in Section 18.6. Now we can understand why: minimizing the sum of squared errors gives the maximum-likelihood straight-line
model, provided that the data are generated with Gaussian noise of fixed variance.

P (y | x) = âˆš

20.2.4 Bayesian parameter learning

HYPOTHESIS PRIOR

Maximum-likelihood learning gives rise to some very simple procedures, but it has some
serious deficiencies with small data sets. For example, after seeing one cherry candy, the
maximum-likelihood hypothesis is that the bag is 100% cherry (i.e., Î¸ = 1.0). Unless oneâ€™s
hypothesis prior is that bags must be either all cherry or all lime, this is not a reasonable
conclusion. It is more likely that the bag is a mixture of lime and cherry. The Bayesian
approach to parameter learning starts by defining a prior probability distribution over the
possible hypotheses. We call this the hypothesis prior. Then, as data arrives, the posterior
probability distribution is updated.

Section 20.2.

Learning with Complete Data

2.5

811

6

[5,5]

5
4

[2,2]

1.5

3

[1,1]

1

[30,10]

P(Î˜ = Î¸)

P(Î˜ = Î¸)

2

2
0.5

1

0

0
0

0.2

0.4
0.6
Parameter Î¸

0.8

(a)
Figure 20.5

BETA DISTRIBUTION
HYPERPARAMETER

[3,1]
0

0.2

0.4
0.6
Parameter Î¸

0.8

1

(b)

Examples of the beta[a, b] distribution for different values of [a, b].

The candy example in Figure 20.2(a) has one parameter, Î¸: the probability that a randomly selected piece of candy is cherry-flavored. In the Bayesian view, Î¸ is the (unknown)
value of a random variable Î˜ that defines the hypothesis space; the hypothesis prior is just
the prior distribution P(Î˜). Thus, P (Î˜ = Î¸) is the prior probability that the bag has a fraction
Î¸ of cherry candies.
If the parameter Î¸ can be any value between 0 and 1, then P(Î˜) must be a continuous
distribution that is nonzero only between 0 and 1 and that integrates to 1. The uniform density
P (Î¸) = Uniform[0, 1](Î¸) is one candidate. (See Chapter 13.) It turns out that the uniform
density is a member of the family of beta distributions. Each beta distribution is defined by
two hyperparameters3 a and b such that
beta[a, b](Î¸) = Î± Î¸ aâˆ’1 (1 âˆ’ Î¸)bâˆ’1 ,

CONJUGATE PRIOR

1

[6,2]

(20.6)

for Î¸ in the range [0, 1]. The normalization constant Î±, which makes the distribution integrate
to 1, depends on a and b. (See Exercise 20.7.) Figure 20.5 shows what the distribution looks
like for various values of a and b. The mean value of the distribution is a/(a + b), so larger
values of a suggest a belief that Î˜ is closer to 1 than to 0. Larger values of a + b make the
distribution more peaked, suggesting greater certainty about the value of Î˜. Thus, the beta
family provides a useful range of possibilities for the hypothesis prior.
Besides its flexibility, the beta family has another wonderful property: if Î˜ has a prior
beta[a, b], then, after a data point is observed, the posterior distribution for Î˜ is also a beta
distribution. In other words, beta is closed under update. The beta family is called the
conjugate prior for the family of distributions for a Boolean variable. 4 Letâ€™s see how this
works. Suppose we observe a cherry candy; then we have
3

They are called hyperparameters because they parameterize a distribution over Î¸, which is itself a parameter.
Other conjugate priors include the Dirichlet family for the parameters of a discrete multivalued distribution
and the Normalâ€“Wishart family for the parameters of a Gaussian distribution. See Bernardo and Smith (1994).
4

812

Chapter 20.

Learning Probabilistic Models

Î˜

Flavor1

Flavor2

Flavor3

Wrapper1

Wrapper2

Wrapper3

Î˜1

Î˜2

Figure 20.6 A Bayesian network that corresponds to a Bayesian learning process. Posterior distributions for the parameter variables Î˜, Î˜1 , and Î˜2 can be inferred from their prior
distributions and the evidence in the Flavor i and Wrapper i variables.

P (Î¸ | D1 = cherry) = Î± P (D1 = cherry | Î¸)P (Î¸)
= Î± Î¸ Â· beta[a, b](Î¸) = Î± Î¸ Â· Î¸ aâˆ’1 (1 âˆ’ Î¸)bâˆ’1
= Î± Î¸ a (1 âˆ’ Î¸)bâˆ’1 = beta[a + 1, b](Î¸) .

VIRTUAL COUNTS

PARAMETER
INDEPENDENCE

Thus, after seeing a cherry candy, we simply increment the a parameter to get the posterior;
similarly, after seeing a lime candy, we increment the b parameter. Thus, we can view the a
and b hyperparameters as virtual counts, in the sense that a prior beta[a, b] behaves exactly
as if we had started out with a uniform prior beta[1, 1] and seen a âˆ’ 1 actual cherry candies
and b âˆ’ 1 actual lime candies.
By examining a sequence of beta distributions for increasing values of a and b, keeping
the proportions fixed, we can see vividly how the posterior distribution over the parameter
Î˜ changes as data arrive. For example, suppose the actual bag of candy is 75% cherry. Figure 20.5(b) shows the sequence beta[3, 1], beta[6, 2], beta[30, 10]. Clearly, the distribution
is converging to a narrow peak around the true value of Î˜. For large data sets, then, Bayesian
learning (at least in this case) converges to the same answer as maximum-likelihood learning.
Now let us consider a more complicated case. The network in Figure 20.2(b) has three
parameters, Î¸, Î¸1 , and Î¸2 , where Î¸1 is the probability of a red wrapper on a cherry candy and
Î¸2 is the probability of a red wrapper on a lime candy. The Bayesian hypothesis prior must
cover all three parametersâ€”that is, we need to specify P(Î˜, Î˜1 , Î˜2 ). Usually, we assume
parameter independence:
P(Î˜, Î˜1 , Î˜2 ) = P(Î˜)P(Î˜1 )P(Î˜2 ) .

Section 20.2.

Learning with Complete Data

813

With this assumption, each parameter can have its own beta distribution that is updated separately as data arrive. Figure 20.6 shows how we can incorporate the hypothesis prior and
any data into one Bayesian network. The nodes Î˜, Î˜1 , Î˜2 have no parents. But each time
we make an observation of a wrapper and corresponding flavor of a piece of candy, we add a
node Flavor i , which is dependent on the flavor parameter Î˜:
P (Flavor i = cherry | Î˜ = Î¸) = Î¸ .
We also add a node Wrapper i , which is dependent on Î˜1 and Î˜2 :
P (Wrapper i = red | Flavor i = cherry, Î˜1 = Î¸1 ) = Î¸1
P (Wrapper i = red | Flavor i = lime, Î˜2 = Î¸2 ) = Î¸2 .
Now, the entire Bayesian learning process can be formulated as an inference problem. We
add new evidence nodes, then query the unknown nodes (in this case, Î˜, Î˜1 , Î˜2 ). This formulation of learning and prediction makes it clear that Bayesian learning requires no extra
â€œprinciples of learning.â€ Furthermore, there is, in essence, just one learning algorithm â€”the
inference algorithm for Bayesian networks. Of course, the nature of these networks is somewhat different from those of Chapter 14 because of the potentially huge number of evidence
variables representing the training set and the prevalence of continuous-valued parameter
variables.

20.2.5 Learning Bayes net structures
So far, we have assumed that the structure of the Bayes net is given and we are just trying to
learn the parameters. The structure of the network represents basic causal knowledge about
the domain that is often easy for an expert, or even a naive user, to supply. In some cases,
however, the causal model may be unavailable or subject to disputeâ€”for example, certain
corporations have long claimed that smoking does not cause cancerâ€”so it is important to
understand how the structure of a Bayes net can be learned from data. This section gives a
brief sketch of the main ideas.
The most obvious approach is to search for a good model. We can start with a model
containing no links and begin adding parents for each node, fitting the parameters with the
methods we have just covered and measuring the accuracy of the resulting model. Alternatively, we can start with an initial guess at the structure and use hill-climbing or simulated
annealing search to make modifications, retuning the parameters after each change in the
structure. Modifications can include reversing, adding, or deleting links. We must not introduce cycles in the process, so many algorithms assume that an ordering is given for the
variables, and that a node can have parents only among those nodes that come earlier in the
ordering (just as in the construction process in Chapter 14). For full generality, we also need
to search over possible orderings.
There are two alternative methods for deciding when a good structure has been found.
The first is to test whether the conditional independence assertions implicit in the structure are
actually satisfied in the data. For example, the use of a naive Bayes model for the restaurant
problem assumes that
P(Fri /Sat , Bar | WillWait) = P(Fri /Sat | WillWait)P(Bar | WillWait)

814

Chapter 20.

Learning Probabilistic Models

and we can check in the data that the same equation holds between the corresponding conditional frequencies. But even if the structure describes the true causal nature of the domain,
statistical fluctuations in the data set mean that the equation will never be satisfied exactly,
so we need to perform a suitable statistical test to see if there is sufficient evidence that the
independence hypothesis is violated. The complexity of the resulting network will depend
on the threshold used for this testâ€”the stricter the independence test, the more links will be
added and the greater the danger of overfitting.
An approach more consistent with the ideas in this chapter is to assess the degree to
which the proposed model explains the data (in a probabilistic sense). We must be careful
how we measure this, however. If we just try to find the maximum-likelihood hypothesis,
we will end up with a fully connected network, because adding more parents to a node cannot decrease the likelihood (Exercise 20.8). We are forced to penalize model complexity in
some way. The MAP (or MDL) approach simply subtracts a penalty from the likelihood of
each structure (after parameter tuning) before comparing different structures. The Bayesian
approach places a joint prior over structures and parameters. There are usually far too many
structures to sum over (superexponential in the number of variables), so most practitioners
use MCMC to sample over structures.
Penalizing complexity (whether by MAP or Bayesian methods) introduces an important
connection between the optimal structure and the nature of the representation for the conditional distributions in the network. With tabular distributions, the complexity penalty for a
nodeâ€™s distribution grows exponentially with the number of parents, but with, say, noisy-OR
distributions, it grows only linearly. This means that learning with noisy-OR (or other compactly parameterized) models tends to produce learned structures with more parents than does
learning with tabular distributions.

20.2.6 Density estimation with nonparametric models

NONPARAMETRIC
DENSITY ESTIMATION

It is possible to learn a probability model without making any assumptions about its structure
and parameterization by adopting the nonparametric methods of Section 18.8. The task of
nonparametric density estimation is typically done in continuous domains, such as that
shown in Figure 20.7(a). The figure shows a probability density function on a space defined
by two continuous variables. In Figure 20.7(b) we see a sample of data points from this
density function. The question is, can we recover the model from the samples?
First we will consider k-nearest-neighbors models. (In Chapter 18 we saw nearestneighbor models for classification and regression; here we see them for density estimation.)
Given a sample of data points, to estimate the unknown probability density at a query point x
we can simply measure the density of the data points in the neighborhood of x. Figure 20.7(b)
shows two query points (small squares). For each query point we have drawn the smallest
circle that encloses 10 neighborsâ€”the 10-nearest-neighborhood. We can see that the central
circle is large, meaning there is a low density there, and the circle on the right is small,
meaning there is a high density there. In Figure 20.8 we show three plots of density estimation
using k-nearest-neighbors, for different values of k. It seems clear that (b) is about right,
while (a) is too spiky (k is too small) and (c) is too smooth (k is too big).

Section 20.2.

Learning with Complete Data

815

Density
18
16
14
12
10
8
6
4
2
0

1
0.9
0.8
0.7

0

0.2

0.4

0.6

0.8

1
0.8
0.6
0.4
0.2
1

0.6
0.5
0.4

0

0.3
0

0.2

0.4

(a)

0.6

0.8

1

(b)

Figure 20.7 (a) A 3D plot of the mixture of Gaussians from Figure 20.11(a). (b) A 128point sample of points from the mixture, together with two query points (small squares) and
their 10-nearest-neighborhoods (medium and large circles).

Density

Density

0 0.2
0.4 0.6

0.8

0

1
0.8
0.6
0.4
0.2

Density

0 0.2
0.4 0.6

(a)

0.8

0

1
0.8
0.6
0.4
0.2

0 0.2
0.4 0.6

(b)

0.8

0

1
0.8
0.6
0.4
0.2

(c)

Figure 20.8 Density estimation using k-nearest-neighbors, applied to the data in Figure 20.7(b), for k = 3, 10, and 40 respectively. k = 3 is too spiky, 40 is too smooth, and
10 is just about right. The best value for k can be chosen by cross-validation.

Density

0 0.2
0.4

Density

0.6 0.8

(a)

0

1
0.8
0.6
0.4
0.2

0 0.2
0.4

Density

0.6 0.8

(b)

0

1
0.8
0.6
0.4
0.2

0 0.2
0.4

0.6 0.8

0

1
0.8
0.6
0.4
0.2

(c)

Figure 20.9 Kernel density estimation for the data in Figure 20.7(b), using Gaussian kernels with w = 0.02, 0.07, and 0.20 respectively. w = 0.07 is about right.

816

Chapter 20.

Learning Probabilistic Models

Another possibility is to use kernel functions, as we did for locally weighted regression. To apply a kernel model to density estimation, assume that each data point generates its
own little density function, using a Gaussian kernel. The estimated density at a query point x
is then the average density as given by each kernel function:
P (x) =

N
1 
K(x, xj ) .
N
j=1

We will assume spherical Gaussians with standard deviation w along each axis:
2

K(x, xj ) =

D(x,xj )
1
âˆš
eâˆ’ 2w2 ,
(w2 2Ï€)d

where d is the number of dimensions in x and D is the Euclidean distance function. We
still have the problem of choosing a suitable value for kernel width w; Figure 20.9 shows
values that are too small, just right, and too large. A good value of w can be chosen by using
cross-validation.

20.3

L EARNING WITH H IDDEN VARIABLES : T HE EM A LGORITHM

LATENT VARIABLE

EXPECTATIONâ€“
MAXIMIZATION

The preceding section dealt with the fully observable case. Many real-world problems have
hidden variables (sometimes called latent variables), which are not observable in the data
that are available for learning. For example, medical records often include the observed
symptoms, the physicianâ€™s diagnosis, the treatment applied, and perhaps the outcome of the
treatment, but they seldom contain a direct observation of the disease itself! (Note that the
diagnosis is not the disease; it is a causal consequence of the observed symptoms, which are in
turn caused by the disease.) One might ask, â€œIf the disease is not observed, why not construct
a model without it?â€ The answer appears in Figure 20.10, which shows a small, fictitious
diagnostic model for heart disease. There are three observable predisposing factors and three
observable symptoms (which are too depressing to name). Assume that each variable has
three possible values (e.g., none, moderate , and severe). Removing the hidden variable
from the network in (a) yields the network in (b); the total number of parameters increases
from 78 to 708. Thus, latent variables can dramatically reduce the number of parameters
required to specify a Bayesian network. This, in turn, can dramatically reduce the amount of
data needed to learn the parameters.
Hidden variables are important, but they do complicate the learning problem. In Figure 20.10(a), for example, it is not obvious how to learn the conditional distribution for
HeartDisease, given its parents, because we do not know the value of HeartDisease in each
case; the same problem arises in learning the distributions for the symptoms. This section
describes an algorithm called expectationâ€“maximization, or EM, that solves this problem
in a very general way. We will show three examples and then provide a general description.
The algorithm seems like magic at first, but once the intuition has been developed, one can
find applications for EM in a huge range of learning problems.

Section 20.3.

Learning with Hidden Variables: The EM Algorithm

2

2

Smoking

2

Diet

2

Exercise

817

2

Smoking

2

Diet

Exercise

54 HeartDisease

6

6

Symptom 1

6

Symptom 2

54

Symptom 3

(a)

162

Symptom 1

486

Symptom 2

Symptom 3

(b)

Figure 20.10 (a) A simple diagnostic network for heart disease, which is assumed to be
a hidden variable. Each variable has three possible values and is labeled with the number
of independent parameters in its conditional distribution; the total number is 78. (b) The
equivalent network with HeartDisease removed. Note that the symptom variables are no
longer conditionally independent given their parents. This network requires 708 parameters.

20.3.1 Unsupervised clustering: Learning mixtures of Gaussians
UNSUPERVISED
CLUSTERING

MIXTURE
DISTRIBUTION
COMPONENT

Unsupervised clustering is the problem of discerning multiple categories in a collection of
objects. The problem is unsupervised because the category labels are not given. For example,
suppose we record the spectra of a hundred thousand stars; are there different types of stars
revealed by the spectra, and, if so, how many types and what are their characteristics? We
are all familiar with terms such as â€œred giantâ€ and â€œwhite dwarf,â€ but the stars do not carry
these labels on their hatsâ€”astronomers had to perform unsupervised clustering to identify
these categories. Other examples include the identification of species, genera, orders, and
so on in the LinnÃ¦an taxonomy and the creation of natural kinds for ordinary objects (see
Chapter 12).
Unsupervised clustering begins with data. Figure 20.11(b) shows 500 data points, each
of which specifies the values of two continuous attributes. The data points might correspond
to stars, and the attributes might correspond to spectral intensities at two particular frequencies. Next, we need to understand what kind of probability distribution might have generated
the data. Clustering presumes that the data are generated from a mixture distribution, P .
Such a distribution has k components, each of which is a distribution in its own right. A
data point is generated by first choosing a component and then generating a sample from that
component. Let the random variable C denote the component, with values 1, . . . , k; then the
mixture distribution is given by
P (x) =

k


P (C = i) P (x | C = i) ,

i=1

MIXTURE OF
GAUSSIANS

where x refers to the values of the attributes for a data point. For continuous data, a natural
choice for the component distributions is the multivariate Gaussian, which gives the so-called
mixture of Gaussians family of distributions. The parameters of a mixture of Gaussians are

818

Chapter 20.

Learning Probabilistic Models

1

1

1

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0
0

0.2

0.4

(a)

0.6

0.8

1

0
0

0.2

0.4

(b)

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

(c)

Figure 20.11 (a) A Gaussian mixture model with three components; the weights (left-toright) are 0.2, 0.3, and 0.5. (b) 500 data points sampled from the model in (a). (c) The model
reconstructed by EM from the data in (b).

wi = P (C = i) (the weight of each component), Î¼i (the mean of each component), and Î£i
(the covariance of each component). Figure 20.11(a) shows a mixture of three Gaussians;
this mixture is in fact the source of the data in (b) as well as being the model shown in
Figure 20.7(a) on page 815.
The unsupervised clustering problem, then, is to recover a mixture model like the one
in Figure 20.11(a) from raw data like that in Figure 20.11(b). Clearly, if we knew which component generated each data point, then it would be easy to recover the component Gaussians:
we could just select all the data points from a given component and then apply (a multivariate
version of) Equation (20.4) (page 809) for fitting the parameters of a Gaussian to a set of data.
On the other hand, if we knew the parameters of each component, then we could, at least in
a probabilistic sense, assign each data point to a component. The problem is that we know
neither the assignments nor the parameters.
The basic idea of EM in this context is to pretend that we know the parameters of the
model and then to infer the probability that each data point belongs to each component. After
that, we refit the components to the data, where each component is fitted to the entire data set
with each point weighted by the probability that it belongs to that component. The process
iterates until convergence. Essentially, we are â€œcompletingâ€ the data by inferring probability
distributions over the hidden variablesâ€”which component each data point belongs toâ€”based
on the current model. For the mixture of Gaussians, we initialize the mixture-model parameters arbitrarily and then iterate the following two steps:
1. E-step: Compute the probabilities pij = P (C = i | xj ), the probability that datum xj
was generated by component i. By Bayesâ€™ rule, we have pij = Î±P (xj | C = i)P (C = i).
The term P (xj | C = i) is just the probability at xj of the ith Gaussian, and
 the term
P (C = i) is just the weight parameter for the ith Gaussian. Define ni = j pij , the
effective number of data points currently assigned to component i.
2. M-step: Compute the new mean, covariance, and component weights using the following steps in sequence:

Section 20.3.

Learning with Hidden Variables: The EM Algorithm
Î¼i â†



819

pij xj /ni

j

Î£i â†



pij (xj âˆ’ Î¼i )(xj âˆ’ Î¼i ) /ni

j

wi â† ni /N

700
600
500
400
300
200
100
0
-100
-200

Log-likelihood L

Log-likelihood L

INDICATOR VARIABLE

where N is the total number of data points. The E-step, or expectation step, can be viewed
as computing the expected values pij of the hidden indicator variables Zij , where Zij is 1 if
datum xj was generated by the ith component and 0 otherwise. The M-step, or maximization
step, finds the new values of the parameters that maximize the log likelihood of the data,
given the expected values of the hidden indicator variables.
The final model that EM learns when it is applied to the data in Figure 20.11(a) is shown
in Figure 20.11(c); it is virtually indistinguishable from the original model from which the
data were generated. Figure 20.12(a) plots the log likelihood of the data according to the
current model as EM progresses.
There are two points to notice. First, the log likelihood for the final learned model
slightly exceeds that of the original model, from which the data were generated. This might
seem surprising, but it simply reflects the fact that the data were generated randomly and
might not provide an exact reflection of the underlying model. The second point is that EM
increases the log likelihood of the data at every iteration. This fact can be proved in general.
Furthermore, under certain conditions (that hold in ost cases), EM can be proven to reach
a local maximum in likelihood. (In rare cases, it could reach a saddle point or even a local
minimum.) In this sense, EM resembles a gradient-based hill-climbing algorithm, but notice
that it has no â€œstep sizeâ€ parameter.

0

5

10
15
Iteration number

(a)

20

-1975
-1980
-1985
-1990
-1995
-2000
-2005
-2010
-2015
-2020
-2025
0

20

40
60
80
Iteration number

100

120

(b)

Figure 20.12 Graphs showing the log likelihood of the data, L, as a function of the EM
iteration. The horizontal line shows the log likelihood according to the true model. (a) Graph
for the Gaussian mixture model in Figure 20.11. (b) Graph for the Bayesian network in
Figure 20.13(a).

820

Chapter 20.

Learning Probabilistic Models

P(Bag=1)

Î¸
Bag P(F=cherry | B)
1

Î¸F1

2

Î¸F2

Flavor

Bag

Wrapper

C

Hole

(a)

X
(b)

Figure 20.13 (a) A mixture model for candy. The proportions of different flavors, wrappers, presence of holes depend on the bag, which is not observed. (b) Bayesian network for
a Gaussian mixture. The mean and covariance of the observable variables X depend on the
component C.

Things do not always go as well as Figure 20.12(a) might suggest. It can happen, for
example, that one Gaussian component shrinks so that it covers just a single data point. Then
its variance will go to zero and its likelihood will go to infinity! Another problem is that
two components can â€œmerge,â€ acquiring identical means and variances and sharing their data
points. These kinds of degenerate local maxima are serious problems, especially in high
dimensions. One solution is to place priors on the model parameters and to apply the MAP
version of EM. Another is to restart a component with new random parameters if it gets too
small or too close to another component. Sensible initialization also helps.

20.3.2 Learning Bayesian networks with hidden variables
To learn a Bayesian network with hidden variables, we apply the same insights that worked
for mixtures of Gaussians. Figure 20.13 represents a situation in which there are two bags of
candies that have been mixed together. Candies are described by three features: in addition
to the Flavor and the Wrapper , some candies have a Hole in the middle and some do not.
The distribution of candies in each bag is described by a naive Bayes model: the features
are independent, given the bag, but the conditional probability distribution for each feature
depends on the bag. The parameters are as follows: Î¸ is the prior probability that a candy
comes from Bag 1; Î¸F 1 and Î¸F 2 are the probabilities that the flavor is cherry, given that the
candy comes from Bag 1 or Bag 2 respectively; Î¸W 1 and Î¸W 2 give the probabilities that the
wrapper is red; and Î¸H1 and Î¸H2 give the probabilities that the candy has a hole. Notice that
the overall model is a mixture model. (In fact, we can also model the mixture of Gaussians
as a Bayesian network, as shown in Figure 20.13(b).) In the figure, the bag is a hidden
variable because, once the candies have been mixed together, we no longer know which bag
each candy came from. In such a case, can we recover the descriptions of the two bags by

Section 20.3.

Learning with Hidden Variables: The EM Algorithm

821

observing candies from the mixture? Let us work through an iteration of EM for this problem.
First, letâ€™s look at the data. We generated 1000 samples from a model whose true parameters
are as follows:
Î¸ = 0.5, Î¸F 1 = Î¸W 1 = Î¸H1 = 0.8, Î¸F 2 = Î¸W 2 = Î¸H2 = 0.3 .

(20.7)

That is, the candies are equally likely to come from either bag; the first is mostly cherries
with red wrappers and holes; the second is mostly limes with green wrappers and no holes.
The counts for the eight possible kinds of candy are as follows:
W = red

W = green

H =1 H =0 H =1 H =0
F = cherry
F = lime

273
79

93
100

104
94

90
167

We start by initializing the parameters. For numerical simplicity, we arbitrarily choose5
(0)

(0)

(0)

(0)

(0)

(0)

Î¸ (0) = 0.6, Î¸F 1 = Î¸W 1 = Î¸H1 = 0.6, Î¸F 2 = Î¸W 2 = Î¸H2 = 0.4 .

(20.8)

First, let us work on the Î¸ parameter. In the fully observable case, we would estimate this
directly from the observed counts of candies from bags 1 and 2. Because the bag is a hidden
variable, we calculate the expected counts instead. The expected count NÌ‚ (Bag = 1) is the
sum, over all candies, of the probability that the candy came from bag 1:
Î¸ (1) = NÌ‚ (Bag = 1)/N =

N


P (Bag = 1 | ï¬‚avor j , wrapper j , holes j )/N .

j=1

These probabilities can be computed by any inference algorithm for Bayesian networks. For
a naive Bayes model such as the one in our example, we can do the inference â€œby hand,â€
using Bayesâ€™ rule and applying conditional independence:
Î¸ (1) =

N
1  P (ï¬‚avor j | Bag = 1)P (wrapper j | Bag = 1)P (holes j | Bag = 1)P (Bag = 1)

.
N
i P (ï¬‚avor j | Bag = i)P (wrapper j | Bag = i)P (holes j | Bag = i)P (Bag = i)
j=1

Applying this formula to, say, the 273 red-wrapped cherry candies with holes, we get a contribution of
(0) (0)

(0)

Î¸ Î¸ Î¸ Î¸ (0)
273
â‰ˆ 0.22797 .
Â· (0) (0) (0) F 1 W 1(0)H1(0) (0)
1000 Î¸ Î¸ Î¸ Î¸ (0) + Î¸ Î¸ Î¸ (1 âˆ’ Î¸ (0) )
F 1 W 1 H1
F 2 W 2 H2
Continuing with the other seven kinds of candy in the table of counts, we obtain Î¸ (1) = 0.6124.
Now let us consider the other parameters, such as Î¸F 1 . In the fully observable case, we
would estimate this directly from the observed counts of cherry and lime candies from bag 1.
The expected count of cherry candies from bag 1 is given by

P (Bag = 1 | Flavor j = cherry, wrapper j , holes j ) .
j:Flavor j = cherry
5

It is better in practice to choose them randomly, to avoid local maxima due to symmetry.

822

Chapter 20.

Learning Probabilistic Models

Again, these probabilities can be calculated by any Bayes net algorithm. Completing this
process, we obtain the new values of all the parameters:
(1)

(1)

(1)

Î¸ (1) = 0.6124, Î¸F 1 = 0.6684, Î¸W 1 = 0.6483, Î¸H1 = 0.6558,
(1)
(1)
(1)
Î¸F 2 = 0.3887, Î¸W 2 = 0.3817, Î¸H2 = 0.3827 .

(20.9)

The log likelihood of the data increases from about âˆ’2044 initially to about âˆ’2021 after
the first iteration, as shown in Figure 20.12(b). That is, the update improves the likelihood
itself by a factor of about e23 â‰ˆ 1010 . By the tenth iteration, the learned model is a better
fit than the original model (L = âˆ’ 1982.214). Thereafter, progress becomes very slow. This
is not uncommon with EM, and many practical systems combine EM with a gradient-based
algorithm such as Newtonâ€“Raphson (see Chapter 4) for the last phase of learning.
The general lesson from this example is that the parameter updates for Bayesian network learning with hidden variables are directly available from the results of inference on
each example. Moreover, only local posterior probabilities are needed for each parameter. Here, â€œlocalâ€ means that the CPT for each variable Xi can be learned from posterior
probabilities involving just Xi and its parents Ui . Defining Î¸ijk to be the CPT parameter
P (Xi = xij | Ui = uik ), the update is given by the normalized expected counts as follows:
Î¸ijk â† NÌ‚ (Xi = xij , Ui = uik )/NÌ‚ (Ui = uik ) .
The expected counts are obtained by summing over the examples, computing the probabilities
P (Xi = xij , Ui = uik ) for each by using any Bayes net inference algorithm. For the exact
algorithmsâ€”including variable eliminationâ€”all these probabilities are obtainable directly as
a by-product of standard inference, with no need for extra computations specific to learning.
Moreover, the information needed for learning is available locally for each parameter.

20.3.3 Learning hidden Markov models
Our final application of EM involves learning the transition probabilities in hidden Markov
models (HMMs). Recall from Section 15.3 that a hidden Markov model can be represented
by a dynamic Bayes net with a single discrete state variable, as illustrated in Figure 20.14.
Each data point consists of an observation sequence of finite length, so the problem is to
learn the transition probabilities from a set of observation sequences (or from just one long
sequence).
We have already worked out how to learn Bayes nets, but there is one complication:
in Bayes nets, each parameter is distinct; in a hidden Markov model, on the other hand, the
individual transition probabilities from state i to state j at time t, Î¸ijt = P (Xt+1 = j | Xt = i),
are repeated across timeâ€”that is, Î¸ijt = Î¸ij for all t. To estimate the transition probability
from state i to state j, we simply calculate the expected proportion of times that the system
undergoes a transition to state j when in state i:


NÌ‚ (Xt+1 = j, Xt = i)/
NÌ‚(Xt = i) .
Î¸ij â†
t

t

The expected counts are computed by an HMM inference algorithm. The forwardâ€“backward
algorithm shown in Figure 15.4 can be modified very easily to compute the necessary probabilities. One important point is that the probabilities required are obtained by smoothing

Section 20.3.

Learning with Hidden Variables: The EM Algorithm

P(R 0 )
0.7
Rain0

R0
t
f

P(R1 )
0.7
0.3

P(R 0 )
0.7

Rain1

Rain0

Umbrella1
R1
t
f

P(U1 )
0.9
0.2

R0
t
f

P(R1 )
0.7
0.3

R1
t
f

823
P(R2 )
0.7
0.3

R2
t
f

P(R3 )
0.7
0.3

R3
t
f

P(R4 )
0.7
0.3

Rain1

Rain2

Rain3

Rain4

Umbrella1

Umbrella2

Umbrella3

Umbrella4

R1
t
f

P(U1 )
0.9
0.2

R2
t
f

P(U2 )
0.9
0.2

R3
t
f

P(U3 )
0.9
0.2

R4
t
f

P(U4 )
0.9
0.2

Figure 20.14 An unrolled dynamic Bayesian network that represents a hidden Markov
model (repeat of Figure 15.16).

rather than filtering; that is, we need to pay attention to subsequent evidence in estimating
the probability that a particular transition occurred. The evidence in a murder case is usually
obtained after the crime (i.e., the transition from state i to state j) has taken place.

20.3.4 The general form of the EM algorithm
We have seen several instances of the EM algorithm. Each involves computing expected
values of hidden variables for each example and then recomputing the parameters, using the
expected values as if they were observed values. Let x be all the observed values in all the
examples, let Z denote all the hidden variables for all the examples, and let Î¸ be all the
parameters for the probability model. Then the EM algorithm is

P (Z = z | x, Î¸ (i) )L(x, Z = z | Î¸) .
Î¸ (i+1) = argmax
Î¸

z

This equation is the EM algorithm in a nutshell. The E-step is the computation of the summation, which is the expectation of the log likelihood of the â€œcompletedâ€ data with respect to the
distribution P (Z = z | x, Î¸ (i) ), which is the posterior over the hidden variables, given the data.
The M-step is the maximization of this expected log likelihood with respect to the parameters. For mixtures of Gaussians, the hidden variables are the Zij s, where Zij is 1 if example j
was generated by component i. For Bayes nets, Zij is the value of unobserved variable Xi in
example j. For HMMs, Zjt is the state of the sequence in example j at time t. Starting from
the general form, it is possible to derive an EM algorithm for a specific application once the
appropriate hidden variables have been identified.
As soon as we understand the general idea of EM, it becomes easy to derive all sorts
of variants and improvements. For example, in many cases the E-stepâ€”the computation of
posteriors over the hidden variablesâ€”is intractable, as in large Bayes nets. It turns out that
one can use an approximate E-step and still obtain an effective learning algorithm. With a
sampling algorithm such as MCMC (see Section 14.5), the learning process is very intuitive:
each state (configuration of hidden and observed variables) visited by MCMC is treated exactly as if it were a complete observation. Thus, the parameters can be updated directly after
each MCMC transition. Other forms of approximate inference, such as variational and loopy
methods, have also proved effective for learning very large networks.

824

Chapter 20.

Learning Probabilistic Models

20.3.5 Learning Bayes net structures with hidden variables

STRUCTURAL EM

In Section 20.2.5, we discussed the problem of learning Bayes net structures with complete
data. When unobserved variables may be influencing the data that are observed, things get
more difficult. In the simplest case, a human expert might tell the learning algorithm that certain hidden variables exist, leaving it to the algorithm to find a place for them in the network
structure. For example, an algorithm might try to learn the structure shown in Figure 20.10(a)
on page 817, given the information that HeartDisease (a three-valued variable) should be included in the model. As in the complete-data case, the overall algorithm has an outer loop that
searches over structures and an inner loop that fits the network parameters given the structure.
If the learning algorithm is not told which hidden variables exist, then there are two
choices: either pretend that the data is really completeâ€”which may force the algorithm to
learn a parameter-intensive model such as the one in Figure 20.10(b)â€”or invent new hidden
variables in order to simplify the model. The latter approach can be implemented by including
new modification choices in the structure search: in addition to modifying links, the algorithm
can add or delete a hidden variable or change its arity. Of course, the algorithm will not know
that the new variable it has invented is called HeartDisease; nor will it have meaningful
names for the values. Fortunately, newly invented hidden variables will usually be connected
to preexisting variables, so a human expert can often inspect the local conditional distributions
involving the new variable and ascertain its meaning.
As in the complete-data case, pure maximum-likelihood structure learning will result in
a completely connected network (moreover, one with no hidden variables), so some form of
complexity penalty is required. We can also apply MCMC to sample many possible network
structures, thereby approximating Bayesian learning. For example, we can learn mixtures of
Gaussians with an unknown number of components by sampling over the number; the approximate posterior distribution for the number of Gaussians is given by the sampling frequencies
of the MCMC process.
For the complete-data case, the inner loop to learn the parameters is very fastâ€”just a
matter of extracting conditional frequencies from the data set. When there are hidden variables, the inner loop may involve many iterations of EM or a gradient-based algorithm, and
each iteration involves the calculation of posteriors in a Bayes net, which is itself an NP-hard
problem. To date, this approach has proved impractical for learning complex models. One
possible improvement is the so-called structural EM algorithm, which operates in much the
same way as ordinary (parametric) EM except that the algorithm can update the structure
as well as the parameters. Just as ordinary EM uses the current parameters to compute the
expected counts in the E-step and then applies those counts in the M-step to choose new
parameters, structural EM uses the current structure to compute expected counts and then applies those counts in the M-step to evaluate the likelihood for potential new structures. (This
contrasts with the outer-loop/inner-loop method, which computes new expected counts for
each potential structure.) In this way, structural EM may make several structural alterations
to the network without once recomputing the expected counts, and is capable of learning nontrivial Bayes net structures. Nonetheless, much work remains to be done before we can say
that the structure-learning problem is solved.

Section 20.4.

20.4

Summary

825

S UMMARY
Statistical learning methods range from simple calculation of averages to the construction of
complex models such as Bayesian networks. They have applications throughout computer
science, engineering, computational biology, neuroscience, psychology, and physics. This
chapter has presented some of the basic ideas and given a flavor of the mathematical underpinnings. The main points are as follows:
â€¢ Bayesian learning methods formulate learning as a form of probabilistic inference,
using the observations to update a prior distribution over hypotheses. This approach
provides a good way to implement Ockhamâ€™s razor, but quickly becomes intractable for
complex hypothesis spaces.
â€¢ Maximum a posteriori (MAP) learning selects a single most likely hypothesis given
the data. The hypothesis prior is still used and the method is often more tractable than
full Bayesian learning.
â€¢ Maximum-likelihood learning simply selects the hypothesis that maximizes the likelihood of the data; it is equivalent to MAP learning with a uniform prior. In simple cases
such as linear regression and fully observable Bayesian networks, maximum-likelihood
solutions can be found easily in closed form. Naive Bayes learning is a particularly
effective technique that scales well.
â€¢ When some variables are hidden, local maximum likelihood solutions can be found
using the EM algorithm. Applications include clustering using mixtures of Gaussians,
learning Bayesian networks, and learning hidden Markov models.
â€¢ Learning the structure of Bayesian networks is an example of model selection. This
usually involves a discrete search in the space of structures. Some method is required
for trading off model complexity against degree of fit.
â€¢ Nonparametric models represent a distribution using the collection of data points.
Thus, the number of parameters grows with the training set. Nearest-neighbors methods
look at the examples nearest to the point in question, whereas kernel methods form a
distance-weighted combination of all the examples.
Statistical learning continues to be a very active area of research. Enormous strides have been
made in both theory and practice, to the point where it is possible to learn almost any model
for which exact or approximate inference is feasible.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

The application of statistical learning techniques in AI was an active area of research in the
early years (see Duda and Hart, 1973) but became separated from mainstream AI as the
latter field concentrated on symbolic methods. A resurgence of interest occurred shortly after
the introduction of Bayesian network models in the late 1980s; at roughly the same time,

826

Chapter 20.

Learning Probabilistic Models

a statistical view of neural network learning began to emerge. In the late 1990s, there was
a noticeable convergence of interests in machine learning, statistics, and neural networks,
centered on methods for creating large probabilistic models from data.
The naive Bayes model is one of the oldest and simplest forms of Bayesian network,
dating back to the 1950s. Its origins were mentioned in Chapter 13. Its surprising success is
partially explained by Domingos and Pazzani (1997). A boosted form of naive Bayes learning won the first KDD Cup data mining competition (Elkan, 1997). Heckerman (1998) gives
an excellent introduction to the general problem of Bayes net learning. Bayesian parameter learning with Dirichlet priors for Bayesian networks was discussed by Spiegelhalter et al.
(1993). The B UGS software package (Gilks et al., 1994) incorporates many of these ideas and
provides a very powerful tool for formulating and learning complex probability models. The
first algorithms for learning Bayes net structures used conditional independence tests (Pearl,
1988; Pearl and Verma, 1991). Spirtes et al. (1993) developed a comprehensive approach
embodied in the T ETRAD package for Bayes net learning. Algorithmic improvements since
then led to a clear victory in the 2001 KDD Cup data mining competition for a Bayes net
learning method (Cheng et al., 2002). (The specific task here was a bioinformatics problem with 139,351 features!) A structure-learning approach based on maximizing likelihood
was developed by Cooper and Herskovits (1992) and improved by Heckerman et al. (1994).
Several algorithmic advances since that time have led to quite respectable performance in
the complete-data case (Moore and Wong, 2003; Teyssier and Koller, 2005). One important
component is an efficient data structure, the AD-tree, for caching counts over all possible
combinations of variables and values (Moore and Lee, 1997). Friedman and Goldszmidt
(1996) pointed out the influence of the representation of local conditional distributions on the
learned structure.
The general problem of learning probability models with hidden variables and missing data was addressed by Hartley (1958), who described the general idea of what was later
called EM and gave several examples. Further impetus came from the Baumâ€“Welch algorithm for HMM learning (Baum and Petrie, 1966), which is a special case of EM. The paper
by Dempster, Laird, and Rubin (1977), which presented the EM algorithm in general form
and analyzed its convergence, is one of the most cited papers in both computer science and
statistics. (Dempster himself views EM as a schema rather than an algorithm, since a good
deal of mathematical work may be required before it can be applied to a new family of distributions.) McLachlan and Krishnan (1997) devote an entire book to the algorithm and its
properties. The specific problem of learning mixture models, including mixtures of Gaussians, is covered by Titterington et al. (1985). Within AI, the first successful system that used
EM for mixture modeling was AUTOCLASS (Cheeseman et al., 1988; Cheeseman and Stutz,
1996). AUTOCLASS has been applied to a number of real-world scientific classification tasks,
including the discovery of new types of stars from spectral data (Goebel et al., 1989) and new
classes of proteins and introns in DNA/protein sequence databases (Hunter and States, 1992).
For maximum-likelihood parameter learning in Bayes nets with hidden variables, EM
and gradient-based methods were introduced around the same time by Lauritzen (1995), Russell et al. (1995), and Binder et al. (1997a). The structural EM algorithm was developed by
Friedman (1998) and applied to maximum-likelihood learning of Bayes net structures with

Exercises

CAUSAL NETWORK

DIRICHLET PROCESS

GAUSSIAN PROCESS

827
latent variables. Friedman and Koller (2003). describe Bayesian structure learning.
The ability to learn the structure of Bayesian networks is closely connected to the issue
of recovering causal information from data. That is, is it possible to learn Bayes nets in
such a way that the recovered network structure indicates real causal influences? For many
years, statisticians avoided this question, believing that observational data (as opposed to data
generated from experimental trials) could yield only correlational informationâ€”after all, any
two variables that appear related might in fact be influenced by a third, unknown causal
factor rather than influencing each other directly. Pearl (2000) has presented convincing
arguments to the contrary, showing that there are in fact many cases where causality can be
ascertained and developing the causal network formalism to express causes and the effects
of intervention as well as ordinary conditional probabilities.
Nonparametric density estimation, also called Parzen window density estimation, was
investigated initially by Rosenblatt (1956) and Parzen (1962). Since that time, a huge literature has developed investigating the properties of various estimators. Devroye (1987) gives a
thorough introduction. There is also a rapidly growing literature on nonparametric Bayesian
methods, originating with the seminal work of Ferguson (1973) on the Dirichlet process,
which can be thought of as a distribution over Dirichlet distributions. These methods are particularly useful for mixtures with unknown numbers of components. Ghahramani (2005) and
Jordan (2005) provide useful tutorials on the many applications of these ideas to statistical
learning. The text by Rasmussen and Williams (2006) covers the Gaussian process, which
gives a way of defining prior distributions over the space of continuous functions.
The material in this chapter brings together work from the fields of statistics and pattern
recognition, so the story has been told many times in many ways. Good texts on Bayesian
statistics include those by DeGroot (1970), Berger (1985), and Gelman et al. (1995). Bishop
(2007) and Hastie et al. (2009) provide an excellent introduction to statistical machine learning. For pattern classification, the classic text for many years has been Duda and Hart (1973),
now updated (Duda et al., 2001). The annual NIPS (Neural Information Processing Conference) conference, whose proceedings are published as the series Advances in Neural Information Processing Systems, is now dominated by Bayesian papers. Papers on learning Bayesian
networks also appear in the Uncertainty in AI and Machine Learning conferences and in several statistics conferences. Journals specific to neural networks include Neural Computation,
Neural Networks, and the IEEE Transactions on Neural Networks. Specifically Bayesian
venues include the Valencia International Meetings on Bayesian Statistics and the journal
Bayesian Analysis.

E XERCISES
20.1 The data used for Figure 20.1 on page 804 can be viewed as being generated by h5 .
For each of the other four hypotheses, generate a data set of length 100 and plot the corresponding graphs for P (hi | d1 , . . . , dN ) and P (DN +1 = lime | d1 , . . . , dN ). Comment on
your results.

828

Chapter 20.

Learning Probabilistic Models

20.2 Suppose that Annâ€™s utilities for cherry and lime candies are cA and A , whereas Bobâ€™s
utilities are cB and B . (But once Ann has unwrapped a piece of candy, Bob wonâ€™t buy
it.) Presumably, if Bob likes lime candies much more than Ann, it would be wise for Ann
to sell her bag of candies once she is sufficiently sure of its lime content. On the other hand,
if Ann unwraps too many candies in the process, the bag will be worth less. Discuss the
problem of determining the optimal point at which to sell the bag. Determine the expected
utility of the optimal procedure, given the prior distribution from Section 20.1.
20.3 Two statisticians go to the doctor and are both given the same prognosis: A 40%
chance that the problem is the deadly disease A, and a 60% chance of the fatal disease B.
Fortunately, there are anti-A and anti-B drugs that are inexpensive, 100% effective, and free
of side-effects. The statisticians have the choice of taking one drug, both, or neither. What
will the first statistician (an avid Bayesian) do? How about the second statistician, who always
uses the maximum likelihood hypothesis?
The doctor does some research and discovers that disease B actually comes in two
versions, dextro-B and levo-B, which are equally likely and equally treatable by the anti-B
drug. Now that there are three hypotheses, what will the two statisticians do?
20.4 Explain how to apply the boosting method of Chapter 18 to naive Bayes learning. Test
the performance of the resulting algorithm on the restaurant learning problem.
20.5 Consider N data points (xj , yj ), where the yj s are generated from the xj s according to
the linear Gaussian model in Equation (20.5). Find the values of Î¸1 , Î¸2 , and Ïƒ that maximize
the conditional log likelihood of the data.
20.6 Consider the noisy-OR model for fever described in Section 14.3. Explain how to
apply maximum-likelihood learning to fit the parameters of such a model to a set of complete
data. (Hint: use the chain rule for partial derivatives.)
20.7

GAMMA FUNCTION

This exercise investigates properties of the Beta distribution defined in Equation (20.6).

a. By integrating over the range [0, 1], show that the normalization constant for the distribution beta[a, b] is given by Î± = Î“(a + b)/Î“(a)Î“(b) where Î“(x) is the Gamma
function, defined by Î“(x + 1) = x Â· Î“(x) and Î“(1) = 1. (For integer x, Î“(x + 1) = x!.)
b. Show that the mean is a/(a + b).
c. Find the mode(s) (the most likely value(s) of Î¸).
d. Describe the distribution beta[, ] for very small . What happens as such a distribution
is updated?
20.8 Consider an arbitrary Bayesian network, a complete data set for that network, and the
likelihood for the data set according to the network. Give a simple proof that the likelihood
of the data cannot decrease if we add a new link to the network and recompute the maximumlikelihood parameter values.
20.9 Consider a single Boolean random variable Y (the â€œclassificationâ€). Let the prior
probability P (Y = true) be Ï€. Letâ€™s try to find Ï€, given a training set D = (y1 , . . . , yN ) with
N independent samples of Y . Furthermore, suppose p of the N are positive and n of the N
are negative.

Exercises

829
a. Write down an expression for the likelihood of D (i.e., the probability of seeing this
particular sequence of examples, given a fixed value of Ï€) in terms of Ï€, p, and n.
b. By differentiating the log likelihood L, find the value of Ï€ that maximizes the likelihood.
c. Now suppose we add in k Boolean random variables X1 , X2 , . . . , Xk (the â€œattributesâ€)
that describe each sample, and suppose we assume that the attributes are conditionally
independent of each other given the goal Y . Draw the Bayes net corresponding to this
assumption.
d. Write down the likelihood for the data including the attributes, using the following
additional notation:
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢

Î±i is P (Xi = true|Y = true).
Î²i is P (Xi = true|Y = f alse).
p+
i is the count of samples for which Xi = true and Y = true.
n+
i is the count of samples for which Xi = f alse and Y = true.
âˆ’
pi is the count of samples for which Xi = true and Y = f alse.
nâˆ’
i is the count of samples for which Xi = f alse and Y = f alse.

[Hint: consider first the probability of seeing a single example with specified values for
X1 , X2 , . . . , Xk and Y .]
e. By differentiating the log likelihood L, find the values of Î±i and Î²i (in terms of the various counts) that maximize the likelihood and say in words what these values represent.
f. Let k = 2, and consider a data set with 4 all four possible examples of theXOR function.
Compute the maximum likelihood estimates of Ï€, Î±1 , Î±2 , Î²1 , and Î²2 .
g. Given these estimates of Ï€, Î±1 , Î±2 , Î²1 , and Î²2 , what are the posterior probabilities
P (Y = true|x1 , x2 ) for each example?
20.10 Consider the application of EM to learn the parameters for the network in Figure 20.13(a), given the true parameters in Equation (20.7).
a. Explain why the EM algorithm would not work if there were just two attributes in the
model rather than three.
b. Show the calculations for the first iteration of EM starting from Equation (20.8).
c. What happens if we start with all the parameters set to the same value p? (Hint: you
may find it helpful to investigate this empirically before deriving the general result.)
d. Write out an expression for the log likelihood of the tabulated candy data on page 821 in
terms of the parameters, calculate the partial derivatives with respect to each parameter,
and investigate the nature of the fixed point reached in part (c).

21

REINFORCEMENT
LEARNING

In which we examine how an agent can learn from success and failure, from reward and punishment.

21.1

I NTRODUCTION

REINFORCEMENT

Chapters 18, 19, and 20 covered methods that learn functions, logical theories, and probability
models from examples. In this chapter, we will study how agents can learn what to do in the
absence of labeled examples of what to do.
Consider, for example, the problem of learning to play chess. A supervised learning
agent needs to be told the correct move for each position it encounters, but such feedback is
seldom available. In the absence of feedback from a teacher, an agent can learn a transition
model for its own moves and can perhaps learn to predict the opponentâ€™s moves, but without
some feedback about what is good and what is bad, the agent will have no grounds for deciding which move to make. The agent needs to know that something good has happened when
it (accidentally) checkmates the opponent, and that something bad has happened when it is
checkmatedâ€”or vice versa, if the game is suicide chess. This kind of feedback is called a
reward, or reinforcement. In games like chess, the reinforcement is received only at the end
of the game. In other environments, the rewards come more frequently. In ping-pong, each
point scored can be considered a reward; when learning to crawl, any forward motion is an
achievement. Our framework for agents regards the reward as part of the input percept, but
the agent must be â€œhardwiredâ€ to recognize that part as a reward rather than as just another
sensory input. Thus, animals seem to be hardwired to recognize pain and hunger as negative
rewards and pleasure and food intake as positive rewards. Reinforcement has been carefully
studied by animal psychologists for over 60 years.
Rewards were introduced in Chapter 17, where they served to define optimal policies
in Markov decision processes (MDPs). An optimal policy is a policy that maximizes the
expected total reward. The task of reinforcement learning is to use observed rewards to learn
an optimal (or nearly optimal) policy for the environment. Whereas in Chapter 17 the agent
has a complete model of the environment and knows the reward function, here we assume no
830

Section 21.1.

Introduction

831

prior knowledge of either. Imagine playing a new game whose rules you donâ€™t know; after a
hundred or so moves, your opponent announces, â€œYou lose.â€ This is reinforcement learning
in a nutshell.
In many complex domains, reinforcement learning is the only feasible way to train a
program to perform at high levels. For example, in game playing, it is very hard for a human
to provide accurate and consistent evaluations of large numbers of positions, which would be
needed to train an evaluation function directly from examples. Instead, the program can be
told when it has won or lost, and it can use this information to learn an evaluation function
that gives reasonably accurate estimates of the probability of winning from any given position.
Similarly, it is extremely difficult to program an agent to fly a helicopter; yet given appropriate
negative rewards for crashing, wobbling, or deviating from a set course, an agent can learn to
fly by itself.
Reinforcement learning might be considered to encompass all of AI: an agent is placed
in an environment and must learn to behave successfully therein. To keep the chapter manageable, we will concentrate on simple environments and simple agent designs. For the most
part, we will assume a fully observable environment, so that the current state is supplied by
each percept. On the other hand, we will assume that the agent does not know how the environment works or what its actions do, and we will allow for probabilistic action outcomes.
Thus, the agent faces an unknown Markov decision process. We will consider three of the
agent designs first introduced in Chapter 2:

Q-LEARNING
Q-FUNCTION

PASSIVE LEARNING

ACTIVE LEARNING
EXPLORATION

â€¢ A utility-based agent learns a utility function on states and uses it to select actions that
maximize the expected outcome utility.
â€¢ A Q-learning agent learns an action-utility function, or Q-function, giving the expected utility of taking a given action in a given state.
â€¢ A reflex agent learns a policy that maps directly from states to actions.
A utility-based agent must also have a model of the environment in order to make decisions,
because it must know the states to which its actions will lead. For example, in order to make
use of a backgammon evaluation function, a backgammon program must know what its legal
moves are and how they affect the board position. Only in this way can it apply the utility
function to the outcome states. A Q-learning agent, on the other hand, can compare the
expected utilities for its available choices without needing to know their outcomes, so it does
not need a model of the environment. On the other hand, because they do not know where
their actions lead, Q-learning agents cannot look ahead; this can seriously restrict their ability
to learn, as we shall see.
We begin in Section 21.2 with passive learning, where the agentâ€™s policy is fixed and
the task is to learn the utilities of states (or stateâ€“action pairs); this could also involve learning
a model of the environment. Section 21.3 covers active learning, where the agent must also
learn what to do. The principal issue is exploration: an agent must experience as much as
possible of its environment in order to learn how to behave in it. Section 21.4 discusses how
an agent can use inductive learning to learn much faster from its experiences. Section 21.5
covers methods for learning direct policy representations in reflex agents. An understanding
of Markov decision processes (Chapter 17) is essential for this chapter.

832

21.2

Chapter 21.

Reinforcement Learning

PASSIVE R EINFORCEMENT L EARNING
To keep things simple, we start with the case of a passive learning agent using a state-based
representation in a fully observable environment. In passive learning, the agentâ€™s policy Ï€
is fixed: in state s, it always executes the action Ï€(s). Its goal is simply to learn how good
the policy isâ€”that is, to learn the utility function U Ï€ (s). We will use as our example the
4 Ã— 3 world introduced in Chapter 17. Figure 21.1 shows a policy for that world and the
corresponding utilities. Clearly, the passive learning task is similar to the policy evaluation
task, part of the policy iteration algorithm described in Section 17.3. The main difference
is that the passive learning agent does not know the transition model P (s | s, a), which
specifies the probability of reaching state s from state s after doing action a; nor does it
know the reward function R(s), which specifies the reward for each state.

3

+1

3

0.812

2

â€“1

2

0.762

1

0.705

0.655

0.611

0.388

1

2

3

4

1

1

2

3

(a)

4

0.868

0.918

+1

0.660

â€“1

(b)

Figure 21.1 (a) A policy Ï€ for the 4 Ã— 3 world; this policy happens to be optimal with
rewards of R(s) = âˆ’ 0.04 in the nonterminal states and no discounting. (b) The utilities of
the states in the 4 Ã— 3 world, given policy Ï€.

TRIAL

The agent executes a set of trials in the environment using its policy Ï€. In each trial, the
agent starts in state (1,1) and experiences a sequence of state transitions until it reaches one
of the terminal states, (4,2) or (4,3). Its percepts supply both the current state and the reward
received in that state. Typical trials might look like this:
(1, 1)-.04 (1, 2)-.04 (1, 3)-.04 (1, 2)-.04 (1, 3)-.04 (2, 3)-.04 (3, 3)-.04 (4, 3)+1
(1, 1)-.04 (1, 2)-.04 (1, 3)-.04 (2, 3)-.04 (3, 3)-.04 (3, 2)-.04 (3, 3)-.04 (4, 3)+1
(1, 1)-.04 (2, 1)-.04 (3, 1)-.04 (3, 2)-.04 (4, 2)-1 .
Note that each state percept is subscripted with the reward received. The object is to use the
information about rewards to learn the expected utility U Ï€ (s) associated with each nonterminal state s. The utility is defined to be the expected sum of (discounted) rewards obtained if

Section 21.2.

Passive Reinforcement Learning
policy Ï€ is followed. As in Equation (17.2) on page 650, we write
"âˆž
#

Ï€
t
Î³ R(St )
U (s) = E

833

(21.1)

t=0

where R(s) is the reward for a state, St (a random variable) is the state reached at time t when
executing policy Ï€, and S0 = s. We will include a discount factor Î³ in all of our equations,
but for the 4 Ã— 3 world we will set Î³ = 1.

21.2.1 Direct utility estimation
DIRECT UTILITY
ESTIMATION
ADAPTIVE CONTROL
THEORY
REWARD-TO-GO

A simple method for direct utility estimation was invented in the late 1950s in the area of
adaptive control theory by Widrow and Hoff (1960). The idea is that the utility of a state
is the expected total reward from that state onward (called the expected reward-to-go), and
each trial provides a sample of this quantity for each state visited. For example, the first trial
in the set of three given earlier provides a sample total reward of 0.72 for state (1,1), two
samples of 0.76 and 0.84 for (1,2), two samples of 0.80 and 0.88 for (1,3), and so on. Thus,
at the end of each sequence, the algorithm calculates the observed reward-to-go for each state
and updates the estimated utility for that state accordingly, just by keeping a running average
for each state in a table. In the limit of infinitely many trials, the sample average will converge
to the true expectation in Equation (21.1).
It is clear that direct utility estimation is just an instance of supervised learning where
each example has the state as input and the observed reward-to-go as output. This means
that we have reduced reinforcement learning to a standard inductive learning problem, as
discussed in Chapter 18. Section 21.4 discusses the use of more powerful kinds of representations for the utility function. Learning techniques for those representations can be applied
directly to the observed data.
Direct utility estimation succeeds in reducing the reinforcement learning problem to
an inductive learning problem, about which much is known. Unfortunately, it misses a very
important source of information, namely, the fact that the utilities of states are not independent! The utility of each state equals its own reward plus the expected utility of its successor
states. That is, the utility values obey the Bellman equations for a fixed policy (see also
Equation (17.10)):

P (s | s, Ï€(s))U Ï€ (s ) .
(21.2)
U Ï€ (s) = R(s) + Î³
s

By ignoring the connections between states, direct utility estimation misses opportunities for
learning. For example, the second of the three trials given earlier reaches the state (3,2),
which has not previously been visited. The next transition reaches (3,3), which is known
from the first trial to have a high utility. The Bellman equation suggests immediately that
(3,2) is also likely to have a high utility, because it leads to (3,3), but direct utility estimation
learns nothing until the end of the trial. More broadly, we can view direct utility estimation
as searching for U in a hypothesis space that is much larger than it needs to be, in that it
includes many functions that violate the Bellman equations. For this reason, the algorithm
often converges very slowly.

834

Chapter 21.

Reinforcement Learning

function PASSIVE -ADP-AGENT(percept ) returns an action
inputs: percept , a percept indicating the current state s  and reward signal r 
persistent: Ï€, a fixed policy
mdp, an MDP with model P , rewards R, discount Î³
U , a table of utilities, initially empty
Nsa , a table of frequencies for stateâ€“action pairs, initially zero
Ns  |sa , a table of outcome frequencies given stateâ€“action pairs, initially zero
s, a, the previous state and action, initially null
if s  is new then U [s  ] â† r  ; R[s  ] â† r 
if s is not null then
increment Nsa [s, a] and Ns  |sa [s  , s, a]
for each t such that Ns  |sa [t , s, a] is nonzero do
P (t | s, a) â† Ns  |sa [t , s, a] / Nsa [s, a]
U â† P OLICY-E VALUATION(Ï€, U , mdp)
if s  .T ERMINAL ? then s, a â† null else s, a â† s  , Ï€[s  ]
return a
Figure 21.2 A passive reinforcement learning agent based on adaptive dynamic programming. The P OLICY-E VALUATION function solves the fixed-policy Bellman equations, as
described on page 657.

21.2.2 Adaptive dynamic programming
ADAPTIVE DYNAMIC
PROGRAMMING

An adaptive dynamic programming (or ADP) agent takes advantage of the constraints
among the utilities of states by learning the transition model that connects them and solving the corresponding Markov decision process using a dynamic programming method. For
a passive learning agent, this means plugging the learned transition model P (s | s, Ï€(s)) and
the observed rewards R(s) into the Bellman equations (21.2) to calculate the utilities of the
states. As we remarked in our discussion of policy iteration in Chapter 17, these equations
are linear (no maximization involved) so they can be solved using any linear algebra package. Alternatively, we can adopt the approach of modified policy iteration (see page 657),
using a simplified value iteration process to update the utility estimates after each change to
the learned model. Because the model usually changes only slightly with each observation,
the value iteration process can use the previous utility estimates as initial values and should
converge quite quickly.
The process of learning the model itself is easy, because the environment is fully observable. This means that we have a supervised learning task where the input is a stateâ€“action
pair and the output is the resulting state. In the simplest case, we can represent the transition model as a table of probabilities. We keep track of how often each action outcome
occurs and estimate the transition probability P (s | s, a) from the frequency with which s
is reached when executing a in s. For example, in the three trials given on page 832, Right
is executed three times in (1,3) and two out of three times the resulting state is (2,3), so
P ((2, 3) | (1, 3), Right ) is estimated to be 2/3.

Section 21.2.

Passive Reinforcement Learning

835

0.6
(4,3)
(3,3)
(1,3)
(1,1)
(3,2)

0.8
0.6
0.4
0.2

0.5
RMS error in utility

Utility estimates

1

0.4
0.3
0.2
0.1

0

0
0

20

40
60
80
Number of trials

(a)

100

0

20

40
60
Number of trials

80

100

(b)

Figure 21.3 The passive ADP learning curves for the 4Ã—3 world, given the optimal policy
shown in Figure 21.1. (a) The utility estimates for a selected subset of states, as a function
of the number of trials. Notice the large changes occurring around the 78th trialâ€”this is the
first time that the agent falls into the âˆ’1 terminal state at (4,2). (b) The root-mean-square
error (see Appendix A) in the estimate for U (1, 1), averaged over 20 runs of 100 trials each.

BAYESIAN
REINFORCEMENT
LEARNING

The full agent program for a passive ADP agent is shown in Figure 21.2. Its performance on the 4 Ã— 3 world is shown in Figure 21.3. In terms of how quickly its value estimates improve, the ADP agent is limited only by its ability to learn the transition model.
In this sense, it provides a standard against which to measure other reinforcement learning
algorithms. It is, however, intractable for large state spaces. In backgammon, for example, it
would involve solving roughly 1050 equations in 1050 unknowns.
A reader familiar with the Bayesian learning ideas of Chapter 20 will have noticed that
the algorithm in Figure 21.2 is using maximum-likelihood estimation to learn the transition
model; moreover, by choosing a policy based solely on the estimated model it is acting as
if the model were correct. This is not necessarily a good idea! For example, a taxi agent
that didnâ€™t know about how traffic lights might ignore a red light once or twice without no
ill effects and then formulate a policy to ignore red lights from then on. Instead, it might
be a good idea to choose a policy that, while not optimal for the model estimated by maximum likelihood, works reasonably well for the whole range of models that have a reasonable
chance of being the true model. There are two mathematical approaches that have this flavor.
The first approach, Bayesian reinforcement learning, assumes a prior probability
P (h) for each hypothesis h about what the true model is; the posterior probability P (h | e) is
obtained in the usual way by Bayesâ€™ rule given the observations to date. Then, if the agent has
decided to stop learning, the optimal policy is the one that gives the highest expected utility.
Let uÏ€h be the expected utility, averaged over all possible start states, obtained by executing
policy Ï€ in model h. Then we have

P (h | e)uÏ€h .
Ï€ âˆ— = argmax
Ï€

h

836

ROBUST CONTROL
THEORY

Chapter 21.

Reinforcement Learning

In some special cases, this policy can even be computed! If the agent will continue learning
in the future, however, then finding an optimal policy becomes considerably more difficult,
because the agent must consider the effects of future observations on its beliefs about the
transition model. The problem becomes a POMDP whose belief states are distributions over
models. This concept provides an analytical foundation for understanding the exploration
problem described in Section 21.3.
The second approach, derived from robust control theory, allows for a set of possible
models H and defines an optimal robust policy as one that gives the best outcome in the worst
case over H:
Ï€ âˆ— = argmax min uÏ€h .
Ï€

h

Often, the set H will be the set of models that exceed some likelihood threshold on P (h | e),
so the robust and Bayesian approaches are related. Sometimes, the robust solution can be
computed efficiently. There are, moreover, reinforcement learning algorithms that tend to
produce robust solutions, although we do not cover them here.

21.2.3 Temporal-difference learning
Solving the underlying MDP as in the preceding section is not the only way to bring the
Bellman equations to bear on the learning problem. Another way is to use the observed
transitions to adjust the utilities of the observed states so that they agree with the constraint
equations. Consider, for example, the transition from (1,3) to (2,3) in the second trial on
page 832. Suppose that, as a result of the first trial, the utility estimates are U Ï€ (1, 3) = 0.84
and U Ï€ (2, 3) = 0.92. Now, if this transition occurred all the time, we would expect the utilities to obey the equation
U Ï€ (1, 3) = âˆ’0.04 + U Ï€ (2, 3) ,
so U Ï€ (1, 3) would be 0.88. Thus, its current estimate of 0.84 might be a little low and should
be increased. More generally, when a transition occurs from state s to state s , we apply the
following update to U Ï€ (s):
U Ï€ (s) â† U Ï€ (s) + Î±(R(s) + Î³ U Ï€ (s ) âˆ’ U Ï€ (s)) .
TEMPORALDIFFERENCE

(21.3)

Here, Î± is the learning rate parameter. Because this update rule uses the difference in utilities
between successive states, it is often called the temporal-difference, or TD, equation.
All temporal-difference methods work by adjusting the utility estimates towards the
ideal equilibrium that holds locally when the utility estimates are correct. In the case of passive learning, the equilibrium is given by Equation (21.2). Now Equation (21.3) does in fact
cause the agent to reach the equilibrium given by Equation (21.2), but there is some subtlety
involved. First, notice that the update involves only the observed successor s , whereas the
actual equilibrium conditions involve all possible next states. One might think that this causes
an improperly large change in U Ï€ (s) when a very rare transition occurs; but, in fact, because
rare transitions occur only rarely, the average value of U Ï€ (s) will converge to the correct
value. Furthermore, if we change Î± from a fixed parameter to a function that decreases as
the number of times a state has been visited increases, then U Ï€ (s) itself will converge to the

Section 21.2.

Passive Reinforcement Learning

837

function PASSIVE -TD-AGENT(percept) returns an action
inputs: percept , a percept indicating the current state s  and reward signal r 
persistent: Ï€, a fixed policy
U , a table of utilities, initially empty
Ns , a table of frequencies for states, initially zero
s, a, r , the previous state, action, and reward, initially null
if s  is new then U [s  ] â† r 
if s is not null then
increment N s [s]
U [s] â† U [s] + Î±(Ns [s])(r + Î³ U [s  ] âˆ’ U [s])

if s .T ERMINAL ? then s, a, r â† null else s, a, r â† s  , Ï€[s  ], r 
return a
Figure 21.4 A passive reinforcement learning agent that learns utility estimates using temporal differences. The step-size function Î±(n) is chosen to ensure convergence, as described
in the text.

correct value.1 This gives us the agent program shown in Figure 21.4. Figure 21.5 illustrates
the performance of the passive TD agent on the 4 Ã— 3 world. It does not learn quite as fast as
the ADP agent and shows much higher variability, but it is much simpler and requires much
less computation per observation. Notice that TD does not need a transition model to perform
its updates. The environment supplies the connection between neighboring states in the form
of observed transitions.
The ADP approach and the TD approach are actually closely related. Both try to make
local adjustments to the utility estimates in order to make each state â€œagreeâ€ with its successors. One difference is that TD adjusts a state to agree with its observed successor (Equation (21.3)), whereas ADP adjusts the state to agree with all of the successors that might
occur, weighted by their probabilities (Equation (21.2)). This difference disappears when
the effects of TD adjustments are averaged over a large number of transitions, because the
frequency of each successor in the set of transitions is approximately proportional to its probability. A more important difference is that whereas TD makes a single adjustment per observed transition, ADP makes as many as it needs to restore consistency between the utility
estimates U and the environment model P . Although the observed transition makes only a
local change in P , its effects might need to be propagated throughout U . Thus, TD can be
viewed as a crude but efficient first approximation to ADP.
Each adjustment made by ADP could be seen, from the TD point of view, as a result of a â€œpseudoexperienceâ€ generated by simulating the current environment model. It
is possible to extend the TD approach to use an environment model to generate several
pseudoexperiencesâ€”transitions that the TD agent can imagine might happen, given its current
model. For each observed transition, the TD agent can generate a large number of imaginary
1

The technical conditions are given on page 725. In Figure 21.5 we have used Î±(n) = 60/(59 + n), which
satisfies the conditions.

838

Chapter 21.

Reinforcement Learning

0.6
(4,3)
(3,3)
(1,3)
(1,1)
(2,1)

0.8
0.6
0.4
0.2

0.5
RMS error in utility

Utility estimates

1

0.4
0.3
0.2
0.1

0

0
0

100

200
300
400
Number of trials

(a)

500

0

20

40
60
Number of trials

80

100

(b)

Figure 21.5 The TD learning curves for the 4 Ã— 3 world. (a) The utility estimates for a
selected subset of states, as a function of the number of trials. (b) The root-mean-square error
in the estimate for U (1, 1), averaged over 20 runs of 500 trials each. Only the first 100 trials
are shown to enable comparison with Figure 21.3.

PRIORITIZED
SWEEPING

transitions. In this way, the resulting utility estimates will approximate more and more closely
those of ADPâ€”of course, at the expense of increased computation time.
In a similar vein, we can generate more efficient versions of ADP by directly approximating the algorithms for value iteration or policy iteration. Even though the value iteration
algorithm is efficient, it is intractable if we have, say, 10100 states. However, many of the
necessary adjustments to the state values on each iteration will be extremely tiny. One possible approach to generating reasonably good answers quickly is to bound the number of
adjustments made after each observed transition. One can also use a heuristic to rank the possible adjustments so as to carry out only the most significant ones. The prioritized sweeping
heuristic prefers to make adjustments to states whose likely successors have just undergone a
large adjustment in their own utility estimates. Using heuristics like this, approximate ADP
algorithms usually can learn roughly as fast as full ADP, in terms of the number of training sequences, but can be several orders of magnitude more efficient in terms of computation. (See
Exercise 21.3.) This enables them to handle state spaces that are far too large for full ADP.
Approximate ADP algorithms have an additional advantage: in the early stages of learning a
new environment, the environment model P often will be far from correct, so there is little
point in calculating an exact utility function to match it. An approximation algorithm can use
a minimum adjustment size that decreases as the environment model becomes more accurate.
This eliminates the very long value iterations that can occur early in learning due to large
changes in the model.

Section 21.3.

21.3

Active Reinforcement Learning

839

ACTIVE R EINFORCEMENT L EARNING
A passive learning agent has a fixed policy that determines its behavior. An active agent must
decide what actions to take. Let us begin with the adaptive dynamic programming agent and
consider how it must be modified to handle this new freedom.
First, the agent will need to learn a complete model with outcome probabilities for all
actions, rather than just the model for the fixed policy. The simple learning mechanism used
by PASSIVE -ADP-AGENT will do just fine for this. Next, we need to take into account the
fact that the agent has a choice of actions. The utilities it needs to learn are those defined by
the optimal policy; they obey the Bellman equations given on page 652, which we repeat here
for convenience:

P (s | s, a)U (s ) .
(21.4)
U (s) = R(s) + Î³ max
a

s

These equations can be solved to obtain the utility function U using the value iteration or
policy iteration algorithms from Chapter 17. The final issue is what to do at each step. Having
obtained a utility function U that is optimal for the learned model, the agent can extract an
optimal action by one-step look-ahead to maximize the expected utility; alternatively, if it
uses policy iteration, the optimal policy is already available, so it should simply execute the
action the optimal policy recommends. Or should it?

21.3.1 Exploration

GREEDY AGENT

EXPLOITATION
EXPLORATION

Figure 21.6 shows the results of one sequence of trials for an ADP agent that follows the
recommendation of the optimal policy for the learned model at each step. The agent does
not learn the true utilities or the true optimal policy! What happens instead is that, in the
39th trial, it finds a policy that reaches the +1 reward along the lower route via (2,1), (3,1),
(3,2), and (3,3). (See Figure 21.6(b).) After experimenting with minor variations, from the
276th trial onward it sticks to that policy, never learning the utilities of the other states and
never finding the optimal route via (1,2), (1,3), and (2,3). We call this agent the greedy agent.
Repeated experiments show that the greedy agent very seldom converges to the optimal policy
for this environment and sometimes converges to really horrendous policies.
How can it be that choosing the optimal action leads to suboptimal results? The answer
is that the learned model is not the same as the true environment; what is optimal in the
learned model can therefore be suboptimal in the true environment. Unfortunately, the agent
does not know what the true environment is, so it cannot compute the optimal action for the
true environment. What, then, is to be done?
What the greedy agent has overlooked is that actions do more than provide rewards
according to the current learned model; they also contribute to learning the true model by affecting the percepts that are received. By improving the model, the agent will receive greater
rewards in the future. 2 An agent therefore must make a tradeoff between exploitation to
maximize its rewardâ€”as reflected in its current utility estimatesâ€”and exploration to maxi2

Notice the direct analogy to the theory of information value in Chapter 16.

840

Chapter 21.

RMS error, policy loss

2

Reinforcement Learning

3

+1

2

â€“1

RMS error
Policy loss

1.5

1

0.5

1
0
0

50 100 150 200 250 300 350 400 450 500
Number of trials

(a)

1

2

3

4

(b)

Figure 21.6 Performance of a greedy ADP agent that executes the action recommended
by the optimal policy for the learned model. (a) RMS error in the utility estimates averaged
over the nine nonterminal squares. (b) The suboptimal policy to which the greedy agent
converges in this particular sequence of trials.

BANDIT PROBLEM

GLIE

mize its long-term well-being. Pure exploitation risks getting stuck in a rut. Pure exploration
to improve oneâ€™s knowledge is of no use if one never puts that knowledge into practice. In the
real world, one constantly has to decide between continuing in a comfortable existence and
striking out into the unknown in the hopes of discovering a new and better life. With greater
understanding, less exploration is necessary.
Can we be a little more precise than this? Is there an optimal exploration policy? This
question has been studied in depth in the subfield of statistical decision theory that deals with
so-called bandit problems. (See sidebar.)
Although bandit problems are extremely difficult to solve exactly to obtain an optimal
exploration method, it is nonetheless possible to come up with a reasonable scheme that
will eventually lead to optimal behavior by the agent. Technically, any such scheme needs
to be greedy in the limit of infinite exploration, or GLIE. A GLIE scheme must try each
action in each state an unbounded number of times to avoid having a finite probability that
an optimal action is missed because of an unusually bad series of outcomes. An ADP agent
using such a scheme will eventually learn the true environment model. A GLIE scheme must
also eventually become greedy, so that the agentâ€™s actions become optimal with respect to the
learned (and hence the true) model.
There are several GLIE schemes; one of the simplest is to have the agent choose a random action a fraction 1/t of the time and to follow the greedy policy otherwise. While this
does eventually converge to an optimal policy, it can be extremely slow. A more sensible
approach would give some weight to actions that the agent has not tried very often, while
tending to avoid actions that are believed to be of low utility. This can be implemented by
altering the constraint equation (21.4) so that it assigns a higher utility estimate to relatively

Section 21.3.

Active Reinforcement Learning

E XPLORATION

AND BANDITS

In Las Vegas, a one-armed bandit is a slot machine. A gambler can insert a coin,
pull the lever, and collect the winnings (if any). An n-armed bandit has n levers.
The gambler must choose which lever to play on each successive coinâ€”the one
that has paid off best, or maybe one that has not been tried?
The n-armed bandit problem is a formal model for real problems in many vitally important areas, such as deciding on the annual budget for AI research and
development. Each arm corresponds to an action (such as allocating $20 million
for the development of new AI textbooks), and the payoff from pulling the arm corresponds to the benefits obtained from taking the action (immense). Exploration,
whether it is exploration of a new research field or exploration of a new shopping
mall, is risky, is expensive, and has uncertain payoffs; on the other hand, failure to
explore at all means that one never discovers any actions that are worthwhile.
To formulate a bandit problem properly, one must define exactly what is meant
by optimal behavior. Most definitions in the literature assume that the aim is to
maximize the expected total reward obtained over the agentâ€™s lifetime. These definitions require that the expectation be taken over the possible worlds that the agent
could be in, as well as over the possible results of each action sequence in any given
world. Here, a â€œworldâ€ is defined by the transition model P (s | s, a). Thus, in order to act optimally, the agent needs a prior distribution over the possible models.
The resulting optimization problems are usually wildly intractable.
In some casesâ€”for example, when the payoff of each machine is independent
and discounted rewards are usedâ€”it is possible to calculate a Gittins index for
each slot machine (Gittins, 1989). The index is a function only of the number of
times the slot machine has been played and how much it has paid off. The index for
each machine indicates how worthwhile it is to invest more; generally speaking, the
higher the expected return and the higher the uncertainty in the utility of a given
choice, the better. Choosing the machine with the highest index value gives an
optimal exploration policy. Unfortunately, no way has been found to extend Gittins
indices to sequential decision problems.
One can use the theory of n-armed bandits to argue for the reasonableness
of the selection strategy in genetic algorithms. (See Chapter 4.) If you consider
each arm in an n-armed bandit problem to be a possible string of genes, and the
investment of a coin in one arm to be the reproduction of those genes, then it can
be proven that genetic algorithms allocate coins optimally, given an appropriate set
of independence assumptions.

841

842

Chapter 21.

Reinforcement Learning

unexplored stateâ€“action pairs. Essentially, this amounts to an optimistic prior over the possible environments and causes the agent to behave initially as if there were wonderful rewards
scattered all over the place. Let us use U + (s) to denote the optimistic estimate of the utility
(i.e., the expected reward-to-go) of the state s, and let N (s, a) be the number of times action
a has been tried in state s. Suppose we are using value iteration in an ADP learning agent;
then we need to rewrite the update equation (Equation (17.6) on page 652) to incorporate the
optimistic estimate. The following equation does this:



+

+

P (s | s, a)U (s ), N (s, a) .
(21.5)
U (s) â† R(s) + Î³ max f
a

EXPLORATION
FUNCTION

s

Here, f (u, n) is called the exploration function. It determines how greed (preference for
high values of u) is traded off against curiosity (preference for actions that have not been
tried often and have low n). The function f (u, n) should be increasing in u and decreasing
in n. Obviously, there are many possible functions that fit these conditions. One particularly
simple definition is
 +
if n < Ne
R
f (u, n) =
u otherwise
where R+ is an optimistic estimate of the best possible reward obtainable in any state and Ne
is a fixed parameter. This will have the effect of making the agent try each actionâ€“state pair
at least Ne times.
The fact that U + rather than U appears on the right-hand side of Equation (21.5) is
very important. As exploration proceeds, the states and actions near the start state might well
be tried a large number of times. If we used U , the more pessimistic utility estimate, then
the agent would soon become disinclined to explore further afield. The use of U + means
that the benefits of exploration are propagated back from the edges of unexplored regions,
so that actions that lead toward unexplored regions are weighted more highly, rather than
just actions that are themselves unfamiliar. The effect of this exploration policy can be seen
clearly in Figure 21.7, which shows a rapid convergence toward optimal performance, unlike
that of the greedy approach. A very nearly optimal policy is found after just 18 trials. Notice
that the utility estimates themselves do not converge as quickly. This is because the agent
stops exploring the unrewarding parts of the state space fairly soon, visiting them only â€œby
accidentâ€ thereafter. However, it makes perfect sense for the agent not to care about the exact
utilities of states that it knows are undesirable and can be avoided.

21.3.2 Learning an action-utility function
Now that we have an active ADP agent, let us consider how to construct an active temporaldifference learning agent. The most obvious change from the passive case is that the agent
is no longer equipped with a fixed policy, so, if it learns a utility function U , it will need to
learn a model in order to be able to choose an action based on U via one-step look-ahead.
The model acquisition problem for the TD agent is identical to that for the ADP agent. What
of the TD update rule itself? Perhaps surprisingly, the update rule (21.3) remains unchanged.
This might seem odd, for the following reason: Suppose the agent takes a step that normally

Active Reinforcement Learning
2.2

(1,1)
(1,2)
(1,3)
(2,3)
(3,2)
(3,3)
(4,3)

2
Utility estimates

843

1.8
1.6
1.4

1.4
RMS error, policy loss

Section 21.3.

1.2
1
0.8

RMS error
Policy loss

1.2
1
0.8
0.6
0.4
0.2

0.6

0
0

20

40
60
80
Number of trials

100

0

(a)

20

40
60
80
Number of trials

100

(b)

Figure 21.7 Performance of the exploratory ADP agent. using R+ = 2 and Ne = 5. (a)
Utility estimates for selected states over time. (b) The RMS error in utility values and the
associated policy loss.

leads to a good destination, but because of nondeterminism in the environment the agent ends
up in a catastrophic state. The TD update rule will take this as seriously as if the outcome had
been the normal result of the action, whereas one might suppose that, because the outcome
was a fluke, the agent should not worry about it too much. In fact, of course, the unlikely
outcome will occur only infrequently in a large set of training sequences; hence in the long
run its effects will be weighted proportionally to its probability, as we would hope. Once
again, it can be shown that the TD algorithm will converge to the same values as ADP as the
number of training sequences tends to infinity.
There is an alternative TD method, called Q-learning, which learns an action-utility
representation instead of learning utilities. We will use the notation Q(s, a) to denote the
value of doing action a in state s. Q-values are directly related to utility values as follows:
U (s) = max Q(s, a) .

(21.6)

a

MODEL-FREE

Q-functions may seem like just another way of storing utility information, but they have a
very important property: a TD agent that learns a Q-function does not need a model of the
form P (s | s, a), either for learning or for action selection. For this reason, Q-learning is
called a model-free method. As with utilities, we can write a constraint equation that must
hold at equilibrium when the Q-values are correct:

P (s | s, a) max
Q(s , a ) .
(21.7)
Q(s, a) = R(s) + Î³

s

a

As in the ADP learning agent, we can use this equation directly as an update equation for
an iteration process that calculates exact Q-values, given an estimated model. This does,
however, require that a model also be learned, because the equation uses P (s | s, a). The
temporal-difference approach, on the other hand, requires no model of state transitionsâ€”all

844

Chapter 21.

Reinforcement Learning

function Q-L EARNING -AGENT(percept) returns an action
inputs: percept , a percept indicating the current state s  and reward signal r 
persistent: Q , a table of action values indexed by state and action, initially zero
Nsa , a table of frequencies for stateâ€“action pairs, initially zero
s, a, r , the previous state, action, and reward, initially null
if T ERMINAL ?(s) then Q [s, None] â† r 
if s is not null then
increment Nsa [s, a]
Q [s, a] â† Q [s, a] + Î±(Nsa [s, a])(r + Î³ maxa Q [s , a  ] âˆ’ Q [s, a])
s, a, r â† s  , argmaxa f (Q [s  , a  ], Nsa [s  , a ]), r 
return a
Figure 21.8 An exploratory Q-learning agent. It is an active learner that learns the value
Q(s, a) of each action in each situation. It uses the same exploration function f as the exploratory ADP agent, but avoids having to learn the transition model because the Q-value of
a state can be related directly to those of its neighbors.

it needs are the Q values. The update equation for TD Q-learning is
Q(s , a ) âˆ’ Q(s, a)) ,
Q(s, a) â† Q(s, a) + Î±(R(s) + Î³ max

a

SARSA

which is calculated whenever action a is executed in state s leading to state s .
The complete agent design for an exploratory Q-learning agent using TD is shown in
Figure 21.8. Notice that it uses exactly the same exploration function f as that used by the
exploratory ADP agentâ€”hence the need to keep statistics on actions taken (the table N ). If
a simpler exploration policy is usedâ€”say, acting randomly on some fraction of steps, where
the fraction decreases over timeâ€”then we can dispense with the statistics.
Q-learning has a close relative called SARSA (for State-Action-Reward-State-Action).
The update rule for SARSA is very similar to Equation (21.8):
Q(s, a) â† Q(s, a) + Î±(R(s) + Î³ Q(s , a ) âˆ’ Q(s, a)) ,
a

OFF-POLICY
ON-POLICY

(21.8)

s .

(21.9)

is the action actually taken in state
The rule is applied at the end of each
where


s, a, r, s , a quintupletâ€”hence the name. The difference from Q-learning is quite subtle:
whereas Q-learning backs up the best Q-value from the state reached in the observed transition, SARSA waits until an action is actually taken and backs up the Q-value for that action.
Now, for a greedy agent that always takes the action with best Q-value, the two algorithms
are identical. When exploration is happening, however, they differ significantly. Because
Q-learning uses the best Q-value, it pays no attention to the actual policy being followedâ€”it
is an off-policy learning algorithm, whereas SARSA is an on-policy algorithm. Q-learning is
more flexible than SARSA, in the sense that a Q-learning agent can learn how to behave well
even when guided by a random or adversarial exploration policy. On the other hand, SARSA
is more realistic: for example, if the overall policy is even partly controlled by other agents, it
is better to learn a Q-function for what will actually happen rather than what the agent would
like to happen.

Section 21.4.

Generalization in Reinforcement Learning

845

Both Q-learning and SARSA learn the optimal policy for the 4 Ã— 3 world, but do so
at a much slower rate than the ADP agent. This is because the local updates do not enforce
consistency among all the Q-values via the model. The comparison raises a general question:
is it better to learn a model and a utility function or to learn an action-utility function with
no model? In other words, what is the best way to represent the agent function? This is
an issue at the foundations of artificial intelligence. As we stated in Chapter 1, one of the
key historical characteristics of much of AI research is its (often unstated) adherence to the
knowledge-based approach. This amounts to an assumption that the best way to represent
the agent function is to build a representation of some aspects of the environment in which
the agent is situated.
Some researchers, both inside and outside AI, have claimed that the availability of
model-free methods such as Q-learning means that the knowledge-based approach is unnecessary. There is, however, little to go on but intuition. Our intuition, for what itâ€™s worth, is that
as the environment becomes more complex, the advantages of a knowledge-based approach
become more apparent. This is borne out even in games such as chess, checkers (draughts),
and backgammon (see next section), where efforts to learn an evaluation function by means
of a model have met with more success than Q-learning methods.

21.4

G ENERALIZATION IN R EINFORCEMENT L EARNING

FUNCTION
APPROXIMATION

BASIS FUNCTION

So far, we have assumed that the utility functions and Q-functions learned by the agents are
represented in tabular form with one output value for each input tuple. Such an approach
works reasonably well for small state spaces, but the time to convergence and (for ADP) the
time per iteration increase rapidly as the space gets larger. With carefully controlled, approximate ADP methods, it might be possible to handle 10,000 states or more. This suffices for
two-dimensional maze-like environments, but more realistic worlds are out of the question.
Backgammon and chess are tiny subsets of the real world, yet their state spaces contain on
the order of 1020 and 1040 states, respectively. It would be absurd to suppose that one must
visit all these states many times in order to learn how to play the game!
One way to handle such problems is to use function approximation, which simply
means using any sort of representation for the Q-function other than a lookup table. The
representation is viewed as approximate because it might not be the case that the true utility
function or Q-function can be represented in the chosen form. For example, in Chapter 5 we
described an evaluation function for chess that is represented as a weighted linear function
of a set of features (or basis functions) f1 , . . . , fn :
UÌ‚Î¸ (s) = Î¸1 f1 (s) + Î¸2 f2 (s) + Â· Â· Â· + Î¸n fn (s) .
A reinforcement learning algorithm can learn values for the parameters Î¸ = Î¸1 , . . . , Î¸n such
that the evaluation function UÌ‚Î¸ approximates the true utility function. Instead of, say, 1040
values in a table, this function approximator is characterized by, say, n = 20 parametersâ€”
an enormous compression. Although no one knows the true utility function for chess, no
one believes that it can be represented exactly in 20 numbers. If the approximation is good

846

Chapter 21.

Reinforcement Learning

enough, however, the agent might still play excellent chess.3 Function approximation makes
it practical to represent utility functions for very large state spaces, but that is not its principal
benefit. The compression achieved by a function approximator allows the learning agent to
generalize from states it has visited to states it has not visited. That is, the most important
aspect of function approximation is not that it requires less space, but that it allows for inductive generalization over input states. To give you some idea of the power of this effect: by
examining only one in every 1012 of the possible backgammon states, it is possible to learn a
utility function that allows a program to play as well as any human (Tesauro, 1992).
On the flip side, of course, there is the problem that there could fail to be any function
in the chosen hypothesis space that approximates the true utility function sufficiently well.
As in all inductive learning, there is a tradeoff between the size of the hypothesis space and
the time it takes to learn the function. A larger hypothesis space increases the likelihood that
a good approximation can be found, but also means that convergence is likely to be delayed.
Let us begin with the simplest case, which is direct utility estimation. (See Section 21.2.)
With function approximation, this is an instance of supervised learning. For example, suppose we represent the utilities for the 4 Ã— 3 world using a simple linear function. The features
of the squares are just their x and y coordinates, so we have
UÌ‚Î¸ (x, y) = Î¸0 + Î¸1 x + Î¸2 y .
(21.10)
Thus, if (Î¸0 , Î¸1 , Î¸2 ) = (0.5, 0.2, 0.1), then UÌ‚Î¸ (1, 1) = 0.8. Given a collection of trials, we obtain a set of sample values of UÌ‚Î¸ (x, y), and we can find the best fit, in the sense of minimizing
the squared error, using standard linear regression. (See Chapter 18.)
For reinforcement learning, it makes more sense to use an online learning algorithm
that updates the parameters after each trial. Suppose we run a trial and the total reward
obtained starting at (1,1) is 0.4. This suggests that UÌ‚Î¸ (1, 1), currently 0.8, is too large and
must be reduced. How should the parameters be adjusted to achieve this? As with neuralnetwork learning, we write an error function and compute its gradient with respect to the
parameters. If uj (s) is the observed total reward from state s onward in the jth trial, then
the error is defined as (half) the squared difference of the predicted total and the actual total:
Ej (s) = (UÌ‚Î¸ (s) âˆ’ uj (s))2 /2. The rate of change of the error with respect to each parameter
Î¸i is âˆ‚Ej /âˆ‚Î¸i , so to move the parameter in the direction of decreasing the error, we want
âˆ‚ UÌ‚Î¸ (s)
âˆ‚Ej (s)
= Î¸i + Î± (uj (s) âˆ’ UÌ‚Î¸ (s))
.
(21.11)
âˆ‚Î¸i
âˆ‚Î¸i
This is called the Widrowâ€“Hoff rule, or the delta rule, for online least-squares. For the
linear function approximator UÌ‚Î¸ (s) in Equation (21.10), we get three simple update rules:
Î¸0 â† Î¸0 + Î± (uj (s) âˆ’ UÌ‚Î¸ (s)) ,
Î¸i â† Î¸i âˆ’ Î±

WIDROWâ€“HOFF RULE
DELTA RULE

Î¸1 â† Î¸1 + Î± (uj (s) âˆ’ UÌ‚Î¸ (s))x ,
Î¸2 â† Î¸2 + Î± (uj (s) âˆ’ UÌ‚Î¸ (s))y .
3

We do know that the exact utility function can be represented in a page or two of Lisp, Java, or C++. That is,
it can be represented by a program that solves the game exactly every time it is called. We are interested only in
function approximators that use a reasonable amount of computation. It might in fact be better to learn a very
simple function approximator and combine it with a certain amount of look-ahead search. The tradeoffs involved
are currently not well understood.

Section 21.4.

Generalization in Reinforcement Learning

847

We can apply these rules to the example where UÌ‚Î¸ (1, 1) is 0.8 and uj (1, 1) is 0.4. Î¸0 , Î¸1 ,
and Î¸2 are all decreased by 0.4Î±, which reduces the error for (1,1). Notice that changing the
parameters Î¸ in response to an observed transition between two states also changes the values
of UÌ‚Î¸ for every other state! This is what we mean by saying that function approximation
allows a reinforcement learner to generalize from its experiences.
We expect that the agent will learn faster if it uses a function approximator, provided
that the hypothesis space is not too large, but includes some functions that are a reasonably
good fit to the true utility function. Exercise 21.5 asks you to evaluate the performance of
direct utility estimation, both with and without function approximation. The improvement in
the 4 Ã— 3 world is noticeable but not dramatic, because this is a very small state space to begin
with. The improvement is much greater in a 10 Ã— 10 world with a +1 reward at (10,10). This
world is well suited for a linear utility function because the true utility function is smooth
and nearly linear. (See Exercise 21.8.) If we put the +1 reward at (5,5), the true utility is
more like a pyramid and the function approximator in Equation (21.10) will fail miserably.
All is not lost, however! Remember that what matters for linear function approximation
is that the function be linear in the parametersâ€”the features themselves can be arbitrary
nonlinear
functions of the state variables. Hence, we can include a term such as Î¸3 f3 (x, y) =


Î¸3 (x âˆ’ xg )2 + (y âˆ’ yg )2 that measures the distance to the goal.
We can apply these ideas equally well to temporal-difference learners. All we need do is
adjust the parameters to try to reduce the temporal difference between successive states. The
new versions of the TD and Q-learning equations (21.3 on page 836 and 21.8 on page 844)
are given by
Î¸i â† Î¸i + Î± [R(s) + Î³ UÌ‚Î¸ (s ) âˆ’ UÌ‚Î¸ (s)]

âˆ‚ UÌ‚Î¸ (s)
âˆ‚Î¸i

(21.12)

for utilities and
âˆ‚ QÌ‚Î¸ (s, a)
(21.13)
a
âˆ‚Î¸i
for Q-values. For passive TD learning, the update rule can be shown to converge to the closest
possible4 approximation to the true function when the function approximator is linear in the
parameters. With active learning and nonlinear functions such as neural networks, all bets
are off: There are some very simple cases in which the parameters can go off to infinity
even though there are good solutions in the hypothesis space. There are more sophisticated
algorithms that can avoid these problems, but at present reinforcement learning with general
function approximators remains a delicate art.
Function approximation can also be very helpful for learning a model of the environment. Remember that learning a model for an observable environment is a supervised learning problem, because the next percept gives the outcome state. Any of the supervised learning
methods in Chapter 18 can be used, with suitable adjustments for the fact that we need to predict a complete state description rather than just a Boolean classification or a single real value.
For a partially observable environment, the learning problem is much more difficult. If we
know what the hidden variables are and how they are causally related to each other and to the
QÌ‚Î¸ (s , a ) âˆ’ QÌ‚Î¸ (s, a)]
Î¸i â† Î¸i + Î± [R(s) + Î³ max


4

The definition of distance between utility functions is rather technical; see Tsitsiklis and Van Roy (1997).

848

Chapter 21.

Reinforcement Learning

observable variables, then we can fix the structure of a dynamic Bayesian network and use the
EM algorithm to learn the parameters, as was described in Chapter 20. Inventing the hidden
variables and learning the model structure are still open problems. Some practical examples
are described in Section 21.6.

21.5

P OLICY S EARCH

POLICY SEARCH

The final approach we will consider for reinforcement learning problems is called policy
search. In some ways, policy search is the simplest of all the methods in this chapter: the
idea is to keep twiddling the policy as long as its performance improves, then stop.
Let us begin with the policies themselves. Remember that a policy Ï€ is a function that
maps states to actions. We are interested primarily in parameterized representations of Ï€ that
have far fewer parameters than there are states in the state space (just as in the preceding
section). For example, we could represent Ï€ by a collection of parameterized Q-functions,
one for each action, and take the action with the highest predicted value:
Ï€(s) = max QÌ‚Î¸ (s, a) .
a

STOCHASTIC POLICY
SOFTMAX FUNCTION

(21.14)

Each Q-function could be a linear function of the parameters Î¸, as in Equation (21.10),
or it could be a nonlinear function such as a neural network. Policy search will then adjust the parameters Î¸ to improve the policy. Notice that if the policy is represented by Qfunctions, then policy search results in a process that learns Q-functions. This process is
not the same as Q-learning! In Q-learning with function approximation, the algorithm finds
a value of Î¸ such that QÌ‚Î¸ is â€œcloseâ€ to Qâˆ— , the optimal Q-function. Policy search, on the
other hand, finds a value of Î¸ that results in good performance; the values found by the two
methods may differ very substantially. (For example, the approximate Q-function defined
by QÌ‚Î¸ (s, a) = Qâˆ— (s, a)/10 gives optimal performance, even though it is not at all close to
Qâˆ— .) Another clear instance of the difference is the case where Ï€(s) is calculated using, say,
depth-10 look-ahead search with an approximate utility function UÌ‚Î¸ . A value of Î¸ that gives
good results may be a long way from making UÌ‚Î¸ resemble the true utility function.
One problem with policy representations of the kind given in Equation (21.14) is that
the policy is a discontinuous function of the parameters when the actions are discrete. (For a
continuous action space, the policy can be a smooth function of the parameters.) That is, there
will be values of Î¸ such that an infinitesimal change in Î¸ causes the policy to switch from one
action to another. This means that the value of the policy may also change discontinuously,
which makes gradient-based search difficult. For this reason, policy search methods often use
a stochastic policy representation Ï€Î¸ (s, a), which specifies the probability of selecting action
a in state s. One popular representation is the softmax function:


Ï€Î¸ (s, a) = eQÌ‚Î¸ (s,a) /
eQÌ‚Î¸ (s,a ) .
a

Softmax becomes nearly deterministic if one action is much better than the others, but it
always gives a differentiable function of Î¸; hence, the value of the policy (which depends in

Section 21.5.

POLICY VALUE

POLICY GRADIENT

Policy Search

849

a continuous fashion on the action selection probabilities) is a differentiable function of Î¸.
Softmax is a generalization of the logistic function (page 725) to multiple variables.
Now let us look at methods for improving the policy. We start with the simplest case: a
deterministic policy and a deterministic environment. Let Ï(Î¸) be the policy value, i.e., the
expected reward-to-go when Ï€Î¸ is executed. If we can derive an expression for Ï(Î¸) in closed
form, then we have a standard optimization problem, as described in Chapter 4. We can follow
the policy gradient vector âˆ‡Î¸ Ï(Î¸) provided Ï(Î¸) is differentiable. Alternatively, if Ï(Î¸) is
not available in closed form, we can evaluate Ï€Î¸ simply by executing it and observing the
accumulated reward. We can follow the empirical gradient by hill climbingâ€”i.e., evaluating
the change in policy value for small increments in each parameter. With the usual caveats,
this process will converge to a local optimum in policy space.
When the environment (or the policy) is stochastic, things get more difficult. Suppose
we are trying to do hill climbing, which requires comparing Ï(Î¸) and Ï(Î¸ + Î”Î¸) for some
small Î”Î¸. The problem is that the total reward on each trial may vary widely, so estimates
of the policy value from a small number of trials will be quite unreliable; trying to compare
two such estimates will be even more unreliable. One solution is simply to run lots of trials,
measuring the sample variance and using it to determine that enough trials have been run
to get a reliable indication of the direction of improvement for Ï(Î¸). Unfortunately, this is
impractical for many real problems where each trial may be expensive, time-consuming, and
perhaps even dangerous.
For the case of a stochastic policy Ï€Î¸ (s, a), it is possible to obtain an unbiased estimate
of the gradient at Î¸, âˆ‡Î¸ Ï(Î¸), directly from the results of trials executed at Î¸. For simplicity,
we will derive this estimate for the simple case of a nonsequential environment in which the
reward R(a) is obtained immediately after doing action a in the start state s0 . In this case,
the policy value is just the expected value of the reward, and we have


âˆ‡Î¸ Ï(Î¸) = âˆ‡Î¸
Ï€Î¸ (s0 , a)R(a) =
(âˆ‡Î¸ Ï€Î¸ (s0 , a))R(a) .
a

a

Now we perform a simple trick so that this summation can be approximated by samples
generated from the probability distribution defined by Ï€Î¸ (s0 , a). Suppose that we have N
trials in all and the action taken on the jth trial is aj . Then
âˆ‡Î¸ Ï(Î¸) =



Ï€Î¸ (s0 , a) Â·

a

N
1  (âˆ‡Î¸ Ï€Î¸ (s0 , aj ))R(aj )
(âˆ‡Î¸ Ï€Î¸ (s0 , a))R(a)
â‰ˆ
.
Ï€Î¸ (s0 , a)
N
Ï€Î¸ (s0 , aj )
j =1

Thus, the true gradient of the policy value is approximated by a sum of terms involving
the gradient of the action-selection probability in each trial. For the sequential case, this
generalizes to
N
1  (âˆ‡Î¸ Ï€Î¸ (s, aj ))Rj (s)
âˆ‡Î¸ Ï(Î¸) â‰ˆ
N
Ï€Î¸ (s, aj )
j=1

for each state s visited, where aj is executed in s on the jth trial and Rj (s) is the total
reward received from state s onwards in the jth trial. The resulting algorithm is called
R EINFORCE (Williams, 1992); it is usually much more effective than hill climbing using
lots of trials at each value of Î¸. It is still much slower than necessary, however.

850

CORRELATED
SAMPLING

21.6

Chapter 21.

Reinforcement Learning

Consider the following task: given two blackjack5 programs, determine which is best.
One way to do this is to have each play against a standard â€œdealerâ€ for a certain number of
hands and then to measure their respective winnings. The problem with this, as we have seen,
is that the winnings of each program fluctuate widely depending on whether it receives good
or bad cards. An obvious solution is to generate a certain number of hands in advance and
have each program play the same set of hands. In this way, we eliminate the measurement
error due to differences in the cards received. This idea, called correlated sampling, underlies a policy-search algorithm called P EGASUS (Ng and Jordan, 2000). The algorithm is
applicable to domains for which a simulator is available so that the â€œrandomâ€ outcomes of
actions can be repeated. The algorithm works by generating in advance N sequences of random numbers, each of which can be used to run a trial of any policy. Policy search is carried
out by evaluating each candidate policy using the same set of random sequences to determine
the action outcomes. It can be shown that the number of random sequences required to ensure
that the value of every policy is well estimated depends only on the complexity of the policy
space, and not at all on the complexity of the underlying domain.

A PPLICATIONS OF R EINFORCEMENT L EARNING
We now turn to examples of large-scale applications of reinforcement learning. We consider
applications in game playing, where the transition model is known and the goal is to learn the
utility function, and in robotics, where the model is usually unknown.

21.6.1 Applications to game playing
The first significant application of reinforcement learning was also the first significant learning program of any kindâ€”the checkers program written by Arthur Samuel (1959, 1967).
Samuel first used a weighted linear function for the evaluation of positions, using up to 16
terms at any one time. He applied a version of Equation (21.12) to update the weights. There
were some significant differences, however, between his program and current methods. First,
he updated the weights using the difference between the current state and the backed-up value
generated by full look-ahead in the search tree. This works fine, because it amounts to viewing the state space at a different granularity. A second difference was that the program did
not use any observed rewards! That is, the values of terminal states reached in self-play were
ignored. This means that it is theoretically possible for Samuelâ€™s program not to converge, or
to converge on a strategy designed to lose rather than to win. He managed to avoid this fate
by insisting that the weight for material advantage should always be positive. Remarkably,
this was sufficient to direct the program into areas of weight space corresponding to good
checkers play.
Gerry Tesauroâ€™s backgammon program TD-G AMMON (1992) forcefully illustrates the
potential of reinforcement learning techniques. In earlier work (Tesauro and Sejnowski,
1989), Tesauro tried learning a neural network representation of Q(s, a) directly from ex5

Also known as twenty-one or pontoon.

Section 21.6.

Applications of Reinforcement Learning

851

Î¸
x
Figure 21.9 Setup for the problem of balancing a long pole on top of a moving cart. The
cart can be jerked left or right by a controller that observes x, Î¸, xÌ‡, and Î¸Ì‡.

amples of moves labeled with relative values by a human expert. This approach proved
extremely tedious for the expert. It resulted in a program, called N EUROGAMMON , that was
strong by computer standards, but not competitive with human experts. The TD-G AMMON
project was an attempt to learn from self-play alone. The only reward signal was given at
the end of each game. The evaluation function was represented by a fully connected neural
network with a single hidden layer containing 40 nodes. Simply by repeated application of
Equation (21.12), TD-G AMMON learned to play considerably better than N EUROGAMMON,
even though the input representation contained just the raw board position with no computed
features. This took about 200,000 training games and two weeks of computer time. Although
that may seem like a lot of games, it is only a vanishingly small fraction of the state space.
When precomputed features were added to the input representation, a network with 80 hidden
nodes was able, after 300,000 training games, to reach a standard of play comparable to that
of the top three human players worldwide. Kit Woolsey, a top player and analyst, said that
â€œThere is no question in my mind that its positional judgment is far better than mine.â€

21.6.2 Application to robot control
CARTâ€“POLE
INVERTED
PENDULUM

BANG-BANG
CONTROL

The setup for the famous cartâ€“pole balancing problem, also known as the inverted pendulum, is shown in Figure 21.9. The problem is to control the position x of the cart so that
the pole stays roughly upright (Î¸ â‰ˆ Ï€/2), while staying within the limits of the cart track
as shown. Several thousand papers in reinforcement learning and control theory have been
published on this seemingly simple problem. The cartâ€“pole problem differs from the problems described earlier in that the state variables x, Î¸, xÌ‡, and Î¸Ì‡ are continuous. The actions are
usually discrete: jerk left or jerk right, the so-called bang-bang control regime.
The earliest work on learning for this problem was carried out by Michie and Chambers (1968). Their B OXES algorithm was able to balance the pole for over an hour after only
about 30 trials. Moreover, unlike many subsequent systems, B OXES was implemented with a

852

Chapter 21.

Reinforcement Learning

real cart and pole, not a simulation. The algorithm first discretized the four-dimensional state
space into boxesâ€”hence the name. It then ran trials until the pole fell over or the cart hit the
end of the track. Negative reinforcement was associated with the final action in the final box
and then propagated back through the sequence. It was found that the discretization caused
some problems when the apparatus was initialized in a position different from those used in
training, suggesting that generalization was not perfect. Improved generalization and faster
learning can be obtained using an algorithm that adaptively partitions the state space according to the observed variation in the reward, or by using a continuous-state, nonlinear function
approximator such as a neural network. Nowadays, balancing a triple inverted pendulum is a
common exerciseâ€”a feat far beyond the capabilities of most humans.
Still more impressive is the application of reinforcement learning to helicopter flight
(Figure 21.10). This work has generally used policy search (Bagnell and Schneider, 2001)
as well as the P EGASUS algorithm with simulation based on a learned transition model (Ng
et al., 2004). Further details are given in Chapter 25.

Figure 21.10 Superimposed time-lapse images of an autonomous helicopter performing
a very difficult â€œnose-in circleâ€ maneuver. The helicopter is under the control of a policy
developed by the P EGASUS policy-search algorithm. A simulator model was developed by
observing the effects of various control manipulations on the real helicopter; then the algorithm was run on the simulator model overnight. A variety of controllers were developed for
different maneuvers. In all cases, performance far exceeded that of an expert human pilot
using remote control. (Image courtesy of Andrew Ng.)

Section 21.7.

21.7

Summary

853

S UMMARY
This chapter has examined the reinforcement learning problem: how an agent can become
proficient in an unknown environment, given only its percepts and occasional rewards. Reinforcement learning can be viewed as a microcosm for the entire AI problem, but it is studied
in a number of simplified settings to facilitate progress. The major points are:
â€¢ The overall agent design dictates the kind of information that must be learned. The
three main designs we covered were the model-based design, using a model P and a
utility function U ; the model-free design, using an action-utility function Q ; and the
reflex design, using a policy Ï€.
â€¢ Utilities can be learned using three approaches:
1. Direct utility estimation uses the total observed reward-to-go for a given state as
direct evidence for learning its utility.
2. Adaptive dynamic programming (ADP) learns a model and a reward function
from observations and then uses value or policy iteration to obtain the utilities or
an optimal policy. ADP makes optimal use of the local constraints on utilities of
states imposed through the neighborhood structure of the environment.
3. Temporal-difference (TD) methods update utility estimates to match those of successor states. They can be viewed as simple approximations to the ADP approach
that can learn without requiring a transition model. Using a learned model to generate pseudoexperiences can, however, result in faster learning.
â€¢ Action-utility functions, or Q-functions, can be learned by an ADP approach or a TD
approach. With TD, Q-learning requires no model in either the learning or actionselection phase. This simplifies the learning problem but potentially restricts the ability
to learn in complex environments, because the agent cannot simulate the results of
possible courses of action.
â€¢ When the learning agent is responsible for selecting actions while it learns, it must
trade off the estimated value of those actions against the potential for learning useful
new information. An exact solution of the exploration problem is infeasible, but some
simple heuristics do a reasonable job.
â€¢ In large state spaces, reinforcement learning algorithms must use an approximate functional representation in order to generalize over states. The temporal-difference signal
can be used directly to update parameters in representations such as neural networks.
â€¢ Policy-search methods operate directly on a representation of the policy, attempting
to improve it based on observed performance. The variation in the performance in a
stochastic domain is a serious problem; for simulated domains this can be overcome by
fixing the randomness in advance.
Because of its potential for eliminating hand coding of control strategies, reinforcement learning continues to be one of the most active areas of machine learning research. Applications
in robotics promise to be particularly valuable; these will require methods for handling con-

854

Chapter 21.

Reinforcement Learning

tinuous, high-dimensional, partially observable environments in which successful behaviors
may consist of thousands or even millions of primitive actions.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Turing (1948, 1950) proposed the reinforcement-learning approach, although he was not convinced of its effectiveness, writing, â€œthe use of punishments and rewards can at best be a part
of the teaching process.â€ Arthur Samuelâ€™s work (1959) was probably the earliest successful
machine learning research. Although this work was informal and had a number of flaws,
it contained most of the modern ideas in reinforcement learning, including temporal differencing and function approximation. Around the same time, researchers in adaptive control
theory (Widrow and Hoff, 1960), building on work by Hebb (1949), were training simple networks using the delta rule. (This early connection between neural networks and reinforcement
learning may have led to the persistent misperception that the latter is a subfield of the former.) The cartâ€“pole work of Michie and Chambers (1968) can also be seen as a reinforcement
learning method with a function approximator. The psychological literature on reinforcement
learning is much older; Hilgard and Bower (1975) provide a good survey. Direct evidence for
the operation of reinforcement learning in animals has been provided by investigations into
the foraging behavior of bees; there is a clear neural correlate of the reward signal in the form
of a large neuron mapping from the nectar intake sensors directly to the motor cortex (Montague et al., 1995). Research using single-cell recording suggests that the dopamine system
in primate brains implements something resembling value function learning (Schultz et al.,
1997). The neuroscience text by Dayan and Abbott (2001) describes possible neural implementations of temporal-difference learning, while Dayan and Niv (2008) survey the latest
evidence from neuroscientific and behavioral experiments.
The connection between reinforcement learning and Markov decision processes was
first made by Werbos (1977), but the development of reinforcement learning in AI stems
from work at the University of Massachusetts in the early 1980s (Barto et al., 1981). The
paper by Sutton (1988) provides a good historical overview. Equation (21.3) in this chapter
is a special case for Î» = 0 of Suttonâ€™s general TD(Î») algorithm. TD(Î») updates the utility
values of all states in a sequence leading up to each transition by an amount that drops off as
Î»t for states t steps in the past. TD(1) is identical to the Widrowâ€“Hoff or delta rule. Boyan
(2002), building on work by Bradtke and Barto (1996), argues that TD(Î») and related algorithms make inefficient use of experiences; essentially, they are online regression algorithms
that converge much more slowly than offline regression. His LSTD (least-squares temporal
differencing) algorithm is an online algorithm for passive reinforcement learning that gives
the same results as offline regression. Least-squares policy iteration, or LSPI (Lagoudakis
and Parr, 2003), combines this idea with the policy iteration algorithm, yielding a robust,
statistically efficient, model-free algorithm for learning policies.
The combination of temporal-difference learning with the model-based generation of
simulated experiences was proposed in Suttonâ€™s DYNA architecture (Sutton, 1990). The idea
of prioritized sweeping was introduced independently by Moore and Atkeson (1993) and

Bibliographical and Historical Notes

CMAC

855

Peng and Williams (1993). Q-learning was developed in Watkinsâ€™s Ph.D. thesis (1989), while
SARSA appeared in a technical report by Rummery and Niranjan (1994).
Bandit problems, which model the problem of exploration for nonsequential decisions,
are analyzed in depth by Berry and Fristedt (1985). Optimal exploration strategies for several
settings are obtainable using the technique called Gittins indices (Gittins, 1989). A variety of exploration methods for sequential decision problems are discussed by Barto et al.
(1995). Kearns and Singh (1998) and Brafman and Tennenholtz (2000) describe algorithms
that explore unknown environments and are guaranteed to converge on near-optimal policies
in polynomial time. Bayesian reinforcement learning (Dearden et al., 1998, 1999) provides
another angle on both model uncertainty and exploration.
Function approximation in reinforcement learning goes back to the work of Samuel,
who used both linear and nonlinear evaluation functions and also used feature-selection methods to reduce the feature space. Later methods include the CMAC (Cerebellar Model Articulation Controller) (Albus, 1975), which is essentially a sum of overlapping local kernel
functions, and the associative neural networks of Barto et al. (1983). Neural networks are
currently the most popular form of function approximator. The best-known application is
TD-Gammon (Tesauro, 1992, 1995), which was discussed in the chapter. One significant
problem exhibited by neural-network-based TD learners is that they tend to forget earlier experiences, especially those in parts of the state space that are avoided once competence is
achieved. This can result in catastrophic failure if such circumstances reappear. Function approximation based on instance-based learning can avoid this problem (Ormoneit and Sen,
2002; Forbes, 2002).
The convergence of reinforcement learning algorithms using function approximation is
an extremely technical subject. Results for TD learning have been progressively strengthened for the case of linear function approximators (Sutton, 1988; Dayan, 1992; Tsitsiklis and
Van Roy, 1997), but several examples of divergence have been presented for nonlinear functions (see Tsitsiklis and Van Roy, 1997, for a discussion). Papavassiliou and Russell (1999)
describe a new type of reinforcement learning that converges with any form of function approximator, provided that a best-fit approximation can be found for the observed data.
Policy search methods were brought to the fore by Williams (1992), who developed the
R EINFORCE family of algorithms. Later work by Marbach and Tsitsiklis (1998), Sutton et al.
(2000), and Baxter and Bartlett (2000) strengthened and generalized the convergence results
for policy search. The method of correlated sampling for comparing different configurations
of a system was described formally by Kahn and Marshall (1953), but seems to have been
known long before that. Its use in reinforcement learning is due to Van Roy (1998) and Ng
and Jordan (2000); the latter paper also introduced the P EGASUS algorithm and proved its
formal properties.
As we mentioned in the chapter, the performance of a stochastic policy is a continuous function of its parameters, which helps with gradient-based search methods. This is not
the only benefit: Jaakkola et al. (1995) argue that stochastic policies actually work better
than deterministic policies in partially observable environments, if both are limited to acting based on the current percept. (One reason is that the stochastic policy is less likely to
get â€œstuckâ€ because of some unseen hindrance.) Now, in Chapter 17 we pointed out that

856

REWARD SHAPING
PSEUDOREWARD

HIERARCHICAL
REINFORCEMENT
LEARNING

PARTIAL PROGRAM

Chapter 21.

Reinforcement Learning

optimal policies in partially observable MDPs are deterministic functions of the belief state
rather than the current percept, so we would expect still better results by keeping track of the
belief state using the filtering methods of Chapter 15. Unfortunately, belief-state space is
high-dimensional and continuous, and effective algorithms have not yet been developed for
reinforcement learning with belief states.
Real-world environments also exhibit enormous complexity in terms of the number
of primitive actions required to achieve significant reward. For example, a robot playing
soccer might make a hundred thousand individual leg motions before scoring a goal. One
common method, used originally in animal training, is called reward shaping. This involves
supplying the agent with additional rewards, called pseudorewards, for â€œmaking progress.â€
For example, in soccer the real reward is for scoring a goal, but pseudorewards might be
given for making contact with the ball or for kicking it toward the goal. Such rewards can
speed up learning enormously and are simple to provide, but there is a risk that the agent
will learn to maximize the pseudorewards rather than the true rewards; for example, standing
next to the ball and â€œvibratingâ€ causes many contacts with the ball. Ng et al. (1999) show
that the agent will still learn the optimal policy provided that the pseudoreward F (s, a, s )
satisfies F (s, a, s ) = Î³Î¦(s ) âˆ’ Î¦(s), where Î¦ is an arbitrary function of the state. Î¦ can be
constructed to reflect any desirable aspects of the state, such as achievement of subgoals or
distance to a goal state.
The generation of complex behaviors can also be facilitated by hierarchical reinforcement learning methods, which attempt to solve problems at multiple levels of abstractionâ€”
much like the HTN planning methods of Chapter 11. For example, â€œscoring a goalâ€ can be
broken down into â€œobtain possession,â€ â€œdribble towards the goal,â€ and â€œshoot;â€ and each of
these can be broken down further into lower-level motor behaviors. The fundamental result
in this area is due to Forestier and Varaiya (1978), who proved that lower-level behaviors
of arbitrary complexity can be treated just like primitive actions (albeit ones that can take
varying amounts of time) from the point of view of the higher-level behavior that invokes
them. Current approaches (Parr and Russell, 1998; Dietterich, 2000; Sutton et al., 2000;
Andre and Russell, 2002) build on this result to develop methods for supplying an agent
with a partial program that constrains the agentâ€™s behavior to have a particular hierarchical
structure. The partial-programming language for agent programs extends an ordinary programming language by adding primitives for unspecified choices that must be filled in by
learning. Reinforcement learning is then applied to learn the best behavior consistent with
the partial program. The combination of function approximation, shaping, and hierarchical
reinforcement learning has been shown to solve large-scale problemsâ€”for example, policies
that execute for 104 steps in state spaces of 10100 states with branching factors of 1030 (Marthi
et al., 2005). One key result (Dietterich, 2000) is that the hierarchical structure provides a
natural additive decomposition of the overall utility function into terms that depend on small
subsets of the variables defining the state space. This is somewhat analogous to the representation theorems underlying the conciseness of Bayes nets (Chapter 14).
The topic of distributed and multiagent reinforcement learning was not touched upon in
the chapter but is of great current interest. In distributed RL, the aim is to devise methods by
which multiple, coordinated agents learn to optimize a common utility function. For example,

Bibliographical and Historical Notes
SUBAGENT

APPRENTICESHIP
LEARNING

INVERSE
REINFORCEMENT
LEARNING

RELATIONAL
REINFORCEMENT
LEARNING

857

can we devise methods whereby separate subagents for robot navigation and robot obstacle
avoidance could cooperatively achieve a combined control system that is globally optimal?
Some basic results in this direction have been obtained (Guestrin et al., 2002; Russell and
Zimdars, 2003). The basic idea is that each subagent learns its own Q-function from its
own stream of rewards. For example, a robot-navigation component can receive rewards for
making progress towards the goal, while the obstacle-avoidance component receives negative
rewards for every collision. Each global decision maximizes the sum of Q-functions and the
whole process converges to globally optimal solutions.
Multiagent RL is distinguished from distributed RL by the presence of agents who
cannot coordinate their actions (except by explicit communicative acts) and who may not
share the same utility function. Thus, multiagent RL deals with sequential game-theoretic
problems or Markov games, as defined in Chapter 17. The consequent requirement for randomized policies is not a significant complication, as we saw on page 848. What does cause
problems is the fact that, while an agent is learning to defeat its opponentâ€™s policy, the opponent is changing its policy to defeat the agent. Thus, the environment is nonstationary
(see page 568). Littman (1994) noted this difficulty when introducing the first RL algorithms
for zero-sum Markov games. Hu and Wellman (2003) present a Q-learning algorithm for
general-sum games that converges when the Nash equilibrium is unique; when there are multiple equilibria, the notion of convergence is not so easy to define (Shoham et al., 2004).
Sometimes the reward function is not easy to define. Consider the task of driving a car.
There are extreme states (such as crashing the car) that clearly should have a large penalty.
But beyond that, it is difficult to be precise about the reward function. However, it is easy
enough for a human to drive for a while and then tell a robot â€œdo it like that.â€ The robot then
has the task of apprenticeship learning; learning from an example of the task done right,
without explicit rewards. Ng et al. (2004) and Coates et al. (2009) show how this technique
works for learning to fly a helicopter; see Figure 25.25 on page 1002 for an example of the
acrobatics the resulting policy is capable of. Russell (1998) describes the task of inverse
reinforcement learningâ€”figuring out what the reward function must be from an example
path through that state space. This is useful as a part of apprenticeship learning, or as a part
of doing scienceâ€”we can understand an animal or robot by working backwards from what it
does to what its reward function must be.
This chapter has dealt only with atomic statesâ€”all the agent knows about a state is the
set of available actions and the utilities of the resulting states (or of state-action pairs). But
it is also possible to apply reinforcement learning to structured representations rather than
atomic ones; this is called relational reinforcement learning (Tadepalli et al., 2004).
The survey by Kaelbling et al. (1996) provides a good entry point to the literature. The
text by Sutton and Barto (1998), two of the fieldâ€™s pioneers, focuses on architectures and algorithms, showing how reinforcement learning weaves together the ideas of learning, planning,
and acting. The somewhat more technical work by Bertsekas and Tsitsiklis (1996) gives a
rigorous grounding in the theory of dynamic programming and stochastic convergence. Reinforcement learning papers are published frequently in Machine Learning, in the Journal of
Machine Learning Research, and in the International Conferences on Machine Learning and
the Neural Information Processing Systems meetings.

858

Chapter 21.

Reinforcement Learning

E XERCISES
21.1 Implement a passive learning agent in a simple environment, such as the 4 Ã— 3 world.
For the case of an initially unknown environment model, compare the learning performance
of the direct utility estimation, TD, and ADP algorithms. Do the comparison for the optimal
policy and for several random policies. For which do the utility estimates converge faster?
What happens when the size of the environment is increased? (Try environments with and
without obstacles.)
21.2 Chapter 17 defined a proper policy for an MDP as one that is guaranteed to reach a
terminal state. Show that it is possible for a passive ADP agent to learn a transition model
for which its policy Ï€ is improper even if Ï€ is proper for the true MDP; with such models,
the P OLICY-E VALUATION step may fail if Î³ = 1. Show that this problem cannot arise if
P OLICY-E VALUATION is applied to the learned model only at the end of a trial.
21.3 Starting with the passive ADP agent, modify it to use an approximate ADP algorithm
as discussed in the text. Do this in two steps:
a. Implement a priority queue for adjustments to the utility estimates. Whenever a state is
adjusted, all of its predecessors also become candidates for adjustment and should be
added to the queue. The queue is initialized with the state from which the most recent
transition took place. Allow only a fixed number of adjustments.
b. Experiment with various heuristics for ordering the priority queue, examining their effect on learning rates and computation time.
21.4

Write out the parameter update equations for TD learning with
	
UÌ‚ (x, y) = Î¸0 + Î¸1 x + Î¸2 y + Î¸3 (x âˆ’ xg )2 + (y âˆ’ yg )2 .

21.5 Implement an exploring reinforcement learning agent that uses direct utility estimation. Make two versionsâ€”one with a tabular representation and one using the function approximator in Equation (21.10). Compare their performance in three environments:
a. The 4 Ã— 3 world described in the chapter.
b. A 10 Ã— 10 world with no obstacles and a +1 reward at (10,10).
c. A 10 Ã— 10 world with no obstacles and a +1 reward at (5,5).
21.6 Devise suitable features for reinforcement learning in stochastic grid worlds (generalizations of the 4 Ã— 3 world) that contain multiple obstacles and multiple terminal states with
rewards of +1 or âˆ’1.
21.7 Extend the standard game-playing environment (Chapter 5) to incorporate a reward
signal. Put two reinforcement learning agents into the environment (they may, of course,
share the agent program) and have them play against each other. Apply the generalized TD
update rule (Equation (21.12)) to update the evaluation function. You might wish to start with
a simple linear weighted evaluation function and a simple game, such as tic-tac-toe.

Exercises

859
21.8 Compute the true utility function and the best linear approximation in x and y (as in
Equation (21.10)) for the following environments:
a.
b.
c.
d.
e.

A 10 Ã— 10 world with a single +1 terminal state at (10,10).
As in (a), but add a âˆ’1 terminal state at (10,1).
As in (b), but add obstacles in 10 randomly selected squares.
As in (b), but place a wall stretching from (5,2) to (5,9).
As in (a), but with the terminal state at (5,5).

The actions are deterministic moves in the four directions. In each case, compare the results
using three-dimensional plots. For each environment, propose additional features (besides x
and y) that would improve the approximation and show the results.
21.9 Implement the R EINFORCE and P EGASUS algorithms and apply them to the 4 Ã— 3
world, using a policy family of your own choosing. Comment on the results.
21.10 Is reinforcement learning an appropriate abstract model for evolution? What connection exists, if any, between hardwired reward signals and evolutionary fitness?

22

NATURAL LANGUAGE
PROCESSING

In which we see how to make use of the copious knowledge that is expressed in
natural language.

KNOWLEDGE
ACQUISITION

LANGUAGE MODEL

22.1

LANGUAGE

GRAMMAR
SEMANTICS

Homo sapiens is set apart from other species by the capacity for language. Somewhere around
100,000 years ago, humans learned how to speak, and about 7,000 years ago learned to write.
Although chimpanzees, dolphins, and other animals have shown vocabularies of hundreds of
signs, only humans can reliably communicate an unbounded number of qualitatively different
messages on any topic using discrete signs.
Of course, there are other attributes that are uniquely human: no other species wears
clothes, creates representational art, or watches three hours of television a day. But when
Alan Turing proposed his Test (see Section 1.1.1), he based it on language, not art or TV.
There are two main reasons why we want our computer agents to be able to process natural
languages: first, to communicate with humans, a topic we take up in Chapter 23, and second,
to acquire information from written language, the focus of this chapter.
There are over a trillion pages of information on the Web, almost all of it in natural
language. An agent that wants to do knowledge acquisition needs to understand (at least
partially) the ambiguous, messy languages that humans use. We examine the problem from
the point of view of specific information-seeking tasks: text classification, information retrieval, and information extraction. One common factor in addressing these tasks is the use of
language models: models that predict the probability distribution of language expressions.

L ANGUAGE M ODELS
Formal languages, such as the programming languages Java or Python, have precisely defined
language models. A language can be defined as a set of strings; â€œprint(2 + 2)â€ is a
legal program in the language Python, whereas â€œ2)+(2 printâ€ is not. Since there are an
infinite number of legal programs, they cannot be enumerated; instead they are specified by a
set of rules called a grammar. Formal languages also have rules that define the meaning or
semantics of a program; for example, the rules say that the â€œmeaningâ€ of â€œ2 + 2â€ is 4, and
the meaning of â€œ1/0â€ is that an error is signaled.
860

Section 22.1.

AMBIGUITY

Language Models

861

Natural languages, such as English or Spanish, cannot be characterized as a definitive
set of sentences. Everyone agrees that â€œNot to be invited is sadâ€ is a sentence of English,
but people disagree on the grammaticality of â€œTo be not invited is sad.â€ Therefore, it is more
fruitful to define a natural language model as a probability distribution over sentences rather
than a definitive set. That is, rather than asking if a string of words is or is not a member of
the set defining the language, we instead ask for P (S = words )â€”what is the probability that
a random sentence would be words.
Natural languages are also ambiguous. â€œHe saw her duckâ€ can mean either that he saw
a waterfowl belonging to her, or that he saw her move to evade something. Thus, again, we
cannot speak of a single meaning for a sentence, but rather of a probability distribution over
possible meanings.
Finally, natural languages are difficult to deal with because they are very large, and
constantly changing. Thus, our language models are, at best, an approximation. We start
with the simplest possible approximations and move up from there.

22.1.1 N-gram character models
CHARACTERS

N -GRAM MODEL

Ultimately, a written text is composed of charactersâ€”letters, digits, punctuation, and spaces
in English (and more exotic characters in some other languages). Thus, one of the simplest
language models is a probability distribution over sequences of characters. As in Chapter 15,
we write P (c1:N ) for the probability of a sequence of N characters, c1 through cN . In one
Web collection, P (â€œtheâ€) = 0.027 and P (â€œzgqâ€) = 0.000000002. A sequence of written symbols of length n is called an n-gram (from the Greek root for writing or letters), with special
case â€œunigramâ€ for 1-gram, â€œbigramâ€ for 2-gram, and â€œtrigramâ€ for 3-gram. A model of the
probability distribution of n-letter sequences is thus called an n-gram model. (But be careful: we can have n-gram models over sequences of words, syllables, or other units; not just
over characters.)
An n-gram model is defined as a Markov chain of order n âˆ’ 1. Recall from page 568
that in a Markov chain the probability of character ci depends only on the immediately preceding characters, not on any other characters. So in a trigram model (Markov chain of
order 2) we have
P (ci | c1:iâˆ’1 ) = P (ci | ciâˆ’2:iâˆ’1 ) .
We can define the probability of a sequence of characters P (c1:N ) under the trigram model
by first factoring with the chain rule and then using the Markov assumption:
P (c1:N ) =

N

i=1

CORPUS

P (ci | c1:iâˆ’1 ) =

N


P (ci | ciâˆ’2:iâˆ’1 ) .

i=1

For a trigram character model in a language with 100 characters, P(Ci |Ciâˆ’2:iâˆ’1 ) has a million
entries, and can be accurately estimated by counting character sequences in a body of text of
10 million characters or more. We call a body of text a corpus (plural corpora), from the
Latin word for body.

862

LANGUAGE
IDENTIFICATION

Chapter

22.

Natural Language Processing

What can we do with n-gram character models? One task for which they are well suited
is language identification: given a text, determine what natural language it is written in. This
is a relatively easy task; even with short texts such as â€œHello, worldâ€ or â€œWie geht es dir,â€ it
is easy to identify the first as English and the second as German. Computer systems identify
languages with greater than 99% accuracy; occasionally, closely related languages, such as
Swedish and Norwegian, are confused.
One approach to language identification is to first build a trigram character model of
each candidate language, P (ci | ciâˆ’2:iâˆ’1 , ), where the variable  ranges over languages. For
each  the model is built by counting trigrams in a corpus of that language. (About 100,000
characters of each language are needed.) That gives us a model of P(Text | Language), but
we want to select the most probable language given the text, so we apply Bayesâ€™ rule followed
by the Markov assumption to get the most probable language:
âˆ— = argmax P ( | c1:N )


= argmax P ()P (c1:N | )


= argmax P ()


N


P (ci | ciâˆ’2:iâˆ’1 , )

i=1

The trigram model can be learned from a corpus, but what about the prior probability P ()?
We may have some estimate of these values; for example, if we are selecting a random Web
page we know that English is the most likely language and that the probability of Macedonian
will be less than 1%. The exact number we select for these priors is not critical because the
trigram model usually selects one language that is several orders of magnitude more probable
than any other.
Other tasks for character models include spelling correction, genre classification, and
named-entity recognition. Genre classification means deciding if a text is a news story, a
legal document, a scientific article, etc. While many features help make this classification,
counts of punctuation and other character n-gram features go a long way (Kessler et al.,
1997). Named-entity recognition is the task of finding names of things in a document and
deciding what class they belong to. For example, in the text â€œMr. Sopersteen was prescribed
aciphex,â€ we should recognize that â€œMr. Sopersteenâ€ is the name of a person and â€œaciphexâ€ is
the name of a drug. Character-level models are good for this task because they can associate
the character sequence â€œex â€ (â€œexâ€ followed by a space) with a drug name and â€œsteen â€ with
a person name, and thereby identify words that they have never seen before.

22.1.2 Smoothing n-gram models
The major complication of n-gram models is that the training corpus provides only an estimate of the true probability distribution. For common character sequences such as â€œ thâ€ any
English corpus will give a good estimate: about 1.5% of all trigrams. On the other hand, â€œ htâ€
is very uncommonâ€”no dictionary words start with ht. It is likely that the sequence would
have a count of zero in a training corpus of standard English. Does that mean we should assign P (â€œ thâ€) = 0? If we did, then the text â€œThe program issues an http requestâ€ would have

Section 22.1.

SMOOTHING

BACKOFF MODEL
LINEAR
INTERPOLATION
SMOOTHING

Language Models

863

an English probability of zero, which seems wrong. We have a problem in generalization: we
want our language models to generalize well to texts they havenâ€™t seen yet. Just because we
have never seen â€œ httpâ€ before does not mean that our model should claim that it is impossible. Thus, we will adjust our language model so that sequences that have a count of zero in
the training corpus will be assigned a small nonzero probability (and the other counts will be
adjusted downward slightly so that the probability still sums to 1). The process od adjusting
the probability of low-frequency counts is called smoothing.
The simplest type of smoothing was suggested by Pierre-Simon Laplace in the 18th century: he said that, in the lack of further information, if a random Boolean variable X has been
false in all n observations so far then the estimate for P (X = true) should be 1/(n + 2). That
is, he assumes that with two more trials, one might be true and one false. Laplace smoothing
(also called add-one smoothing) is a step in the right direction, but performs relatively poorly.
A better approach is a backoff model, in which we start by estimating n-gram counts, but for
any particular sequence that has a low (or zero) count, we back off to (n âˆ’ 1)-grams. Linear
interpolation smoothing is a backoff model that combines trigram, bigram, and unigram
models by linear interpolation. It defines the probability estimate as
P*(ci |ciâˆ’2:iâˆ’1 ) = Î»3 P (ci |ciâˆ’2:iâˆ’1 ) + Î»2 P (ci |ciâˆ’1 ) + Î»1 P (ci ) ,
where Î»3 + Î»2 + Î»1 = 1. The parameter values Î»i can be fixed, or they can be trained with
an expectationâ€“maximization algorithm. It is also possible to have the values of Î»i depend
on the counts: if we have a high count of trigrams, then we weigh them relatively more; if
only a low count, then we put more weight on the bigram and unigram models. One camp of
researchers has developed ever more sophisticated smoothing models, while the other camp
suggests gathering a larger corpus so that even simple smoothing models work well. Both are
getting at the same goal: reducing the variance in the language model.
One complication: note that the expression P (ci | ciâˆ’2:iâˆ’1 ) asks for P (c1 | c-1:0 ) when
i = 1, but there are no characters before c1 . We can introduce artificial characters, for
example, defining c0 to be a space character or a special â€œbegin textâ€ character. Or we can
fall back on lower-order Markov models, in effect defining c-1:0 to be the empty sequence
and thus P (c1 | c-1:0 ) = P (c1 ).

22.1.3 Model evaluation

PERPLEXITY

With so many possible n-gram modelsâ€”unigram, bigram, trigram, interpolated smoothing
with different values of Î», etc.â€”how do we know what model to choose? We can evaluate a
model with cross-validation. Split the corpus into a training corpus and a validation corpus.
Determine the parameters of the model from the training data. Then evaluate the model on
the validation corpus.
The evaluation can be a task-specific metric, such as measuring accuracy on language
identification. Alternatively we can have a task-independent model of language quality: calculate the probability assigned to the validation corpus by the model; the higher the probability the better. This metric is inconvenient because the probability of a large corpus will
be a very small number, and floating-point underflow becomes an issue. A different way of
describing the probability of a sequence is with a measure called perplexity, defined as

864

Chapter

22.

Natural Language Processing

1

Perplexity(c1:N ) = P (c1:N )âˆ’ N .
Perplexity can be thought of as the reciprocal of probability, normalized by sequence length.
It can also be thought of as the weighted average branching factor of a model. Suppose there
are 100 characters in our language, and our model says they are all equally likely. Then for
a sequence of any length, the perplexity will be 100. If some characters are more likely than
others, and the model reflects that, then the model will have a perplexity less than 100.

22.1.4 N-gram word models
VOCABULARY

OUT OF
VOCABULARY

Now we turn to n-gram models over words rather than characters. All the same mechanism
applies equally to word and character models. The main difference is that the vocabularyâ€”
the set of symbols that make up the corpus and the modelâ€”is larger. There are only about
100 characters in most languages, and sometimes we build character models that are even
more restrictive, for example by treating â€œAâ€ and â€œaâ€ as the same symbol or by treating all
punctuation as the same symbol. But with word models we have at least tens of thousands of
symbols, and sometimes millions. The wide range is because it is not clear what constitutes a
word. In English a sequence of letters surrounded by spaces is a word, but in some languages,
like Chinese, words are not separated by spaces, and even in English many decisions must be
made to have a clear policy on word boundaries: how many words are in â€œneâ€™er-do-wellâ€? Or
in â€œ(Tel:1-800-960-5660x123)â€?
Word n-gram models need to deal with out of vocabulary words. With character models, we didnâ€™t have to worry about someone inventing a new letter of the alphabet.1 But
with word models there is always the chance of a new word that was not seen in the training
corpus, so we need to model that explicitly in our language model. This can be done by
adding just one new word to the vocabulary: <UNK>, standing for the unknown word. We
can estimate n-gram counts for <UNK> by this trick: go through the training corpus, and
the first time any individual word appears it is previously unknown, so replace it with the
symbol <UNK>. All subsequent appearances of the word remain unchanged. Then compute
n-gram counts for the corpus as usual, treating <UNK> just like any other word. Then when
an unknown word appears in a test set, we look up its probability under <UNK>. Sometimes
multiple unknown-word symbols are used, for different classes. For example, any string of
digits might be replaced with <NUM>, or any email address with <EMAIL>.
To get a feeling for what word models can do, we built unigram, bigram, and trigram
models over the words in this book and then randomly sampled sequences of words from the
models. The results are
Unigram: logical are as are confusion a may right tries agent goal the was . . .
Bigram: systems are very similar computational approach would be represented . . .
Trigram: planning and scheduling are integrated the success of naive bayes model is . . .
Even with this small sample, it should be clear that the unigram model is a poor approximation
of either English or the content of an AI textbook, and that the bigram and trigram models are
1

With the possible exception of the groundbreaking work of T. Geisel (1955).

Section 22.2.

Text Classification

865

much better. The models agree with this assessment: the perplexity was 891 for the unigram
model, 142 for the bigram model and 91 for the trigram model.
With the basics of n-gram modelsâ€”both character- and word-basedâ€”established, we
can turn now to some language tasks.

22.2

T EXT C LASSIFICATION

TEXT
CLASSIFICATION

SPAM DETECTION

We now consider in depth the task of text classification, also known as categorization: given
a text of some kind, decide which of a predefined set of classes it belongs to. Language identification and genre classification are examples of text classification, as is sentiment analysis
(classifying a movie or product review as positive or negative) and spam detection (classifying an email message as spam or not-spam). Since â€œnot-spamâ€ is awkward, researchers have
coined the term ham for not-spam. We can treat spam detection as a problem in supervised
learning. A training set is readily available: the positive (spam) examples are in my spam
folder, the negative (ham) examples are in my inbox. Here is an excerpt:
Spam: Wholesale Fashion Watches -57% today. Designer watches for cheap ...
Spam: You can buy ViagraFr$1.85 All Medications at unbeatable prices! ...
Spam: WE CAN TREAT ANYTHING YOU SUFFER FROM JUST TRUST US ...
Spam: Sta.rt earn*ing the salary yo,u d-eserve by oâ€™btaining the prope,r credeâ€™ntials!
Ham: The practical significance of hypertree width in identifying more ...
Ham: Abstract: We will motivate the problem of social identity clustering: ...
Ham: Good to see you my friend. Hey Peter, It was good to hear from you. ...
Ham: PDS implies convexity of the resulting optimization problem (Kernel Ridge ...

From this excerpt we can start to get an idea of what might be good features to include in
the supervised learning model. Word n-grams such as â€œfor cheapâ€ and â€œYou can buyâ€ seem
to be indicators of spam (although they would have a nonzero probability in ham as well).
Character-level features also seem important: spam is more likely to be all uppercase and to
have punctuation embedded in words. Apparently the spammers thought that the word bigram
â€œyou deserveâ€ would be too indicative of spam, and thus wrote â€œyo,u d-eserveâ€ instead. A
character model should detect this. We could either create a full character n-gram model
of spam and ham, or we could handcraft features such as â€œnumber of punctuation marks
embedded in words.â€
Note that we have two complementary ways of talking about classification. In the
language-modeling approach, we define one n-gram language model for P(Message | spam)
by training on the spam folder, and one model for P(Message | ham) by training on the inbox.
Then we can classify a new message with an application of Bayesâ€™ rule:
argmax
câˆˆ{spam,ham}

P (c | message) =

argmax

P (message | c) P (c) .

câˆˆ{spam,ham}

where P (c) is estimated just by counting the total number of spam and ham messages. This
approach works well for spam detection, just as it did for language identification.

866

BAG OF WORDS

FEATURE SELECTION

Chapter

22.

Natural Language Processing

In the machine-learning approach we represent the message as a set of feature/value
pairs and apply a classification algorithm h to the feature vector X. We can make the
language-modeling and machine-learning approaches compatible by thinking of the n-grams
as features. This is easiest to see with a unigram model. The features are the words in the
vocabulary: â€œa,â€ â€œaardvark,â€ . . ., and the values are the number of times each word appears
in the message. That makes the feature vector large and sparse. If there are 100,000 words in
the language model, then the feature vector has length 100,000, but for a short email message
almost all the features will have count zero. This unigram representation has been called the
bag of words model. You can think of the model as putting the words of the training corpus
in a bag and then selecting words one at a time. The notion of order of the words is lost; a
unigram model gives the same probability to any permutation of a text. Higher-order n-gram
models maintain some local notion of word order.
With bigrams and trigrams the number of features is squared or cubed, and we can add
in other, non-n-gram features: the time the message was sent, whether a URL or an image
is part of the message, an ID number for the sender of the message, the senderâ€™s number of
previous spam and ham messages, and so on. The choice of features is the most important part
of creating a good spam detectorâ€”more important than the choice of algorithm for processing
the features. In part this is because there is a lot of training data, so if we can propose a
feature, the data can accurately determine if it is good or not. It is necessary to constantly
update features, because spam detection is an adversarial task; the spammers modify their
spam in response to the spam detectorâ€™s changes.
It can be expensive to run algorithms on a very large feature vector, so often a process
of feature selection is used to keep only the features that best discriminate between spam and
ham. For example, the bigram â€œof theâ€ is frequent in English, and may be equally frequent in
spam and ham, so there is no sense in counting it. Often the top hundred or so features do a
good job of discriminating between classes.
Once we have chosen a set of features, we can apply any of the supervised learning
techniques we have seen; popular ones for text categorization include k-nearest-neighbors,
support vector machines, decision trees, naive Bayes, and logistic regression. All of these
have been applied to spam detection, usually with accuracy in the 98%â€“99% range. With a
carefully designed feature set, accuracy can exceed 99.9%.

22.2.1 Classification by data compression
DATA COMPRESSION

Another way to think about classification is as a problem in data compression. A lossless
compression algorithm takes a sequence of symbols, detects repeated patterns in it, and writes
a description of the sequence that is more compact than the original. For example, the text
â€œ0.142857142857142857â€ might be compressed to â€œ0.[142857]*3.â€ Compression algorithms
work by building dictionaries of subsequences of the text, and then referring to entries in the
dictionary. The example here had only one dictionary entry, â€œ142857.â€
In effect, compression algorithms are creating a language model. The LZW algorithm
in particular directly models a maximum-entropy probability distribution. To do classification
by compression, we first lump together all the spam training messages and compress them as

Section 22.3.

Information Retrieval

867

a unit. We do the same for the ham. Then when given a new message to classify, we append
it to the spam messages and compress the result. We also append it to the ham and compress
that. Whichever class compresses betterâ€”adds the fewer number of additional bytes for the
new messageâ€”is the predicted class. The idea is that a spam message will tend to share
dictionary entries with other spam messages and thus will compress better when appended to
a collection that already contains the spam dictionary.
Experiments with compression-based classification on some of the standard corpora for
text classificationâ€”the 20-Newsgroups data set, the Reuters-10 Corpora, the Industry Sector
corporaâ€”indicate that whereas running off-the-shelf compression algorithms like gzip, RAR,
and LZW can be quite slow, their accuracy is comparable to traditional classification algorithms. This is interesting in its own right, and also serves to point out that there is promise
for algorithms that use character n-grams directly with no preprocessing of the text or feature
selection: they seem to be captiring some real patterns.

22.3

I NFORMATION R ETRIEVAL

INFORMATION
RETRIEVAL

IR

Information retrieval is the task of finding documents that are relevant to a userâ€™s need for
information. The best-known examples of information retrieval systems are search engines
on the World Wide Web. A Web user can type a query such as [AI book]2 into a search engine
and see a list of relevant pages. In this section, we will see how such systems are built. An
information retrieval (henceforth IR) system can be characterized by
1. A corpus of documents. Each system must decide what it wants to treat as a document:
a paragraph, a page, or a multipage text.
2. Queries posed in a query language. A query specifies what the user wants to know.
The query language can be just a list of words, such as [AI book]; or it can specify
a phrase of words that must be adjacent, as in [â€œAI bookâ€]; it can contain Boolean
operators as in [AI AND book]; it can include non-Boolean operators such as [AI NEAR
book] or [AI book site:www.aaai.org].
3. A result set. This is the subset of documents that the IR system judges to be relevant to
the query. By relevant, we mean likely to be of use to the person who posed the query,
for the particular information need expressed in the query.
4. A presentation of the result set. This can be as simple as a ranked list of document
titles or as complex as a rotating color map of the result set projected onto a threedimensional space, rendered as a two-dimensional display.

QUERY LANGUAGE

RESULT SET
RELEVANT

PRESENTATION

BOOLEAN KEYWORD
MODEL

The earliest IR systems worked on a Boolean keyword model. Each word in the document
collection is treated as a Boolean feature that is true of a document if the word occurs in the
document and false if it does not. So the feature â€œretrievalâ€ is true for the current chapter
but false for Chapter 15. The query language is the language of Boolean expressions over
2

We denote a search query as [query]. Square brackets are used rather than quotation marks so that we can
distinguish the query [â€œtwo wordsâ€] from [two words].

868

Chapter

22.

Natural Language Processing

features. A document is relevant only if the expression evaluates to true. For example, the
query [information AND retrieval] is true for the current chapter and false for Chapter 15.
This model has the advantage of being simple to explain and implement. However,
it has some disadvantages. First, the degree of relevance of a document is a single bit, so
there is no guidance as to how to order the relevant documents for presentation. Second,
Boolean expressions are unfamiliar to users who are not programmers or logicians. Users
find it unintuitive that when they want to know about farming in the states of Kansas and
Nebraska they need to issue the query [farming (Kansas OR Nebraska)]. Third, it can be
hard to formulate an appropriate query, even for a skilled user. Suppose we try [information
AND retrieval AND models AND optimization] and get an empty result set. We could try
[information OR retrieval OR models OR optimization], but if that returns too many results,
it is difficult to know what to try next.

22.3.1 IR scoring functions
BM25 SCORING
FUNCTION

Most IR systems have abandoned the Boolean model and use models based on the statistics of
word counts. We describe the BM25 scoring function, which comes from the Okapi project
of Stephen Robertson and Karen Sparck Jones at Londonâ€™s City College, and has been used
in search engines such as the open-source Lucene project.
A scoring function takes a document and a query and returns a numeric score; the most
relevant documents have the highest scores. In the BM25 function, the score is a linear
weighted combination of scores for each of the words that make up the query. Three factors
affect the weight of a query term: First, the frequency with which a query term appears in
a document (also known as TF for term frequency). For the query [farming in Kansas],
documents that mention â€œfarmingâ€ frequently will have higher scores. Second, the inverse
document frequency of the term, or IDF . The word â€œinâ€ appears in almost every document,
so it has a high document frequency, and thus a low inverse document frequency, and thus it
is not as important to the query as â€œfarmingâ€ or â€œKansas.â€ Third, the length of the document.
A million-word document will probably mention all the query words, but may not actually be
about the query. A short document that mentions all the words is a much better candidate.
The BM25 function takes all three of these into account. We assume we have created
an index of the N documents in the corpus so that we can look up TF (qi , dj ), the count of
the number of times word qi appears in document dj . We also assume a table of document
frequency counts, DF (qi ), that gives the number of documents that contain the word qi .
Then, given a document dj and a query consisting of the words q1:N , we have
BM 25(dj , q1:N ) =

N

i=1

IDF (qi ) Â·

TF (qi , dj ) Â· (k + 1)
TF (qi , dj ) + k Â· (1 âˆ’ b + b Â·

|dj |
L )

,

where |dj | is the length
 of document dj in words, and L is the average document length
in the corpus: L =
i |di |/N . We have two parameters, k and b, that can be tuned by
cross-validation; typical values are k = 2.0 and b = 0.75. IDF (qi ) is the inverse document

Section 22.3.

INDEX
HIT LIST

Information Retrieval

869

frequency of word qi , given by
N âˆ’ DF (qi ) + 0.5
.
IDF (qi ) = log
DF (qi ) + 0.5
Of course, it would be impractical to apply the BM25 scoring function to every document
in the corpus. Instead, systems create an index ahead of time that lists, for each vocabulary
word, the documents that contain the word. This is called the hit list for the word. Then when
given a query, we intersect the hit lists of the query words and only score the documents in
the intersection.

22.3.2 IR system evaluation

PRECISION

RECALL

How do we know whether an IR system is performing well? We undertake an experiment in
which the system is given a set of queries and the result sets are scored with respect to human
relevance judgments. Traditionally, there have been two measures used in the scoring: recall
and precision. We explain them with the help of an example. Imagine that an IR system has
returned a result set for a single query, for which we know which documents are and are not
relevant, out of a corpus of 100 documents. The document counts in each category are given
in the following table:
In result set Not in result set
Relevant
30
20
Not relevant
10
40
Precision measures the proportion of documents in the result set that are actually relevant.
In our example, the precision is 30/(30 + 10) = .75. The false positive rate is 1 âˆ’ .75 = .25.
Recall measures the proportion of all the relevant documents in the collection that are in
the result set. In our example, recall is 30/(30 + 20) = .60. The false negative rate is 1 âˆ’
.60 = .40. In a very large document collection, such as the World Wide Web, recall is difficult
to compute, because there is no easy way to examine every page on the Web for relevance.
All we can do is either estimate recall by sampling or ignore recall completely and just judge
precision. In the case of a Web search engine, there may be thousands of documents in the
result set, so it makes more sense to measure precision for several different sizes, such as
â€œP@10â€ (precision in the top 10 results) or â€œP@50,â€ rather than to estimate precision in the
entire result set.
It is possible to trade off precision against recall by varying the size of the result set
returned. In the extreme, a system that returns every document in the document collection is
guaranteed a recall of 100%, but will have low precision. Alternately, a system could return
a single document and have low recall, but a decent chance at 100% precision. A summary
of both measures is the F1 score, a single number that is the harmonic mean of precision and
recall, 2P R/(P + R).

22.3.3 IR refinements
There are many possible refinements to the system described here, and indeed Web search
engines are continually updating their algorithms as they discover new approaches and as the
Web grows and changes.

870

CASE FOLDING
STEMMING

SYNONYM

METADATA

LINKS

Chapter

22.

Natural Language Processing

One common refinement is a better model of the effect of document length on relevance.
Singhal et al. (1996) observed that simple document length normalization schemes tend to
favor short documents too much and long documents not enough. They propose a pivoted
document length normalization scheme; the idea is that the pivot is the document length at
which the old-style normalization is correct; documents shorter than that get a boost and
longer ones get a penalty.
The BM25 scoring function uses a word model that treats all words as completely independent, but we know that some words are correlated: â€œcouchâ€ is closely related to both
â€œcouchesâ€ and â€œsofa.â€ Many IR systems attempt to account for these correlations.
For example, if the query is [couch], it would be a shame to exclude from the result set
those documents that mention â€œCOUCHâ€ or â€œcouchesâ€ but not â€œcouch.â€ Most IR systems
do case folding of â€œCOUCHâ€ to â€œcouch,â€ and some use a stemming algorithm to reduce
â€œcouchesâ€ to the stem form â€œcouch,â€ both in the query and the documents. This typically
yields a small increase in recall (on the order of 2% for English). However, it can harm
precision. For example, stemming â€œstockingâ€ to â€œstockâ€ will tend to decrease precision for
queries about either foot coverings or financial instruments, although it could improve recall
for queries about warehousing. Stemming algorithms based on rules (e.g., remove â€œ-ingâ€)
cannot avoid this problem, but algorithms based on dictionaries (donâ€™t remove â€œ-ingâ€ if the
word is already listed in the dictionary) can. While stemming has a small effect in English,
it is more important in other languages. In German, for example, it is not uncommon to
see words like â€œLebensversicherungsgesellschaftsangestellterâ€ (life insurance company employee). Languages such as Finnish, Turkish, Inuit, and Yupik have recursive morphological
rules that in principle generate words of unbounded length.
The next step is to recognize synonyms, such as â€œsofaâ€ for â€œcouch.â€ As with stemming,
this has the potential for small gains in recall, but can hurt precision. A user who gives the
query [Tim Couch] wants to see results about the football player, not sofas. The problem is
that â€œlanguages abhor absolute synonyms just as nature abhors a vacuumâ€ (Cruse, 1986). That
is, anytime there are two words that mean the same thing, speakers of the language conspire
to evolve the meanings to remove the confusion. Related words that are not synonyms also
play an important role in rankingâ€”terms like â€œleatherâ€, â€œwooden,â€ or â€œmodernâ€ can serve
to confirm that the document really is about â€œcouch.â€ Synonyms and related words can be
found in dictionaries or by looking for correlations in documents or in queriesâ€”if we find
that many users who ask the query [new sofa] follow it up with the query [new couch], we
can in the future alter [new sofa] to be [new sofa OR new couch].
As a final refinement, IR can be improved by considering metadataâ€”data outside of
the text of the document. Examples include human-supplied keywords and publication data.
On the Web, hypertext links between documents are a crucial source of information.

22.3.4 The PageRank algorithm
PAGERANK

PageRank3 was one of the two original ideas that set Googleâ€™s search apart from other Web
search engines when it was introduced in 1997. (The other innovation was the use of anchor
3

The name stands both for Web pages and for coinventor Larry Page (Brin and Page, 1998).

Section 22.3.

Information Retrieval

871

function HITS(query) returns pages with hub and authority numbers
pages â† E XPAND -PAGES(R ELEVANT-PAGES(query))
for each p in pages do
p.AUTHORITY â† 1
p.H UB â† 1
repeat until convergence do
for each p in pages do

p.AUTHORITY
 â† i I NLINKi (p).H UB
p.H UB â† i O UTLINKi (p).AUTHORITY
N ORMALIZE(pages)
return pages
Figure 22.1 The HITS algorithm for computing hubs and authorities with respect to a
query. R ELEVANT-PAGES fetches the pages that match the query, and E XPAND -PAGES adds
in every page that links to or is linked from one of the relevant pages. N ORMALIZE divides
each pageâ€™s score by the sum of the squares of all pagesâ€™ scores (separately for both the
authority and hubs scores).

textâ€”the underlined text in a hyperlinkâ€”to index a page, even though the anchor text was on
a different page than the one being indexed.) PageRank was invented to solve the problem of
the tyranny of TF scores: if the query is [IBM], how do we make sure that IBMâ€™s home page,
ibm.com, is the first result, even if another page mentions the term â€œIBMâ€ more frequently?
The idea is that ibm.com has many in-links (links to the page), so it should be ranked higher:
each in-link is a vote for the quality of the linked-to page. But if we only counted in-links,
then it would be possible for a Web spammer to create a network of pages and have them all
point to a page of his choosing, increasing the score of that page. Therefore, the PageRank
algorithm is designed to weight links from high-quality sites more heavily. What is a highquality site? One that is linked to by other high-quality sites. The definition is recursive, but
we will see that the recursion bottoms out properly. The PageRank for a page p is defined as:
P R(p) =

 P R(in i )
1âˆ’d
+d
,
N
C(in i )
i

RANDOM SURFER
MODEL

where P R(p) is the PageRank of page p, N is the total number of pages in the corpus, in i
are the pages that link in to p, and C(in i ) is the count of the total number of out-links on
page in i . The constant d is a damping factor. It can be understood through the random
surfer model: imagine a Web surfer who starts at some random page and begins exploring.
With probability d (weâ€™ll assume d = 0.85) the surfer clicks on one of the links on the page
(choosing uniformly among them), and with probability 1 âˆ’ d she gets bored with the page
and restarts on a random page anywhere on the Web. The PageRank of page p is then the
probability that the random surfer will be at page p at any point in time. PageRank can be
computed by an iterative procedure: start with all pages having P R(p) = 1, and iterate the
algorithm, updating ranks until they converge.

872

Chapter

22.

Natural Language Processing

22.3.5 The HITS algorithm

AUTHORITY
HUB

The Hyperlink-Induced Topic Search algorithm, also known as â€œHubs and Authoritiesâ€ or
HITS, is another influential link-analysis algorithm (see Figure 22.1). HITS differs from
PageRank in several ways. First, it is a query-dependent measure: it rates pages with respect
to a query. That means that it must be computed anew for each queryâ€”a computational
burden that most search engines have elected not to take on. Given a query, HITS first finds
a set of pages that are relevant to the query. It does that by intersecting hit lists of query
words, and then adding pages in the link neighborhood of these pagesâ€”pages that link to or
are linked from one of the pages in the original relevant set.
Each page in this set is considered an authority on the query to the degree that other
pages in the relevant set point to it. A page is considered a hub to the degree that it points
to other authoritative pages in the relevant set. Just as with PageRank, we donâ€™t want to
merely count the number of links; we want to give more value to the high-quality hubs and
authorities. Thus, as with PageRank, we iterate a process that updates the authority score of
a page to be the sum of the hub scores of the pages that point to it, and the hub score to be
the sum of the authority scores of the pages it points to. If we then normalize the scores and
repeat k times, the process will converge.
Both PageRank and HITS played important roles in developing our understanding of
Web information retrieval. These algorithms and their extensions are used in ranking billions
of queries daily as search engines steadily develop better ways of extracting yet finer signals
of search relevance.

22.3.6 Question answering
QUESTION
ANSWERING

Information retrieval is the task of finding documents that are relevant to a query, where the
query may be a question, or just a topic area or concept. Question answering is a somewhat
different task, in which the query really is a question, and the answer is not a ranked list
of documents but rather a short responseâ€”a sentence, or even just a phrase. There have
been question-answering NLP (natural language processing) systems since the 1960s, but
only since 2001 have such systems used Web information retrieval to radically increase their
breadth of coverage.
The A SK MSR system (Banko et al., 2002) is a typical Web-based question-answering
system. It is based on the intuition that most questions will be answered many times on the
Web, so question answering should be thought of as a problem in precision, not recall. We
donâ€™t have to deal with all the different ways that an answer might be phrasedâ€”we only
have to find one of them. For example, consider the query [Who killed Abraham Lincoln?]
Suppose a system had to answer that question with access only to a single encyclopedia,
whose entry on Lincoln said
John Wilkes Booth altered history with a bullet. He will forever be known as the man
who ended Abraham Lincolnâ€™s life.

To use this passage to answer the question, the system would have to know that ending a life
can be a killing, that â€œHeâ€ refers to Booth, and several other linguistic and semantic facts.

Section 22.4.

Information Extraction

873

A SK MSR does not attempt this kind of sophisticationâ€”it knows nothing about pronoun
reference, or about killing, or any other verb. It does know 15 different kinds of questions, and
how they can be rewritten as queries to a search engine. It knows that [Who killed Abraham
Lincoln] can be rewritten as the query [* killed Abraham Lincoln] and as [Abraham Lincoln
was killed by *]. It issues these rewritten queries and examines the results that come backâ€”
not the full Web pages, just the short summaries of text that appear near the query terms.
The results are broken into 1-, 2-, and 3-grams and tallied for frequency in the result sets and
for weight: an n-gram that came back from a very specific query rewrite (such as the exact
phrase match query [â€œAbraham Lincoln was killed by *â€]) would get more weight than one
from a general query rewrite, such as [Abraham OR Lincoln OR killed]. We would expect
that â€œJohn Wilkes Boothâ€ would be among the highly ranked n-grams retrieved, but so would
â€œAbraham Lincolnâ€ and â€œthe assassination ofâ€ and â€œFordâ€™s Theatre.â€
Once the n-grams are scored, they are filtered by expected type. If the original query
starts with â€œwho,â€ then we filter on names of people; for â€œhow manyâ€ we filter on numbers, for
â€œwhen,â€ on a date or time. There is also a filter that says the answer should not be part of the
question; together these should allow us to return â€œJohn Wilkes Boothâ€ (and not â€œAbraham
Lincolnâ€) as the highest-scoring response.
In some cases the answer will be longer than three words; since the components responses only go up to 3-grams, a longer response would have to be pieced together from
shorter pieces. For example, in a system that used only bigrams, the answer â€œJohn Wilkes
Boothâ€ could be pieced together from high-scoring pieces â€œJohn Wilkesâ€ and â€œWilkes Booth.â€
At the Text Retrieval Evaluation Conference (TREC), A SK MSR was rated as one of
the top systems, beating out competitors with the ability to do far more complex language
understanding. A SK MSR relies upon the breadth of the content on the Web rather than on
its own depth of understanding. It wonâ€™t be able to handle complex inference patterns like
associating â€œwho killedâ€ with â€œended the life of.â€ But it knows that the Web is so vast that it
can afford to ignore passages like that and wait for a simple passage it can handle.

22.4
INFORMATION
EXTRACTION

I NFORMATION E XTRACTION
Information extraction is the process of acquiring knowledge by skimming a text and looking for occurrences of a particular class of object and for relationships among objects. A
typical task is to extract instances of addresses from Web pages, with database fields for
street, city, state, and zip code; or instances of storms from weather reports, with fields for
temperature, wind speed, and precipitation. In a limited domain, this can be done with high
accuracy. As the domain gets more general, more complex linguistic models and more complex learning techniques are necessary. We will see in Chapter 23 how to define complex
language models of the phrase structure (noun phrases and verb phrases) of English. But so
far there are no complete models of this kind, so for the limited needs of information extraction, we define limited models that approximate the full English model, and concentrate
on just the parts that are needed for the task at hand. The models we describe in this sec-

874

Chapter

22.

Natural Language Processing

tion are approximations in the same way that the simple 1-CNF logical model in Figure 7.21
(page 271) is an approximations of the full, wiggly, logical model.
In this section we describe six different approaches to information extraction, in order
of increasing complexity on several dimensions: deterministic to stochastic, domain-specific
to general, hand-crafted to learned, and small-scale to large-scale.

22.4.1 Finite-state automata for information extraction
ATTRIBUTE-BASED
EXTRACTION

TEMPLATE
REGULAR
EXPRESSION

The simplest type of information extraction system is an attribute-based extraction system
that assumes that the entire text refers to a single object and the task is to extract attributes of
that object. For example, we mentioned in Section 12.7 the problem of extracting from the
text â€œIBM ThinkBook 970. Our price: $399.00â€ the set of attributes {Manufacturer=IBM,
Model=ThinkBook970, Price=$399.00}. We can address this problem by defining a template (also known as a pattern) for each attribute we would like to extract. The template is
defined by a finite state automaton, the simplest example of which is the regular expression,
or regex. Regular expressions are used in Unix commands such as grep, in programming
languages such as Perl, and in word processors such as Microsoft Word. The details vary
slightly from one tool to another and so are best learned from the appropriate manual, but
here we show how to build up a regular expression template for prices in dollars:
[0-9]
[0-9]+
[.][0-9][0-9]
([.][0-9][0-9])?
[$][0-9]+([.][0-9][0-9])?

RELATIONAL
EXTRACTION

matches any digit from 0 to 9
matches one or more digits
matches a period followed by two digits
matches a period followed by two digits, or nothing
matches $249.99 or $1.23 or $1000000 or . . .

Templates are often defined with three parts: a prefix regex, a target regex, and a postfix regex.
For prices, the target regex is as above, the prefix would look for strings such as â€œprice:â€ and
the postfix could be empty. The idea is that some clues about an attribute come from the
attribute value itself and some come from the surrounding text.
If a regular expression for an attribute matches the text exactly once, then we can pull
out the portion of the text that is the value of the attribute. If there is no match, all we can do
is give a default value or leave the attribute missing; but if there are several matches, we need
a process to choose among them. One strategy is to have several templates for each attribute,
ordered by priority. So, for example, the top-priority template for price might look for the
prefix â€œour price:â€; if that is not found, we look for the prefix â€œprice:â€ and if that is not found,
the empty prefix. Another strategy is to take all the matches and find some way to choose
among them. For example, we could take the lowest price that is within 50% of the highest
price. That will select $78.00 as the target from the text â€œList price $99.00, special sale price
$78.00, shipping $3.00.â€
One step up from attribute-based extraction systems are relational extraction systems,
which deal with multiple objects and the relations among them. Thus, when these systems
see the text â€œ$249.99,â€ they need to determine not just that it is a price, but also which object
has that price. A typical relational-based extraction system is FASTUS, which handles news
stories about corporate mergers and acquisitions. It can read the story

Section 22.4.

Information Extraction

875

Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local
concern and a Japanese trading house to produce golf clubs to be shipped to Japan.

and extract the relations:
e âˆˆ JointVentures âˆ§ Product (e, â€œgolf clubsâ€) âˆ§ Date(e, â€œF ridayâ€)
âˆ§ Member (e, â€œBridgestone Sports Coâ€) âˆ§ Member (e, â€œa local concernâ€)
âˆ§ Member (e, â€œa Japanese trading houseâ€) .
CASCADED
FINITE-STATE
TRANSDUCERS

A relational extraction system can be built as a series of cascaded finite-state transducers.
That is, the system consists of a series of small, efficient finite-state automata (FSAs), where
each automaton receives text as input, transduces the text into a different format, and passes
it along to the next automaton. FASTUS consists of five stages:
1.
2.
3.
4.
5.

Tokenization
Complex-word handling
Basic-group handling
Complex-phrase handling
Structure merging

FASTUSâ€™s first stage is tokenization, which segments the stream of characters into tokens
(words, numbers, and punctuation). For English, tokenization can be fairly simple; just separating characters at white space or punctuation does a fairly good job. Some tokenizers also
deal with markup languages such as HTML, SGML, and XML.
The second stage handles complex words, including collocations such as â€œset upâ€ and
â€œjoint venture,â€ as well as proper names such as â€œBridgestone Sports Co.â€ These are recognized by a combination of lexical entries and finite-state grammar rules. For example, a
company name might be recognized by the rule
CapitalizedWord+ (â€œCompanyâ€ | â€œCoâ€ | â€œIncâ€ | â€œLtdâ€)

The third stage handles basic groups, meaning noun groups and verb groups. The idea is
to chunk these into units that will be managed by the later stages. We will see how to write
a complex description of noun and verb phrases in Chapter 23, but here we have simple
rules that only approximate the complexity of English, but have the advantage of being representable by finite state automata. The example sentence would emerge from this stage as
the following sequence of tagged groups:
1
2
3
4
5
6
7
8
9

NG:
VG:
NG:
NG:
VG:
NG:
PR:
NG:
PR:

Bridgestone Sports Co.
said
Friday
it
had set up
a joint venture
in
Taiwan
with

10
11
12
13
14
15
16
17

NG:
CJ:
NG:
VG:
NG:
VG:
PR:
NG:

a local concern
and
a Japanese trading house
to produce
golf clubs
to be shipped
to
Japan

Here NG means noun group, VG is verb group, PR is preposition, and CJ is conjunction.

876

Chapter

22.

Natural Language Processing

The fourth stage combines the basic groups into complex phrases. Again, the aim
is to have rules that are finite-state and thus can be processed quickly, and that result in
unambiguous (or nearly unambiguous) output phrases. One type of combination rule deals
with domain-specific events. For example, the rule
Company+ SetUp JointVenture (â€œwithâ€ Company+)?

captures one way to describe the formation of a joint venture. This stage is the first one in
the cascade where the output is placed into a database template as well as being placed in the
output stream. The final stage merges structures that were built up in the previous step. If
the next sentence says â€œThe joint venture will start production in January,â€ then this step will
notice that there are two references to a joint venture, and that they should be merged into
one. This is an instance of the identity uncertainty problem discussed in Section 14.6.3.
In general, finite-state template-based information extraction works well for a restricted
domain in which it is possible to predetermine what subjects will be discussed, and how they
will be mentioned. The cascaded transducer model helps modularize the necessary knowledge, easing construction of the system. These systems work especially well when they are
reverse-engineering text that has been generated by a program. For example, a shopping site
on the Web is generated by a program that takes database entries and formats them into Web
pages; a template-based extractor then recovers the original database. Finite-state information extraction is less successful at recovering information in highly variable format, such as
text written by humans on a variety of subjects.

22.4.2 Probabilistic models for information extraction
When information extraction must be attempted from noisy or varied input, simple finite-state
approaches fare poorly. It is too hard to get all the rules and their priorities right; it is better
to use a probabilistic model rather than a rule-based model. The simplest probabilistic model
for sequences with hidden state is the hidden Markov model, or HMM.
Recall from Section 15.3 that an HMM models a progression through a sequence of
hidden states, xt , with an observation et at each step. To apply HMMs to information extraction, we can either build one big HMM for all the attributes or build a separate HMM
for each attribute. Weâ€™ll do the second. The observations are the words of the text, and the
hidden states are whether we are in the target, prefix, or postfix part of the attribute template,
or in the background (not part of a template). For example, here is a brief text and the most
probable (Viterbi) path for that text for two HMMs, one trained to recognize the speaker in a
talk announcement, and one trained to recognize dates. The â€œ-â€ indicates a background state:
Text:
There will be a seminar by Dr.
Andrew McCallum on
Friday
PRE TARGET TARGET TARGET
POST Speaker: - - PRE
PRE TARGET
Date:
- - HMMs have two big advantages over FSAs for extraction. First, HMMs are probabilistic, and
thus tolerant to noise. In a regular expression, if a single expected character is missing, the
regex fails to match; with HMMs there is graceful degradation with missing characters/words,
and we get a probability indicating the degree of match, not just a Boolean match/fail. Second,

Section 22.4.

Information Extraction

who
speaker
speak
5409
appointment

1.0

877

:
with
;
about
how

dr
professor
robert
michael
mr

0.99

0.99
0.76

seminar
reminder
theater
artist
additionally

1.0

that
by
speakers
/
here

0.24

w
cavalier
stevens
christel
l

0.56

will
(
received
has
is

0.44

Prefix

Target

Postfix

Figure 22.2 Hidden Markov model for the speaker of a talk announcement. The two
square states are the target (note the second target state has a self-loop, so the target can
match a string of any length), the four circles to the left are the prefix, and the one on the
right is the postfix. For each state, only a few of the high-probability words are shown. From
Freitag and McCallum (2000).

HMMs can be trained from data; they donâ€™t require laborious engineering of templates, and
thus they can more easily be kept up to date as text changes over time.
Note that we have assumed a certain level of structure in our HMM templates: they all
consist of one or more target states, and any prefix states must precede the targets, postfix
states most follow the targets, and other states must be background. This structure makes
it easier to learn HMMs from examples. With a partially specified structure, the forwardâ€“
backward algorithm can be used to learn both the transition probabilities P(Xt | Xtâˆ’1 ) between states and the observation model, P(Et | Xt ), which says how likely each word is in
each state. For example, the word â€œFridayâ€ would have high probability in one or more of
the target states of the date HMM, and lower probability elsewhere.
With sufficient training data, the HMM automatically learns a structure of dates that we
find intuitive: the date HMM might have one target state in which the high-probability words
are â€œMonday,â€ â€œTuesday,â€ etc., and which has a high-probability transition to a target state
with words â€œJanâ€, â€œJanuary,â€ â€œFeb,â€ etc. Figure 22.2 shows the HMM for the speaker of a
talk announcement, as learned from data. The prefix covers expressions such as â€œSpeaker:â€
and â€œseminar by,â€ and the target has one state that covers titles and first names and another
state that covers initials and last names.
Once the HMMs have been learned, we can apply them to a text, using the Viterbi
algorithm to find the most likely path through the HMM states. One approach is to apply
each attribute HMM separately; in this case you would expect most of the HMMs to spend
most of their time in background states. This is appropriate when the extraction is sparseâ€”
when the number of extracted words is small compared to the length of the text.

878

Chapter

22.

Natural Language Processing

The other approach is to combine all the individual attributes into one big HMM, which
would then find a path that wanders through different target attributes, first finding a speaker
target, then a date target, etc. Separate HMMs are better when we expect just one of each
attribute in a text and one big HMM is better when the texts are more free-form and dense
with attributes. With either approach, in the end we have a collection of target attribute
observations, and have to decide what to do with them. If every expected attribute has one
target filler then the decision is easy: we have an instance of the desired relation. If there
are multiple fillers, we need to decide which to choose, as we discussed with template-based
systems. HMMs have the advantage of supplying probability numbers that can help make
the choice. If some targets are missing, we need to decide if this is an instance of the desired
relation at all, or if the targets found are false positives. A machine learning algorithm can be
trained to make this choice.

22.4.3 Conditional random fields for information extraction

CONDITIONAL
RANDOM FIELD

LINEAR-CHAIN
CONDITIONAL
RANDOM FIELD

One issue with HMMs for the information extraction task is that they model a lot of probabilities that we donâ€™t really need. An HMM is a generative model; it models the full joint
probability of observations and hidden states, and thus can be used to generate samples. That
is, we can use the HMM model not only to parse a text and recover the speaker and date,
but also to generate a random instance of a text containing a speaker and a date. Since weâ€™re
not interested in that task, it is natural to ask whether we might be better off with a model
that doesnâ€™t bother modeling that possibility. All we need in order to understand a text is a
discriminative model, one that models the conditional probability of the hidden attributes
given the observations (the text). Given a text e1:N , the conditional model finds the hidden
state sequence X1:N that maximizes P (X1:N | e1:N ).
Modeling this directly gives us some freedom. We donâ€™t need the independence assumptions of the Markov modelâ€”we can have an xt that is dependent on x1 . A framework
for this type of model is the conditional random field, or CRF, which models a conditional
probability distribution of a set of target variables given a set of observed variables. Like
Bayesian networks, CRFs can represent many different structures of dependencies among the
variables. One common structure is the linear-chain conditional random field for representing Markov dependencies among variables in a temporal sequence. Thus, HMMs are the
temporal version of naive Bayes models, and linear-chain CRFs are the temporal version of
logistic regression, where the predicted target is an entire state sequence rather than a single
binary variable.
Let e1:N be the observations (e.g., words in a document), and x1:N be the sequence of
hidden states (e.g., the prefix, target, and postfix states). A linear-chain conditional random
field defines a conditional probability distribution:
PN
P(x |e ) = Î± e[ i=1 F (xiâˆ’1 ,xi ,e,i)] ,
1:N

1:N

where Î± is a normalization factor (to make sure the probabilities sum to 1), and F is a feature
function defined as the weighted sum of a collection of k component feature functions:

Î»k fk (xiâˆ’1 , xi , e, i) .
F (xiâˆ’1 , xi , e, i) =
k

Section 22.4.

Information Extraction

879

The Î»k parameter values are learned with a MAP (maximum a posteriori) estimation procedure that maximizes the conditional likelihood of the training data. The feature functions are
the key components of a CRF. The function fk has access to a pair of adjacent states, xiâˆ’1 and
xi , but also the entire observation (word) sequence e, and the current position in the temporal
sequence, i. This gives us a lot of flexibility in defining features. We can define a simple
feature function, for example one that produces a value of 1 if the current word is A NDREW
and the current state is SPEAKER:

1 if xi = SPEAKER and ei = A NDREW
f1 (xiâˆ’1 , xi , e, i) =
0 otherwise
How are features like these used? It depends on their corresponding weights. If Î»1 > 0, then
whenever f1 is true, it increases the probability of the hidden state sequence x1:N . This is
another way of saying â€œthe CRF model should prefer the target state SPEAKER for the word
A NDREW.â€ If on the other hand Î»1 < 0, the CRF model will try to avoid this association,
and if Î»1 = 0, this feature is ignored. Parameter values can be set manually or can be learned
from data. Now consider a second feature function:

1 if xi = SPEAKER and ei+1 = SAID
f2 (xiâˆ’1 , xi , e, i) =
0 otherwise
This feature is true if the current state is SPEAKER and the next word is â€œsaid.â€ One would
therefore expect a positive Î»2 value to go with the feature. More interestingly, note that both
f1 and f2 can hold at the same time for a sentence like â€œAndrew said . . . .â€ In this case, the
two features overlap each other and both boost the belief in x1 = SPEAKER. Because of the
independence assumption, HMMs cannot use overlapping features; CRFs can. Furthermore,
a feature in a CRF can use any part of the sequence e1:N . Features can also be defined over
transitions between states. The features we defined here were binary, but in general, a feature
function can be any real-valued function. For domains where we have some knowledge about
the types of features we would like to include, the CRF formalism gives us a great deal of
flexibility in defining them. This flexibility can lead to accuracies that are higher than with
less flexible models such as HMMs.

22.4.4 Ontology extraction from large corpora
So far we have thought of information extraction as finding a specific set of relations (e.g.,
speaker, time, location) in a specific text (e.g., a talk announcement). A different application of extraction technology is building a large knowledge base or ontology of facts from
a corpus. This is different in three ways: First it is open-endedâ€”we want to acquire facts
about all types of domains, not just one specific domain. Second, with a large corpus, this
task is dominated by precision, not recallâ€”just as with question answering on the Web (Section 22.3.6). Third, the results can be statistical aggregates gathered from multiple sources,
rather than being extracted from one specific text.
For example, Hearst (1992) looked at the problem of learning an ontology of concept
categories and subcategories from a large corpus. (In 1992, a large corpus was a 1000-page
encyclopedia; today it would be a 100-million-page Web corpus.) The work concentrated on
templates that are very general (not tied to a specific domain) and have high precision (are

880

Chapter

22.

Natural Language Processing

almost always correct when they match) but low recall (do not always match). Here is one of
the most productive templates:
NP such as NP (, NP )* (,)? ((and | or) NP)? .

Here the bold words and commas must appear literally in the text, but the parentheses are
for grouping, the asterisk means repetition of zero or more, and the question mark means
optional. NP is a variable standing for a noun phrase; Chapter 23 describes how to identify
noun phrases; for now just assume that we know some words are nouns and other words (such
as verbs) that we can reliably assume are not part of a simple noun phrase. This template
matches the texts â€œdiseases such as rabies affect your dogâ€ and â€œsupports network protocols
such as DNS,â€ concluding that rabies is a disease and DNS is a network protocol. Similar
templates can be constructed with the key words â€œincluding,â€ â€œespecially,â€ and â€œor other.â€ Of
course these templates will fail to match many relevant passages, like â€œRabies is a disease.â€
That is intentional. The â€œNP is a NP â€ template does indeed sometimes denote a subcategory
relation, but it often means something else, as in â€œThere is a Godâ€ or â€œShe is a little tired.â€
With a large corpus we can afford to be picky; to use only the high-precision templates. Weâ€™ll
miss many statements of a subcategory relationship, but most likely weâ€™ll find a paraphrase
of the statement somewhere else in the corpus in a form we can use.

22.4.5 Automated template construction
The subcategory relation is so fundamental that is worthwhile to handcraft a few templates to
help identify instances of it occurring in natural language text. But what about the thousands
of other relations in the world? There arenâ€™t enough AI grad students in the world to create
and debug templates for all of them. Fortunately, it is possible to learn templates from a few
examples, then use the templates to learn more examples, from which more templates can be
learned, and so on. In one of the first experiments of this kind, Brin (1999) started with a data
set of just five examples:
(â€œIsaac Asimovâ€, â€œThe Robots of Dawnâ€)
(â€œDavid Brinâ€, â€œStartide Risingâ€)
(â€œJames Gleickâ€, â€œChaosâ€”Making a New Scienceâ€)
(â€œCharles Dickensâ€, â€œGreat Expectationsâ€)
(â€œWilliam Shakespeareâ€, â€œThe Comedy of Errorsâ€)

Clearly these are examples of the authorâ€“title relation, but the learning system had no knowledge of authors or titles. The words in these examples were used in a search over a Web
corpus, resulting in 199 matches. Each match is defined as a tuple of seven strings,
(Author, Title, Order, Prefix, Middle, Postfix, URL) ,
where Order is true if the author came first and false if the title came first, Middle is the
characters between the author and title, Prefix is the 10 characters before the match, Suffix is
the 10 characters after the match, and URL is the Web address where the match was made.
Given a set of matches, a simple template-generation scheme can find templates to
explain the matches. The language of templates was designed to have a close mapping to the
matches themselves, to be amenable to automated learning, and to emphasize high precision

Section 22.4.

Information Extraction

881

(possibly at the risk of lower recall). Each template has the same seven components as a
match. The Author and Title are regexes consisting of any characters (but beginning and
ending in letters) and constrained to have a length from half the minimum length of the
examples to twice the maximum length. The prefix, middle, and postfix are restricted to
literal strings, not regexes. The middle is the easiest to learn: each distinct middle string in
the set of matches is a distinct candidate template. For each such candidate, the templateâ€™s
Prefix is then defined as the longest common suffix of all the prefixes in the matches, and the
Postfix is defined as the longest common prefix of all the postfixes in the matches. If either of
these is of length zero, then the template is rejected. The URL of the template is defined as
the longest prefix of the URLs in the matches.
In the experiment run by Brin, the first 199 matches generated three templates. The
most productive template was
<LI><B> Title </B> by Author (
URL: www.sff.net/locus/c

The three templates were then used to retrieve 4047 more (author, title) examples. The examples were then used to generate more templates, and so on, eventually yielding over 15,000
titles. Given a good set of templates, the system can collect a good set of examples. Given a
good set of examples, the system can build a good set of templates.
The biggest weakness in this approach is the sensitivity to noise. If one of the first
few templates is incorrect, errors can propagate quickly. One way to limit this problem is to
not accept a new example unless it is verified by multiple templates, and not accept a new
template unless it discovers multiple examples that are also found by other templates.

22.4.6 Machine reading

MACHINE READING

Automated template construction is a big step up from handcrafted template construction, but
it still requires a handful of labeled examples of each relation to get started. To build a large
ontology with many thousands of relations, even that amount of work would be onerous; we
would like to have an extraction system with no human input of any kindâ€”a system that could
read on its own and build up its own database. Such a system would be relation-independent;
would work for any relation. In practice, these systems work on all relations in parallel,
because of the I/O demands of large corpora. They behave less like a traditional informationextraction system that is targeted at a few relations and more like a human reader who learns
from the text itself; because of this the field has been called machine reading.
A representative machine-reading system is T EXT RUNNER (Banko and Etzioni, 2008).
T EXT RUNNER uses cotraining to boost its performance, but it needs something to bootstrap
from. In the case of Hearst (1992), specific patterns (e.g., such as) provided the bootstrap, and
for Brin (1998), it was a set of five authorâ€“title pairs. For T EXT RUNNER , the original inspiration was a taxonomy of eight very general syntactic templates, as shown in Figure 22.3. It
was felt that a small number of templates like this could cover most of the ways that relationships are expressed in English. The actual bootsrapping starts from a set of labelled examples
that are extracted from the Penn Treebank, a corpus of parsed sentences. For example, from
the parse of the sentence â€œEinstein received the Nobel Prize in 1921,â€ T EXT RUNNER is able

882

Chapter

22.

Natural Language Processing

to extract the relation (â€œEinstein,â€ â€œreceived,â€ â€œNobel Prizeâ€).
Given a set of labeled examples of this type, T EXT RUNNER trains a linear-chain CRF
to extract further examples from unlabeled text. The features in the CRF include function
words like â€œtoâ€ and â€œofâ€ and â€œthe,â€ but not nouns and verbs (and not noun phrases or verb
phrases). Because T EXT RUNNER is domain-independent, it cannot rely on predefined lists
of nouns and verbs.
Type

Template

Example

Frequency

Verb
Nounâ€“Prep
Verbâ€“Prep
Infinitive
Modifier
Noun-Coordinate
Verb-Coordinate
Appositive

NP 1
NP 1
NP 1
NP 1
NP 1
NP 1
NP 1
NP 1

X established Y
X settlement with Y
X moved to Y
X plans to acquire Y
X is Y winner
X-Y deal
X, Y merge
X hometown : Y

38%
23%
16%
9%
5%
2%
1%
1%

Verb NP 2
NP Prep NP 2
Verb Prep NP 2
to Verb NP 2
Verb NP 2 Noun
(, | and | - | :) NP 2 NP
(,| and) NP 2 Verb
NP (:| ,)? NP 2

Figure 22.3 Eight general templates that cover about 95% of the ways that relations are
expressed in English.

T EXT RUNNER achieves a precision of 88% and recall of 45% (F1 of 60%) on a large
Web corpus. T EXT RUNNER has extracted hundreds of millions of facts from a corpus of a
half-billion Web pages. For example, even though it has no predefined medical knowledge,
it has extracted over 2000 answers to the query [what kills bacteria]; correct answers include
antibiotics, ozone, chlorine, Cipro, and broccoli sprouts. Questionable answers include â€œwater,â€ which came from the sentence â€œBoiling water for at least 10 minutes will kill bacteria.â€
It would be better to attribute this to â€œboiling waterâ€ rather than just â€œwater.â€
With the techniques outlined in this chapter and continual new inventions, we are starting to get closer to the goal of machine reading.

22.5

S UMMARY
The main points of this chapter are as follows:
â€¢ Probabilistic language models based on n-grams recover a surprising amount of information about a language. They can perform well on such diverse tasks as language
identification, spelling correction, genre classification, and named-entity recognition.
â€¢ These language models can have millions of features, so feature selection and preprocessing of the data to reduce noise is important.
â€¢ Text classification can be done with naive Bayes n-gram models or with any of the
classification algorithms we have previously discussed. Classification can also be seen
as a problem in data compression.

Bibliographical and Historical Notes

883

â€¢ Information retrieval systems use a very simple language model based on bags of
words, yet still manage to perform well in terms of recall and precision on very large
corpora of text. On Web corpora, link-analysis algorithms improve performance.
â€¢ Question answering can be handled by an approach based on information retrieval, for
questions that have multiple answers in the corpus. When more answers are available
in the corpus, we can use techniques that emphasize precision rather than recall.
â€¢ Information-extraction systems use a more complex model that includes limited notions of syntax and semantics in the form of templates. They can be built from finitestate automata, HMMs, or conditional random fields, and can be learned from examples.
â€¢ In building a statistical language system, it is best to devise a model that can make good
use of available data, even if the model seems overly simplistic.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

N -gram letter models for language modeling were proposed by Markov (1913). Claude
Shannon (Shannon and Weaver, 1949) was the first to generate n-gram word models of English. Chomsky (1956, 1957) pointed out the limitations of finite-state models compared with
context-free models, concluding, â€œProbabilistic models give no particular insight into some
of the basic problems of syntactic structure.â€ This is true, but probabilistic models do provide
insight into some other basic problemsâ€”problems that context-free models ignore. Chomskyâ€™s remarks had the unfortunate effect of scaring many people away from statistical models
for two decades, until these models reemerged for use in speech recognition (Jelinek, 1976).
Kessler et al. (1997) show how to apply character n-gram models to genre classification,
and Klein et al. (2003) describe named-entity recognition with character models. Franz and
Brants (2006) describe the Google n-gram corpus of 13 million unique words from a trillion
words of Web text; it is now publicly available. The bag of words model gets its name from
a passage from linguist Zellig Harris (1954), â€œlanguage is not merely a bag of words but
a tool with particular properties.â€ Norvig (2009) gives some examples of tasks that can be
accomplished with n-gram models.
Add-one smoothing, first suggested by Pierre-Simon Laplace (1816), was formalized by
Jeffreys (1948), and interpolation smoothing is due to Jelinek and Mercer (1980), who used
it for speech recognition. Other techniques include Wittenâ€“Bell smoothing (1991), Goodâ€“
Turing smoothing (Church and Gale, 1991) and Kneserâ€“Ney smoothing (1995). Chen and
Goodman (1996) and Goodman (2001) survey smoothing techniques.
Simple n-gram letter and word models are not the only possible probabilistic models.
Blei et al. (2001) describe a probabilistic text model called latent Dirichlet allocation that
views a document as a mixture of topics, each with its own distribution of words. This model
can be seen as an extension and rationalization of the latent semantic indexing model of
(Deerwester et al., 1990) (see also Papadimitriou et al. (1998)) and is also related to the
multiple-cause mixture model of (Sahami et al., 1996).

884

Chapter

22.

Natural Language Processing

Manning and SchuÌˆtze (1999) and Sebastiani (2002) survey text-classification techniques.
Joachims (2001) uses statistical learning theory and support vector machines to give a theoretical analysis of when classification will be successful. ApteÌ et al. (1994) report an accuracy
of 96% in classifying Reuters news articles into the â€œEarningsâ€ category. Koller and Sahami
(1997) report accuracy up to 95% with a naive Bayes classifier, and up to 98.6% with a Bayes
classifier that accounts for some dependencies among features. Lewis (1998) surveys forty
years of application of naive Bayes techniques to text classification and retrieval. Schapire
and Singer (2000) show that simple linear classifiers can often achieve accuracy almost as
good as more complex models and are more efficient to evaluate. Nigam et al. (2000) show
how to use the EM algorithm to label unlabeled documents, thus learning a better classification model. Witten et al. (1999) describe compression algorithms for classification, and
show the deep connection between the LZW compression algorithm and maximum-entropy
language models.
Many of the n-gram model techniques are also used in bioinformatics problems. Biostatistics and probabilistic NLP are coming closer together, as each deals with long, structured
sequences chosen from an alphabet of constituents.
The field of information retrieval is experiencing a regrowth in interest, sparked by
the wide usage of Internet searching. Robertson (1977) gives an early overview and introduces the probability ranking principle. Croft et al. (2009) and Manning et al. (2008) are
the first textbooks to cover Web-based search as well as traditional IR. Hearst (2009) covers
user interfaces for Web search. The TREC conference, organized by the U.S. governmentâ€™s
National Institute of Standards and Technology (NIST), hosts an annual competition for IR
systems and publishes proceedings with results. In the first seven years of the competition,
performance roughly doubled.
The most popular model for IR is the vector space model (Salton et al., 1975). Saltonâ€™s
work dominated the early years of the field. There are two alternative probabilistic models,
one due to Ponte and Croft (1998) and one by Maron and Kuhns (1960) and Robertson and
Sparck Jones (1976). Lafferty and Zhai (2001) show that the models are based on the same
joint probability distribution, but that the choice of model has implications for training the
parameters. Craswell et al. (2005) describe the BM25 scoring function and Svore and Burges
(2009) describe how BM25 can be improved with a machine learning approach that incorporates click dataâ€”examples of past search queies and the results that were clicked on.
Brin and Page (1998) describe the PageRank algorithm and the implementation of a
Web search engine. Kleinberg (1999) describes the HITS algorithm. Silverstein et al. (1998)
investigate a log of a billion Web searches. The journal Information Retrieval and the proceedings of the annual SIGIR conference cover recent developments in the field.
Early information extraction programs include G US (Bobrow et al., 1977) and F RUMP
(DeJong, 1982). Recent information extraction has been pushed forward by the annual Message Understand Conferences (MUC), sponsored by the U.S. government. The FASTUS
finite-state system was done by Hobbs et al. (1997). It was based in part on the idea from
Pereira and Wright (1991) of using FSAs as approximations to phrase-structure grammars.
Surveys of template-based systems are given by Roche and Schabes (1997), Appelt (1999),

Exercises

885
and Muslea (1999). Large databases of facts were extracted by Craven et al. (2000), Pasca
et al. (2006), Mitchell (2007), and Durme and Pasca (2008).
Freitag and McCallum (2000) discuss HMMs for Information Extraction. CRFs were
introduced by Lafferty et al. (2001); an example of their use for information extraction is
described in (McCallum, 2003) and a tutorial with practical guidance is given by (Sutton and
McCallum, 2007). Sarawagi (2007) gives a comprehensive survey.
Banko et al. (2002) present the A SK MSR question-answering system; a similar system is due to Kwok et al. (2001). Pasca and Harabagiu (2001) discuss a contest-winning
question-answering system. Two early influential approaches to automated knowledge engineering were by Riloff (1993), who showed that an automatically constructed dictionary performed almost as well as a carefully handcrafted domain-specific dictionary, and by Yarowsky
(1995), who showed that the task of word sense classification (see page 756) could be accomplished through unsupervised training on a corpus of unlabeled text with accuracy as good as
supervised methods.
The idea of simultaneously extracting templates and examples from a handful of labeled
examples was developed independently and simultaneously by Blum and Mitchell (1998),
who called it cotraining and by Brin (1998), who called it DIPRE (Dual Iterative Pattern
Relation Extraction). You can see why the term cotraining has stuck. Similar early work,
under the name of bootstrapping, was done by Jones et al. (1999). The method was advanced
by the QX TRACT (Agichtein and Gravano, 2003) and K NOW I TA LL (Etzioni et al., 2005)
systems. Machine reading was introduced by Mitchell (2005) and Etzioni et al. (2006) and is
the focus of the T EXT RUNNER project (Banko et al., 2007; Banko and Etzioni, 2008).
This chapter has focused on natural language text, but it is also possible to do information extraction based on the physical structure or layout of text rather than on the linguistic
structure. HTML lists and tables in both HTML and relational databases are home to data
that can be extracted and consolidated (Hurst, 2000; Pinto et al., 2003; Cafarella et al., 2008).
The Association for Computational Linguistics (ACL) holds regular conferences and
publishes the journal Computational Linguistics. There is also an International Conference
on Computational Linguistics (COLING). The textbook by Manning and SchuÌˆtze (1999) covers statistical language processing, while Jurafsky and Martin (2008) give a comprehensive
introduction to speech and natural language processing.

E XERCISES
22.1 This exercise explores the quality of the n-gram model of language. Find or create a
monolingual corpus of 100,000 words or more. Segment it into words, and compute the frequency of each word. How many distinct words are there? Also count frequencies of bigrams
(two consecutive words) and trigrams (three consecutive words). Now use those frequencies
to generate language: from the unigram, bigram, and trigram models, in turn, generate a 100word text by making random choices according to the frequency counts. Compare the three
generated texts with actual language. Finally, calculate the perplexity of each model.

886

Chapter

22.

Natural Language Processing

22.2 Write a program to do segmentation of words without spaces. Given a string, such
as the URL â€œthelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com,â€ return a
list of component words: [â€œthe,â€ â€œlongest,â€ â€œlist,â€ . . .]. This task is useful for parsing URLs,
for spelling correction when words runtogether, and for languages such as Chinese that do
not have spaces between words. It can be solved with a unigram or bigram word model and
a dynamic programming algorithm similar to the Viterbi algorithm.

STYLOMETRY

22.3 (Adapted from Jurafsky and Martin (2000).) In this exercise you will develop a classifier for authorship: given a text, the classifier predicts which of two candidate authors wrote
the text. Obtain samples of text from two different authors. Separate them into training and
test sets. Now train a language model on the training set. You can choose what features to
use; n-grams of words or letters are the easiest, but you can add additional features that you
think may help. Then compute the probability of the text under each language model and
chose the most probable model. Assess the accuracy of this technique. How does accuracy
change as you alter the set of features? This subfield of linguistics is called stylometry; its
successes include the identification of the author of the disputed Federalist Papers (Mosteller
and Wallace, 1964) and some disputed works of Shakespeare (Hope, 1994). Khmelev and
Tweedie (2001) produce good results with a simple letter bigram model.
22.4 This exercise concerns the classification of spam email. Create a corpus of spam email
and one of non-spam mail. Examine each corpus and decide what features appear to be useful
for classification: unigram words? bigrams? message length, sender, time of arrival? Then
train a classification algorithm (decision tree, naive Bayes, SVM, logistic regression, or some
other algorithm of your choosing) on a training set and report its accuracy on a test set.
22.5 Create a test set of ten queries, and pose them to three major Web search engines.
Evaluate each one for precision at 1, 3, and 10 documents. Can you explain the differences
between engines?
22.6 Try to ascertain which of the search engines from the previous exercise are using case
folding, stemming, synonyms, and spelling correction.
22.7 Write a regular expression or a short program to extract company names. Test it on a
corpus of business news articles. Report your recall and precision.
22.8 Consider the problem of trying to evaluate the quality of an IR system that returns a
ranked list of answers (like most Web search engines). The appropriate measure of quality
depends on the presumed model of what the searcher is trying to achieve, and what strategy
she employs. For each of the following models, propose a corresponding numeric measure.
a. The searcher will look at the first twenty answers returned, with the objective of getting
as much relevant information as possible.
b. The searcher needs only one relevant document, and will go down the list until she finds
the first one.
c. The searcher has a fairly narrow query and is able to examine all the answers retrieved.
She wants to be sure that she has seen everything in the document collection that is

Exercises

887
relevant to her query. (E.g., a lawyer wants to be sure that she has found all relevant
precedents, and is willing to spend considerable resources on that.)
d. The searcher needs just one document relevant to the query, and can afford to pay a
research assistant for an hourâ€™s work looking through the results. The assistant can look
through 100 retrieved documents in an hour. The assistant will charge the searcher for
the full hour regardless of whether he finds it immediately or at the end of the hour.
e. The searcher will look through all the answers. Examining a document has cost $A;
finding a relevant document has value $B; failing to find a relevant document has cost
$C for each relevant document not found.
f. The searcher wants to collect as many relevant documents as possible, but needs steady
encouragement. She looks through the documents in order. If the documents she has
looked at so far are mostly good, she will continue; otherwise, she will stop.

23

NATURAL LANGUAGE
FOR COMMUNICATION

In which we see how humans communicate with one another in natural language,
and how computer agents might join in the conversation.

COMMUNICATION
SIGN

23.1

Communication is the intentional exchange of information brought about by the production
and perception of signs drawn from a shared system of conventional signs. Most animals use
signs to represent important messages: food here, predator nearby, approach, withdraw, letâ€™s
mate. In a partially observable world, communication can help agents be successful because
they can learn information that is observed or inferred by others. Humans are the most chatty
of all species, and if computer agents are to be helpful, theyâ€™ll need to learn to speak the
language. In this chapter we look at language models for communication. Models aimed at
deep understanding of a conversation necessarily need to be more complex than the simple
models aimed at, say, spam classification. We start with grammatical models of the phrase
structure of sentences, add semantics to the model, and then apply it to machine translation
and speech recognition.

P HRASE S TRUCTURE G RAMMARS

LEXICAL CATEGORY

SYNTACTIC
CATEGORIES
PHRASE STRUCTURE

The n-gram language models of Chapter 22 were based on sequences of words. The big
issue for these models is data sparsityâ€”with a vocabulary of, say, 105 words, there are 1015
trigram probabilities to estimate, and so a corpus of even a trillion words will not be able to
supply reliable estimates for all of them. We can address the problem of sparsity through
generalization. From the fact that â€œblack dogâ€ is more frequent than â€œdog blackâ€ and similar
observations, we can form the generalization that adjectives tend to come before nouns in
English (whereas they tend to follow nouns in French: â€œchien noirâ€ is more frequent). Of
course there are always exceptions; â€œgaloreâ€ is an adjective that follows the noun it modifies.
Despite the exceptions, the notion of a lexical category (also known as a part of speech) such
as noun or adjective is a useful generalizationâ€”useful in its own right, but more so when we
string together lexical categories to form syntactic categories such as noun phrase or verb
phrase, and combine these syntactic categories into trees representing the phrase structure
of sentences: nested phrases, each marked with a category.
888

Section 23.1.

Phrase Structure Grammars

G ENERATIVE

CAPACITY

Grammatical formalisms can be classified by their generative capacity: the set of
languages they can represent. Chomsky (1957) describes four classes of grammatical formalisms that differ only in the form of the rewrite rules. The classes can
be arranged in a hierarchy, where each class can be used to describe all the languages that can be described by a less powerful class, as well as some additional
languages. Here we list the hierarchy, most powerful class first:
Recursively enumerable grammars use unrestricted rules: both sides of the
rewrite rules can have any number of terminal and nonterminal symbols, as in the
rule A B C â†’ D E . These grammars are equivalent to Turing machines in their
expressive power.
Context-sensitive grammars are restricted only in that the right-hand side
must contain at least as many symbols as the left-hand side. The name â€œcontextsensitiveâ€ comes from the fact that a rule such as A X B â†’ A Y B says that
an X can be rewritten as a Y in the context of a preceding A and a following B.
Context-sensitive grammars can represent languages such as an bn cn (a sequence
of n copies of a followed by the same number of bs and then cs).
In context-free grammars (or CFGs), the left-hand side consists of a single nonterminal symbol. Thus, each rule licenses rewriting the nonterminal as
the right-hand side in any context. CFGs are popular for natural-language and
programming-language grammars, although it is now widely accepted that at least
some natural languages have constructions that are not context-free (Pullum, 1991).
Context-free grammars can represent an bn , but not an bn cn .
Regular grammars are the most restricted class. Every rule has a single nonterminal on the left-hand side and a terminal symbol optionally followed by a nonterminal on the right-hand side. Regular grammars are equivalent in power to finitestate machines. They are poorly suited for programming languages, because they
cannot represent constructs such as balanced opening and closing parentheses (a
variation of the an bn language). The closest they can come is representing aâˆ— bâˆ— , a
sequence of any number of as followed by any number of bs.
The grammars higher up in the hierarchy have more expressive power, but
the algorithms for dealing with them are less efficient. Up to the 1980s, linguists
focused on context-free and context-sensitive languages. Since then, there has been
renewed interest in regular grammars, brought about by the need to process and
learn from gigabytes or terabytes of online text very quickly, even at the cost of
a less complete analysis. As Fernando Pereira put it, â€œThe older I get, the further
down the Chomsky hierarchy I go.â€ To see what he means, compare Pereira and
Warren (1980) with Mohri, Pereira, and Riley (2002) (and note that these three
authors all now work on large text corpora at Google).

889

890

PROBABILISTIC
CONTEXT-FREE
GRAMMAR
GRAMMAR
LANGUAGE

Chapter 23.

Natural Language for Communication

There have been many competing language models based on the idea of phrase structure; we will describe a popular model called the probabilistic context-free grammar, or
PCFG.1 A grammar is a collection of rules that defines a language as a set of allowable
strings of words. â€œContext-freeâ€ is described in the sidebar on page 889, and â€œprobabilisticâ€
means that the grammar assigns a probability to every string. Here is a PCFG rule:
VP â†’ Verb [0.70]
| VP NP [0.30] .

NON-TERMINAL
SYMBOLS
TERMINAL SYMBOL

Here VP (verb phrase) and NP (noun phrase) are non-terminal symbols. The grammar
also refers to actual words, which are called terminal symbols. This rule is saying that with
probability 0.70 a verb phrase consists solely of a verb, and with probability 0.30 it is a VP
followed by an NP. Appendix B describes non-probabilistic context-free grammars.
We now define a grammar for a tiny fragment of English that is suitable for communication between agents exploring the wumpus world. We call this language E0 . Later sections
improve on E0 to make it slightly closer to real English. We are unlikely ever to devise a
complete grammar for English, if only because no two persons would agree entirely on what
constitutes valid English.

23.1.1 The lexicon of E0
LEXICON

OPEN CLASS
CLOSED CLASS

First we define the lexicon, or list of allowable words. The words are grouped into the lexical
categories familiar to dictionary users: nouns, pronouns, and names to denote things; verbs
to denote events; adjectives to modify nouns; adverbs to modify verbs; and function words:
articles (such as the), prepositions (in), and conjunctions (and). Figure 23.1 shows a small
lexicon for the language E0 .
Each of the categories ends in . . . to indicate that there are other words in the category.
For nouns, names, verbs, adjectives, and adverbs, it is infeasible even in principle to list all
the words. Not only are there tens of thousands of members in each class, but new onesâ€“
like iPod or biodieselâ€”are being added constantly. These five categories are called open
classes. For the categories of pronoun, relative pronoun, article, preposition, and conjunction
we could have listed all the words with a little more work. These are called closed classes;
they have a small number of words (a dozen or so). Closed classes change over the course
of centuries, not months. For example, â€œtheeâ€ and â€œthouâ€ were commonly used pronouns in
the 17th century, were on the decline in the 19th, and are seen today only in poetry and some
regional dialects.

23.1.2 The Grammar of E0

PARSE TREE

The next step is to combine the words into phrases. Figure 23.2 shows a grammar for E0 ,
with rules for each of the six syntactic categories and an example for each rewrite rule. 2
Figure 23.3 shows a parse tree for the sentence â€œEvery wumpus smells.â€ The parse tree
1

PCFGs are also known as stochastic context-free grammars, or SCFGs.
A relative clause follows and modifies a noun phrase. It consists of a relative pronoun (such as â€œwhoâ€ or
â€œthatâ€) followed by a verb phrase. An example of a relative clause is that stinks in â€œThe wumpus that stinks is in
2 2.â€ Another kind of relative clause has no relative pronoun, e.g., I know in â€œthe man I know.â€
2

Section 23.1.

Phrase Structure Grammars

Noun
Verb
Adjective
Adverb
Pronoun
RelPro
Name
Article
Prep
Conj
Digit

â†’
â†’
â†’
â†’
â†’
â†’
â†’
â†’
â†’
â†’
â†’

891

stench [0.05] | breeze [0.10] | wumpus [0.15] | pits [0.05] | . . .
is [0.10] | feel [0.10] | smells [0.10] | stinks [0.05] | . . .
right [0.10] | dead [0.05] | smelly [0.02] | breezy [0.02] . . .
here [0.05] | ahead [0.05] | nearby [0.02] | . . .
me [0.10] | you [0.03] | I [0.10] | it [0.10] | . . .
that [0.40] | which [0.15] | who [0.20] | whom [0.02] âˆ¨ . . .
John [0.01] | Mary [0.01] | Boston [0.01] | . . .
the [0.40] | a [0.30] | an [0.10] | every [0.05] | . . .
to [0.20] | in [0.10] | on [0.05] | near [0.10] | . . .
and [0.50] | or [0.10] | but [0.20] | yet [0.02] âˆ¨ . . .
0 [0.20] | 1 [0.20] | 2 [0.20] | 3 [0.20] | 4 [0.20] | . . .

Figure 23.1 The lexicon for E0 . RelPro is short for relative pronoun, Prep for preposition,
and Conj for conjunction. The sum of the probabilities for each category is 1.

E0 :

S â†’ NP VP
| S Conj S

[0.90] I + feel a breeze
[0.10] I feel a breeze + and + It stinks

NP â†’
|
|
|
|
|
|
|

Pronoun
Name
Noun
Article Noun
Article Adjs Noun
Digit Digit
NP PP
NP RelClause

[0.30]
[0.10]
[0.10]
[0.25]
[0.05]
[0.05]
[0.10]
[0.05]

I
John
pits
the + wumpus
the + smelly dead + wumpus
34
the wumpus + in 1 3
the wumpus + that is smelly

VP â†’
|
|
|
|

Verb
VP NP
VP Adjective
VP PP
VP Adverb

[0.40]
[0.35]
[0.05]
[0.10]
[0.10]

stinks
feel + a breeze
smells + dead
is + in 1 3
go + ahead

Adjective
Adjective Adjs
Prep NP
RelPro VP

[0.80]
[0.20]
[1.00]
[1.00]

smelly
smelly + dead
to + the east
that + is smelly

Adjs â†’
|
PP â†’
RelClause â†’

Figure 23.2 The grammar for E0 , with example phrases for each rule. The syntactic categories are sentence (S ), noun phrase (NP ), verb phrase (VP), list of adjectives (Adjs),
prepositional phrase (PP ), and relative clause (RelClause).

892

Chapter 23.

Natural Language for Communication

S
0.90

NP

VP
0.40

0.25

Article

Noun

0.05

Every

0.15

wumpus

Verb
0.10

smells

Figure 23.3 Parse tree for the sentence â€œEvery wumpus smellsâ€ according to the grammar
E0 . Each interior node of the tree is labeled with its probability. The probability of the tree
as a whole is 0.9 Ã— 0.25 Ã— 0.05 Ã— 0.15 Ã— 0.40 Ã— 0.10 = 0.0000675. Since this tree is the only
parse of the sentence, that number is also the probability of the sentence. The tree can also
be written in linear form as [S [NP [Article every] [Noun wumpus]][VP [Verb smells]]].

OVERGENERATION
UNDERGENERATION

23.2
PARSING

gives a constructive proof that the string of words is indeed a sentence according to the rules
of E0 . The E0 grammar generates a wide range of English sentences such as the following:
John is in the pit
The wumpus that stinks is in 2 2
Mary is in Boston and the wumpus is near 3 2
Unfortunately, the grammar overgenerates: that is, it generates sentences that are not grammatical, such as â€œMe go Bostonâ€ and â€œI smell pits wumpus John.â€ It also undergenerates:
there are many sentences of English that it rejects, such as â€œI think the wumpus is smelly.â€
We will see how to learn a better grammar later; for now we concentrate on what we can do
with the grammar we have.

S YNTACTIC A NALYSIS (PARSING )
Parsing is the process of analyzing a string of words to uncover its phrase structure, according
to the rules of a grammar. Figure 23.4 shows that we can start with the S symbol and search
top down for a tree that has the words as its leaves, or we can start with the words and search
bottom up for a tree that culminates in an S . Both top-down and bottom-up parsing can be
inefficient, however, because they can end up repeating effort in areas of the search space that
lead to dead ends. Consider the following two sentences:
Have the students in section 2 of Computer Science 101 take the exam.
Have the students in section 2 of Computer Science 101 taken the exam?
Even though they share the first 10 words, these sentences have very different parses, because
the first is a command and the second is a question. A left-to-right parsing algorithm would
have to guess whether the first word is part of a command or a question and will not be able
to tell if the guess is correct until at least the eleventh word, take or taken. If the algorithm
guesses wrong, it will have to backtrack all the way to the first word and reanalyze the whole
sentence under the other interpretation.

Section 23.2.

Syntactic Analysis (Parsing)

893

List of items

Rule

S
NP VP
NP VP Adjective
NP Verb Adjective
NP Verb dead
NP is dead
Article Noun is dead
Article wumpus is dead
the wumpus is dead

S â†’ NP VP
VP â†’ VP Adjective
VP â†’ Verb
Adjective â†’ dead
Verb â†’ is
NP â†’ Article Noun
Noun â†’ wumpus
Article â†’ the

Figure 23.4 Trace of the process of finding a parse for the string â€œThe wumpus is deadâ€
as a sentence, according to the grammar E0 . Viewed as a top-down parse, we start with the
list of items being S and, on each step, match an item X with a rule of the form (X â†’ . . . )
and replace X in the list of items with (. . . ). Viewed as a bottom-up parse, we start with the
list of items being the words of the sentence, and, on each step, match a string of tokens (. . . )
in the list against a rule of the form (X â†’ . . . ) and replace (. . . ) with X .

CHART

CYK ALGORITHM

CHOMSKY NORMAL
FORM

To avoid this source of inefficiency we can use dynamic programming: every time we
analyze a substring, store the results so we wonâ€™t have to reanalyze it later. For example,
once we discover that â€œthe students in section 2 of Computer Science 101â€ is an NP , we can
record that result in a data structure known as a chart. Algorithms that do this are called chart
parsers. Because we are dealing with context-free grammars, any phrase that was found in
the context of one branch of the search space can work just as well in any other branch of the
search space. There are many types of chart parsers; we describe a bottom-up version called
the CYK algorithm, after its inventors, John Cocke, Daniel Younger, and Tadeo Kasami.
The CYK algorithm is shown in Figure 23.5. Note that it requires a grammar with all
rules in one of two very specific formats: lexical rules of the form X â†’ word, and syntactic
rules of the form X â†’ Y Z . This grammar format, called Chomsky Normal Form, may
seem restrictive, but it is not: any context-free grammar can be automatically transformed
into Chomsky Normal Form. Exercise 23.8 leads you through the process.
The CYK algorithm uses space of O(n2 m) for the P table, where n is the number of
words in the sentence, and m is the number of nonterminal symbols in the grammar, and takes
time O(n3 m). (Since m is constant for a particular grammar, this is commonly described as
O(n3 ).) No algorithm can do better for general context-free grammars, although there are
faster algorithms on more restricted grammars. In fact, it is quite a trick for the algorithm to
complete in O(n3 ) time, given that it is possible for a sentence to have an exponential number
of parse trees. Consider the sentence
Fall leaves fall and spring leaves spring.
It is ambiguous because each word (except â€œandâ€) can be either a noun or a verb, and â€œfallâ€
and â€œspringâ€ can be adjectives as well. (For example, one meaning of â€œFall leaves fallâ€ is

894

Chapter 23.

Natural Language for Communication

function CYK-PARSE (words, grammar ) returns P , a table of probabilities
N â† L ENGTH(words)
M â† the number of nonterminal symbols in grammar
P â† an array of size [M , N , N ], initially all 0
/* Insert lexical rules for each word */
for i = 1 to N do
for each rule of form (X â†’ words i [p]) do
P [X , i, 1] â† p
/* Combine first and second parts of right-hand sides of rules, from short to long */
for length = 2 to N do
for start = 1 to N âˆ’ length + 1 do
for len1 = 1 to N âˆ’ 1 do
len2 â† length âˆ’ len1
for each rule of the form (X â†’ Y Z [p]) do
P [X , start, length] â† M AX(P [X , start, length],
P [Y , start , len1 ] Ã— P [Z , start + len1 , len2 ] Ã— p)
return P
Figure 23.5 The CYK algorithm for parsing. Given a sequence of words, it finds the
most probable derivation for the whole sequence and for each subsequence. It returns the
whole table, P , in which an entry P [X , start, len] is the probability of the most probable
X of length len starting at position start . If there is no X of that size at that location, the
probability is 0.

equivalent to â€œAutumn abandons autumn.) With E0 the sentence has four parses:
[S
[S
[S
[S

[S
[S
[S
[S

[NP Fall leaves] fall] and [S
[NP Fall leaves] fall] and [S
Fall [VP leaves fall]] and [S
Fall [VP leaves fall]] and [S

[NP spring leaves] spring]
spring [VP leaves spring]]
[NP spring leaves] spring]
spring [VP leaves spring]] .

If we had c two-ways-ambiguous conjoined subsentences, we would have 2c ways of choosing parses for the subsentences.3 How does the CYK algorithm process these 2c parse trees
in O(c3 ) time? The answer is that it doesnâ€™t examine all the parse trees; all it has to do is
compute the probability of the most probable tree. The subtrees are all represented in the P
table, and with a little work we could enumerate them all (in exponential time), but the beauty
of the CYK algorithm is that we donâ€™t have to enumerate them unless we want to.
In practice we are usually not interested in all parses; just the best one or best few. Think
of the CYK algorithm as defining the complete state space defined by the â€œapply grammar
ruleâ€ operator. It is possible to search just part of this space using Aâˆ— search. Each state
in this space is a list of items (words or categories), as shown in the bottom-up parse table
(Figure 23.4). The start state is a list of words, and a goal state is the single item S . The
3

There also would be O(c!) ambiguity in the way the components conjoinâ€”for example, (X and (Y and Z))
versus ((X and Y ) and Z). But that is another story, one told well by Church and Patil (1982).

Section 23.2.

Syntactic Analysis (Parsing)

895

[ [S [NP-SBJ-2 Her eyes]
[VP were
[VP glazed
[NP *-2]
[SBAR-ADV as if
[S [NP-SBJ she]
[VP did nâ€™t
[VP [VP hear [NP *-1]]
or
[VP [ADVP even] see [NP *-1]]
[NP-1 him]]]]]]]]
.]
Figure 23.6 Annotated tree for the sentence â€œHer eyes were glazed as if she didnâ€™t hear
or even see him.â€ from the Penn Treebank. Note that in this grammar there is a distinction
between an object noun phrase (NP) and a subject noun phrase (NP-SBJ). Note also a grammatical phenomenon we have not covered yet: the movement of a phrase from one part of
the tree to another. This tree analyzes the phrase â€œhear or even see himâ€ as consisting of two
constituent VPs, [VP hear [NP *-1]] and [VP [ADVP even] see [NP *-1]], both of which
have a missing object, denoted *-1, which refers to the NP labeled elsewhere in the tree as
[NP-1 him].

cost of a state is the inverse of its probability as defined by the rules applied so far, and there
are various heuristics to estimate the remaining distance to the goal; the best heuristics come
from machine learning applied to a corpus of sentences. With the Aâˆ— algorithm we donâ€™t have
to search the entire state space, and we are guaranteed that the first parse found will be the
most probable.

23.2.1 Learning probabilities for PCFGs

TREEBANK

A PCFG has many rules, with a probability for each rule. This suggests that learning the
grammar from data might be better than a knowledge engineering approach. Learning is easiest if we are given a corpus of correctly parsed sentences, commonly called a treebank. The
Penn Treebank (Marcus et al., 1993) is the best known; it consists of 3 million words which
have been annotated with part of speech and parse-tree structure, using human labor assisted
by some automated tools. Figure 23.6 shows an annotated tree from the Penn Treebank.
Given a corpus of trees, we can create a PCFG just by counting (and smoothing). In the
example above, there are two nodes of the form [S [NP . . .][VP . . .]]. We would count these,
and all the other subtrees with root S in the corpus. If there are 100,000 S nodes of which
60,000 are of this form, then we create the rule:
S â†’ NP VP [0.60] .
What if a treebank is not available, but we have a corpus of raw unlabeled sentences? It is
still possible to learn a grammar from such a corpus, but it is more difficult. First of all,
we actually have two problems: learning the structure of the grammar rules and learning the

896

INSIDEâ€“OUTSIDE
ALGORITHM

Chapter 23.

Natural Language for Communication

probabilities associated with each rule. (We have the same distinction in learning Bayes nets.)
Weâ€™ll assume that weâ€™re given the lexical and syntactic category names. (If not, we can just
assume categories X1 , . . . Xn and use cross-validation to pick the best value of n.) We can
then assume that the grammar includes every possible (X â†’ Y Z ) or (X â†’ word) rule,
although many of these rules will have probability 0 or close to 0.
We can then use an expectationâ€“maximization (EM) approach, just as we did in learning
HMMs. The parameters we are trying to learn are the rule probabilities; we start them off at
random or uniform values. The hidden variables are the parse trees: we donâ€™t know whether
a string of words wi . . . wj is or is not generated by a rule (X â†’ . . .). The E step estimates
the probability that each subsequence is generated by each rule. The M step then estimates
the probability of each rule. The whole computation can be done in a dynamic-programming
fashion with an algorithm called the insideâ€“outside algorithm in analogy to the forwardâ€“
backward algorithm for HMMs.
The insideâ€“outside algorithm seems magical in that it induces a grammar from unparsed
text. But it has several drawbacks. First, the parses that are assigned by the induced grammars
are often difficult to understand and unsatisfying to linguists. This makes it hard to combine
handcrafted knowledge with automated induction. Second, it is slow: O(n3 m3 ), where n is
the number of words in a sentence and m is the number of grammar categories. Third, the
space of probability assignments is very large, and empirically it seems that getting stuck in
local maxima is a severe problem. Alternatives such as simulated annealing can get closer to
the global maximum, at a cost of even more computation. Lari and Young (1990) conclude
that insideâ€“outside is â€œcomputationally intractable for realistic problems.â€
However, progress can be made if we are willing to step outside the bounds of learning
solely from unparsed text. One approach is to learn from prototypes: to seed the process with
a dozen or two rules, similar to the rules in E1 . From there, more complex rules can be learned
more easily, and the resulting grammar parses English with an overall recall and precision for
sentences of about 80% (Haghighi and Klein, 2006). Another approach is to use treebanks,
but in addition to learning PCFG rules directly from the bracketings, also learning distinctions
that are not in the treebank. For example, not that the tree in Figure 23.6 makes the distinction
between NP and NP âˆ’ SBJ . The latter is used for the pronoun â€œshe,â€ the former for the
pronoun â€œher.â€ We will explore this issue in Section 23.6; for now let us just say that there
are many ways in which it would be useful to split a category like NPâ€”grammar induction
systems that use treebanks but automatically split categories do better than those that stick
with the original category set (Petrov and Klein, 2007c). The error rates for automatically
learned grammars are still about 50% higher than for hand-constructed grammar, but the gap
is decreasing.

23.2.2 Comparing context-free and Markov models
The problem with PCFGs is that they are context-free. That means that the difference between
P (â€œeat a bananaâ€) and P (â€œeat a bandannaâ€) depends only on P (Noun â†’ â€œbananaâ€) versus
P (Noun â†’ â€œbandannaâ€) and not on the relation between â€œeatâ€ and the respective objects.
A Markov model of order two or more, given a sufficiently large corpus, will know that â€œeat

Section 23.3.

Augmented Grammars and Semantic Interpretation

897

a bananaâ€ is more probable. We can combine a PCFG and Markov model to get the best of
both. The simplest approach is to estimate the probability of a sentence with the geometric
mean of the probabilities computed by both models. Then we would know that â€œeat a bananaâ€
is probable from both the grammatical and lexical point of view. But it still wouldnâ€™t pick up
the relation between â€œeatâ€ and â€œbananaâ€ in â€œeat a slightly aging but still palatable bananaâ€
because here the relation is more than two words away. Increasing the order of the Markov
model wonâ€™t get at the relation precisely; to do that we can use a lexicalized PCFG, as
described in the next section.
Another problem with PCFGs is that they tend to have too strong a preference for shorter
sentences. In a corpus such as the Wall Street Journal, the average length of a sentence
is about 25 words. But a PCFG will usually assign fairly high probability to many short
sentences, such as â€œHe slept,â€ whereas in the Journal weâ€™re more likely to see something like
â€œIt has been reported by a reliable source that the allegation that he slept is credible.â€ It seems
that the phrases in the Journal really are not context-free; instead the writers have an idea of
the expected sentence length and use that length as a soft global constraint on their sentences.
This is hard to reflect in a PCFG.

23.3

AUGMENTED G RAMMARS AND S EMANTIC I NTERPRETATION
In this section we see how to extend context-free grammarsâ€”to say that, for example, not
every NP is independent of context, but rather, certain NPs are more likely to appear in one
context, and others in another context.

23.3.1 Lexicalized PCFGs
LEXICALIZED PCFG

HEAD

AUGMENTED
GRAMMAR

To get at the relationship between the verb â€œeatâ€ and the nouns â€œbananaâ€ versus â€œbandanna,â€
we can use a lexicalized PCFG, in which the probabilities for a rule depend on the relationship between words in the parse tree, not just on the adjacency of words in a sentence. Of
course, we canâ€™t have the probability depend on every word in the tree, because we wonâ€™t
have enough training data to estimate all those probabilities. It is useful to introduce the notion of the head of a phraseâ€”the most important word. Thus, â€œeatâ€ is the head of the VP
â€œeat a bananaâ€ and â€œbananaâ€ is the head of the NP â€œa banana.â€ We use the notation VP (v)
to denote a phrase with category VP whose head word is v. We say that the category VP
is augmented with the head variable v. Here is an augmented grammar that describes the
verbâ€“object relation:
VP (v) â†’ Verb(v) NP (n)
VP (v) â†’ Verb(v)
NP (n) â†’ Article(a) Adjs(j) Noun(n)
Noun(banana) â†’ banana
...

[P1 (v, n)]
[P2 (v)]
[P3 (n, a)]
[pn ]
...

Here the probability P1 (v, n) depends on the head words v and n. We would set this probability to be relatively high when v is â€œeatâ€ and n is â€œbanana,â€ and low when n is â€œbandanna.â€

898

Chapter 23.

Natural Language for Communication

Note that since we are considering only heads, the distinction between â€œeat a bananaâ€ and
â€œeat a rancid bananaâ€ will not be caught by these probabilities. Another issue with this approach is that, in a vocabulary with, say, 20,000 nouns and 5,000 verbs, P1 needs 100 million
probability estimates. Only a few percent of these can come from a corpus; the rest will have
to come from smoothing (see Section 22.1.2). For example, we can estimateP1 (v, n) for a
(v, n) pair that we have not seen often (or at all) by backing off to a model that depends
only on v. These objectless probabilities are still very useful; they can capture the distinction
between a transitive verb like â€œeatâ€â€”which will have a high value for P1 and a low value for
P2 â€”and an intransitive verb like â€œsleep,â€ which will have the reverse. It is quite feasible to
learn these probabilities from a treebank.

23.3.2 Formal definition of augmented grammar rules

DEFINITE CLAUSE
GRAMMAR

Augmented rules are complicated, so we will give them a formal definition by showing how
an augmented rule can be translated into a logical sentence. The sentence will have the form
of a definite clause (see page 256), so the result is called a definite clause grammar, or DCG.
Weâ€™ll use as an example a version of a rule from the lexicalized grammar for NP with one
new piece of notation:
NP (n) â†’ Article(a) Adjs(j) Noun(n) {Compatible (j, n)} .
The new aspect here is the notation {constraint } to denote a logical constraint on some of the
variables; the rule only holds when the constraint is true. Here the predicate Compatible (j, n)
is meant to test whether adjective j and noun n are compatible; it would be defined by a series
of assertions such as Compatible (black, dog). We can convert this grammar rule into a definite clause by (1) reversing the order of right- and left-hand sides, (2) making a conjunction
of all the constituents and constraints, (3) adding a variable si to the list of arguments for each
constituent to represent the sequence of words spanned by the constituent, (4) adding a term
for the concatenation of words, Append (s1 , . . .), to the list of arguments for the root of the
tree. That gives us
Article(a, s1 ) âˆ§ Adjs(j, s2 ) âˆ§ Noun(n, s3 ) âˆ§ Compatible (j, n)
â‡’ NP(n, Append (s1 , s2 , s3 )) .
This definite clause says that if the predicate Article is true of a head word a and a string s1 ,
and Adjs is similarly true of a head word j and a string s2 , and Noun is true of a head word
n and a string s3 , and if j and n are compatible, then the predicate N P is true of the head
word n and the result of appending strings s1 , s2 , and s3 .
The DCG translation left out the probabilities, but we could put them back in: just augment each constituent with one more variable representing the probability of the constituent,
and augment the root with a variable that is the product of the constituent probabilities times
the rule probability.
The translation from grammar rule to definite clause allows us to talk about parsing
as logical inference. This makes it possible to reason about languages and strings in many
different ways. For example, it means we can do bottom-up parsing using forward chaining or
top-down parsing using backward chaining. In fact, parsing natural language with DCGs was

Section 23.3.

LANGUAGE
GENERATION

Augmented Grammars and Semantic Interpretation

899

one of the first applications of (and motivations for) the Prolog logic programming language.
It is sometimes possible to run the process backward and do language generation as well as
parsing. For example, skipping ahead to Figure 23.10 (page 903), a logic program could be
given the semantic form Loves(John, Mary) and apply the definite-clause rules to deduce
S(Loves(John, Mary), [John, loves, Mary]) .
This works for toy examples, but serious language-generation systems need more control over
the process than is afforded by the DCG rules alone.
E1 :

E2 :

S
NP S
NP O
VP
PP
Pronoun S
Pronoun O

â†’
â†’
â†’
â†’
â†’
â†’
â†’

NP S VP | . . .
Pronoun S | Name | Noun | . . .
Pronoun O | Name | Noun | . . .
VP NP O | . . .
Prep NP O
I | you | he | she | it | . . .
me | you | him | her | it | . . .
...

S (head )
NP (c, pn, head )
VP (pn, head )
PP (head )
Pronoun(Sbj , 1S , I)
Pronoun(Sbj , 1P , we)
Pronoun(Obj , 1S , me)
Pronoun(Obj , 3P , them)

â†’
â†’
â†’
â†’
â†’
â†’
â†’
â†’

NP (Sbj , pn, h) VP(pn, head ) | . . .
Pronoun(c, pn, head ) | Noun(c, pn, head ) | . . .
VP (pn, head ) NP(Obj , p, h) | . . .
Prep(head ) NP(Obj , pn, h)
I
we
me
them
...

Figure 23.7 Top: part of a grammar for the language E1 , which handles subjective and
objective cases in noun phrases and thus does not overgenerate quite as badly as E0 . The
portions that are identical to E0 have been omitted. Bottom: part of an augmented grammar
for E2 , with three augmentations: case agreement, subjectâ€“verb agreement, and head word.
Sbj, Obj, 1S, 1P and 3P are constants, and lowercase names are variables.

23.3.3 Case agreement and subjectâ€“verb agreement
We saw in Section 23.1 that the simple grammar for E0 overgenerates, producing nonsentences such as â€œMe smell a stench.â€ To avoid this problem, our grammar would have to know
that â€œmeâ€ is not a valid NP when it is the subject of a sentence. Linguists say that the pronoun
â€œIâ€ is in the subjective case, and â€œmeâ€ is in the objective case.4 We can account for this by
4

The subjective case is also sometimes called the nominative case and the objective case is sometimes called
the accusative case. Many languages also have a dative case for words in the indirect object position.

900

CASE AGREEMENT

SUBJECTâ€“VERB
AGREEMENT

Chapter 23.

Natural Language for Communication

splitting NP into two categories, NP S and NP O , to stand for noun phrases in the subjective
and objective case, respectively. We would also need to split the category Pronoun into the
two categories Pronoun S (which includes â€œIâ€) and Pronoun O (which includes â€œmeâ€). The
top part of Figure 23.7 shows the grammar for case agreement; we call the resulting language
E1 . Notice that all the NP rules must be duplicated, once for NP S and once for NP O .
Unfortunately, E1 still overgenerates. English requires subjectâ€“verb agreement for
person and number of the subject and main verb of a sentence. For example, if â€œIâ€ is the
subject, then â€œI smellâ€ is grammatical, but â€œI smellsâ€ is not. If â€œitâ€ is the subject, we get the
reverse. In English, the agreement distinctions are minimal: most verbs have one form for
third-person singular subjects (he, she, or it), and a second form for all other combinations
of person and number. There is one exception: the verb â€œto beâ€ has three forms, â€œI am / you
are / he is.â€ So one distinction (case) splits N P two ways, another distinction (person and
number) splits N P three ways, and as we uncover other distinctions we would end up with an
exponential number of subscripted N P forms if we took the approach of E1 . Augmentations
are a better approach: they can represent an exponential number of forms as a single rule.
In the bottom of Figure 23.7 we see (part of) an augmented grammar for the language
E2 , which handles case agreement, subjectâ€“verb agreement, and head words. We have just
one N P category, but NP(c, pn, head ) has three augmentations: c is a parameter for case,
pn is a parameter for person and number, and head is a parameter for the head word of
the phrase. The other categories also are augmented with heads and other arguments. Letâ€™s
consider one rule in detail:
S (head ) â†’ NP(Sbj , pn, h) VP (pn, head ) .
This rule is easiest to understand right-to-left: when an NP and a VP are conjoined they form
an S, but only if the NP has the subjective (Sbj) case and the person and number (pn) of the
NP and VP are identical. If that holds, then we have an S whose head is the same as the
head of the VP. Note the head of the NP, denoted by the dummy variable h, is not part of the
augmentation of the S. The lexical rules for E2 fill in the values of the parameters and are also
best read right-to-left. For example, the rule
Pronoun(Sbj , 1S , I) â†’ I
says that â€œIâ€ can be interpreted as a Pronoun in the subjective case, first-person singular, with
head â€œI.â€ For simplicity we have omitted the probabilities for these rules, but augmentation
does work with probabilities. Augmentation can also work with automated learning mechanisms. Petrov and Klein (2007c) show how a learning algorithm can automatically split the
NP category into NP S and NP O .

23.3.4 Semantic interpretation
To show how to add semantics to a grammar, we start with an example that is simpler than
English: the semantics of arithmetic expressions. Figure 23.8 shows a grammar for arithmetic
expressions, where each rule is augmented with a variable indicating the semantic interpretation of the phrase. The semantics of a digit such as â€œ3â€ is the digit itself. The semantics of an
expression such as â€œ3 + 4â€ is the operator â€œ+â€ applied to the semantics of the phrase â€œ3â€ and

Section 23.3.

Augmented Grammars and Semantic Interpretation

901

Exp(x) â†’ Exp(x1 ) Operator (op) Exp(x2 ) {x = Apply(op, x1 , x2 )}
Exp(x) â†’ ( Exp(x) )
Exp(x) â†’ Number (x)
Number (x) â†’ Digit(x)
Number (x) â†’ Number(x1 ) Digit(x2 ) {x = 10 Ã— x1 + x2 }
Digit(x) â†’ x {0 â‰¤ x â‰¤ 9}
Operator (x) â†’ x {x âˆˆ {+, âˆ’, Ã·, Ã—}}
Figure 23.8 A grammar for arithmetic expressions, augmented with semantics. Each variable xi represents the semantics of a constituent. Note the use of the {test} notation to define
logical predicates that must be satisfied, but that are not constituents.

Exp(5)
Exp(2)
Exp(2)
Exp(3)

Exp(4)

Number(3)
Digit(3)

3
Figure 23.9
COMPOSITIONAL
SEMANTICS

Exp(2)

Number(4)
Operator(+)

+

Number(2)

Digit(4) Operator(Ã·)

(

4

Ã·

Digit(2)

2

)

Parse tree with semantic interpretations for the string â€œ3 + (4 Ã· 2)â€.

the phrase â€œ4.â€ The rules obey the principle of compositional semanticsâ€”the semantics of
a phrase is a function of the semantics of the subphrases. Figure 23.9 shows the parse tree for
3 + (4 Ã· 2) according to this grammar. The root of the parse tree is Exp(5), an expression
whose semantic interpretation is 5.
Now letâ€™s move on to the semantics of English, or at least of E0 . We start by determining what semantic representations we want to associate with what phrases. We use the simple
example sentence â€œJohn loves Mary.â€ The NP â€œJohnâ€ should have as its semantic interpretation the logical term John, and the sentence as a whole should have as its interpretation the
logical sentence Loves(John, Mary). That much seems clear. The complicated part is the
VP â€œloves Mary.â€ The semantic interpretation of this phrase is neither a logical term nor a
complete logical sentence. Intuitively, â€œloves Maryâ€ is a description that might or might not

902

Chapter 23.

Natural Language for Communication

apply to a particular person. (In this case, it applies to John.) This means that â€œloves Maryâ€
is a predicate that, when combined with a term that represents a person (the person doing
the loving), yields a complete logical sentence. Using the Î»-notation (see page 294), we can
represent â€œloves Maryâ€ as the predicate
Î»x Loves(x, Mary) .
Now we need a rule that says â€œan NP with semantics obj followed by a VP with semantics
pred yields a sentence whose semantics is the result of applying pred to obj :â€
S(pred (obj )) â†’ NP (obj ) VP(pred ) .
The rule tells us that the semantic interpretation of â€œJohn loves Maryâ€ is
(Î»x Loves(x, Mary))(John) ,
which is equivalent to Loves(John, Mary).
The rest of the semantics follows in a straightforward way from the choices we have
made so far. Because VP s are represented as predicates, it is a good idea to be consistent and
represent verbs as predicates as well. The verb â€œlovesâ€ is represented as Î»y Î»x Loves(x, y),
the predicate that, when given the argument Mary, returns the predicate Î»x Loves(x, Mary).
We end up with the grammar shown in Figure 23.10 and the parse tree shown in Figure 23.11.
We could just as easily have added semantics to E2 ; we chose to work with E0 so that the
reader can focus on one type of augmentation at a time.
Adding semantic augmentations to a grammar by hand is laborious and error prone.
Therefore, there have been several projects to learn semantic augmentations from examples.
C HILL (Zelle and Mooney, 1996) is an inductive logic programming (ILP) program that
learns a grammar and a specialized parser for that grammar from examples. The target domain
is natural language database queries. The training examples consist of pairs of word strings
and corresponding semantic formsâ€”for example;
What is the capital of the state with the largest population?
Answer(c, Capital (s, c) âˆ§ Largest (p, State(s) âˆ§ Population(s, p)))
C HILL â€™s task is to learn a predicate Parse(words , semantics) that is consistent with the examples and, hopefully, generalizes well to other examples. Applying ILP directly to learn
this predicate results in poor performance: the induced parser has only about 20% accuracy.
Fortunately, ILP learners can improve by adding knowledge. In this case, most of the Parse
predicate was defined as a logic program, and C HILL â€™s task was reduced to inducing the
control rules that guide the parser to select one parse over another. With this additional background knowledge, C HILL can learn to achieve 70% to 85% accuracy on various database
query tasks.

23.3.5 Complications
TIME AND TENSE

The grammar of real English is endlessly complex. We will briefly mention some examples.
Time and tense: Suppose we want to represent the difference between â€œJohn loves
Maryâ€ and â€œJohn loved Mary.â€ English uses verb tenses (past, present, and future) to indicate

Section 23.3.

Augmented Grammars and Semantic Interpretation

903

S(pred (obj )) â†’ NP (obj ) VP(pred )
VP (pred (obj )) â†’ Verb(pred ) NP (obj )
NP (obj ) â†’ Name(obj )
Name(John) â†’ John
Name(Mary) â†’ Mary
Verb(Î»y Î»x Loves(x, y)) â†’ loves
Figure 23.10 A grammar that can derive a parse tree and semantic interpretation for â€œJohn
loves Maryâ€ (and three other sentences). Each category is augmented with a single argument
representing the semantics.

S(Loves(John,Mary))
VP(Î»x Loves(x,Mary))
NP(John)

NP(Mary)

Name(John) Verb(Î»y Î»x Loves(x,y))
John
Figure 23.11

loves

Name(Mary)
Mary

A parse tree with semantic interpretations for the string â€œJohn loves Maryâ€.

the relative time of an event. One good choice to represent the time of events is the event
calculus notation of Section 12.3. In event calculus we have
John loves mary: E1 âˆˆ Loves(John, Mary) âˆ§ During(Now , Extent(E1 ))
John loved mary: E2 âˆˆ Loves(John, Mary) âˆ§ After (Now , Extent(E2 )) .
This suggests that our two lexical rules for the words â€œlovesâ€ and â€œlovedâ€ should be these:

QUANTIFICATION

Verb(Î»y Î»x e âˆˆ Loves(x, y) âˆ§ During(Now , e)) â†’ loves
Verb(Î»y Î»x e âˆˆ Loves(x, y) âˆ§ After(Now , e)) â†’ loved .
Other than this change, everything else about the grammar remains the same, which is encouraging news; it suggests we are on the right track if we can so easily add a complication
like the tense of verbs (although we have just scratched the surface of a complete grammar
for time and tense). It is also encouraging that the distinction between processes and discrete
events that we made in our discussion of knowledge representation in Section 12.3.1 is actually reflected in language use. We can say â€œJohn slept a lot last night,â€ where Sleeping is a
process category, but it is odd to say â€œJohn found a unicorn a lot last night,â€ where Finding
is a discrete event category. A grammar would reflect that fact by having a low probability
for adding the adverbial phrase â€œa lotâ€ to discrete events.
Quantification: Consider the sentence â€œEvery agent feels a breeze.â€ The sentence has
only one syntactic parse under E0 , but it is actually semantically ambiguous; the preferred

904

QUASI-LOGICAL
FORM

PRAGMATICS

INDEXICAL

SPEECH ACT

LONG-DISTANCE
DEPENDENCIES

TRACE

AMBIGUITY

Chapter 23.

Natural Language for Communication

meaning is â€œFor every agent there exists a breeze that the agent feels,â€ but an acceptable
alternative meaning is â€œThere exists a breeze that every agent feels.â€ 5 The two interpretations
can be represented as
âˆ€ a a âˆˆ Agents â‡’
âˆƒ b b âˆˆ Breezes âˆ§ âˆƒ e e âˆˆ Feel (a, b) âˆ§ During(Now , e) ;
âˆƒ b b âˆˆ Breezes âˆ€ a a âˆˆ Agents â‡’
âˆƒ e e âˆˆ Feel (a, b) âˆ§ During(Now , e) .
The standard approach to quantification is for the grammar to define not an actual logical
semantic sentence, but rather a quasi-logical form that is then turned into a logical sentence
by algorithms outside of the parsing process. Those algorithms can have preference rules for
preferring one quantifier scope over anotherâ€”preferences that need not be reflected directly
in the grammar.
Pragmatics: We have shown how an agent can perceive a string of words and use a
grammar to derive a set of possible semantic interpretations. Now we address the problem
of completing the interpretation by adding context-dependent information about the current
situation. The most obvious need for pragmatic information is in resolving the meaning of
indexicals, which are phrases that refer directly to the current situation. For example, in the
sentence â€œI am in Boston today,â€ both â€œIâ€ and â€œtodayâ€ are indexicals. The word â€œIâ€ would be
represented by the fluent Speaker , and it would be up to the hearer to resolve the meaning of
the fluentâ€”that is not considered part of the grammar but rather an issue of pragmatics; of
using the context of the current situation to interpret fluents.
Another part of pragmatics is interpreting the speakerâ€™s intent. The speakerâ€™s action is
considered a speech act, and it is up to the hearer to decipher what type of action it isâ€”a
question, a statement, a promise, a warning, a command, and so on. A command such as
â€œgo to 2 2â€ implicitly refers to the hearer. So far, our grammar for S covers only declarative
sentences. We can easily extend it to cover commands. A command can be formed from
a VP , where the subject is implicitly the hearer. We need to distinguish commands from
statements, so we alter the rules for S to include the type of speech act:
S(Statement (Speaker , pred (obj ))) â†’ NP (obj ) VP (pred )
S(Command (Speaker , pred (Hearer ))) â†’ VP(pred ) .
Long-distance dependencies: Questions introduce a new grammatical complexity. In
â€œWho did the agent tell you to give the gold to?â€ the final word â€œtoâ€ should be parsed as
[PP to ], where the â€œ â€ denotes a gap or trace where an NP is missing; the missing NP
is licensed by the first word of the sentence, â€œwho.â€ A complex system of augmentations is
used to make sure that the missing NP s match up with the licensing words in just the right
way, and prohibit gaps in the wrong places. For example, you canâ€™t have a gap in one branch
of an NP conjunction: â€œWhat did he play [NP Dungeons and ]?â€ is ungrammatical. But
you can have the same gap in both branches of a VP conjunction: â€œWhat did you [VP [VP
smell ] and [VP shoot an arrow at ]]?â€
Ambiguity: In some cases, hearers are consciously aware of ambiguity in an utterance.
Here are some examples taken from newspaper headlines:
5

If this interpretation seems unlikely, consider â€œEvery Protestant believes in a just God.â€

Section 23.3.

Augmented Grammars and Semantic Interpretation

905

Squad helps dog bite victim.
Police begin campaign to run down jaywalkers.
Helicopter powered by human flies.
Once-sagging cloth diaper industry saved by full dumps.
Portable toilet bombed; police have nothing to go on.
Teacher strikes idle kids.
Include your children when baking cookies.
Hospitals are sued by 7 foot doctors.
Milk drinkers are turning to powder.
Safety experts say school bus passengers should be belted.

LEXICAL AMBIGUITY

SYNTACTIC
AMBIGUITY

SEMANTIC
AMBIGUITY

METONYMY

But most of the time the language we hear seems unambiguous. Thus, when researchers first
began to use computers to analyze language in the 1960s, they were quite surprised to learn
that almost every utterance is highly ambiguous, even though the alternative interpretations
might not be apparent to a native speaker. A system with a large grammar and lexicon might
find thousands of interpretations for a perfectly ordinary sentence. Lexical ambiguity, in
which a word has more than one meaning, is quite common; â€œbackâ€ can be an adverb (go
back), an adjective (back door), a noun (the back of the room) or a verb (back up your files).
â€œJackâ€ can be a name, a noun (a playing card, a six-pointed metal game piece, a nautical flag,
a fish, a socket, or a device for raising heavy objects), or a verb (to jack up a car, to hunt with
a light, or to hit a baseball hard). Syntactic ambiguity refers to a phrase that has multiple
parses: â€œI smelled a wumpus in 2,2â€ has two parses: one where the prepositional phrase â€œin
2,2â€ modifies the noun and one where it modifies the verb. The syntactic ambiguity leads to a
semantic ambiguity, because one parse means that the wumpus is in 2,2 and the other means
that a stench is in 2,2. In this case, getting the wrong interpretation could be a deadly mistake
for the agent.
Finally, there can be ambiguity between literal and figurative meanings. Figures of
speech are important in poetry, but are surprisingly common in everyday speech as well. A
metonymy is a figure of speech in which one object is used to stand for another. When
we hear â€œChrysler announced a new model,â€ we do not interpret it as saying that companies can talk; rather we understand that a spokesperson representing the company made the
announcement. Metonymy is common and is often interpreted unconsciously by human hearers. Unfortunately, our grammar as it is written is not so facile. To handle the semantics of
metonymy properly, we need to introduce a whole new level of ambiguity. We do this by providing two objects for the semantic interpretation of every phrase in the sentence: one for the
object that the phrase literally refers to (Chrysler) and one for the metonymic reference (the
spokesperson). We then have to say that there is a relation between the two. In our current
grammar, â€œChrysler announcedâ€ gets interpreted as
x = Chrysler âˆ§ e âˆˆ Announce(x) âˆ§ After(Now , Extent(e)) .
We need to change that to
x = Chrysler âˆ§ e âˆˆ Announce(m) âˆ§ After (Now , Extent(e))
âˆ§ Metonymy(m, x) .

906

Chapter 23.

Natural Language for Communication

This says that there is one entity x that is equal to Chrysler, and another entity m that did
the announcing, and that the two are in a metonymy relation. The next step is to define what
kinds of metonymy relations can occur. The simplest case is when there is no metonymy at
allâ€”the literal object x and the metonymic object m are identical:
âˆ€ m, x (m = x) â‡’ Metonymy(m, x) .
For the Chrysler example, a reasonable generalization is that an organization can be used to
stand for a spokesperson of that organization:
âˆ€ m, x x âˆˆ Organizations âˆ§ Spokesperson (m, x) â‡’ Metonymy(m, x) .

METAPHOR

DISAMBIGUATION

Other metonymies include the author for the works (I read Shakespeare) or more generally
the producer for the product (I drive a Honda) and the part for the whole (The Red Sox need
a strong arm). Some examples of metonymy, such as â€œThe ham sandwich on Table 4 wants
another beer,â€ are more novel and are interpreted with respect to a situation.
A metaphor is another figure of speech, in which a phrase with one literal meaning is
used to suggest a different meaning by way of an analogy. Thus, metaphor can be seen as a
kind of metonymy where the relation is one of similarity.
Disambiguation is the process of recovering the most probable intended meaning of
an utterance. In one sense we already have a framework for solving this problem: each rule
has a probability associated with it, so the probability of an interpretation is the product of
the probabilities of the rules that led to the interpretation. Unfortunately, the probabilities
reflect how common the phrases are in the corpus from which the grammar was learned,
and thus reflect general knowledge, not specific knowledge of the current situation. To do
disambiguation properly, we need to combine four models:
1. The world model: the likelihood that a proposition occurs in the world. Given what we
know about the world, it is more likely that a speaker who says â€œIâ€™m deadâ€ means â€œI
am in big troubleâ€ rather than â€œMy life ended, and yet I can still talk.â€
2. The mental model: the likelihood that the speaker forms the intention of communicating a certain fact to the hearer. This approach combines models of what the speaker
believes, what the speaker believes the hearer believes, and so on. For example, when
a politician says, â€œI am not a crook,â€ the world model might assign a probability of
only 50% to the proposition that the politician is not a criminal, and 99.999% to the
proposition that he is not a hooked shepherdâ€™s staff. Nevertheless, we select the former
interpretation because it is a more likely thing to say.
3. The language model: the likelihood that a certain string of words will be chosen, given
that the speaker has the intention of communicating a certain fact.
4. The acoustic model: for spoken communication, the likelihood that a particular sequence of sounds will be generated, given that the speaker has chosen a given string of
words. Section 23.5 covers speech recognition.

Section 23.4.

23.4

Machine Translation

907

M ACHINE T RANSLATION
Machine translation is the automatic translation of text from one natural language (the source)
to another (the target). It was one of the first application areas envisioned for computers
(Weaver, 1949), but it is only in the past decade that the technology has seen widespread
usage. Here is a passage from page 1 of this book:
AI is one of the newest fields in science and engineering. Work started in earnest soon
after World War II, and the name itself was coined in 1956. Along with molecular biology, AI is regularly cited as the â€œfield I would most like to be inâ€ by scientists in other
disciplines.

And here it is translated from English to Danish by an online tool, Google Translate:
AI er en af de nyeste omraÌŠder inden for videnskab og teknik. Arbejde startede for alvor
lige efter Anden Verdenskrig, og navnet i sig selv var opfundet i 1956. Sammen med
molekylÃ¦r biologi, er AI jÃ¦vnligt nÃ¦vnt som â€œfeltet Jeg ville de fleste gerne vÃ¦re iâ€ af
forskere i andre discipliner.

For those who donâ€™t read Danish, here is the Danish translated back to English. The words
that came out different are in italics:
AI is one of the newest fields of science and engineering. Work began in earnest just after
the Second World War, and the name itself was invented in 1956. Together with molecular
biology, AI is frequently mentioned as â€œfield I would most like to be inâ€ by researchers
in other disciplines.

The differences are all reasonable paraphrases, such as frequently mentioned for regularly
cited. The only real error is the omission of the article the, denoted by the symbol. This is
typical accuracy: of the two sentences, one has an error that would not be made by a native
speaker, yet the meaning is clearly conveyed.
Historically, there have been three main applications of machine translation. Rough
translation, as provided by free online services, gives the â€œgistâ€ of a foreign sentence or
document, but contains errors. Pre-edited translation is used by companies to publish their
documentation and sales materials in multiple languages. The original source text is written
in a constrained language that is easier to translate automatically, and the results are usually
edited by a human to correct any errors. Restricted-source translation works fully automatically, but only on highly stereotypical language, such as a weather report.
Translation is difficult because, in the fully general case, it requires in-depth understanding of the text. This is true even for very simple textsâ€”even â€œtextsâ€ of one word. Consider
the word â€œOpenâ€ on the door of a store. 6 It communicates the idea that the store is accepting
customers at the moment. Now consider the same word â€œOpenâ€ on a large banner outside a
newly constructed store. It means that the store is now in daily operation, but readers of this
sign would not feel misled if the store closed at night without removing the banner. The two
signs use the identical word to convey different meanings. In German the sign on the door
would be â€œOffenâ€ while the banner would read â€œNeu EroÌˆffnet.â€
6

This example is due to Martin Kay.

908

INTERLINGUA

Chapter 23.

Natural Language for Communication

The problem is that different languages categorize the world differently. For example,
the French word â€œdouxâ€ covers a wide range of meanings corresponding approximately to
the English words â€œsoft,â€ â€œsweet,â€ and â€œgentle.â€ Similarly, the English word â€œhardâ€ covers
virtually all uses of the German word â€œhartâ€ (physically recalcitrant, cruel) and some uses
of the word â€œschwierigâ€ (difficult). Therefore, representing the meaning of a sentence is
more difficult for translation than it is for single-language understanding. An English parsing
system could use predicates like Open(x), but for translation, the representation language
would have to make more distinctions, perhaps with Open 1 (x) representing the â€œOffenâ€ sense
and Open 2 (x) representing the â€œNeu EroÌˆffnetâ€ sense. A representation language that makes
all the distinctions necessary for a set of languages is called an interlingua.
A translator (human or machine) often needs to understand the actual situation described in the source, not just the individual words. For example, to translate the English
word â€œhim,â€ into Korean, a choice must be made between the humble and honorific form, a
choice that depends on the social relationship between the speaker and the referent of â€œhim.â€
In Japanese, the honorifics are relative, so the choice depends on the social relationships between the speaker, the referent, and the listener. Translators (both machine and human) sometimes find it difficult to make this choice. As another example, to translate â€œThe baseball hit
the window. It broke.â€ into French, we must choose the feminine â€œelleâ€ or the masculine
â€œilâ€ for â€œit,â€ so we must decide whether â€œitâ€ refers to the baseball or the window. To get the
translation right, one must understand physics as well as language.
Sometimes there is no choice that can yield a completely satisfactory translation. For
example, an Italian love poem that uses the masculine â€œil soleâ€ (sun) and feminine â€œla lunaâ€
(moon) to symbolize two lovers will necessarily be altered when translated into German,
where the genders are reversed, and further altered when translated into a language where the
genders are the same.7

23.4.1 Machine translation systems

TRANSFER MODEL

All translation systems must model the source and target languages, but systems vary in the
type of models they use. Some systems attempt to analyze the source language text all the way
into an interlingua knowledge representation and then generate sentences in the target language from that representation. This is difficult because it involves three unsolved problems:
creating a complete knowledge representation of everything; parsing into that representation;
and generating sentences from that representation.
Other systems are based on a transfer model. They keep a database of translation rules
(or examples), and whenever the rule (or example) matches, they translate directly. Transfer
can occur at the lexical, syntactic, or semantic level. For example, a strictly syntactic rule
maps English [Adjective Noun] to French [Noun Adjective]. A mixed syntactic and lexical
rule maps French [S1 â€œet puisâ€ S2 ] to English [S1 â€œand thenâ€ S2 ]. Figure 23.12 diagrams the
various transfer points.
7

Warren Weaver (1949) reports that Max Zeldner points out that the great Hebrew poet H. N. Bialik once said
that translation â€œis like kissing the bride through a veil.â€

Section 23.4.

Machine Translation

909
Interlingua Semantics
Attraction(NamedJohn, NamedMary, High)

English Semantics
Loves(John, Mary)
English Syntax
S(NP(John), VP(loves, NP(Mary)))

French Semantics
Aime(Jean, Marie)
French Syntax
S(NP(Jean), VP(aime, NP(Marie)))

English Words
John loves Mary

French Words
Jean aime Marie

Figure 23.12 The Vauquois triangle: schematic diagram of the choices for a machine
translation system (Vauquois, 1968). We start with English text at the top. An interlinguabased system follows the solid lines, parsing English first into a syntactic form, then into
a semantic representation and an interlingua representation, and then through generation to
a semantic, syntactic, and lexical form in French. A transfer-based system uses the dashed
lines as a shortcut. Different systems make the transfer at different points; some make it at
multiple points.

23.4.2 Statistical machine translation
Now that we have seen how complex the translation task can be, it should come as no surprise that the most successful machine translation systems are built by training a probabilistic
model using statistics gathered from a large corpus of text. This approach does not need
a complex ontology of interlingua concepts, nor does it need handcrafted grammars of the
source and target languages, nor a hand-labeled treebank. All it needs is dataâ€”sample translations from which a translation model can be learned. To translate a sentence in, say, English
(e) into French (f ), we find the string of words f âˆ— that maximizes
f âˆ— = argmax P (f | e) = argmax P (e | f ) P (f ) .
f

LANGUAGE MODEL
TRANSLATION
MODEL

Here the factor P (f ) is the target language model for French; it says how probable a given
sentence is in French. P (e|f ) is the translation model; it says how probable an English
sentence is as a translation for a given French sentence. Similarly, P (f | e) is a translation
model from English to French.
Should we work directly on P (f | e), or apply Bayesâ€™ rule and work on P (e | f ) P (f )?
In diagnostic applications like medicine, it is easier to model the domain in the causal direction: P (symptoms | disease) rather than P (disease | symptoms). But in translation both
directions are equally easy. The earliest work in statistical machine translation did apply
Bayesâ€™ ruleâ€”in part because the researchers had a good language model, P (f ), and wanted
to make use of it, and in part because they came from a background in speech recognition,
which is a diagnostic problem. We follow their lead in this chapter, but we note that recent work in statistical machine translation often optimizes P (f | e) directly, using a more
sophisticated model that takes into account many of the features from the language model.

910

BILINGUAL CORPUS

DISTORTION

Chapter 23.

Natural Language for Communication

The language model, P (f ), could address any level(s) on the right-hand side of Figure 23.12, but the easiest and most common approach is to build an n-gram model from a
French corpus, as we have seen before. This captures only a partial, local idea of French
sentences; however, that is often sufficient for rough translation.8
The translation model is learned from a bilingual corpusâ€”a collection of parallel texts,
each an English/French pair. Now, if we had an infinitely large corpus, then translating a
sentence would just be a lookup task: we would have seen the English sentence before in the
corpus, so we could just return the paired French sentence. But of course our resources are
finite, and most of the sentences we will be asked to translate will be novel. However, they
will be composed of phrases that we have seen before (even if some phrases are as short as
one word). For example, in this book, common phrases include â€œin this exercise we will,â€
â€œsize of the state space,â€ â€œas a function of theâ€ and â€œnotes at the end of the chapter.â€ If asked
to translate the novel sentence â€œIn this exercise we will compute the size of the state space as a
function of the number of actions.â€ into French, we should be able to break the sentence into
phrases, find the phrases in the English corpus (this book), find the corresponding French
phrases (from the French translation of the book), and then reassemble the French phrases
into an order that makes sense in French. In other words, given a source English sentence, e,
finding a French translation f is a matter of three steps:
1. Break the English sentence into phrases e1 , . . . , en .
2. For each phrase ei , choose a corresponding French phrase fi . We use the notation
P (fi | ei ) for the phrasal probability that fi is a translation of ei .
3. Choose a permutation of the phrases f1 , . . . , fn . We will specify this permutation in a
way that seems a little complicated, but is designed to have a simple probability distribution: For each fi , we choose a distortion di , which is the number of words that
phrase fi has moved with respect to fiâˆ’1 ; positive for moving to the right, negative for
moving to the left, and zero if fi immediately follows fiâˆ’1 .
Figure 23.13 shows an example of the process. At the top, the sentence â€œThere is a smelly
wumpus sleeping in 2 2â€ is broken into five phrases, e1 , . . . , e5 . Each of them is translated
into a corresponding phrase fi , and then these are permuted into the order f1 , f3 , f4 , f2 , f5 .
We specify the permutation in terms of the distortions di of each French phrase, defined as
di = S TART (fi ) âˆ’ E ND(fiâˆ’1 ) âˆ’ 1 ,
where S TART (fi ) is the ordinal number of the first word of phrase fi in the French sentence,
and E ND (fiâˆ’1 ) is the ordinal number of the last word of phrase fiâˆ’1 . In Figure 23.13 we see
that f5 , â€œaÌ€ 2 2,â€ immediately follows f4 , â€œqui dort,â€ and thus d5 = 0. Phrase f2 , however, has
moved one words to the right of f1 , so d2 = 1. As a special case we have d1 = 0, because f1
starts at position 1 and E ND(f0 ) is defined to be 0 (even though f0 does not exist).
Now that we have defined the distortion, di , we can define the probability distribution
for distortion, P(di ). Note that for sentences bounded by length n we have |di | â‰¤ n , and
8 For the finer points of translation, n-grams are clearly not enough. Marcel Proustâ€™s 4000-page novel A la
reÌcherche du temps perdu begins and ends with the same word (longtemps), so some translators have decided to
do the same, thus basing the translation of the final word on one that appeared roughly 2 million words earlier.

Section 23.4.

Machine Translation

911

so the full probability distribution P(di ) has only 2n + 1 elements, far fewer numbers to
learn than the number of permutations, n!. That is why we defined the permutation in this
circuitous way. Of course, this is a rather impoverished model of distortion. It doesnâ€™t say
that adjectives are usually distorted to appear after the noun when we are translating from
English to Frenchâ€”that fact is represented in the French language model, P (f ). The distortion probability is completely independent of the words in the phrasesâ€”it depends only on
the integer value di . The probability distribution provides a summary of the volatility of the
permutations; how likely a distortion of P (d = 2) is, compared to P (d = 0), for example.
Weâ€™re ready now to put it all together: we can define P (f, d | e), the probability that
the sequence of phrases f with distortions d is a translation of the sequence of phrases e. We
make the assumption that each phrase translation and each distortion is independent of the
others, and thus we can factor the expression as

P (fi | ei ) P (di )
P (f, d | e) =
i

e2

e1

e3

smelly

There is a

wumpus

e4

e5

sleeping

in 2 2

f1

f3

Il y a un

wumpus

malodorant

qui dort

Ã 22

d1 = 0

d3 = -2

d2 = +1

d4 = +1

d5 = 0

f2

f4

f5

Figure 23.13 Candidate French phrases for each phrase of an English sentence, with distortion (d) values for each French phrase.

That gives us a way to compute the probability P (f, d | e) for a candidate translation f
and distortion d. But to find the best f and d we canâ€™t just enumerate sentences; with maybe
100 French phrases for each English phrase in the corpus, there are 1005 different 5-phrase
translations, and 5! reorderings for each of those. We will have to search for a good solution.
A local beam search (see page 125) with a heuristic that estimates probability has proven
effective at finding a nearly-most-probable translation.
All that remains is to learn the phrasal and distortion probabilities. We sketch the procedure; see the notes at the end of the chapter for details.
1. Find parallel texts: First, gather a parallel bilingual corpus. For example, a Hansard9
is a record of parliamentary debate. Canada, Hong Kong, and other countries produce bilingual Hansards, the European Union publishes its official documents in 11
languages, and the United Nations publishes multilingual documents. Bilingual text is
also available online; some Web sites publish parallel content with parallel URLs, for

HANSARD

9

Named after William Hansard, who first published the British parliamentary debates in 1811.

912

Chapter 23.

2.

3.

4.

5.

6.

23.5
SPEECH
RECOGNITION

Natural Language for Communication

example, /en/ for the English page and /fr/ for the corresponding French page. The
leading statistical translation systems train on hundreds of millions of words of parallel
text and billions of words of monolingual text.
Segment into sentences: The unit of translation is a sentence, so we will have to break
the corpus into sentences. Periods are strong indicators of the end of a sentence, but
consider â€œDr. J. R. Smith of Rodeo Dr. paid $29.99 on 9.9.09.â€; only the final period
ends a sentence. One way to decide if a period ends a sentence is to train a model
that takes as features the surrounding words and their parts of speech. This approach
achieves about 98% accuracy.
Align sentences: For each sentence in the English version, determine what sentence(s)
it corresponds to in the French version. Usually, the next sentence of English corresponds to the next sentence of French in a 1:1 match, but sometimes there is variation:
one sentence in one language will be split into a 2:1 match, or the order of two sentences
will be swapped, resulting in a 2:2 match. By looking at the sentence lengths alone (i.e.
short sentences should align with short sentences), it is possible to align them (1:1, 1:2,
or 2:2, etc.) with accuracy in the 90% to 99% range using a variation on the Viterbi
algorithm. Even better alignment can be achieved by using landmarks that are common
to both languages, such as numbers, dates, proper names, or words that we know from
a bilingual dictionary have an unambiguous translation. For example, if the 3rd English
and 4th French sentences contain the string â€œ1989â€ and neighboring sentences do not,
that is good evidence that the sentences should be aligned together.
Align phrases: Within a sentence, phrases can be aligned by a process that is similar to
that used for sentence alignment, but requiring iterative improvement. When we start,
we have no way of knowing that â€œqui dortâ€ aligns with â€œsleeping,â€ but we can arrive at
that alignment by a process of aggregation of evidence. Over all the example sentences
we have seen, we notice that â€œqui dortâ€ and â€œsleepingâ€ co-occur with high frequency,
and that in the pair of aligned sentences, no phrase other than â€œqui dortâ€ co-occurs so
frequently in other sentences with â€œsleeping.â€ A complete phrase alignment over our
corpus gives us the phrasal probabilities (after appropriate smoothing).
Extract distortions: Once we have an alignment of phrases we can define distortion
probabilities. Simply count how often distortion occurs in the corpus for each distance
d = 0, Â±1, Â±2, . . ., and apply smoothing.
Improve estimates with EM: Use expectationâ€“maximization to improve the estimates
of P (f | e) and P (d) values. We compute the best alignments with the current values
of these parameters in the E step, then update the estimates in the M step and iterate the
process until convergence.

S PEECH R ECOGNITION
Speech recognition is the task of identifying a sequence of words uttered by a speaker, given
the acoustic signal. It has become one of the mainstream applications of AIâ€”millions of

Section 23.5.

SEGMENTATION

COARTICULATION

HOMOPHONES

Speech Recognition

913

people interact with speech recognition systems every day to navigate voice mail systems,
search the Web from mobile phones, and other applications. Speech is an attractive option
when hands-free operation is necessary, as when operating machinery.
Speech recognition is difficult because the sounds made by a speaker are ambiguous
and, well, noisy. As a well-known example, the phrase â€œrecognize speechâ€ sounds almost
the same as â€œwreck a nice beachâ€ when spoken quickly. Even this short example shows
several of the issues that make speech problematic. First, segmentation: written words in
English have spaces between them, but in fast speech there are no pauses in â€œwreck a niceâ€
that would distinguish it as a multiword phrase as opposed to the single word â€œrecognize.â€
Second, coarticulation: when speaking quickly the â€œsâ€ sound at the end of â€œniceâ€ merges
with the â€œbâ€ sound at the beginning of â€œbeach,â€ yielding something that is close to a â€œsp.â€
Another problem that does not show up in this example is homophonesâ€”words like â€œto,â€
â€œtoo,â€ and â€œtwoâ€ that sound the same but differ in meaning.
We can view speech recognition as a problem in most-likely-sequence explanation. As
we saw in Section 15.2, this is the problem of computing the most likely sequence of state
variables, x1:t , given a sequence of observations e1:t . In this case the state variables are the
words, and the observations are sounds. More precisely, an observation is a vector of features
extracted from the audio signal. As usual, the most likely sequence can be computed with the
help of Bayesâ€™ rule to be:
argmax P (word 1:t | sound 1:t ) = argmax P (sound 1:t | word 1:t )P (word 1:t ) .
word 1:t

ACOUSTIC MODEL

LANGUAGE MODEL

NOISY CHANNEL
MODEL

word 1:t

Here P (sound 1:t |word 1:t ) is the acoustic model. It describes the sounds of wordsâ€”that
â€œceilingâ€ begins with a soft â€œcâ€ and sounds the same as â€œsealing.â€ P (word 1:t ) is known as
the language model. It specifies the prior probability of each utteranceâ€”for example, that
â€œceiling fanâ€ is about 500 times more likely as a word sequence than â€œsealing fan.â€
This approach was named the noisy channel model by Claude Shannon (1948). He
described a situation in which an original message (the words in our example) is transmitted
over a noisy channel (such as a telephone line) such that a corrupted message (the sounds
in our example) are received at the other end. Shannon showed that no matter how noisy
the channel, it is possible to recover the original message with arbitrarily small error, if we
encode the original message in a redundant enough way. The noisy channel approach has
been applied to speech recognition, machine translation, spelling correction, and other tasks.
Once we define the acoustic and language models, we can solve for the most likely
sequence of words using the Viterbi algorithm (Section 15.2.3 on page 576). Most speech
recognition systems use a language model that makes the Markov assumptionâ€”that the current state Word t depends only on a fixed number n of previous statesâ€”and represent Word t
as a single random variable taking on a finite set of values, which makes it a Hidden Markov
Model (HMM). Thus, speech recognition becomes a simple application of the HMM methodology, as described in Section 15.3â€”simple that is, once we define the acoustic and language
models. We cover them next.

914

Chapter 23.
Vowels

Natural Language for Communication

Consonants Bâ€“N

Consonants Pâ€“Z

Phone

Example

Phone

Example

Phone

Example

[iy]
[ih]
[eh]
[Ã¦]
[ah]
[ao]
[ow]
[uh]
[ey]
[er]
[ay]
[oy]
[axr]
[aw]
[ax]
[ix]
[aa]

beat
bit
bet
bat
but
bought
boat
book
bait
Bert
buy
boy
diner
down
about
roses
cot

[b]
[ch]
[d]
[f]
[g]
[hh]
[hv]
[jh]
[k]
[l]
[el]
[m]
[em]
[n]
[en]
[ng]
[eng]

bet
Chet
debt
fat
get
hat
high
jet
kick
let
bottle
met
bottom
net
button
sing
washing

[p]
[r]
[s]
[sh]
[t]
[th]
[dh]
[dx]
[v]
[w]
[wh]
[y]
[z]
[zh]

pet
rat
set
shoe
ten
thick
that
butter
vet
wet
which
yet
zoo
measure

[-]

silence

Figure 23.14 The ARPA phonetic alphabet, or ARPAbet, listing all the phones used in
American English. There are several alternative notations, including an International Phonetic Alphabet (IPA), which contains the phones in all known languages.

23.5.1 Acoustic model

SAMPLING RATE

QUANTIZATION
FACTOR

PHONE

Sound waves are periodic changes in pressure that propagate through the air. When these
waves strike the diaphragm of a microphone, the back-and-forth movement generates an
electric current. An analog-to-digital converter measures the size of the currentâ€”which approximates the amplitude of the sound waveâ€”at discrete intervals called the sampling rate.
Speech sounds, which are mostly in the range of 100 Hz (100 cycles per second) to 1000 Hz,
are typically sampled at a rate of 8 kHz. (CDs and mp3 files are sampled at 44.1 kHz.) The
precision of each measurement is determined by the quantization factor; speech recognizers
typically keep 8 to 12 bits. That means that a low-end system, sampling at 8 kHz with 8-bit
quantization, would require nearly half a megabyte per minute of speech.
Since we only want to know what words were spoken, not exactly what they sounded
like, we donâ€™t need to keep all that information. We only need to distinguish between different speech sounds. Linguists have identified about 100 speech sounds, or phones, that can be
composed to form all the words in all known human languages. Roughly speaking, a phone
is the sound that corresponds to a single vowel or consonant, but there are some complications: combinations of letters, such as â€œthâ€ and â€œngâ€ produce single phones, and some letters
produce different phones in different contexts (e.g., the â€œaâ€ in rat and rate. Figure 23.14 lists

Section 23.5.
PHONEME

FRAME

FEATURE

MEL FREQUENCY
CEPSTRAL
COEFFICIENT (MFCC)

PHONE MODEL

Speech Recognition

915

all the phones that are used in English, with an example of each. A phoneme is the smallest
unit of sound that has a distinct meaning to speakers of a particular language. For example,
the â€œtâ€ in â€œstickâ€ sounds similar enough to the â€œtâ€ in â€œtickâ€ that speakers of English consider
them the same phoneme. But the difference is significant in the Thai language, so there they
are two phonemes. To represent spoken English we want a representation that can distinguish
between different phonemes, but one that need not distinguish the nonphonemic variations in
sound: loud or soft, fast or slow, male or female voice, etc.
First, we observe that although the sound frequencies in speech may be several kHz,
the changes in the content of the signal occur much less often, perhaps at no more than 100
Hz. Therefore, speech systems summarize the properties of the signal over time slices called
frames. A frame length of about 10 milliseconds (i.e., 80 samples at 8 kHz) is short enough
to ensure that few short-duration phenomena will be missed. Overlapping frames are used to
make sure that we donâ€™t miss a signal because it happens to fall on a frame boundary.
Each frame is summarized by a vector of features. Picking out features from a speech
signal is like listening to an orchestra and saying â€œhere the French horns are playing loudly
and the violins are playing softly.â€ Weâ€™ll give a brief overview of the features in a typical
system. First, a Fourier transform is used to determine the amount of acoustic energy at
about a dozen frequencies. Then we compute a measure called the mel frequency cepstral
coefficient (MFCC) or MFCC for each frequency. We also compute the total energy in
the frame. That gives thirteen features; for each one we compute the difference between
this frame and the previous frame, and the difference between differences, for a total of 39
features. These are continuous-valued; the easiest way to fit them into the HMM framework
is to discretize the values. (It is also possible to extend the HMM model to handle continuous
mixtures of Gaussians.) Figure 23.15 shows the sequence of transformations from the raw
sound to a sequence of frames with discrete features.
We have seen how to go from the raw acoustic signal to a series of observations, et .
Now we have to describe the (unobservable) states of the HMM and define the transition
model, P(Xt | Xtâˆ’1 ), and the sensor model, P(Et | Xt ). The transition model can be broken
into two levels: word and phone. Weâ€™ll start from the bottom: the phone model describes

Analog acoustic signal:

Sampled, quantized
digital signal:

10 15 38
Frames with features:

22 63 24
52 47 82

10 12 73
89 94 11

Figure 23.15 Translating the acoustic signal into a sequence of frames. In this diagram
each frame is described by the discretized values of three acoustic features; a real system
would have dozens of features.

916

Chapter 23.

Natural Language for Communication

Phone HMM for [m]:
0.9

0.3

Onset

0.7

Mid

0.4
0.1

End

0.6

FINAL

Output probabilities for the phone HMM:
Onset :
C1: 0.5
C2: 0.2
C3: 0.3

Mid:
C3: 0.2
C4: 0.7
C5: 0.1

End:
C4: 0.1
C6: 0.5
C7: 0.4

Figure 23.16 An HMM for the three-state phone [m]. Each state has several possible
outputs, each with its own probability. The MFCC feature labels C1 through C7 are arbitrary,
standing for some combination of feature values.

(a) Word model with dialect variation:
0.5
[t]

1.0

[ow]

1.0

[ey]

1.0

[m]

[t]
0.5

[aa]

1.0

[ow]

1.0

:
(b) Word model with coarticulation and dialect variations

0.2

[ow]

1.0

[t]

0.5

[ey]

1.0

[m]
0.8

[ah]

1.0

[t]
0.5

[aa]

1.0

[ow]

1.0

Figure 23.17 Two pronunciation models of the word â€œtomato.â€ Each model is shown as
a transition diagram with states as circles and arrows showing allowed transitions with their
associated probabilities. (a) A model allowing for dialect differences. The 0.5 numbers are
estimates based on the two authorsâ€™ preferred pronunciations. (b) A model with a coarticulation effect on the first vowel, allowing either the [ow] or the [ah] phone.

Section 23.5.

PRONUNCIATION
MODEL

Speech Recognition

917

a phone as three states, the onset, middle, and end. For example, the [t] phone has a silent
beginning, a small explosive burst of sound in the middle, and (usually) a hissing at the end.
Figure 23.16 shows an example for the phone [m]. Note that in normal speech, an average
phone has a duration of 50â€“100 milliseconds, or 5â€“10 frames. The self-loops in each state
allows for variation in this duration. By taking many self-loops (especially in the mid state),
we can represent a long â€œmmmmmmmmmmmâ€ sound. Bypassing the self-loops yields a
short â€œmâ€ sound.
In Figure 23.17 the phone models are strung together to form a pronunciation model
for a word. According to Gershwin (1937), you say [t ow m ey t ow] and I say [t ow m aa t
ow]. Figure 23.17(a) shows a transition model that provides for this dialect variation. Each
of the circles in this diagram represents a phone model like the one in Figure 23.16.
In addition to dialect variation, words can have coarticulation variation. For example,
the [t] phone is produced with the tongue at the top of the mouth, whereas the [ow] has the
tongue near the bottom. When speaking quickly, the tongue doesnâ€™t have time to get into
position for the [ow], and we end up with [t ah] rather than [t ow]. Figure 23.17(b) gives
a model for â€œtomatoâ€ that takes this coarticulation effect into account. More sophisticated
phone models take into account the context of the surrounding phones.
There can be substantial variation in pronunciation for a word. The most common
pronunciation of â€œbecauseâ€ is [b iy k ah z], but that only accounts for about a quarter of
uses. Another quarter (approximately) substitutes [ix], [ih] or [ax] for the first vowel, and the
remainder substitute [ax] or [aa] for the second vowel, [zh] or [s] for the final [z], or drop
â€œbeâ€ entirely, leaving â€œcuz.â€

23.5.2 Language model
For general-purpose speech recognition, the language model can be an n-gram model of
text learned from a corpus of written sentences. However, spoken language has different
characteristics than written language, so it is better to get a corpus of transcripts of spoken
language. For task-specific speech recognition, the corpus should be task-specific: to build
your airline reservation system, get transcripts of prior calls. It also helps to have task-specific
vocabulary, such as a list of all the airports and cities served, and all the flight numbers.
Part of the design of a voice user interface is to coerce the user into saying things from a
limited set of options, so that the speech recognizer will have a tighter probability distribution
to deal with. For example, asking â€œWhat city do you want to go to?â€ elicits a response with
a highly constrained language model, while asking â€œHow can I help you?â€ does not.

23.5.3 Building a speech recognizer
The quality of a speech recognition system depends on the quality of all of its componentsâ€”
the language model, the word-pronunciation models, the phone models, and the signalprocessing algorithms used to extract spectral features from the acoustic signal. We have
discussed how the language model can be constructed from a corpus of written text, and we
leave the details of signal processing to other textbooks. We are left with the pronunciation
and phone models. The structure of the pronunciation modelsâ€”such as the tomato models in

918

Chapter 23.

Natural Language for Communication

Figure 23.17â€”is usually developed by hand. Large pronunciation dictionaries are now available for English and other languages, although their accuracy varies greatly. The structure
of the three-state phone models is the same for all phones, as shown in Figure 23.16. That
leaves the probabilities themselves.
As usual, we will acquire the probabilities from a corpus, this time a corpus of speech.
The most common type of corpus to obtain is one that includes the speech signal for each
sentence paired with a transcript of the words. Building a model from this corpus is more
difficult than building an n-gram model of text, because we have to build a hidden Markov
modelâ€”the phone sequence for each word and the phone state for each time frame are hidden
variables. In the early days of speech recognition, the hidden variables were provided by
laborious hand-labeling of spectrograms. Recent systems use expectationâ€“maximization to
automatically supply the missing data. The idea is simple: given an HMM and an observation
sequence, we can use the smoothing algorithms from Sections 15.2 and 15.3 to compute the
probability of each state at each time step and, by a simple extension, the probability of each
stateâ€“state pair at consecutive time steps. These probabilities can be viewed as uncertain
labels. From the uncertain labels, we can estimate new transition and sensor probabilities,
and the EM procedure repeats. The method is guaranteed to increase the fit between model
and data on each iteration, and it generally converges to a much better set of parameter values
than those provided by the initial, hand-labeled estimates.
The systems with the highest accuracy work by training a different model for each
speaker, thereby capturing differences in dialect as well as male/female and other variations.
This training can require several hours of interaction with the speaker, so the systems with
the most widespread adoption do not create speaker-specific models.
The accuracy of a system depends on a number of factors. First, the quality of the signal
matters: a high-quality directional microphone aimed at a stationary mouth in a padded room
will do much better than a cheap microphone transmitting a signal over phone lines from a
car in traffic with the radio playing. The vocabulary size matters: when recognizing digit
strings with a vocabulary of 11 words (1-9 plus â€œohâ€ and â€œzeroâ€), the word error rate will be
below 0.5%, whereas it rises to about 10% on news stories with a 20,000-word vocabulary,
and 20% on a corpus with a 64,000-word vocabulary. The task matters too: when the system
is trying to accomplish a specific taskâ€”book a flight or give directions to a restaurantâ€”the
task can often be accomplished perfectly even with a word error rate of 10% or more.

23.6

S UMMARY
Natural language understanding is one of the most important subfields of AI. Unlike most
other areas of AI, natural language understanding requires an empirical investigation of actual
human behaviorâ€”which turns out to be complex and interesting.
â€¢ Formal language theory and phrase structure grammars (and in particular, contextfree grammar) are useful tools for dealing with some aspects of natural language. The
probabilistic context-free grammar (PCFG) formalism is widely used.

Bibliographical and Historical Notes

919

â€¢ Sentences in a context-free language can be parsed in O(n3 ) time by a chart parser
such as the CYK algorithm, which requires grammar rules to be in Chomsky Normal
Form.
â€¢ A treebank can be used to learn a grammar. It is also possible to learn a grammar from
an unparsed corpus of sentences, but this is less successful.
â€¢ A lexicalized PCFG allows us to represent that some relationships between words are
more common than others.
â€¢ It is convenient to augment a grammar to handle such problems as subjectâ€“verb agreement and pronoun case. Definite clause grammar (DCG) is a formalism that allows for
augmentations. With DCG, parsing and semantic interpretation (and even generation)
can be done using logical inference.
â€¢ Semantic interpretation can also be handled by an augmented grammar.
â€¢ Ambiguity is a very important problem in natural language understanding; most sentences have many possible interpretations, but usually only one is appropriate. Disambiguation relies on knowledge about the world, about the current situation, and about
language use.
â€¢ Machine translation systems have been implemented using a range of techniques,
from full syntactic and semantic analysis to statistical techniques based on phrase frequencies. Currently the statistical models are most popular and most successful.
â€¢ Speech recognition systems are also primarily based on statistical principles. Speech
systems are popular and useful, albeit imperfect.
â€¢ Together, machine translation and speech recognition are two of the big successes of
natural language technology. One reason that the models perform well is that large
corpora are availableâ€”both translation and speech are tasks that are performed â€œin the
wildâ€ by people every day. In contrast, tasks like parsing sentences have been less
successful, in part because no large corpora of parsed sentences are available â€œin the
wildâ€ and in part because parsing is not useful in and of itself.

B IBLIOGRAPHICAL

ATTRIBUTE
GRAMMAR

AND

H ISTORICAL N OTES

Like semantic networks, context-free grammars (also known as phrase structure grammars)
are a reinvention of a technique first used by ancient Indian grammarians (especially Panini,
ca. 350 B . C .) studying Shastric Sanskrit (Ingerman, 1967). They were reinvented by Noam
Chomsky (1956) for the analysis of English syntax and independently by John Backus for
the analysis of Algol-58 syntax. Peter Naur extended Backusâ€™s notation and is now credited
(Backus, 1996) with the â€œNâ€ in BNF, which originally stood for â€œBackus Normal Form.â€
Knuth (1968) defined a kind of augmented grammar called attribute grammar that is useful for programming languages. Definite clause grammars were introduced by Colmerauer (1975) and developed and popularized by Pereira and Shieber (1987).
Probabilistic context-free grammars were investigated by Booth (1969) and Salomaa (1969). Other algorithms for PCFGs are presented in the excellent short monograph by

920

Chapter 23.

Natural Language for Communication

Charniak (1993) and the excellent long textbooks by Manning and SchuÌˆtze (1999) and Jurafsky and Martin (2008). Baker (1979) introduces the insideâ€“outside algorithm for learning a
PCFG, and Lari and Young (1990) describe its uses and limitations. Stolcke and Omohundro
(1994) show how to learn grammar rules with Bayesian model merging; Haghighi and Klein
(2006) describe a learning system based on prototypes.
Lexicalized PCFGs (Charniak, 1997; Hwa, 1998) combine the best aspects of PCFGs
and n-gram models. Collins (1999) describes PCFG parsing that is lexicalized with head
features. Petrov and Klein (2007a) show how to get the advantages of lexicalization without
actual lexical augmentations by learning specific syntactic categories from a treebank that has
general categories; for example, the treebank has the category NP, from which more specific
categories such as NP O and NP S can be learned.
There have been many attempts to write formal grammars of natural languages, both
in â€œpureâ€ linguistics and in computational linguistics. There are several comprehensive but
informal grammars of English (Quirk et al., 1985; McCawley, 1988; Huddleston and Pullum,
2002). Since the mid-1980s, there has been a trend toward putting more information in the
lexicon and less in the grammar. Lexical-functional grammar, or LFG (Bresnan, 1982) was
the first major grammar formalism to be highly lexicalized. If we carry lexicalization to an
extreme, we end up with categorial grammar (Clark and Curran, 2004), in which there can
be as few as two grammar rules, or with dependency grammar (Smith and Eisner, 2008;
KuÌˆbler et al., 2009) in which there are no syntactic categories, only relations between words.
Sleator and Temperley (1993) describe a dependency parser. Paskin (2001) shows that a
version of dependency grammar is easier to learn than PCFGs.
The first computerized parsing algorithms were demonstrated by Yngve (1955). Efficient algorithms were developed in the late 1960s, with a few twists since then (Kasami,
1965; Younger, 1967; Earley, 1970; Graham et al., 1980). Maxwell and Kaplan (1993) show
how chart parsing with augmentations can be made efficient in the average case. Church
and Patil (1982) address the resolution of syntactic ambiguity. Klein and Manning (2003)
describe Aâˆ— parsing, and Pauls and Klein (2009) extend that to K-best Aâˆ— parsing, in which
the result is not a single parse but the K best.
Leading parsers today include those by Petrov and Klein (2007b), which achieved
90.6% accuracy on the Wall Street Journal corpus, Charniak and Johnson (2005), which
achieved 92.0%, and Koo et al. (2008), which achieved 93.2% on the Penn treebank. These
numbers are not directly comparable, and there is some criticism of the field that it is focusing
too narrowly on a few select corpora, and perhaps overfitting on them.
Formal semantic interpretation of natural languages originates within philosophy and
formal logic, particularly Alfred Tarskiâ€™s (1935) work on the semantics of formal languages.
Bar-Hillel (1954) was the first to consider the problems of pragmatics and propose that they
could be handled by formal logic. For example, he introduced C. S. Peirceâ€™s (1902) term
indexical into linguistics. Richard Montagueâ€™s essay â€œEnglish as a formal languageâ€ (1970)
is a kind of manifesto for the logical analysis of language, but the books by Dowty et al.
(1991) and Portner and Partee (2002) are more readable.
The first NLP system to solve an actual task was probably the B ASEBALL question
answering system (Green et al., 1961), which handled questions about a database of baseball

Bibliographical and Historical Notes

UNIVERSAL
GRAMMAR

921

statistics. Close after that was Woodsâ€™s (1973) L UNAR, which answered questions about the
rocks brought back from the moon by the Apollo program. Roger Schank and his students
built a series of programs (Schank and Abelson, 1977; Schank and Riesbeck, 1981) that
all had the task of understanding language. Modern approaches to semantic interpretation
usually assume that the mapping from syntax to semantics will be learned from examples
(Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005).
Hobbs et al. (1993) describes a quantitative nonprobabilistic framework for interpretation. More recent work follows an explicitly probabilistic framework (Charniak and Goldman, 1992; Wu, 1993; Franz, 1996). In linguistics, optimality theory (Kager, 1999) is based
on the idea of building soft constraints into the grammar, giving a natural ranking to interpretations (similar to a probability distribution), rather than having the grammar generate all
possibilities with equal rank. Norvig (1988) discusses the problems of considering multiple
simultaneous interpretations, rather than settling for a single maximum-likelihood interpretation. Literary critics (Empson, 1953; Hobbs, 1990) have been ambiguous about whether
ambiguity is something to be resolved or cherished.
Nunberg (1979) outlines a formal model of metonymy. Lakoff and Johnson (1980) give
an engaging analysis and catalog of common metaphors in English. Martin (1990) and Gibbs
(2006) offer computational models of metaphor interpretation.
The first important result on grammar induction was a negative one: Gold (1967)
showed that it is not possible to reliably learn a correct context-free grammar, given a set of
strings from that grammar. Prominent linguists, such as Chomsky (1957) and Pinker (2003),
have used Goldâ€™s result to argue that there must be an innate universal grammar that all
children have from birth. The so-called Poverty of the Stimulus argument says that children
arenâ€™t given enough input to learn a CFG, so they must already â€œknowâ€ the grammar and be
merely tuning some of its parameters. While this argument continues to hold sway throughout
much of Chomskyan linguistics, it has been dismissed by some other linguists (Pullum, 1996;
Elman et al., 1997) and most computer scientists. As early as 1969, Horning showed that it
is possible to learn, in the sense of PAC learning, a probabilistic context-free grammar. Since
then, there have been many convincing empirical demonstrations of learning from positive
examples alone, such as the ILP work of Mooney (1999) and Muggleton and De Raedt (1994),
the sequence learning of Nevill-Manning and Witten (1997), and the remarkable Ph.D. theses
of SchuÌˆtze (1995) and de Marcken (1996). There is an annual International Conference on
Grammatical Inference (ICGI). It is possible to learn other grammar formalisms, such as
regular languages (Denis, 2001) and finite state automata (Parekh and Honavar, 2001). Abney
(2007) is a textbook introduction to semi-supervised learning for language models.
Wordnet (Fellbaum, 2001) is a publicly available dictionary of about 100,000 words and
phrases, categorized into parts of speech and linked by semantic relations such as synonym,
antonym, and part-of. The Penn Treebank (Marcus et al., 1993) provides parse trees for a
3-million-word corpus of English. Charniak (1996) and Klein and Manning (2001) discuss
parsing with treebank grammars. The British National Corpus (Leech et al., 2001) contains
100 million words, and the World Wide Web contains several trillion words; (Brants et al.,
2007) describe n-gram models over a 2-trillion-word Web corpus.

922

Chapter 23.

Natural Language for Communication

In the 1930s Petr Troyanskii applied for a patent for a â€œtranslating machine,â€ but there
were no computers available to implement his ideas. In March 1947, the Rockefeller Foundationâ€™s Warren Weaver wrote to Norbert Wiener, suggesting that machine translation might be
possible. Drawing on work in cryptography and information theory, Weaver wrote, â€œWhen I
look at an article in Russian, I say: â€˜This is really written in English, but it has been coded in
strange symbols. I will now proceed to decode.â€â€™ For the next decade, the community tried
to decode in this way. IBM exhibited a rudimentary system in 1954. Bar-Hillel (1960) describes the enthusiasm of this period. However, the U.S. government subsequently reported
(ALPAC, 1966) that â€œthere is no immediate or predictable prospect of useful machine translation.â€ However, limited work continued, and starting in the 1980s, computer power had
increased to the point where the ALPAC findings were no longer correct.
The basic statistical approach we describe in the chapter is based on early work by the
IBM group (Brown et al., 1988, 1993) and the recent work by the ISI and Google research
groups (Och and Ney, 2004; Zollmann et al., 2008). A textbook introduction on statistical
machine translation is given by Koehn (2009), and a short tutorial by Kevin Knight (1999) has
been influential. Early work on sentence segmentation was done by Palmer and Hearst (1994).
Och and Ney (2003) and Moore (2005) cover bilingual sentence alignment.
The prehistory of speech recognition began in the 1920s with Radio Rex, a voiceactivated toy dog. Rex jumped out of his doghouse in response to the word â€œRex!â€ (or
actually almost any sufficiently loud word). Somewhat more serious work began after World
War II. At AT&T Bell Labs, a system was built for recognizing isolated digits (Davis et al.,
1952) by means of simple pattern matching of acoustic features. Starting in 1971, the Defense
Advanced Research Projects Agency (DARPA) of the United States Department of Defense
funded four competing five-year projects to develop high-performance speech recognition
systems. The winner, and the only system to meet the goal of 90% accuracy with a 1000-word
vocabulary, was the H ARPY system at CMU (Lowerre and Reddy, 1980). The final version
of H ARPY was derived from a system called D RAGON built by CMU graduate student James
Baker (1975); D RAGON was the first to use HMMs for speech. Almost simultaneously, Jelinek (1976) at IBM had developed another HMM-based system. Recent years have been
characterized by steady incremental progress, larger data sets and models, and more rigorous competitions on more realistic speech tasks. In 1997, Bill Gates predicted, â€œThe PC five
years from nowâ€”you wonâ€™t recognize it, because speech will come into the interface.â€ That
didnâ€™t quite happen, but in 2008 he predicted â€œIn five years, Microsoft expects more Internet
searches to be done through speech than through typing on a keyboard.â€ History will tell if
he is right this time around.
Several good textbooks on speech recognition are available (Rabiner and Juang, 1993;
Jelinek, 1997; Gold and Morgan, 2000; Huang et al., 2001). The presentation in this chapter
drew on the survey by Kay, Gawron, and Norvig (1994) and on the textbook by Jurafsky and
Martin (2008). Speech recognition research is published in Computer Speech and Language,
Speech Communications, and the IEEE Transactions on Acoustics, Speech, and Signal Processing and at the DARPA Workshops on Speech and Natural Language Processing and the
Eurospeech, ICSLP, and ASRU conferences.

Exercises

923
Ken Church (2004) shows that natural language research has cycled between concentrating on the data (empiricism) and concentrating on theories (rationalism). The linguist
John Firth (1957) proclaimed â€œYou shall know a word by the company it keeps,â€ and linguistics of the 1940s and early 1950s was based largely on word frequencies, although without
the computational power we have available today. Then Noam (Chomsky, 1956) showed
the limitations of finite-state models, and sparked an interest in theoretical studies of syntax,
disregarding frequency counts. This approach dominated for twenty years, until empiricism
made a comeback based on the success of work in statistical speech recognition (Jelinek,
1976). Today, most work accepts the statistical framework, but there is great interest in building statistical models that consider higher-level models, such as syntactic trees and semantic
relations, not just sequences of words.
Work on applications of language processing is presented at the biennial Applied Natural Language Processing conference (ANLP), the conference on Empirical Methods in Natural Language Processing (EMNLP), and the journal Natural Language Engineering. A broad
range of NLP work appears in the journal Computational Linguistics and its conference, ACL,
and in the Computational Linguistics (COLING) conference.

E XERCISES
23.1 Read the following text once for understanding, and remember as much of it as you
can. There will be a test later.
The procedure is actually quite simple. First you arrange things into different groups. Of
course, one pile may be sufficient depending on how much there is to do. If you have to go
somewhere else due to lack of facilities that is the next step, otherwise you are pretty well
set. It is important not to overdo things. That is, it is better to do too few things at once
than too many. In the short run this may not seem important but complications can easily
arise. A mistake is expensive as well. At first the whole procedure will seem complicated.
Soon, however, it will become just another facet of life. It is difficult to foresee any end
to the necessity for this task in the immediate future, but then one can never tell. After the
procedure is completed one arranges the material into different groups again. Then they
can be put into their appropriate places. Eventually they will be used once more and the
whole cycle will have to be repeated. However, this is part of life.

23.2 An HMM grammar is essentially a standard HMM whose state variable is N (nonterminal, with values such as Det, Adjective, N oun and so on) and whose evidence variable is
W (word, with values such as is, duck, and so on). The HMM model includes a prior P(N0 ),
a transition model P(Nt+1 |Nt ), and a sensor model P(Wt |Nt ). Show that every HMM grammar can be written as a PCFG. [Hint: start by thinking about how the HMM prior can be
represented by PCFG rules for the sentence symbol. You may find it helpful to illustrate for
the particular HMM with values A, B for N and values x, y for W .]

924

Chapter 23.
23.3

a.
b.
c.
d.

Natural Language for Communication

Consider the following PCFG for simple verb phrases:
0.1 : V P â†’ V erb
0.2 : V P â†’ Copula Adjective
0.5 : V P â†’ V erb the N oun
0.2 : V P â†’ V P Adverb
0.5 : V erb â†’ is
0.5 : V erb â†’ shoots
0.8 : Copula â†’ is
0.2 : Copula â†’ seems
0.5 : Adjective â†’ unwell
0.5 : Adjective â†’ well
0.5 : Adverb â†’ well
0.5 : Adverb â†’ badly
0.6 : N oun â†’ duck
0.4 : N oun â†’ well
Which of the following have a nonzero probability as a VP? (i) shoots the duck well
well well (ii) seems the well well (iii) shoots the unwell well badly
What is the probability of generating â€œis well wellâ€?
What types of ambiguity are exhibited by the phrase in (b)?
Given any PCFG, is it possible to calculate the probability that the PCFG generates a
string of exactly 10 words?

23.4 Outline the major differences between Java (or any other computer language with
which you are familiar) and English, commenting on the â€œunderstandingâ€ problem in each
case. Think about such things as grammar, syntax, semantics, pragmatics, compositionality, context-dependence, lexical ambiguity, syntactic ambiguity, reference finding (including
pronouns), background knowledge, and what it means to â€œunderstandâ€ in the first place.
23.5

This exercise concerns grammars for very simple languages.

a. Write a context-free grammar for the language an bn .
b. Write a context-free grammar for the palindrome language: the set of all strings whose
second half is the reverse of the first half.
c. Write a context-sensitive grammar for the duplicate language: the set of all strings
whose second half is the same as the first half.
23.6 Consider the sentence â€œSomeone walked slowly to the supermarketâ€ and a lexicon
consisting of the following words:
Pronoun â†’ someone Verb â†’ walked
Adv â†’ slowly
Prep â†’ to
Article â†’ the
Noun â†’ supermarket
Which of the following three grammars, combined with the lexicon, generates the given sentence? Show the corresponding parse tree(s).

Exercises

925
(A):
S â†’ NP VP
NP â†’ Pronoun
NP â†’ Article Noun
VP â†’ VP PP
VP â†’ VP Adv Adv
VP â†’ Verb
PP â†’ Prep NP
NP â†’ Noun

(B):
(C):
S â†’ NP VP
S â†’ NP VP
NP â†’ Pronoun
NP â†’ Pronoun
NP â†’ Noun
NP â†’ Article NP
NP â†’ Article NP
VP â†’ Verb Adv
VP â†’ Verb Vmod
Adv â†’ Adv Adv
Vmod â†’ Adv Vmod
Adv â†’ PP
Vmod â†’ Adv
PP â†’ Prep NP
Adv â†’ PP
NP â†’ Noun
PP â†’ Prep NP
For each of the preceding three grammars, write down three sentences of English and three
sentences of non-English generated by the grammar. Each sentence should be significantly
different, should be at least six words long, and should include some new lexical entries
(which you should define). Suggest ways to improve each grammar to avoid generating the
non-English sentences.
23.7 Collect some examples of time expressions, such as â€œtwo oâ€™clock,â€ â€œmidnight,â€ and
â€œ12:46.â€ Also think up some examples that are ungrammatical, such as â€œthirteen oâ€™clockâ€ or
â€œhalf past two fifteen.â€ Write a grammar for the time language.
23.8 In this exercise you will transform E0 into Chomsky Normal Form (CNF). There are
five steps: (a) Add a new start symbol, (b) Eliminate  rules, (c) Eliminate multiple words
on right-hand sides, (d) Eliminate rules of the form (X â†’ Y ), (e) Convert long right-hand
sides into binary rules.
a. The start symbol, S, can occur only on the left-hand side in CNF. Add a new rule of the
form S  â†’ S , using a new symbol S  .
b. The empty string,  cannot appear on the right-hand side in CNF. E0 does not have any
rules with , so this is not an issue.
c. A word can appear on the right-hand side in a rule only of the form (X â†’ word).
Replace each rule of the form (X â†’ . . . word . . . ) with (X â†’ . . . W  . . . ) and (W 
â†’ word), using a new symbol W  .
d. A rule (X â†’ Y ) is not allowed in CNF; it must be (X â†’ Y Z ) or (X â†’ word).
Replace each rule of the form (X â†’ Y ) with a set of rules of the form (X â†’ . . . ),
one for each rule (Y â†’ . . . ), where (. . . ) indicates one or more symbols.
e. Replace each rule of the form (X â†’ Y Z . . . ) with two rules, (X â†’ Y Z  ) and (Z 
â†’ Z . . . ), where Z  is a new symbol.
Show each step of the process and the final set of rules.
23.9 Using DCG notation, write a grammar for a language that is just like E1 , except that
it enforces agreement between the subject and verb of a sentence and thus does not generate
ungrammatical sentences such as â€œI smells the wumpus.â€

926

Chapter 23.
23.10

Natural Language for Communication

Consider the following PCFG:
S â†’ NP VP [1.0]
NP â†’ Noun [0.6] | Pronoun [0.4]
VP â†’ Verb NP [0.8] | Modal Verb [0.2]
Noun â†’ can [0.1] | fish [0.3] | . . .
Pronoun â†’ I [0.4] | . . .
Verb â†’ can [0.01] | fish [0.1] | . . .
Modal â†’ can [0.3] | . . .

The sentence â€œI can fishâ€ has two parse trees with this grammar. Show the two trees, their
prior probabilities, and their conditional probabilities, given the sentence.
23.11 An augmented context-free grammar can represent languages that a regular contextfree grammar cannot. Show an augmented context-free grammar for the language an bn cn .
The allowable values for augmentation variables are 1 and S UCCESSOR (n), where n is a
value. The rule for a sentence in this language is
S(n) â†’ A(n) B(n) C(n) .
Show the rule(s) for each of A, B, and C .
23.12 Augment the E1 grammar so that it handles articleâ€“noun agreement. That is, make
sure that â€œagentsâ€ and â€œan agentâ€ are NPs, but â€œagentâ€ and â€œan agentsâ€ are not.
23.13

Consider the following sentence (from The New York Times, July 28, 2008):
Banks struggling to recover from multibillion-dollar loans on real estate are curtailing loans to American businesses, depriving even healthy companies of money
for expansion and hiring.

a.
b.
c.
d.
23.14
a.
b.
c.
d.
e.

Which of the words in this sentence are lexically ambiguous?
Find two cases of syntactic ambiguity in this sentence (there are more than two.)
Give an instance of metaphor in this sentence.
Can you find semantic ambiguity?
Without looking back at Exercise 23.1, answer the following questions:
What are the four steps that are mentioned?
What step is left out?
What is â€œthe materialâ€ that is mentioned in the text?
What kind of mistake would be expensive?
Is it better to do too few things or too many? Why?

23.15 Select five sentences and submit them to an online translation service. Translate
them from English to another language and back to English. Rate the resulting sentences for
grammaticality and preservation of meaning. Repeat the process; does the second round of

Exercises

927
iteration give worse results or the same results? Does the choice of intermediate language
make a difference to the quality of the results? If you know a foreign language, look at the
translation of one paragraph into that language. Count and describe the errors made, and
conjecture why these errors were made.
23.16 The Di values for the sentence in Figure 23.13 sum to 0. Will that be true of every
translation pair? Prove it or give a counterexample.
23.17 (Adapted from Knight (1999).) Our translation model assumes that, after the phrase
translation model selects phrases and the distortion model permutes them, the language model
can unscramble the permutation. This exercise investigates how sensible that assumption is.
Try to unscramble these proposed lists of phrases into the correct order:
a. have, programming, a, seen, never, I, language, better
b. loves, john, mary
c. is the, communication, exchange of, intentional, information brought, by, about, the
production, perception of, and signs, from, drawn, a, of, system, signs, conventional,
shared
d. created, that, we hold these, to be, all men, truths, are, equal, self-evident
Which ones could you do? What type of knowledge did you draw upon? Train a bigram
model from a training corpus, and use it to find the highest-probability permutation of some
sentences from a test corpus. Report on the accuracy of this model.
23.18 Calculate the most probable path through the HMM in Figure 23.16 for the output
sequence [C1 , C2 , C3 , C4 , C4 , C6 , C7 ]. Also give its probability.
23.19 We forgot to mention that the text in Exercise 23.1 is entitled â€œWashing Clothes.â€
Reread the text and answer the questions in Exercise 23.14. Did you do better this time?
Bransford and Johnson (1973) used this text in a controlled experiment and found that the title
helped significantly. What does this tell you about how language and memory works?

24

PERCEPTION

In which we connect the computer to the raw, unwashed world.

PERCEPTION
SENSOR

OBJECT MODEL

RENDERING MODEL

Perception provides agents with information about the world they inhabit by interpreting the
response of sensors. A sensor measures some aspect of the environment in a form that can
be used as input by an agent program. The sensor could be as simple as a switch, which gives
one bit telling whether it is on or off, or as complex as the eye. A variety of sensory modalities
are available to artificial agents. Those they share with humans include vision, hearing, and
touch. Modalities that are not available to the unaided human include radio, infrared, GPS,
and wireless signals. Some robots do active sensing, meaning they send out a signal, such as
radar or ultrasound, and sense the reflection of this signal off of the environment. Rather than
trying to cover all of these, this chapter will cover one modality in depth: vision.
We saw in our description of POMDPs (Section 17.4, page 658) that a model-based
decision-theoretic agent in a partially observable environment has a sensor modelâ€”a probability distribution P(E | S) over the evidence that its sensors provide, given a state of the
world. Bayesâ€™ rule can then be used to update the estimation of the state.
For vision, the sensor model can be broken into two components: An object model
describes the objects that inhabit the visual worldâ€”people, buildings, trees, cars, etc. The
object model could include a precise 3D geometric model taken from a computer-aided design
(CAD) system, or it could be vague constraints, such as the fact that human eyes are usually 5
to 7 cm apart. A rendering model describes the physical, geometric, and statistical processes
that produce the stimulus from the world. Rendering models are quite accurate, but they are
ambiguous. For example, a white object under low light may appear as the same color as a
black object under intense light. A small nearby object may look the same as a large distant
object. Without additional evidence, we cannot tell if the image that fills the frame is a toy
Godzilla or a real monster.
Ambiguity can be managed with prior knowledgeâ€”we know Godzilla is not real, so
the image must be a toyâ€”or by selectively choosing to ignore the ambiguity. For example,
the vision system for an autonomous car may not be able to interpret objects that are far in
the distance, but the agent can choose to ignore the problem, because it is unlikely to crash
into an object that is miles away.

928

Section 24.1.

FEATURE
EXTRACTION

RECOGNITION

RECONSTRUCTION

24.1

Image Formation

929

A decision-theoretic agent is not the only architecture that can make use of vision sensors. For example, fruit flies (Drosophila) are in part reflex agents: they have cervical giant
fibers that form a direct pathway from their visual system to the wing muscles that initiate an
escape responseâ€”an immediate reaction, without deliberation. Flies and many other flying
animals make used of a closed-loop control architecture to land on an object. The visual
system extracts an estimate of the distance to the object, and the control system adjusts the
wing muscles accordingly, allowing very fast changes of direction, with no need for a detailed
model of the object.
Compared to the data from other sensors (such as the single bit that tells the vacuum
robot that it has bumped into a wall), visual observations are extraordinarily rich, both in
the detail they can reveal and in the sheer amount of data they produce. A video camera
for robotic applications might produce a million 24-bit pixels at 60 Hz; a rate of 10 GB per
minute. The problem for a vision-capable agent then is: Which aspects of the rich visual
stimulus should be considered to help the agent make good action choices, and which aspects
should be ignored? Visionâ€”and all perceptionâ€”serves to further the agentâ€™s goals, not as
an end to itself.
We can characterize three broad approaches to the problem. The feature extraction
approach, as exhibited by Drosophila, emphasizes simple computations applied directly to
the sensor observations. In the recognition approach an agent draws distinctions among the
objects it encounters based on visual and other information. Recognition could mean labeling
each image with a yes or no as to whether it contains food that we should forage, or contains
Grandmaâ€™s face. Finally, in the reconstruction approach an agent builds a geometric model
of the world from an image or a set of images.
The last thirty years of research have produced powerful tools and methods for addressing these approaches. Understanding these methods requires an understanding of the
processes by which images are formed. Therefore, we now cover the physical and statistical
phenomena that occur in the production of an image.

I MAGE F ORMATION
Imaging distorts the appearance of objects. For example, a picture taken looking down a
long straight set of railway tracks will suggest that the rails converge and meet. As another
example, if you hold your hand in front of your eye, you can block out the moon, which is
not smaller than your hand. As you move your hand back and forth or tilt it, your hand will
seem to shrink and grow in the image, but it is not doing so in reality (Figure 24.1). Models
of these effects are essential for both recognition and reconstruction.

24.1.1 Images without lenses: The pinhole camera
SCENE
IMAGE

Image sensors gather light scattered from objects in a scene and create a two-dimensional
image. In the eye, the image is formed on the retina, which consists of two types of cells:
about 100 million rods, which are sensitive to light at a wide range of wavelengths, and 5

930

Chapter

24.

Perception

Figure 24.1 Imaging distorts geometry. Parallel lines appear to meet in the distance, as
in the image of the railway tracks on the left. In the center, a small hand blocks out most of
a large moon. On the right is a foreshortening effect: the hand is tilted away from the eye,
making it appear shorter than in the center figure.

PIXEL

PINHOLE CAMERA

PERSPECTIVE
PROJECTION

million cones. Cones, which are essential for color vision, are of three main types, each of
which is sensitive to a different set of wavelengths. In cameras, the image is formed on an
image plane, which can be a piece of film coated with silver halides or a rectangular grid
of a few million photosensitive pixels, each a complementary metal-oxide semiconductor
(CMOS) or charge-coupled device (CCD). Each photon arriving at the sensor produces an
effect, whose strength depends on the wavelength of the photon. The output of the sensor
is the sum of all effects due to photons observed in some time window, meaning that image
sensors report a weighted average of the intensity of light arriving at the sensor.
To see a focused image, we must ensure that all the photons from approximately the
same spot in the scene arrive at approximately the same point in the image plane. The simplest
way to form a focused image is to view stationary objects with a pinhole camera, which
consists of a pinhole opening, O, at the front of a box, and an image plane at the back of the
box (Figure 24.2). Photons from the scene must pass through the pinhole, so if it is small
enough then nearby photons in the scene will be nearby in the image plane, and the image
will be in focus.
The geometry of scene and image is easiest to understand with the pinhole camera. We
use a three-dimensional coordinate system with the origin at the pinhole, and consider a point
P in the scene, with coordinates (X, Y, Z). P gets projected to the point P  in the image
plane with coordinates (x, y, z). If f is the distance from the pinhole to the image plane, then
by similar triangles, we can derive the following equations:
X âˆ’y
Y
âˆ’f X
âˆ’f Y
âˆ’x
= ,
=
â‡’ x=
, y=
.
f
Z f
Z
Z
Z
These equations define an image-formation process known as perspective projection. Note
that the Z in the denominator means that the farther away an object is, the smaller its image

Section 24.1.

Image Formation

Image
plane

931

Y

P
X
Z

Pâ€²

Pinhole

f

Figure 24.2 Each light-sensitive element in the image plane at the back of a pinhole camera receives light from a the small range of directions that passes through the pinhole. If the
pinhole is small enough, the result is a focused image at the back of the pinhole. The process
of projection means that large, distant objects look the same as smaller, nearby objects. Note
that the image is projected upside down.

will be. Also, note that the minus signs mean that the image is inverted, both leftâ€“right and
upâ€“down, compared with the scene.
Under perspective projection, distant objects look small. This is what allows you to
cover the moon with your hand (Figure 24.1). An important result of this effect is that parallel
lines converge to a point on the horizon. (Think of railway tracks, Figure 24.1.) A line in the
scene in the direction (U, V, W ) and passing through the point (X0 , Y0 , Z0 ) can be described
as the set of points (X0 + Î»U, Y0 + Î»V, Z0 + Î»W ), with Î» varying between âˆ’âˆž and +âˆž.
Different choices of (X0 , Y0 , Z0 ) yield different lines parallel to one another. The projection
of a point PÎ» from this line onto the image plane is given by


Y0 + Î»V
X0 + Î»U
,f
.
f
Z0 + Î»W Z0 + Î»W

VANISHING POINT

As Î» â†’ âˆž or Î» â†’ âˆ’âˆž, this becomes pâˆž = (f U/W, f V /W ) if W = 0. This means that
two parallel lines leaving different points in space will converge in the imageâ€”for large Î»,
the image points are nearly the same, whatever the value of (X0 , Y0 , Z0 ) (again, think railway
tracks, Figure 24.1). We call pâˆž the vanishing point associated with the family of straight
lines with direction (U, V, W ). Lines with the same direction share the same vanishing point.

24.1.2 Lens systems

MOTION BLUR

The drawback of the pinhole camera is that we need a small pinhole to keep the image in
focus. But the smaller the pinhole, the fewer photons get through, meaning the image will be
dark. We can gather more photons by keeping the pinhole open longer, but then we will get
motion blurâ€”objects in the scene that move will appear blurred because they send photons
to multiple locations on the image plane. If we canâ€™t keep the pinhole open longer, we can
try to make it bigger. More light will enter, but light from a small patch of object in the scene
will now be spread over a patch on the image plane, causing a blurred image.

932

Chapter

24.

Perception

Image plane
Light Source
Iris
Cornea

Fovea
Visual Axis
Lens

Optic Nerve

Optical Axis
Lens
System

Retina

Figure 24.3 Lenses collect the light leaving a scene point in a range of directions, and steer
it all to arrive at a single point on the image plane. Focusing works for points lying close to
a focal plane in space; other points will not be focused properly. In cameras, elements of
the lens system move to change the focal plane, whereas in the eye, the shape of the lens is
changed by specialized muscles.
LENS

DEPTH OF FIELD
FOCAL PLANE

Vertebrate eyes and modern cameras use a lens system to gather sufficient light while
keeping the image in focus. A large opening is covered with a lens that focuses light from
nearby object locations down to nearby locations in the image plane. However, lens systems
have a limited depth of field: they can focus light only from points that lie within a range
of depths (centered around a focal plane). Objects outside this range will be out of focus in
the image. To move the focal plane, the lens in the eye can change shape (Figure 24.3); in a
camera, the lenses move back and forth.

24.1.3 Scaled orthographic projection

SCALED
ORTHOGRAPHIC
PROJECTION

Perspective effects arenâ€™t always pronounced. For example, spots on a distant leopard may
look small because the leopard is far away, but two spots that are next to each other will have
about the same size. This is because the difference in distance to the spots is small compared
to the distance to them, and so we can simplify the projection model. The appropriate model
is scaled orthographic projection. The idea is as follows: If the depth Z of points on the
object varies within some range Z0 Â± Î”Z, with Î”Z * Z0 , then the perspective scaling
factor f /Z can be approximated by a constant s = f /Z0 . The equations for projection from
the scene coordinates (X, Y, Z) to the image plane become x = sX and y = sY . Scaled
orthographic projection is an approximation that is valid only for those parts of the scene with
not much internal depth variation. For example, scaled orthographic projection can be a good
model for the features on the front of a distant building.

24.1.4 Light and shading
The brightness of a pixel in the image is a function of the brightness of the surface patch in
the scene that projects to the pixel. We will assume a linear model (current cameras have nonlinearities at the extremes of light and dark, but are linear in the middle). Image brightness is

Section 24.1.

Image Formation

933

Diffuse reflection, bright

Specularities

Diffuse reflection, dark

Cast shadow

Figure 24.4 A variety of illumination effects. There are specularities on the metal spoon
and on the milk. The bright diffuse surface is bright because it faces the light direction. The
dark diffuse surface is dark because it is tangential to the illumination direction. The shadows
appear at surface points that cannot see the light source. Photo by Mike Linksvayer (mlinksva
on flickr).

OVERALL INTENSITY

REFLECT

SHADING

DIFFUSE
REFLECTION

SPECULAR
REFLECTION
SPECULARITIES

a strong, if ambiguous, cue to the shape of an object, and from there to its identity. People are
usually able to distinguish the three main causes of varying brightness and reverse-engineer
the objectâ€™s properties. The first cause is overall intensity of the light. Even though a white
object in shadow may be less bright than a black object in direct sunlight, the eye can distinguish relative brightness well, and perceive the white object as white. Second, different points
in the scene may reflect more or less of the light. Usually, the result is that people perceive
these points as lighter or darker, and so see texture or markings on the object. Third, surface
patches facing the light are brighter than surface patches tilted away from the light, an effect
known as shading. Typically, people can tell that this shading comes from the geometry of
the object, but sometimes get shading and markings mixed up. For example, a streak of dark
makeup under a cheekbone will often look like a shading effect, making the face look thinner.
Most surfaces reflect light by a process of diffuse reflection. Diffuse reflection scatters light evenly across the directions leaving a surface, so the brightness of a diffuse surface
doesnâ€™t depend on the viewing direction. Most cloth, paints, rough wooden surfaces, vegetation, and rough stone are diffuse. Mirrors are not diffuse, because what you see depends on
the direction in which you look at the mirror. The behavior of a perfect mirror is known as
specular reflection. Some surfacesâ€”such as brushed metal, plastic, or a wet floorâ€”display
small patches where specular reflection has occurred, called specularities. These are easy to
identify, because they are small and bright (Figure 24.4). For almost all purposes, it is enough
to model all surfaces as being diffuse with specularities.

934

Chapter

Î¸
A

24.

Perception

Î¸
B

Figure 24.5 Two surface patches are illuminated by a distant point source, whose rays are
shown as gray arrowheads. Patch A is tilted away from the source (Î¸ is close to 900 ) and
collects less energy, because it cuts fewer light rays per unit surface area. Patch B, facing the
source (Î¸ is close to 00 ), collects more energy.

DISTANT POINT
LIGHT SOURCE

DIFFUSE ALBEDO

LAMBERTâ€™S COSINE
LAW

The main source of illumination outside is the sun, whose rays all travel parallel to one
another. We model this behavior as a distant point light source. This is the most important
model of lighting, and is quite effective for indoor scenes as well as outdoor scenes. The
amount of light collected by a surface patch in this model depends on the angle Î¸ between the
illumination direction and the normal to the surface.
A diffuse surface patch illuminated by a distant point light source will reflect some
fraction of the light it collects; this fraction is called the diffuse albedo. White paper and
snow have a high albedo, about 0.90, whereas flat black velvet and charcoal have a low albedo
of about 0.05 (which means that 95% of the incoming light is absorbed within the fibers of
the velvet or the pores of the charcoal). Lambertâ€™s cosine law states that the brightness of a
diffuse patch is given by
I = ÏI0 cos Î¸ ,

SHADOW

INTERREFLECTIONS

AMBIENT
ILLUMINATION

where Ï is the diffuse albedo, I0 is the intensity of the light source and Î¸ is the angle between
the light source direction and the surface normal (see Figure 24.5). Lampertâ€™s law predicts
bright image pixels come from surface patches that face the light directly and dark pixels
come from patches that see the light only tangentially, so that the shading on a surface provides some shape information. We explore this cue in Section 24.4.5. If the surface is not
reached by the light source, then it is in shadow. Shadows are very seldom a uniform black,
because the shadowed surface receives some light from other sources. Outdoors, the most
important such source is the sky, which is quite bright. Indoors, light reflected from other
surfaces illuminates shadowed patches. These interreflections can have a significant effect
on the brightness of other surfaces, too. These effects are sometimes modeled by adding a
constant ambient illumination term to the predicted intensity.

Section 24.2.

Early Image-Processing Operations

935

24.1.5 Color

PRINCIPLE OF
TRICHROMACY

COLOR CONSTANCY

24.2

Fruit is a bribe that a tree offers to animals to carry its seeds around. Trees have evolved to
have fruit that turns red or yellow when ripe, and animals have evolved to detect these color
changes. Light arriving at the eye has different amounts of energy at different wavelengths;
this can be represented by a spectral energy density function. Human eyes respond to light in
the 380â€“750nm wavelength region, with three different types of color receptor cells, which
have peak receptiveness at 420mm (blue), 540nm (green), and 570nm (red). The human eye
can capture only a small fraction of the full spectral energy density functionâ€”but it is enough
to tell when the fruit is ripe.
The principle of trichromacy states that for any spectral energy density, no matter how
complicated, it is possible to construct another spectral energy density consisting of a mixture
of just three colorsâ€”usually red, green, and blueâ€”such that a human canâ€™t tell the difference
between the two. That means that our TVs and computer displays can get by with just the
three red/green/blue (or R/G/B) color elements. It makes our computer vision algorithms
easier, too. Each surface can be modeled with three different albedos for R/G/B. Similarly,
each light source can be modeled with three R/G/B intensities. We then apply Lambertâ€™s
cosine law to each to get three R/G/B pixel values. This model predicts, correctly, that the
same surface will produce different colored image patches under different-colored lights. In
fact, human observers are quite good at ignoring the effects of different colored lights and are
able to estimate the color of the surface under white light, an effect known as color constancy.
Quite accurate color constancy algorithms are now available; simple versions show up in the
â€œauto white balanceâ€ function of your camera. Note that if we wanted to build a camera for
mantis shrimp, we would need 12 different pixel colors, corresponding to the 12 types of
color receptors of the crustacean.

E ARLY I MAGE -P ROCESSING O PERATIONS
We have seen how light reflects off objects in the scene to form an image consisting of, say,
five million 3-byte pixels. With all sensors there will be noise in the image, and in any case
there is a lot of data to deal with. So how do we get started on analyzing this data?
In this section we will study three useful image-processing operations: edge detection,
texture analysis, and computation of optical flow. These are called â€œearlyâ€ or â€œlow-levelâ€
operations because they are the first in a pipeline of operations. Early vision operations are
characterized by their local nature (they can be carried out in one part of the image without
regard for anything more than a few pixels away) and by their lack of knowledge: we can
perform these operations without consideration of the objects that might be present in the
scene. This makes the low-level operations good candidates for implementation in parallel
hardwareâ€”either in a graphics processor unit (GPU) or an eye. We will then look at one
mid-level operation: segmenting the image into regions.

936

Chapter

2

AB

3
4

Perception

1

2

1

24.

1

Figure 24.6 Different kinds of edges: (1) depth discontinuities; (2) surface orientation
discontinuities; (3) reflectance discontinuities; (4) illumination discontinuities (shadows).

24.2.1 Edge detection
EDGE

Edges are straight lines or curves in the image plane across which there is a â€œsignificantâ€
change in image brightness. The goal of edge detection is to abstract away from the messy,
multimegabyte image and toward a more compact, abstract representation, as in Figure 24.6.
The motivation is that edge contours in the image correspond to important scene contours.
In the figure we have three examples of depth discontinuity, labeled 1; two surface-normal
discontinuities, labeled 2; a reflectance discontinuity, labeled 3; and an illumination discontinuity (shadow), labeled 4. Edge detection is concerned only with the image, and thus does
not distinguish between these different types of scene discontinuities; later processing will.
Figure 24.7(a) shows an image of a scene containing a stapler resting on a desk, and
(b) shows the output of an edge-detection algorithm on this image. As you can see, there
is a difference between the output and an ideal line drawing. There are gaps where no edge
appears, and there are â€œnoiseâ€ edges that do not correspond to anything of significance in the
scene. Later stages of processing will have to correct for these errors.
How do we detect edges in an image? Consider the profile of image brightness along a
one-dimensional cross-section perpendicular to an edgeâ€”for example, the one between the
left edge of the desk and the wall. It looks something like what is shown in Figure 24.8 (top).
Edges correspond to locations in images where the brightness undergoes a sharp change,
so a naive idea would be to differentiate the image and look for places where the magnitude
of the derivative I  (x) is large. That almost works. In Figure 24.8 (middle), we see that there
is indeed a peak at x = 50, but there are also subsidiary peaks at other locations (e.g., x = 75).
These arise because of the presence of noise in the image. If we smooth the image first, the
spurious peaks are diminished, as we see in the bottom of the figure.

Section 24.2.

Early Image-Processing Operations

937

(a)
Figure 24.7

(b)

(a) Photograph of a stapler. (b) Edges computed from (a).

2
1
0
âˆ’1

0

10

20

30

40

50

60

70

80

90

100

0

10

20

30

40

50

60

70

80

90

100

0

10

20

30

40

50

60

70

80

90

100

1

0
âˆ’1

1

0
âˆ’1

Figure 24.8 Top: Intensity profile I(x) along a one-dimensional section across an edge at
x = 50. Middle: The derivative of intensity, I  (x). Large values of this function correspond
to edges, but the function is noisy. Bottom: The derivative of a smoothed version of the
intensity, (I âˆ— GÏƒ ) , which can be computed in one step as the convolution I âˆ— GÏƒ . The noisy
candidate edge at x = 75 has disappeared.

The measurement of brightness at a pixel in a CCD camera is based on a physical
process involving the absorption of photons and the release of electrons; inevitably there
will be statistical fluctuations of the measurementâ€”noise. The noise can be modeled with

938

GAUSSIAN FILTER

Chapter

Perception

a Gaussian probability distribution, with each pixel independent of the others. One way to
smooth an image is to assign to each pixel the average of its neighbors. This tends to cancel
out extreme values. But how many neighbors should we considerâ€”one pixel away, or two, or
more? One good answer is a weighted average that weights the nearest pixels the most, then
gradually decreases the weight for more distant pixels. The Gaussian filter does just that.
(Users of Photoshop recognize this as the Gaussian blur operation.) Recall that the Gaussian
function with standard deviation Ïƒ and mean 0 is
2
2
1
eâˆ’x /2Ïƒ
in one dimension, or
NÏƒ (x) = âˆš2Ï€Ïƒ
NÏƒ (x, y) =

CONVOLUTION

24.

2
2
2
1
eâˆ’(x +y )/2Ïƒ
2Ï€Ïƒ2

in two dimensions.

The application of the Gaussian filter replaces the intensity I(x0 , y0 ) with the sum, over all
(x, y) pixels, of I(x, y) NÏƒ (d), where d is the distance from (x0 , y0 ) to (x, y). This kind of
weighted sum is so common that there is a special name and notation for it. We say that the
function h is the convolution of two functions f and g (denoted f âˆ— g) if we have
h(x) = (f âˆ— g)(x) =

+âˆž


f (u) g(x âˆ’ u)

in one dimension, or

u=âˆ’âˆž

h(x, y) = (f âˆ— g)(x, y) =

+âˆž


+âˆž


f (u, v) g(x âˆ’ u, y âˆ’ v)

in two.

u=âˆ’âˆž v=âˆ’âˆž

So the smoothing function is achieved by convolving the image with the Gaussian, I âˆ— NÏƒ . A
Ïƒ of 1 pixel is enough to smooth over a small amount of noise, whereas 2 pixels will smooth a
larger amount, but at the loss of some detail. Because the Gaussianâ€™s influence fades quickly
at a distance, we can replace the Â±âˆž in the sums with Â±3Ïƒ.
We can optimize the computation by combining smoothing and edge finding into a single operation. It is a theorem that for any functions f and g, the derivative of the convolution,
(f âˆ— g) , is equal to the convolution with the derivative, f âˆ— (g ). So rather than smoothing
the image and then differentiating, we can just convolve the image with the derivative of the
smoothing function, NÏƒ . We then mark as edges those peaks in the response that are above
some threshold.
There is a natural generalization of this algorithm from one-dimensional cross sections
to general two-dimensional images. In two dimensions edges may be at any angle Î¸. Considering the image brightness as a scalar function of the variables x, y, its gradient is a vector
 âˆ‚I

 
Ix
âˆ‚x
.
=
âˆ‡I = âˆ‚I
Iy
âˆ‚y
Edges correspond to locations in images where the brightness undergoes a sharp change, and
so the magnitude of the gradient, +âˆ‡I+, should be large at an edge point. Of independent
interest is the direction of the gradient


âˆ‡I
cos Î¸
=
.
sin Î¸
+âˆ‡I+
ORIENTATION

This gives us a Î¸ = Î¸(x, y) at every pixel, which defines the edge orientation at that pixel.

Section 24.2.

Early Image-Processing Operations

939

As in one dimension, to form the gradient we donâ€™t compute âˆ‡I, but rather âˆ‡(I âˆ— NÏƒ ),
the gradient after smoothing the image by convolving it with a Gaussian. And again, the
shortcut is that this is equivalent to convolving the image with the partial derivatives of a
Gaussian. Once we have computed the gradient, we can obtain edges by finding edge points
and linking them together. To tell whether a point is an edge point, we must look at other
points a small distance forward and back along the direction of the gradient. If the gradient
magnitude at one of these points is larger, then we could get a better edge point by shifting
the edge curve very slightly. Furthermore, if the gradient magnitude is too small, the point
cannot be an edge point. So at an edge point, the gradient magnitude is a local maximum
along the direction of the gradient, and the gradient magnitude is above a suitable threshold.
Once we have marked edge pixels by this algorithm, the next stage is to link those pixels
that belong to the same edge curves. This can be done by assuming that any two neighboring
edge pixels with consistent orientations must belong to the same edge curve.

24.2.2 Texture
TEXTURE

In everyday language, texture is the visual feel of a surfaceâ€”what you see evokes what
the surface might feel like if you touched it (â€œtextureâ€ has the same root as â€œtextileâ€). In
computational vision, texture refers to a spatially repeating pattern on a surface that can be
sensed visually. Examples include the pattern of windows on a building, stitches on a sweater,
spots on a leopard, blades of grass on a lawn, pebbles on a beach, and people in a stadium.
Sometimes the arrangement is quite periodic, as in the stitches on a sweater; in other cases,
such as pebbles on a beach, the regularity is only statistical.
Whereas brightness is a property of individual pixels, the concept of texture makes sense
only for a multipixel patch. Given such a patch, we could compute the orientation at each
pixel, and then characterize the patch by a histogram of orientations. The texture of bricks in
a wall would have two peaks in the histogram (one vertical and one horizontal), whereas the
texture of spots on a leopardâ€™s skin would have a more uniform distribution of orientations.
Figure 24.9 shows that orientations are largely invariant to changes in illumination. This
makes texture an important clue for object recognition, because other clues, such as edges,
can yield different results in different lighting conditions.
In images of textured objects, edge detection does not work as well as it does for smooth
objects. This is because the most important edges can be lost among the texture elements.
Quite literally, we may miss the tiger for the stripes. The solution is to look for differences in
texture properties, just the way we look for differences in brightness. A patch on a tiger and
a patch on the grassy background will have very different orientation histograms, allowing us
to find the boundary curve between them.

24.2.3 Optical flow

OPTICAL FLOW

Next, let us consider what happens when we have a video sequence, instead of just a single
static image. When an object in the video is moving, or when the camera is moving relative
to an object, the resulting apparent motion in the image is called optical flow. Optical flow
describes the direction and speed of motion of features in the imageâ€”the optical flow of a

940

Chapter

(a)

24.

Perception

(b)

Figure 24.9 Two images of the same texture of crumpled rice paper, with different illumination levels. The gradient vector field (at every eighth pixel) is plotted on top of each one.
Notice that, as the light gets darker, all the gradient vectors get shorter. The vectors do not
rotate, so the gradient orientations do not change.

SUM OF SQUARED
DIFFERENCES

video of a race car would be measured in pixels per second, not miles per hour. The optical
flow encodes useful information about scene structure. For example, in a video of scenery
taken from a moving train, distant objects have slower apparent motion than close objects;
thus, the rate of apparent motion can tell us something about distance. Optical flow also
enables us to recognize actions. In Figure 24.10(a) and (b), we show two frames from a video
of a tennis player. In (c) we display the optical flow vectors computed from these images,
showing that the racket and front leg are moving fastest.
The optical flow vector field can be represented at any point (x, y) by its components
vx (x, y) in the x direction and vy (x, y) in the y direction. To measure optical flow we need to
find corresponding points between one time frame and the next. A simple-minded technique
is based on the fact that image patches around corresponding points have similar intensity
patterns. Consider a block of pixels centered at pixel p, (x0 , y0 ), at time t0 . This block
of pixels is to be compared with pixel blocks centered at various candidate pixels at (x0 +
Dx , y0 + Dy ) at time t0 + Dt . One possible measure of similarity is the sum of squared
differences (SSD):

SSD(Dx , Dy ) =
(I(x, y, t) âˆ’ I(x + Dx , y + Dy , t + Dt ))2 .
(x,y)

Here, (x, y) ranges over pixels in the block centered at (x0 , y0 ). We find the (Dx , Dy ) that
minimizes the SSD. The optical flow at (x0 , y0 ) is then (vx , vy ) = (Dx /Dt , Dy /Dt ). Note
that for this to work, there needs to be some texture or variation in the scene. If one is looking
at a uniform white wall, then the SSD is going to be nearly the same for the different can-

Section 24.2.

Early Image-Processing Operations

941

Figure 24.10 Two frames of a video sequence. On the right is the optical flow field corresponding to the displacement from one frame to the other. Note how the movement of
the tennis racket and the front leg is captured by the directions of the arrows. (Courtesy of
Thomas Brox.)

didate matches, and the algorithm is reduced to making a blind guess. The best-performing
algorithms for measuring optical flow rely on a variety of additional constraints when the
scene is only partially textured.

24.2.4 Segmentation of images
SEGMENTATION
REGIONS

Segmentation is the process of breaking an image into regions of similar pixels. Each image
pixel can be associated with certain visual properties, such as brightness, color, and texture.
Within an object, or a single part of an object, these attributes vary relatively little, whereas
across an inter-object boundary there is typically a large change in one or more of these attributes. There are two approaches to segmentation, one focusing on detecting the boundaries
of these regions, and the other on detecting the regions themselves (Figure 24.11).
A boundary curve passing through a pixel (x, y) will have an orientation Î¸, so one way
to formalize the problem of detecting boundary curves is as a machine learning classification
problem. Based on features from a local neighborhood, we want to compute the probability
Pb (x, y, Î¸) that indeed there is a boundary curve at that pixel along that orientation. Consider
a circular disk centered at (x, y), subdivided into two half disks by a diameter oriented at Î¸.
If there is a boundary at (x, y, Î¸) the two half disks might be expected to differ significantly
in their brightness, color, and texture. Martin, Fowlkes, and Malik (2004) used features based
on differences in histograms of brightness, color, and texture values measured in these two
half disks, and then trained a classifier. For this they used a data set of natural images where
humans had marked the â€œground truthâ€ boundaries, and the goal of the classifier was to mark
exactly those boundaries marked by humans and no others.
Boundaries detected by this technique turn out to be significantly better than those found
using the simple edge-detection technique described previously. But still there are two limitations. (1) The boundary pixels formed by thresholding Pb (x, y, Î¸) are not guaranteed to form
closed curves, so this approach doesnâ€™t deliver regions, and (2) the decision making exploits
only local context and does not use global consistency constraints.

942

Chapter

(a)

(b)

(c)

24.

Perception

(d)

Figure 24.11 (a) Original image. (b) Boundary contours, where the higher the Pb value,
the darker the contour. (c) Segmentation into regions, corresponding to a fine partition of
the image. Regions are rendered in their mean colors. (d) Segmentation into regions, corresponding to a coarser partition of the image, resulting in fewer regions. (Courtesy of Pablo
Arbelaez, Michael Maire, Charles Fowlkes, and Jitendra Malik)

SUPERPIXELS

24.3
APPEARANCE

The alternative approach is based on trying to â€œclusterâ€ the pixels into regions based on
their brightness, color, and texture. Shi and Malik (2000) set this up as a graph partitioning
problem. The nodes of the graph correspond to pixels, and edges to connections between
pixels. The weight Wij on the edge connecting a pair of pixels i and j is based on how similar
the two pixels are in brightness, color, texture, etc. Partitions that minimize a normalized cut
criterion are then found. Roughly speaking, the criterion for partitioning the graph is to
minimize the sum of weights of connections across the groups of pixels and maximize the
sum of weights of connections within the groups.
Segmentation based purely on low-level, local attributes such as brightness and color
cannot be expected to deliver the final correct boundaries of all the objects in the scene. To
reliably find object boundaries we need high-level knowledge of the likely kinds of objects
in the scene. Representing this knowledge is a topic of active research. A popular strategy is
to produce an over-segmentation of an image, containing hundreds of homogeneous regions
known as superpixels. From there, knowledge-based algorithms can take over; they will
find it easier to deal with hundreds of superpixels rather than millions of raw pixels. How to
exploit high-level knowledge of objects is the subject of the next section.

O BJECT R ECOGNITION BY A PPEARANCE
Appearance is shorthand for what an object tends to look like. Some object categoriesâ€”for
example, baseballsâ€”vary rather little in appearance; all of the objects in the category look
about the same under most circumstances. In this case, we can compute a set of features
describing each class of images likely to contain the object, then test it with a classifier.

Section 24.3.

SLIDING WINDOW

Object Recognition by Appearance

943

Other object categoriesâ€”for example, houses or ballet dancersâ€”vary greatly. A house
can have different size, color, and shape and can look different from different angles. A dancer
looks different in each pose, or when the stage lights change colors. A useful abstraction is to
say that some objects are made up of local patterns which tend to move around with respect to
one another. We can then find the object by looking at local histograms of detector responses,
which expose whether some part is present but suppress the details of where it is.
Testing each class of images with a learned classifier is an important general recipe.
It works extremely well for faces looking directly at the camera, because at low resolution
and under reasonable lighting, all such faces look quite similar. The face is round, and quite
bright compared to the eye sockets; these are dark, because they are sunken, and the mouth is
a dark slash, as are the eyebrows. Major changes of illumination can cause some variations in
this pattern, but the range of variation is quite manageable. That makes it possible to detect
face positions in an image that contains faces. Once a computational challenge, this feature
is now commonplace in even inexpensive digital cameras.
For the moment, we will consider only faces where the nose is oriented vertically; we
will deal with rotated faces below. We sweep a round window of fixed size over the image,
compute features for it, and present the features to a classifier. This strategy is sometimes
called the sliding window. Features need to be robust to shadows and to changes in brightness
caused by illumination changes. One strategy is to build features out of gradient orientations.
Another is to estimate and correct the illumination in each image window. To find faces of
different sizes, repeat the sweep over larger or smaller versions of the image. Finally, we
postprocess the responses across scales and locations to produce the final set of detections.
Postprocessing is important, because it is unlikely that we have chosen a window size
that is exactly the right size for a face (even if we use multiple sizes). Thus, we will likely
have several overlapping windows that each report a match for a face. However, if we use
a classifier that can report strength of response (for example, logistic regression or a support
vector machine) we can combine these partial overlapping matches at nearby locations to
yield a single high-quality match. That gives us a face detector that can search over locations
and scales. To search rotations as well, we use two steps. We train a regression procedure
to estimate the best orientation of any face present in a window. Now, for each window, we
estimate the orientation, reorient the window, then test whether a vertical face is present with
our classifier. All this yields a system whose architecture is sketched in Figure 24.12.
Training data is quite easily obtained. There are several data sets of marked-up face
images, and rotated face windows are easy to build (just rotate a window from a training
data set). One trick that is widely used is to take each example window, then produce new
examples by changing the orientation of the window, the center of the window, or the scale
very slightly. This is an easy way of getting a bigger data set that reflects real images fairly
well; the trick usually improves performance significantly. Face detectors built along these
lines now perform very well for frontal faces (side views are harder).

944

Chapter

24.

Perception

Non-maximal
suppresion

Image

Responses

Detections

Estimate
orientation

Correct
illumination

Rotate
window

Features

Classifier

Figure 24.12 Face finding systems vary, but most follow the architecture illustrated in
two parts here. On the top, we go from images to responses, then apply non-maximum
suppression to find the strongest local response. The responses are obtained by the process
illustrated on the bottom. We sweep a window of fixed size over larger and smaller versions
of the image, so as to find smaller or larger faces, respectively. The illumination in the
window is corrected, and then a regression engine (quite often, a neural net) predicts the
orientation of the face. The window is corrected to this orientation and then presented to a
classifier. Classifier outputs are then postprocessed to ensure that only one face is placed at
each location in the image.

24.3.1 Complex appearance and pattern elements
Many objects produce much more complex patterns than faces do. This is because several
effects can move features around in an image of the object. Effects include (Figure 24.13)
â€¢ Foreshortening, which causes a pattern viewed at a slant to be significantly distorted.
â€¢ Aspect, which causes objects to look different when seen from different directions.
Even as simple an object as a doughnut has several aspects; seen from the side, it looks
like a flattened oval, but from above it is an annulus.
â€¢ Occlusion, where some parts are hidden from some viewing directions. Objects can
occlude one another, or parts of an object can occlude other parts, an effect known as
self-occlusion.
â€¢ Deformation, where internal degrees of freedom of the object change its appearance.
For example, people can move their arms and legs around, generating a very wide range
of different body configurations.
However, our recipe of searching across location and scale can still work. This is because
some structure will be present in the images produced by the object. For example, a picture
of a car is likely to show some of headlights, doors, wheels, windows, and hubcaps, though
they may be in somewhat different arrangements in different pictures. This suggests modeling
objects with pattern elementsâ€”collections of parts. These pattern elements may move around

Section 24.3.

Object Recognition by Appearance

945

Foreshortening

Aspect

Occlusion

Deformation

Figure 24.13 Sources of appearance variation. First, elements can foreshorten, like the
circular patch on the top left. This patch is viewed at a slant, and so is elliptical in the
image. Second, objects viewed from different directions can change shape quite dramatically,
a phenomenon known as aspect. On the top right are three different aspects of a doughnut.
Occlusion causes the handle of the mug on the bottom left to disappear when the mug is
rotated. In this case, because the body and handle belong to the same mug, we have selfocclusion. Finally, on the bottom right, some objects can deform dramatically.

with respect to one another, but if most of the pattern elements are present in about the right
place, then the object is present. An object recognizer is then a collection of features that can
tell whether the pattern elements are present, and whether they are in about the right place.
The most obvious approach is to represent the image window with a histogram of the
pattern elements that appear there. This approach does not work particularly well, because
too many patterns get confused with one another. For example, if the pattern elements are
color pixels, the French, UK, and Netherlands flags will get confused because they have
approximately the same color histograms, though the colors are arranged in very different
ways. Quite simple modifications of histograms yield very useful features. The trick is to
preserve some spatial detail in the representation; for example, headlights tend to be at the
front of a car and wheels tend to be at the bottom. Histogram-based features have been
successful in a wide variety of recognition applications; we will survey pedestrian detection.

24.3.2 Pedestrian detection with HOG features
The World Bank estimates that each year car accidents kill about 1.2 million people, of whom
about two thirds are pedestrians. This means that detecting pedestrians is an important application problem, because cars that can automatically detect and avoid pedestrians might save
many lives. Pedestrians wear many different kinds of clothing and appear in many different
configurations, but, at relatively low resolution, pedestrians can have a fairly characteristic
appearance. The most usual cases are lateral or frontal views of a walk. In these cases,

946

Chapter

Image

Orientation
histograms

Positive
components

24.

Perception

Negative
components

Figure 24.14 Local orientation histograms are a powerful feature for recognizing even
quite complex objects. On the left, an image of a pedestrian. On the center left, local orientation histograms for patches. We then apply a classifier such as a support vector machine
to find the weights for each histogram that best separate the positive examples of pedestrians
from non-pedestrians. We see that the positively weighted components look like the outline
of a person. The negative components are less clear; they represent all the patterns that are
c IEEE.
not pedestrians. Figure from Dalal and Triggs (2005) 

we see either a â€œlollipopâ€ shape â€” the torso is wider than the legs, which are together in
the stance phase of the walk â€” or a â€œscissorâ€ shape â€” where the legs are swinging in the
walk. We expect to see some evidence of arms and legs, and the curve around the shoulders
and head also tends to visible and quite distinctive. This means that, with a careful feature
construction, we can build a useful moving-window pedestrian detector.
There isnâ€™t always a strong contrast between the pedestrian and the background, so it
is better to use orientations than edges to represent the image window. Pedestrians can move
their arms and legs around, so we should use a histogram to suppress some spatial detail in
the feature. We break up the window into cells, which could overlap, and build an orientation
histogram in each cell. Doing so will produce a feature that can tell whether the head-andshoulders curve is at the top of the window or at the bottom, but will not change if the head
moves slightly.
One further trick is required to make a good feature. Because orientation features are
not affected by illumination brightness, we cannot treat high-contrast edges specially. This
means that the distinctive curves on the boundary of a pedestrian are treated in the same way
as fine texture detail in clothing or in the background, and so the signal may be submerged
in noise. We can recover contrast information by counting gradient orientations with weights
that reflect how significant a gradient is compared to other gradients in the same cell. We
will write || âˆ‡Ix || for the gradient magnitude at point x in the image, write C for the cell
whose histogram we wish to compute, and write wx,C for the weight that we will use for the

Section 24.4.

Reconstructing the 3D World

947

Figure 24.15 Another example of object recognition, this one using the SIFT feature
(Scale Invariant Feature Transform), an earlier version of the HOG feature. On the left, images of a shoe and a telephone that serve as object models. In the center, a test image. On the
right, the shoe and the telephone have been detected by: finding points in the image whose
SIFT feature descriptions match a model; computing an estimate of pose of the model; and
verifying that estimate. A strong match is usually verified with rare false positives. Images
c IEEE.
from Lowe (1999) 

orientation at x for this cell. A natural choice of weight is
|| âˆ‡Ix ||
.
wx,C = 
uâˆˆC || âˆ‡Iu ||

HOG FEATURE

24.4

This compares the gradient magnitude to others in the cell, so gradients that are large compared to their neighbors get a large weight. The resulting feature is usually called a HOG
feature (for Histogram Of Gradient orientations).
This feature construction is the main way in which pedestrian detection differs from
face detection. Otherwise, building a pedestrian detector is very like building a face detector.
The detector sweeps a window across the image, computes features for that window, then
presents it to a classifier. Non-maximum suppression needs to be applied to the output. In
most applications, the scale and orientation of typical pedestrians is known. For example, in
driving applications in which a camera is fixed to the car, we expect to view mainly vertical
pedestrians, and we are interested only in nearby pedestrians. Several pedestrian data sets
have been published, and these can be used for training the classifier.
Pedestrians are not the only type of object we can detect. In Figure 24.15 we see that
similar techniques can be used to find a variety of objects in different contexts.

R ECONSTRUCTING THE 3D W ORLD
In this section we show how to go from the two-dimensional image to a three-dimensional
representation of the scene. The fundamental question is this: Given that all points in the
scene that fall along a ray to the pinhole are projected to the same point in the image, how do
we recover three-dimensional information? Two ideas come to our rescue:

948

Chapter

24.

Perception

â€¢ If we have two (or more) images from different camera positions, then we can triangulate to find the position of a point in the scene.
â€¢ We can exploit background knowledge about the physical scene that gave rise to the
image. Given an object model P(Scene) and a rendering model P(Image | Scene), we
can compute a posterior distribution P(Scene | Image).
There is as yet no single unified theory for scene reconstruction. We survey eight commonly
used visual cues: motion, binocular stereopsis, multiple views, texture, shading, contour,
and familiar objects.

24.4.1 Motion parallax
If the camera moves relative to the three-dimensional scene, the resulting apparent motion in
the image, optical flow, can be a source of information for both the movement of the camera
and depth in the scene. To understand this, we state (without proof) an equation that relates
the optical flow to the viewerâ€™s translational velocity T and the depth in the scene.
The components of the optical flow field are
âˆ’Tx + xTz
âˆ’Ty + yTz
,
vy (x, y) =
,
vx (x, y) =
Z(x, y)
Z(x, y)

FOCUS OF
EXPANSION

where Z(x, y) is the z-coordinate of the point in the scene corresponding to the point in the
image at (x, y).
Note that both components of the optical flow, vx (x, y) and vy (x, y), are zero at the
point x = Tx /Tz , y = Ty /Tz . This point is called the focus of expansion of the flow
field. Suppose we change the origin in the xâ€“y plane to lie at the focus of expansion; then
the expressions for optical flow take on a particularly simple form. Let (x , y  ) be the new
coordinates defined by x = x âˆ’ Tx /Tz , y  = y âˆ’ Ty /Tz . Then
x Tz
y  Tz
 
,
v
.
(x
,
y
)
=
y
Z(x , y  )
Z(x , y  )
Note that there is a scale-factor ambiguity here. If the camera was moving twice as fast, and
every object in the scene was twice as big and at twice the distance to the camera, the optical
flow field would be exactly the same. But we can still extract quite useful information.
vx (x , y  ) =

1. Suppose you are a fly trying to land on a wall and you want to know the time-tocontact at the current velocity. This time is given by Z/Tz . Note that although the
instantaneous optical flow field cannot provide either the distance Z or the velocity
component Tz , it can provide the ratio of the two and can therefore be used to control
the landing approach. There is considerable experimental evidence that many different
animal species exploit this cue.
2. Consider two points at depths Z1 , Z2 , respectively. We may not know the absolute
value of either of these, but by considering the inverse of the ratio of the optical flow
magnitudes at these points, we can determine the depth ratio Z1 /Z2 . This is the cue of
motion parallax, one we use when we look out of the side window of a moving car or
train and infer that the slower moving parts of the landscape are farther away.

Section 24.4.

Reconstructing the 3D World

949
Perceived object

Right image

Left image

Left

Right

Disparity

(a)

(b)

Figure 24.16 Translating a camera parallel to the image plane causes image features to
move in the camera plane. The disparity in positions that results is a cue to depth. If we
superimpose left and right image, as in (b), we see the disparity.

24.4.2 Binocular stereopsis

BINOCULAR
STEREOPSIS

DISPARITY

Most vertebrates have two eyes. This is useful for redundancy in case of a lost eye, but it
helps in other ways too. Most prey have eyes on the side of the head to enable a wider field
of vision. Predators have the eyes in the front, enabling them to use binocular stereopsis.
The idea is similar to motion parallax, except that instead of using images over time, we use
two (or more) images separated in space. Because a given feature in the scene will be in a
different place relative to the z-axis of each image plane, if we superpose the two images,
there will be a disparity in the location of the image feature in the two images. You can see
this in Figure 24.16, where the nearest point of the pyramid is shifted to the left in the right
image and to the right in the left image.
Note that to measure disparity we need to solve the correspondence problem, that is,
determine for a point in the left image, the point in the right image that results from the
projection of the same scene point. This is analogous to what one has to do in measuring
optical flow, and the most simple-minded approaches are somewhat similar and based on
comparing blocks of pixels around corresponding points using the sum of squared differences.
In practice, we use much more sophisticated algorithms, which exploit additional constraints.
Assuming that we can measure disparity, how does this yield information about depth
in the scene? We will need to work out the geometrical relationship between disparity and
depth. First, we will consider the case when both the eyes (or cameras) are looking forward
with their optical axes parallel. The relationship of the right camera to the left camera is then
just a displacement along the x-axis by an amount b, the baseline. We can use the optical
flow equations from the previous section, if we think of this as resulting from a translation

950

Chapter
Left
eye

Perception

Î´Î¸/2

PL

Î¸

b

Right
eye

24.

P0

P

PR
Z

Î´Z

Figure 24.17 The relation between disparity and depth in stereopsis. The centers of projection of the two eyes are b apart, and the optical axes intersect at the fixation point P0 . The
point P in the scene projects to points PL and PR in the two eyes. In angular terms, the
disparity between these is Î´Î¸. See text.

FIXATE

vector T acting for time Î´t, with Tx = b/Î´t and Ty = Tz = 0. The horizontal and vertical
disparity are given by the optical flow components, multiplied by the time step Î´t, H = vx Î´t,
V = vy Î´t. Carrying out the substitutions, we get the result that H = b/Z, V = 0. In words,
the horizontal disparity is equal to the ratio of the baseline to the depth, and the vertical
disparity is zero. Given that we know b, we can measure H and recover the depth Z.
Under normal viewing conditions, humans fixate; that is, there is some point in the
scene at which the optical axes of the two eyes intersect. Figure 24.17 shows two eyes fixated
at a point P0 , which is at a distance Z from the midpoint of the eyes. For convenience,
we will compute the angular disparity, measured in radians. The disparity at the point of
fixation P0 is zero. For some other point P in the scene that is Î´Z farther away, we can
compute the angular displacements of the left and right images of P , which we will call PL
and PR , respectively. If each of these is displaced by an angle Î´Î¸/2 relative to P0 , then the
displacement between PL and PR , which is the disparity of P , is just Î´Î¸. From Figure 24.17,
b/2
tan Î¸ = b/2
Z and tan(Î¸ âˆ’ Î´Î¸/2) = Z+Î´Z , but for small angles, tan Î¸ â‰ˆ Î¸, so
b/2
bÎ´Z
b/2
âˆ’
â‰ˆ
Z
Z + Î´Z
2Z 2
and, since the actual disparity is Î´Î¸, we have
bÎ´Z
disparity = 2 .
Z
In humans, b (the baseline distance between the eyes) is about 6 cm. Suppose that Z is about
100 cm. If the smallest detectable Î´Î¸ (corresponding to the pixel size) is about 5 seconds
of arc, this gives a Î´Z of 0.4 mm. For Z = 30 cm, we get the impressively small value
Î´Z = 0.036 mm. That is, at a distance of 30 cm, humans can discriminate depths that differ
by as little as 0.036 mm, enabling us to thread needles and the like.
Î´Î¸/2 =

BASELINE

Section 24.4.

Reconstructing the 3D World

951

Figure 24.18 (a) Four frames from a video sequence in which the camera is moved and
rotated relative to the object. (b) The first frame of the sequence, annotated with small boxes
highlighting the features found by the feature detector. (Courtesy of Carlo Tomasi.)

24.4.3 Multiple views
Shape from optical flow or binocular disparity are two instances of a more general framework,
that of exploiting multiple views for recovering depth. In computer vision, there is no reason
for us to be restricted to differential motion or to only use two cameras converging at a fixation
point. Therefore, techniques have been developed that exploit the information available in
multiple views, even from hundreds or thousands of cameras. Algorithmically, there are
three subproblems that need to be solved:
â€¢ The correspondence problem, i.e., identifying features in the different images that are
projections of the same feature in the three-dimensional world.
â€¢ The relative orientation problem, i.e., determining the transformation (rotation and
translation) between the coordinate systems fixed to the different cameras.
â€¢ The depth estimation problem, i.e., determining the depths of various points in the world
for which image plane projections were available in at least two views
The development of robust matching procedures for the correspondence problem, accompanied by numerically stable algorithms for solving for relative orientations and scene depth, is
one of the success stories of computer vision. Results from one such approach due to Tomasi
and Kanade (1992) are shown in Figures 24.18 and 24.19.

24.4.4 Texture

TEXEL

Earlier we saw how texture was used for segmenting objects. It can also be used to estimate
distances. In Figure 24.20 we see that a homogeneous texture in the scene results in varying
texture elements, or texels, in the image. All the paving tiles in (a) are identical in the scene.
They appear different in the image for two reasons:

952

Chapter

(a)

24.

Perception

(b)

Figure 24.19 (a) Three-dimensional reconstruction of the locations of the image features
in Figure 24.18, shown from above. (b) The real house, taken from the same position.

1. Differences in the distances of the texels from the camera. Distant objects appear smaller
by a scaling factor of 1/Z.
2. Differences in the foreshortening of the texels. If all the texels are in the ground plane
then distance ones are viewed at an angle that is farther off the perpendicular, and so
are more foreshortened. The magnitude of the foreshortening effect is proportional to
cos Ïƒ, where Ïƒ is the slant, the angle between the Z-axis and n, the surface normal to
the texel.
Researchers have developed various algorithms that try to exploit the variation in the
appearance of the projected texels as a basis for determining surface normals. However, the
accuracy and applicability of these algorithms is not anywhere as general as those based on
using multiple views.

24.4.5 Shading
Shadingâ€”variation in the intensity of light received from different portions of a surface in a
sceneâ€”is determined by the geometry of the scene and by the reflectance properties of the
surfaces. In computer graphics, the objective is to compute the image brightness I(x, y),
given the scene geometry and reflectance properties of the objects in the scene. Computer
vision aims to invert the processâ€”that is, to recover the geometry and reflectance properties,
given the image brightness I(x, y). This has proved to be difficult to do in anything but the
simplest cases.
From the physical model of section 24.1.4, we know that if a surface normal points
toward the light source, the surface is brighter, and if it points away, the surface is darker.
We cannot conclude that a dark patch has its normal pointing away from the light; instead,
it could have low albedo. Generally, albedo changes quite quickly in images, and shading

Section 24.4.

Reconstructing the 3D World

(a)

953

(b)

Figure 24.20 (a) A textured scene. Assuming that the real texture is uniform allows recovery of the surface orientation. The computed surface orientation is indicated by overlaying a
black circle and pointer, transformed as if the circle were painted on the surface at that point.
(b) Recovery of shape from texture for a curved surface (white circle and pointer this time).
Images courtesy of Jitendra Malik and Ruth Rosenholtz (1994).

changes rather slowly, and humans seem to be quite good at using this observation to tell
whether low illumination, surface orientation, or albedo caused a surface patch to be dark.
To simplify the problem, let us assume that the albedo is known at every surface point. It
is still difficult to recover the normal, because the image brightness is one measurement but
the normal has two unknown parameters, so we cannot simply solve for the normal. The key
to this situation seems to be that nearby normals will be similar, because most surfaces are
smoothâ€”they do not have sharp changes.
The real difficulty comes in dealing with interreflections. If we consider a typical indoor
scene, such as the objects inside an office, surfaces are illuminated not only by the light
sources, but also by the light reflected from other surfaces in the scene that effectively serve
as secondary light sources. These mutual illumination effects are quite significant and make
it quite difficult to predict the relationship between the normal and the image brightness. Two
surface patches with the same normal might have quite different brightnesses, because one
receives light reflected from a large white wall and the other faces only a dark bookcase.
Despite these difficulties, the problem is important. Humans seem to be able to ignore the
effects of interreflections and get a useful perception of shape from shading, but we know
frustratingly little about algorithms to do this.

24.4.6 Contour
When we look at a line drawing, such as Figure 24.21, we get a vivid perception of threedimensional shape and layout. How? It is a combination of recognition of familiar objects in
the scene and the application of generic constraints such as the following:
â€¢ Occluding contours, such as the outlines of the hills. One side of the contour is nearer
to the viewer, the other side is farther away. Features such as local convexity and sym-

954

Chapter

Figure 24.21
FIGURE-GROUND

GROUND PLANE

24.

Perception

An evocative line drawing. (Courtesy of Isha Malik.)

metry provide cues to solving the figure-ground problemâ€”assigning which side of the
contour is figure (nearer), and which is ground (farther). At an occluding contour, the
line of sight is tangential to the surface in the scene.
â€¢ T-junctions. When one object occludes another, the contour of the farther object is
interrupted, assuming that the nearer object is opaque. A T-junction results in the image.
â€¢ Position on the ground plane. Humans, like many other terrestrial animals, are very
often in a scene that contains a ground plane, with various objects at different locations
on this plane. Because of gravity, typical objects donâ€™t float in air but are supported by
this ground plane, and we can exploit the very special geometry of this viewing scenario.
Let us work out the projection of objects of different heights and at different locations on the ground plane. Suppose that the eye, or camera, is at a height hc above
the ground plane. Consider an object of height Î´Y resting on the ground plane, whose
bottom is at (X, âˆ’hc , Z) and top is at (X, Î´Y âˆ’ hc , Z). The bottom projects to the
image point (f X/Z, âˆ’f hc /Z) and the top to (f X/Z, f (Î´Y âˆ’ hc )/Z). The bottoms of
nearer objects (small Z) project to points lower in the image plane; farther objects have
bottoms closer to the horizon.

24.4.7 Objects and the geometric structure of scenes
A typical adult human head is about 9 inches long. This means that for someone who is 43
feet away, the angle subtended by the head at the camera is 1 degree. If we see a person whose
head appears to subtend just half a degree, Bayesian inference suggests we are looking at a
normal person who is 86 feet away, rather than someone with a half-size head. This line of
reasoning supplies us with a method to check the results of a pedestrian detector, as well as a
method to estimate the distance to an object. For example, all pedestrians are about the same
height, and they tend to stand on a ground plane. If we know where the horizon is in an image,
we can rank pedestrians by distance to the camera. This works because we know where their

Section 24.4.

Reconstructing the 3D World

955
Image plane

Horizon

Ground plane

C

C
A

B
B

A

Figure 24.22 In an image of people standing on a ground plane, the people whose feet
are closer to the horizon in the image must be farther away (top drawing). This means they
must look smaller in the image (left lower drawing). This means that the size and location of
real pedestrians in an image depend upon one another and on the location of the horizon. To
exploit this, we need to identify the ground plane, which is done using shape-from-texture
methods. From this information, and from some likely pedestrians, we can recover a horizon
as shown in the center image. On the right, acceptable pedestrian boxes given this geometric
context. Notice that pedestrians who are higher in the scene must be smaller. If they are not,
c IEEE.
then they are false positives. Images from Hoiem et al. (2008) 

feet are, and pedestrians whose feet are closer to the horizon in the image are farther away
from the camera (Figure 24.22). Pedestrians who are farther away from the camera must also
be smaller in the image. This means we can rule out some detector responses â€” if a detector
finds a pedestrian who is large in the image and whose feet are close to the horizon, it has
found an enormous pedestrian; these donâ€™t exist, so the detector is wrong. In fact, many or
most image windows are not acceptable pedestrian windows, and need not even be presented
to the detector.
There are several strategies for finding the horizon, including searching for a roughly
horizontal line with a lot of blue above it, and using surface orientation estimates obtained
from texture deformation. A more elegant strategy exploits the reverse of our geometric
constraints. A reasonably reliable pedestrian detector is capable of producing estimates of the
horizon, if there are several pedestrians in the scene at different distances from the camera.
This is because the relative scaling of the pedestrians is a cue to where the horizon is. So we
can extract a horizon estimate from the detector, then use this estimate to prune the pedestrian
detectorâ€™s mistakes.

956

ALIGNMENT METHOD

Chapter

24.

Perception

If the object is familiar, we can estimate more than just the distance to it, because what it
looks like in the image depends very strongly on its pose, i.e., its position and orientation with
respect to the viewer. This has many applications. For instance, in an industrial manipulation
task, the robot arm cannot pick up an object until the pose is known. In the case of rigid
objects, whether three-dimensional or two-dimensional, this problem has a simple and welldefined solution based on the alignment method, which we now develop.
The object is represented by M features or distinguished points m1 , m2 , . . . , mM in
three-dimensional spaceâ€”perhaps the vertices of a polyhedral object. These are measured
in some coordinate system that is natural for the object. The points are then subjected to
an unknown three-dimensional rotation R, followed by translation by an unknown amount t
and then projection to give rise to image feature points p1 , p2 , . . . , pN on the image plane.
In general, N = M , because some model points may be occluded, and the feature detector
could miss some features (or invent false ones due to noise). We can express this as
pi = Î (Rmi + t) = Q(mi )
for a three-dimensional model point mi and the corresponding image point pi . Here, R
is a rotation matrix, t is a translation, and Î  denotes perspective projection or one of its
approximations, such as scaled orthographic projection. The net result is a transformation Q
that will bring the model point mi into alignment with the image point pi . Although we do
not know Q initially, we do know (for rigid objects) that Q must be the same for all the model
points.
We can solve for Q, given the three-dimensional coordinates of three model points and
their two-dimensional projections. The intuition is as follows: we can write down equations
relating the coordinates of pi to those of mi . In these equations, the unknown quantities
correspond to the parameters of the rotation matrix R and the translation vector t. If we have
enough equations, we ought to be able to solve for Q. We will not give a proof here; we
merely state the following result:
Given three noncollinear points m1 , m2 , and m3 in the model, and their scaled
orthographic projections p1 , p2 , and p3 on the image plane, there exist exactly
two transformations from the three-dimensional model coordinate frame to a twodimensional image coordinate frame.
These transformations are related by a reflection around the image plane and can be computed
by a simple closed-form solution. If we could identify the corresponding model features for
three features in the image, we could compute Q, the pose of the object.
Let us specify position and orientation in mathematical terms. The position of a point P
in the scene is characterized by three numbers, the (X, Y, Z) coordinates of P in a coordinate
frame with its origin at the pinhole and the Z-axis along the optical axis (Figure 24.2 on
page 931). What we have available is the perspective projection (x, y) of the point in the
image. This specifies the ray from the pinhole along which P lies; what we do not know is
the distance. The term â€œorientationâ€ could be used in two senses:
1. The orientation of the object as a whole. This can be specified in terms of a threedimensional rotation relating its coordinate frame to that of the camera.

Section 24.5.

SLANT
TILT

SHAPE

24.5

DEFORMABLE
TEMPLATE

Object Recognition from Structural Information

957

2. The orientation of the surface of the object at P . This can be specified by a normal
vector, nâ€”which is a vector specifying the direction that is perpendicular to the surface.
Often we express the surface orientation using the variables slant and tilt. Slant is the
angle between the Z-axis and n. Tilt is the angle between the X-axis and the projection
of n on the image plane.
When the camera moves relative to an object, both the objectâ€™s distance and its orientation
change. What is preserved is the shape of the object. If the object is a cube, that fact is
not changed when the object moves. Geometers have been attempting to formalize shape for
centuries, the basic concept being that shape is what remains unchanged under some group of
transformationsâ€”for example, combinations of rotations and translations. The difficulty lies
in finding a representation of global shape that is general enough to deal with the wide variety
of objects in the real worldâ€”not just simple forms like cylinders, cones, and spheresâ€”and yet
can be recovered easily from the visual input. The problem of characterizing the local shape
of a surface is much better understood. Essentially, this can be done in terms of curvature:
how does the surface normal change as one moves in different directions on the surface? For
a plane, there is no change at all. For a cylinder, if one moves parallel to the axis, there is
no change, but in the perpendicular direction, the surface normal rotates at a rate inversely
proportional to the radius of the cylinder, and so on. All this is studied in the subject called
differential geometry.
The shape of an object is relevant for some manipulation tasks (e.g., deciding where to
grasp an object), but its most significant role is in object recognition, where geometric shape
along with color and texture provide the most significant cues to enable us to identify objects,
classify what is in the image as an example of some class one has seen before, and so on.

O BJECT R ECOGNITION FROM S TRUCTURAL I NFORMATION
Putting a box around pedestrians in an image may well be enough to avoid driving into them.
We have seen that we can find a box by pooling the evidence provided by orientations, using
histogram methods to suppress potentially confusing spatial detail. If we want to know more
about what someone is doing, we will need to know where their arms, legs, body, and head lie
in the picture. Individual body parts are quite difficult to detect on their own using a moving
window method, because their color and texture can vary widely and because they are usually
small in images. Often, forearms and shins are as small as two to three pixels wide. Body
parts do not usually appear on their own, and representing what is connected to what could
be quite powerful, because parts that are easy to find might tell us where to look for parts that
are small and hard to detect.
Inferring the layout of human bodies in pictures is an important task in vision, because
the layout of the body often reveals what people are doing. A model called a deformable
template can tell us which configurations are acceptable: the elbow can bend but the head is
never joined to the foot. The simplest deformable template model of a person connects lower
arms to upper arms, upper arms to the torso, and so on. There are richer models: for example,

958

Chapter

24.

Perception

we could represent the fact that left and right upper arms tend to have the same color and
texture, as do left and right legs. These richer models remain difficult to work with, however.

24.5.1 The geometry of bodies: Finding arms and legs

POSE

PICTORIAL
STRUCTURE MODEL

For the moment, we assume that we know what the personâ€™s body parts look like (e.g., we
know the color and texture of the personâ€™s clothing). We can model the geometry of the
body as a tree of eleven segments (upper and lower left and right arms and legs respectively,
a torso, a face, and hair on top of the face) each of which is rectangular. We assume that
the position and orientation (pose) of the left lower arm is independent of all other segments
given the pose of the left upper arm; that the pose of the left upper arm is independent of
all segments given the pose of the torso; and extend these assumptions in the obvious way
to include the right arm and the legs, the face, and the hair. Such models are often called
â€œcardboard peopleâ€ models. The model forms a tree, which is usually rooted at the torso. We
will search the image for the best match to this cardboard person using inference methods for
a tree-structured Bayes net (see Chapter 14).
There are two criteria for evaluating a configuration. First, an image rectangle should
look like its segment. For the moment, we will remain vague about precisely what that means,
but we assume we have a function Ï†i that scores how well an image rectangle matches a body
segment. For each pair of related segments, we have another function Ïˆ that scores how
well relations between a pair of image rectangles match those to be expected from the body
segments. The dependencies between segments form a tree, so each segment has only one
parent, and we could write Ïˆi,pa(i) . All the functions will be larger if the match is better,
so we can think of them as being like a log probability. The cost of a particular match that
allocates image rectangle mi to body segment i is then


Ï†i (mi ) +
Ïˆi,pa(i) (mi , mpa(i) ) .
iâˆˆsegments
iâˆˆsegments
Dynamic programming can find the best match, because the relational model is a tree.
It is inconvenient to search a continuous space, and we will discretize the space of image
rectangles. We do so by discretizing the location and orientation of rectangles of fixed size
(the sizes may be different for different segments). Because ankles and knees are different,
we need to distinguish between a rectangle and the same rectangle rotated by 180 â—¦ . One
could visualize the result as a set of very large stacks of small rectangles of image, cut out at
different locations and orientations. There is one stack per segment. We must now find the
best allocation of rectangles to segments. This will be slow, because there are many image
rectangles and, for the model we have given, choosing the right torso will be O(M 6 ) if there
are M image rectangles. However, various speedups are available for an appropriate choice
of Ïˆ, and the method is practical (Figure 24.23). The model is usually known as a pictorial
structure model.
Recall our assumption that we know what we need to know about what the person looks
like. If we are matching a person in a single image, the most useful feature for scoring segment matches turns out to be color. Texture features donâ€™t work well in most cases, because
folds on loose clothing produce strong shading patterns that overlay the image texture. These

Section 24.5.

Object Recognition from Structural Information

959

Figure 24.23 A pictorial structure model evaluates a match between a set of image rectangles and a cardboard person (shown on the left) by scoring the similarity in appearance
between body segments and image segments and the spatial relations between the image segments. Generally, a match is better if the image segments have about the right appearance and
are in about the right place with respect to one another. The appearance model uses average
colors for hair, head, torso, and upper and lower arms and legs. The relevant relations are
shown as arrows. On the right, the best match for a particular image, obtained using dynamic
programming. The match is a fair estimate of the configuration of the body. Figure from
c IEEE.
Felzenszwalb and Huttenlocher (2000) 

APPEARANCE
MODEL

patterns are strong enough to disrupt the true texture of the cloth. In current work, Ïˆ typically
reflects the need for the ends of the segments to be reasonably close together, but there are
usually no constraints on the angles. Generally, we donâ€™t know what a person looks like,
and must build a model of segment appearances. We call the description of what a person
looks like the appearance model. If we must report the configuration of a person in a single
image, we can start with a poorly tuned appearance model, estimate configuration with this,
then re-estimate appearance, and so on. In video, we have many frames of the same person,
and this will reveal their appearance.

24.5.2 Coherent appearance: Tracking people in video
Tracking people in video is an important practical problem. If we could reliably report the
location of arms, legs, torso, and head in video sequences, we could build much improved
game interfaces and surveillance systems. Filtering methods have not had much success
with this problem, because people can produce large accelerations and move quite fast. This
means that for 30 Hz video, the configuration of the body in frame i doesnâ€™t constrain the
configuration of the body in frame i+1 all that strongly. Currently, the most effective methods
exploit the fact that appearance changes very slowly from frame to frame. If we can infer an
appearance model of an individual from the video, then we can use this information in a
pictorial structure model to detect that person in each frame of the video. We can then link
these locations across time to make a track.

960

Chapter

24.

Perception

torso

arm

Lateral walking
detector

Appearance
model

Body part
maps

Detected figure

motion blur
& interlacing

Figure 24.24 We can track moving people with a pictorial structure model by first obtaining an appearance model, then applying it. To obtain the appearance model, we scan the
image to find a lateral walking pose. The detector does not need to be very accurate, but
should produce few false positives. From the detector response, we can read off pixels that
lie on each body segment, and others that do not lie on that segment. This makes it possible to
build a discriminative model of the appearance of each body part, and these are tied together
into a pictorial structure model of the person being tracked. Finally, we can reliably track by
detecting this model in each frame. As the frames in the lower part of the image suggest, this
procedure can track complicated, fast-changing body configurations, despite degradation of
c IEEE.
the video signal due to motion blur. Figure from Ramanan et al. (2007) 

There are several ways to infer a good appearance model. We regard the video as a
large stack of pictures of the person we wish to track. We can exploit this stack by looking
for appearance models that explain many of the pictures. This would work by detecting
body segments in each frame, using the fact that segments have roughly parallel edges. Such
detectors are not particularly reliable, but the segments we want to find are special. They
will appear at least once in most of the frames of video; such segments can be found by
clustering the detector responses. It is best to start with the torso, because it is big and
because torso detectors tend to be reliable. Once we have a torso appearance model, upper
leg segments should appear near the torso, and so on. This reasoning yields an appearance
model, but it can be unreliable if people appear against a near-fixed background where the
segment detector generates lots of false positives. An alternative is to estimate appearance
for many of the frames of video by repeatedly reestimating configuration and appearance; we
then see if one appearance model explains many frames. Another alternative, which is quite

Section 24.6.

Using Vision

961

Figure 24.25 Some complex human actions produce consistent patterns of appearance
and motion. For example, drinking involves movements of the hand in front of the face. The
first three images are correct detections of drinking; the fourth is a false-positive (the cook is
looking into the coffee pot, but not drinking from it). Figure from Laptev and Perez (2007)
c IEEE.


reliable in practice, is to apply a detector for a fixed body configuration to all of the frames. A
good choice of configuration is one that is easy to detect reliably, and where there is a strong
chance the person will appear in that configuration even in a short sequence (lateral walking
is a good choice). We tune the detector to have a low false positive rate, so we know when it
responds that we have found a real person; and because we have localized their torso, arms,
legs, and head, we know what these segments look like.

24.6

BACKGROUND
SUBTRACTION

U SING V ISION
If vision systems could analyze video and understood what people are doing, we would be
able to: design buildings and public places better by collecting and using data about what
people do in public; build more accurate, more secure, and less intrusive surveillance systems;
build computer sports commentators; and build human-computer interfaces that watch people
and react to their behavior. Applications for reactive interfaces range from computer games
that make a player get up and move around to systems that save energy by managing heat and
light in a building to match where the occupants are and what they are doing.
Some problems are well understood. If people are relatively small in the video frame,
and the background is stable, it is easy to detect the people by subtracting a background image
from the current frame. If the absolute value of the difference is large, this background
subtraction declares the pixel to be a foreground pixel; by linking foreground blobs over
time, we obtain a track.
Structured behaviors like ballet, gymnastics, or tai chi have specific vocabularies of actions. When performed against a simple background, videos of these actions are easy to deal
with. Background subtraction identifies the major moving regions, and we can build HOG
features (keeping track of flow rather than orientation) to present to a classifier. We can detect
consistent patterns of action with a variant of our pedestrian detector, where the orientation
features are collected into histogram buckets over time as well as space (Figure 24.25).
More general problems remain open. The big research question is to link observations
of the body and the objects nearby to the goals and intentions of the moving people. One
source of difficulty is that we lack a simple vocabulary of human behavior. Behavior is a lot

962

Chapter

24.

Perception

like color, in that people tend to think they know a lot of behavior names but canâ€™t produce
long lists of such words on demand. There is quite a lot of evidence that behaviors combineâ€”
you can, for example, drink a milkshake while visiting an ATMâ€”but we donâ€™t yet know
what the pieces are, how the composition works, or how many composites there might be.
A second source of difficulty is that we donâ€™t know what features expose what is happening.
For example, knowing someone is close to an ATM may be enough to tell that theyâ€™re visiting
the ATM. A third difficulty is that the usual reasoning about the relationship between training
and test data is untrustworthy. For example, we cannot argue that a pedestrian detector is
safe simply because it performs well on a large data set, because that data set may well omit
important, but rare, phenomena (for example, people mounting bicycles). We wouldnâ€™t want
our automated driver to run over a pedestrian who happened to do something unusual.

24.6.1 Words and pictures
Many Web sites offer collections of images for viewing. How can we find the images we
want? Letâ€™s suppose the user enters a text query, such as â€œbicycle race.â€ Some of the images
will have keywords or captions attached, or will come from Web pages that contain text near
the image. For these, image retrieval can be like text retrieval: ignore the images and match
the imageâ€™s text against the query (see Section 22.3 on page 867).
However, keywords are usually incomplete. For example, a picture of a cat playing in
the street might be tagged with words like â€œcatâ€ and â€œstreet,â€ but it is easy to forget to mention
the â€œgarbage canâ€ or the â€œfish bones.â€ Thus an interesting task is to annotate an image (which
may already have a few keywords) with additional appropriate keywords.
In the most straightforward version of this task, we have a set of correctly tagged example images, and we wish to tag some test images. This problem is sometimes known as
auto-annotation. The most accurate solutions are obtained using nearest-neighbors methods.
One finds the training images that are closest to the test image in a feature space metric that
is trained using examples, then reports their tags.
Another version of the problem involves predicting which tags to attach to which regions in a test image. Here we do not know which regions produced which tags for the training data. We can use a version of expectation maximization to guess an initial correspondence
between text and regions, and from that estimate a better decomposition into regions, and so
on.

24.6.2 Reconstruction from many views
Binocular stereopsis works because for each point we have four measurements constraining
three unknown degrees of freedom. The four measurements are the (x, y) positions of the
point in each view, and the unknown degrees of freedom are the (x, y, z) coordinate values of
the point in the scene. This rather crude argument suggests, correctly, that there are geometric
constraints that prevent most pairs of points from being acceptable matches. Many images of
a set of points should reveal their positions unambiguously.
We donâ€™t always need a second picture to get a second view of a set of points. If we
believe the original set of points comes from a familiar rigid 3D object, then we might have

Section 24.6.

Using Vision

963

an object model available as a source of information. If this object model consists of a set of
3D points or of a set of pictures of the object, and if we can establish point correspondences,
we can determine the parameters of the camera that produced the points in the original image.
This is very powerful information. We could use it to evaluate our original hypothesis that
the points come from an object model. We do this by using some points to determine the
parameters of the camera, then projecting model points in this camera and checking to see
whether there are image points nearby.
We have sketched here a technology that is now very highly developed. The technology
can be generalized to deal with views that are not orthographic; to deal with points that are
observed in only some views; to deal with unknown camera properties like focal length; to
exploit various sophisticated searches for appropriate correspondences; and to do reconstruction from very large numbers of points and of views. If the locations of points in the images
are known with some accuracy and the viewing directions are reasonable, very high accuracy
camera and point information can be obtained. Some applications are
â€¢ Model-building: For example, one might build a modeling system that takes a video
sequence depicting an object and produces a very detailed three-dimensional mesh of
textured polygons for use in computer graphics and virtual reality applications. Models
like this can now be built from apparently quite unpromising sets of pictures. For example, Figure 24.26 shows a model of the Statue of Liberty built from pictures found
on the Internet.
â€¢ Matching moves: To place computer graphics characters into real video, we need to
know how the camera moved for the real video, so that we can render the character
correctly.
â€¢ Path reconstruction: Mobile robots need to know where they have been. If they are
moving in a world of rigid objects, then performing a reconstruction and keeping the
camera information is one way to obtain a path.

24.6.3 Using vision for controlling movement
One of the principal uses of vision is to provide information both for manipulating objectsâ€”
picking them up, grasping them, twirling them, and so onâ€”and for navigating while avoiding
obstacles. The ability to use vision for these purposes is present in the most primitive of
animal visual systems. In many cases, the visual system is minimal, in the sense that it
extracts from the available light field just the information the animal needs to inform its
behavior. Quite probably, modern vision systems evolved from early, primitive organisms
that used a photosensitive spot at one end to orient themselves toward (or away from) the
light. We saw in Section 24.4 that flies use a very simple optical flow detection system to
land on walls. A classic study, What the Frogâ€™s Eye Tells the Frogâ€™s Brain (Lettvin et al.,
1959), observes of a frog that, â€œHe will starve to death surrounded by food if it is not moving.
His choice of food is determined only by size and movement.â€
Let us consider a vision system for an automated vehicle driving on a freeway. The
tasks faced by the driver include the following:

964

Chapter

a

b

(a)

(b)

24.

Perception

c

(c)

Figure 24.26 The state of the art in multiple-view reconstruction is now highly advanced.
This figure outlines a system built by Michael Goesele and colleagues from the University
of Washington, TU Darmstadt, and Microsoft Research. From a collection of pictures of a
monument taken by a large community of users and posted on the Internet (a), their system
can determine the viewing directions for those pictures, shown by the small black pyramids
in (b) and a comprehensive 3D reconstruction shown in (c).

1. Lateral controlâ€”ensure that the vehicle remains securely within its lane or changes
lanes smoothly when required.
2. Longitudinal controlâ€”ensure that there is a safe distance to the vehicle in front.
3. Obstacle avoidanceâ€”monitor vehicles in neighboring lanes and be prepared for evasive
maneuvers if one of them decides to change lanes.
The problem for the driver is to generate appropriate steering, acceleration, and braking actions to best accomplish these tasks.
For lateral control, one needs to maintain a representation of the position and orientation
of the car relative to the lane. We can use edge-detection algorithms to find edges corresponding to the lane-marker segments. We can then fit smooth curves to these edge elements. The
parameters of these curves carry information about the lateral position of the car, the direction it is pointing relative to the lane, and the curvature of the lane. This information, along
with information about the dynamics of the car, is all that is needed by the steering-control
system. If we have good detailed maps of the road, then the vision system serves to confirm
our position (and to watch for obstacles that are not on the map).
For longitudinal control, one needs to know distances to the vehicles in front. This can
be accomplished with binocular stereopsis or optical flow. Using these techniques, visioncontrolled cars can now drive reliably at highway speeds.
The more general case of mobile robots navigating in various indoor and outdoor environments has been studied, too. One particular problem, localizing the robot in its environment, now has pretty good solutions. A group at Sarnoff has developed a system based on
two cameras looking forward that track feature points in 3D and use that to reconstruct the

Section 24.7.

Summary

965

position of the robot relative to the environment. In fact, they have two stereoscopic camera
systems, one looking front and one looking backâ€”this gives greater robustness in case the
robot has to go through a featureless patch due to dark shadows, blank walls, and the like. It is
unlikely that there are no features either in the front or in the back. Now of course, that could
happen, so a backup is provided by using an inertial motion unit (IMU) somewhat akin to the
mechanisms for sensing acceleration that we humans have in our inner ears. By integrating
the sensed acceleration twice, one can keep track of the change in position. Combining the
data from vision and the IMU is a problem of probabilistic evidence fusion and can be tackled
using techniques, such as Kalman filtering, we have studied elsewhere in the book.
In the use of visual odometry (estimation of change in position), as in other problems
of odometry, there is the problem of â€œdrift,â€ positional errors accumulating over time. The
solution for this is to use landmarks to provide absolute position fixes: as soon as the robot
passes a location in its internal map, it can adjust its estimate of its position appropriately.
Accuracies on the order of centimeters have been demonstrated with the these techniques.
The driving example makes one point very clear: for a specific task, one does not need
to recover all the information that, in principle, can be recovered from an image. One does
not need to recover the exact shape of every vehicle, solve for shape-from-texture on the grass
surface adjacent to the freeway, and so on. Instead, a vision system should compute just what
is needed to accomplish the task.

24.7

S UMMARY
Although perception appears to be an effortless activity for humans, it requires a significant
amount of sophisticated computation. The goal of vision is to extract information needed for
tasks such as manipulation, navigation, and object recognition.
â€¢ The process of image formation is well understood in its geometric and physical aspects. Given a description of a three-dimensional scene, we can easily produce a picture
of it from some arbitrary camera position (the graphics problem). Inverting the process
by going from an image to a description of the scene is more difficult.
â€¢ To extract the visual information necessary for the tasks of manipulation, navigation,
and recognition, intermediate representations have to be constructed. Early vision
image-processing algorithms extract primitive features from the image, such as edges
and regions.
â€¢ There are various cues in the image that enable one to obtain three-dimensional information about the scene: motion, stereopsis, texture, shading, and contour analysis.
Each of these cues relies on background assumptions about physical scenes to provide
nearly unambiguous interpretations.
â€¢ Object recognition in its full generality is a very hard problem. We discussed brightnessbased and feature-based approaches. We also presented a simple algorithm for pose
estimation. Other possibilities exist.

966

Chapter

B IBLIOGRAPHICAL

AND

24.

Perception

H ISTORICAL N OTES

The eye developed in the Cambrian explosion (530 million years ago), apparently in a common ancestor. Since then, endless variations have developed in different creatures, but the
same gene, Pax-6, regulates the development of the eye in animals as diverse as humans,
mice, and Drosophila.
Systematic attempts to understand human vision can be traced back to ancient times.
Euclid (ca. 300 B . C .) wrote about natural perspectiveâ€”the mapping that associates, with
each point P in the three-dimensional world, the direction of the ray OP joining the center of
projection O to the point P . He was well aware of the notion of motion parallax. The use of
perspective in art was developed in ancient Roman culture, as evidenced by art found in the
ruins of Pompeii (A.D. 79), but was then largely lost for 1300 years. The mathematical understanding of perspective projection, this time in the context of projection onto planar surfaces,
had its next significant advance in the 15th-century in Renaissance Italy. Brunelleschi (1413)
is usually credited with creating the first paintings based on geometrically correct projection
of a three-dimensional scene. In 1435, Alberti codified the rules and inspired generations of
artists whose artistic achievements amaze us to this day. Particularly notable in their development of the science of perspective, as it was called in those days, were Leonardo da Vinci and
Albrecht DuÌˆrer. Leonardoâ€™s late 15th century descriptions of the interplay of light and shade
(chiaroscuro), umbra and penumbra regions of shadows, and aerial perspective are still worth
reading in translation (Kemp, 1989). Stork (2004) analyzes the creation of various pieces of
Renaissance art using computer vision techniques.
Although perspective was known to the ancient Greeks, they were curiously confused
by the role of the eyes in vision. Aristotle thought of the eyes as devices emitting rays, rather
in the manner of modern laser range finders. This mistaken view was laid to rest by the work
of Arab scientists, such as Abu Ali Alhazen, in the 10th century. Alhazen also developed the
camera obscura, a room (camera is Latin for â€œroomâ€ or â€œchamberâ€) with a pinhole that casts
an image on the opposite wall. Of course the image was inverted, which caused no end of
confusion. If the eye was to be thought of as such an imaging device, how do we see rightside up? This enigma exercised the greatest minds of the era (including Leonardo). Kepler
first proposed that the lens of the eye focuses an image on the retina, and Descartes surgically
removed an ox eye and demonstrated that Kepler was right. There was still puzzlement as to
why we do not see everything upside down; today we realize it is just a question of accessing
the retinal data structure in the right way.
In the first half of the 20th century, the most significant research results in vision were
obtained by the Gestalt school of psychology, led by Max Wertheimer. They pointed out the
importance of perceptual organization: for a human observer, the image is not a collection
of pointillist photoreceptor outputs (pixels in computer vision terminology); rather it is organized into coherent groups. One could trace the motivation in computer vision of finding
regions and curves back to this insight. The Gestaltists also drew attention to the â€œfigureâ€“
groundâ€ phenomenonâ€”a contour separating two image regions that, in the world, are at
different depths, appears to belong only to the nearer region, the â€œfigure,â€ and not the farther

Bibliographical and Historical Notes

GENERALIZED
CYLINDER

967

region, the â€œground.â€ The computer vision problem of classifying image curves according to
their significance in the scene can be thought of as a generalization of this insight.
The period after World War II was marked by renewed activity. Most significant was
the work of J. J. Gibson (1950, 1979), who pointed out the importance of optical flow, as well
as texture gradients in the estimation of environmental variables such as surface slant and tilt.
He reemphasized the importance of the stimulus and how rich it was. Gibson emphasized the
role of the active observer whose self-directed movement facilitates the pickup of information
about the external environment.
Computer vision was founded in the 1960s. Robertsâ€™s (1963) thesis at MIT was one
of the earliest publications in the field, introducing key ideas such as edge detection and
model-based matching. There is an urban legend that Marvin Minsky assigned the problem
of â€œsolvingâ€ computer vision to a graduate student as a summer project. According to Minsky
the legend is untrueâ€”it was actually an undergraduate student. But it was an exceptional
undergraduate, Gerald Jay Sussman (who is now a professor at MIT) and the task was not to
â€œsolveâ€ vision, but to investigate some aspects of it.
In the 1960s and 1970s, progress was slow, hampered considerably by the lack of computational and storage resources. Low-level visual processing received a lot of attention. The
widely used Canny edge-detection technique was introduced in Canny (1986). Techniques
for finding texture boundaries based on multiscale, multiorientation filtering of images date to
work such as Malik and Perona (1990). Combining multiple cluesâ€”brightness, texture and
colorâ€”for finding boundary curves in a learning framework was shown by Martin, Fowlkes
and Malik (2004) to considerably improve performance.
The closely related problem of finding regions of coherent brightness, color, and texture, naturally lends itself to formulations in which finding the best partition becomes an
optimization problem. Three leading examples are the Markov Random Fields approach of
Geman and Geman (1984), the variational formulation of Mumford and Shah (1989), and
normalized cuts by Shi and Malik (2000).
Through much of the 1960s, 1970s and 1980s, there were two distinct paradigms in
which visual recognition was pursued, dictated by different perspectives on what was perceived to be the primary problem. Computer vision research on object recognition largely focused on issues arising from the projection of three-dimensional objects onto two-dimensional
images. The idea of alignment, also first introduced by Roberts, resurfaced in the 1980s in the
work of Lowe (1987) and Huttenlocher and Ullman (1990). Also popular was an approach
based on describing shapes in terms of volumetric primitives, with generalized cylinders,
introduced by Tom Binford (1971), proving particularly popular.
In contrast, the pattern recognition community viewed the 3D-to-2D aspects of the problem as not significant. Their motivating examples were in domains such as optical character
recognition and handwritten zip code recognition where the primary concern is that of learning the typical variations characteristic of a class of objects and separating them from other
classes. See LeCun et al. (1995) for a comparison of approaches.
In the late 1990s, these two paradigms started to converge, as both sides adopted the
probabilistic modeling and learning techniques that were becoming popular throughout AI.
Two lines of work contributed significantly. One was research on face detection, such as that

968

Chapter

24.

Perception

of Rowley, Baluja and Kanade (1996), and of Viola and Jones (2002b) which demonstrated
the power of pattern recognition techniques on clearly important and useful tasks. The other
was the development of point descriptors, which enable one to construct feature vectors from
parts of objects. This was pioneered by Schmid and Mohr (1996). Loweâ€™s (2004) SIFT
descriptor is widely used. The HOG descriptor is due to Dalal and Triggs (2005).
Ullman (1979) and Longuet-Higgins (1981) are influential early works in reconstruction from multiple images. Concerns about the stability of structure from motion were significantly allayed by the work of Tomasi and Kanade (1992) who showed that with the use of
multiple frames shape could be recovered quite accurately. In the 1990s, with great increase
in computer speed and storage, motion analysis found many new applications. Building geometrical models of real-world scenes for rendering by computer graphics techniques proved
particularly popular, led by reconstruction algorithms such as the one developed by Debevec,
Taylor, and Malik (1996). The books by Hartley and Zisserman (2000) and Faugeras et al.
(2001) provide a comprehensive treatment of the geometry of multiple views.
For single images, inferring shape from shading was first studied by Horn (1970), and
Horn and Brooks (1989) present an extensive survey of the main papers from a period when
this was a much-studied problem. Gibson (1950) was the first to propose texture gradients
as a cue to shape, though a comprehensive analysis for curved surfaces first appears in Garding (1992) and Malik and Rosenholtz (1997). The mathematics of occluding contours, and
more generally understanding the visual events in the projection of smooth curved objects,
owes much to the work of Koenderink and van Doorn, which finds an extensive treatment in
Koenderinkâ€™s (1990) Solid Shape. In recent years, attention has turned to treating the problem
of shape and surface recovery from a single image as a probabilistic inference problem, where
geometrical cues are not modeled explicitly, but used implicitly in a learning framework. A
good representative is the work of Hoiem, Efros, and Hebert (2008).
For the reader interested in human vision, Palmer (1999) provides the best comprehensive treatment; Bruce et al. (2003) is a shorter textbook. The books by Hubel (1988) and
Rock (1984) are friendly introductions centered on neurophysiology and perception respectively. David Marrâ€™s book Vision (Marr, 1982) played a historical role in connecting computer
vision to psychophysics and neurobiology. While many of his specific models havenâ€™t stood
the test of time, the theoretical perspective from which each task is analyzed at an informational, computational, and implementation level is still illuminating.
For computer vision, the most comprehensive textbook is Forsyth and Ponce (2002).
Trucco and Verri (1998) is a shorter account. Horn (1986) and Faugeras (1993) are two older
and still useful textbooks.
The main journals for computer vision are IEEE Transactions on Pattern Analysis and
Machine Intelligence and International Journal of Computer Vision. Computer vision conferences include ICCV (International Conference on Computer Vision), CVPR (Computer
Vision and Pattern Recognition), and ECCV (European Conference on Computer Vision).
Research with a machine learning component is also published in the NIPS (Neural Information Processing Systems) conference, and work on the interface with computer graphics often
appears at the ACM SIGGRAPH (Special Interest Group in Graphics) conference.

Exercises

969

E XERCISES
24.1 In the shadow of a tree with a dense, leafy canopy, one sees a number of light spots.
Surprisingly, they all appear to be circular. Why? After all, the gaps between the leaves
through which the sun shines are not likely to be circular.
24.2 Consider a picture of a white sphere floating in front of a black backdrop. The image curve separating white pixels from black pixels is sometimes called the â€œoutlineâ€ of the
sphere. Show that the outline of a sphere, viewed in a perspective camera, can be an ellipse.
Why do spheres not look like ellipses to you?
24.3 Consider an infinitely long cylinder of radius r oriented with its axis along the y-axis.
The cylinder has a Lambertian surface and is viewed by a camera along the positive z-axis.
What will you expect to see in the image if the cylinder is illuminated by a point source
at infinity located on the positive x-axis? Draw the contours of constant brightness in the
projected image. Are the contours of equal brightness uniformly spaced?
24.4 Edges in an image can correspond to a variety of events in a scene. Consider Figure 24.4 (page 933), and assume that it is a picture of a real three-dimensional scene. Identify
ten different brightness edges in the image, and for each, state whether it corresponds to a
discontinuity in (a) depth, (b) surface orientation, (c) reflectance, or (d) illumination.
24.5 A stereoscopic system is being contemplated for terrain mapping. It will consist of two
CCD cameras, each having 512 Ã— 512 pixels on a 10 cm Ã— 10 cm square sensor. The lenses
to be used have a focal length of 16 cm, with the focus fixed at infinity. For corresponding
points (u1 , v1 ) in the left image and (u2 , v2 ) in the right image, v1 = v2 because the x-axes
in the two image planes are parallel to the epipolar linesâ€”the lines from the object to the
camera. The optical axes of the two cameras are parallel. The baseline between the cameras
is 1 meter.
a. If the nearest distance to be measured is 16 meters, what is the largest disparity that will
occur (in pixels)?
b. What is the distance resolution at 16 meters, due to the pixel spacing?
c. What distance corresponds to a disparity of one pixel?
24.6

Which of the following are true, and which are false?

a. Finding corresponding points in stereo images is the easiest phase of the stereo depthfinding process.
b. Shape-from-texture can be done by projecting a grid of light-stripes onto the scene.
c. Lines with equal lengths in the scene always project to equal lengths in the image.
d. Straight lines in the image necessarily correspond to straight lines in the scene.

970

Chapter

24.

Perception

D
A
X

Y

B
C
E

Figure 24.27
behind it.

Top view of a two-camera vision system observing a bottle with a wall

24.7 (Courtesy of Pietro Perona.) Figure 24.27 shows two cameras at X and Y observing a
scene. Draw the image seen at each camera, assuming that all named points are in the same
horizontal plane. What can be concluded from these two images about the relative distances
of points A, B, C, D, and E from the camera baseline, and on what basis?

25

ROBOTICS

In which agents are endowed with physical effectors with which to do mischief.

25.1

I NTRODUCTION

ROBOT
EFFECTOR

SENSOR

MANIPULATOR

MOBILE ROBOT

UGV
PLANETARY ROVER

UAV

Robots are physical agents that perform tasks by manipulating the physical world. To do so,
they are equipped with effectors such as legs, wheels, joints, and grippers. Effectors have
a single purpose: to assert physical forces on the environment.1 Robots are also equipped
with sensors, which allow them to perceive their environment. Present day robotics employs a diverse set of sensors, including cameras and lasers to measure the environment, and
gyroscopes and accelerometers to measure the robotâ€™s own motion.
Most of todayâ€™s robots fall into one of three primary categories. Manipulators, or robot
arms (Figure 25.1(a)), are physically anchored to their workplace, for example in a factory
assembly line or on the International Space Station. Manipulator motion usually involves
a chain of controllable joints, enabling such robots to place their effectors in any position
within the workplace. Manipulators are by far the most common type of industrial robots,
with approximately one million units installed worldwide. Some mobile manipulators are
used in hospitals to assist surgeons. Few car manufacturers could survive without robotic
manipulators, and some manipulators have even been used to generate original artwork.
The second category is the mobile robot. Mobile robots move about their environment
using wheels, legs, or similar mechanisms. They have been put to use delivering food in
hospitals, moving containers at loading docks, and similar tasks. Unmanned ground vehicles, or UGVs, drive autonomously on streets, highways, and off-road. The planetary rover
shown in Figure 25.2(b) explored Mars for a period of 3 months in 1997. Subsequent NASA
robots include the twin Mars Exploration Rovers (one is depicted on the cover of this book),
which landed in 2003 and were still operating six years later. Other types of mobile robots
include unmanned air vehicles (UAVs), commonly used for surveillance, crop-spraying, and
1

In Chapter 2 we talked about actuators, not effectors. Here we distinguish the effector (the physical device)
from the actuator (the control line that communicates a command to the effector).

971

972

Chapter

(a)

25.

Robotics

(b)

Figure 25.1 (a) An industrial robotic manipulator for stacking bags on a pallet. Image
courtesy of Nachi Robotic Systems. (b) Hondaâ€™s P3 and Asimo humanoid robots.

(a)

(b)

Figure 25.2 (a) Predator, an unmanned aerial vehicle (UAV) used by the U.S. Military.
Image courtesy of General Atomics Aeronautical Systems. (b) NASAâ€™s Sojourner, a mobile
robot that explored the surface of Mars in July 1997.

AUV

MOBILE
MANIPULATOR
HUMANOID ROBOT

military operations. Figure 25.2(a) shows a UAV commonly used by the U.S. military. Autonomous underwater vehicles (AUVs) are used in deep sea exploration. Mobile robots
deliver packages in the workplace and vacuum the floors at home.
The third type of robot combines mobility with manipulation, and is often called a
mobile manipulator. Humanoid robots mimic the human torso. Figure 25.1(b) shows two
early humanoid robots, both manufactured by Honda Corp. in Japan. Mobile manipulators

Section 25.2.

Robot Hardware

973

can apply their effectors further afield than anchored manipulators can, but their task is made
harder because they donâ€™t have the rigidity that the anchor provides.
The field of robotics also includes prosthetic devices (artificial limbs, ears, and eyes
for humans), intelligent environments (such as an entire house that is equipped with sensors
and effectors), and multibody systems, wherein robotic action is achieved through swarms of
small cooperating robots.
Real robots must cope with environments that are partially observable, stochastic, dynamic, and continuous. Many robot environments are sequential and multiagent as well.
Partial observability and stochasticity are the result of dealing with a large, complex world.
Robot cameras cannot see around corners, and motion commands are subject to uncertainty
due to gears slipping, friction, etc. Also, the real world stubbornly refuses to operate faster
than real time. In a simulated environment, it is possible to use simple algorithms (such as the
Q-learning algorithm described in Chapter 21) to learn in a few CPU hours from millions of
trials. In a real environment, it might take years to run these trials. Furthermore, real crashes
really hurt, unlike simulated ones. Practical robotic systems need to embody prior knowledge
about the robot, its physical environment, and the tasks that the robot will perform so that the
robot can learn quickly and perform safely.
Robotics brings together many of the concepts we have seen earlier in the book, including probabilistic state estimation, perception, planning, unsupervised learning, and reinforcement learning. For some of these concepts robotics serves as a challenging example
application. For other concepts this chapter breaks new ground in introducing the continuous
version of techniques that we previously saw only in the discrete case.

25.2

ROBOT H ARDWARE
So far in this book, we have taken the agent architectureâ€”sensors, effectors, and processorsâ€”
as given, and we have concentrated on the agent program. The success of real robots depends
at least as much on the design of sensors and effectors that are appropriate for the task.

25.2.1 Sensors
PASSIVE SENSOR

ACTIVE SENSOR

RANGE FINDER
SONAR SENSORS

Sensors are the perceptual interface between robot and environment. Passive sensors, such
as cameras, are true observers of the environment: they capture signals that are generated by
other sources in the environment. Active sensors, such as sonar, send energy into the environment. They rely on the fact that this energy is reflected back to the sensor. Active sensors
tend to provide more information than passive sensors, but at the expense of increased power
consumption and with a danger of interference when multiple active sensors are used at the
same time. Whether active or passive, sensors can be divided into three types, depending on
whether they sense the environment, the robotâ€™s location, or the robotâ€™s internal configuration.
Range finders are sensors that measure the distance to nearby objects. In the early
days of robotics, robots were commonly equipped with sonar sensors. Sonar sensors emit
directional sound waves, which are reflected by objects, with some of the sound making it

974

Chapter

(a)

25.

Robotics

(b)

Figure 25.3 (a) Time of flight camera; image courtesy of Mesa Imaging GmbH. (b) 3D
range image obtained with this camera. The range image makes it possible to detect obstacles
and objects in a robotâ€™s vicinity.

STEREO VISION

TIME OF FLIGHT
CAMERA

SCANNING LIDARS

TACTILE SENSORS

LOCATION SENSORS
GLOBAL
POSITIONING
SYSTEM

back into the sensor. The time and intensity of the returning signal indicates the distance
to nearby objects. Sonar is the technology of choice for autonomous underwater vehicles.
Stereo vision (see Section 24.4.2) relies on multiple cameras to image the environment from
slightly different viewpoints, analyzing the resulting parallax in these images to compute the
range of surrounding objects. For mobile ground robots, sonar and stereo vision are now
rarely used, because they are not reliably accurate.
Most ground robots are now equipped with optical range finders. Just like sonar sensors,
optical range sensors emit active signals (light) and measure the time until a reflection of this
signal arrives back at the sensor. Figure 25.3(a) shows a time of flight camera. This camera
acquires range images like the one shown in Figure 25.3(b) at up to 60 frames per second.
Other range sensors use laser beams and special 1-pixel cameras that can be directed using
complex arrangements of mirrors or rotating elements. These sensors are called scanning
lidars (short for light detection and ranging). Scanning lidars tend to provide longer ranges
than time of flight cameras, and tend to perform better in bright daylight.
Other common range sensors include radar, which is often the sensor of choice for
UAVs. Radar sensors can measure distances of multiple kilometers. On the other extreme
end of range sensing are tactile sensors such as whiskers, bump panels, and touch-sensitive
skin. These sensors measure range based on physical contact, and can be deployed only for
sensing objects very close to the robot.
A second important class of sensors is location sensors. Most location sensors use
range sensing as a primary component to determine location. Outdoors, the Global Positioning System (GPS) is the most common solution to the localization problem. GPS measures
the distance to satellites that emit pulsed signals. At present, there are 31 satellites in orbit,
transmitting signals on multiple frequencies. GPS receivers can recover the distance to these
satellites by analyzing phase shifts. By triangulating signals from multiple satellites, GPS

Section 25.2.

DIFFERENTIAL GPS

PROPRIOCEPTIVE
SENSOR

SHAFT DECODER

ODOMETRY

INERTIAL SENSOR

FORCE SENSOR
TORQUE SENSOR

Robot Hardware

975

receivers can determine their absolute location on Earth to within a few meters. Differential
GPS involves a second ground receiver with known location, providing millimeter accuracy
under ideal conditions. Unfortunately, GPS does not work indoors or underwater. Indoors,
localization is often achieved by attaching beacons in the environment at known locations.
Many indoor environments are full of wireless base stations, which can help robots localize
through the analysis of the wireless signal. Underwater, active sonar beacons can provide a
sense of location, using sound to inform AUVs of their relative distances to those beacons.
The third important class is proprioceptive sensors, which inform the robot of its own
motion. To measure the exact configuration of a robotic joint, motors are often equipped
with shaft decoders that count the revolution of motors in small increments. On robot arms,
shaft decoders can provide accurate information over any period of time. On mobile robots,
shaft decoders that report wheel revolutions can be used for odometryâ€”the measurement of
distance traveled. Unfortunately, wheels tend to drift and slip, so odometry is accurate only
over short distances. External forces, such as the current for AUVs and the wind for UAVs,
increase positional uncertainty. Inertial sensors, such as gyroscopes, rely on the resistance
of mass to the change of velocity. They can help reduce uncertainty.
Other important aspects of robot state are measured by force sensors and torque sensors. These are indispensable when robots handle fragile objects or objects whose exact shape
and location is unknown. Imagine a one-ton robotic manipulator screwing in a light bulb. It
would be all too easy to apply too much force and break the bulb. Force sensors allow the
robot to sense how hard it is gripping the bulb, and torque sensors allow it to sense how hard
it is turning. Good sensors can measure forces in all three translational and three rotational
directions. They do this at a frequency of several hundred times a second, so that a robot can
quickly detect unexpected forces and correct its actions before it breaks a light bulb.

25.2.2 Effectors

DEGREE OF
FREEDOM

KINEMATIC STATE
POSE
DYNAMIC STATE

Effectors are the means by which robots move and change the shape of their bodies. To
understand the design of effectors, it will help to talk about motion and shape in the abstract,
using the concept of a degree of freedom (DOF) We count one degree of freedom for each
independent direction in which a robot, or one of its effectors, can move. For example, a rigid
mobile robot such as an AUV has six degrees of freedom, three for its (x, y, z) location in
space and three for its angular orientation, known as yaw, roll, and pitch. These six degrees
define the kinematic state2 or pose of the robot. The dynamic state of a robot includes these
six plus an additional six dimensions for the rate of change of each kinematic dimension, that
is, their velocities.
For nonrigid bodies, there are additional degrees of freedom within the robot itself. For
example, the elbow of a human arm possesses two degree of freedom. It can flex the upper
arm towards or away, and can rotate right or left. The wrist has three degrees of freedom. It
can move up and down, side to side, and can also rotate. Robot joints also have one, two,
or three degrees of freedom each. Six degrees of freedom are required to place an object,
such as a hand, at a particular point in a particular orientation. The arm in Figure 25.4(a)
2

â€œKinematicâ€ is from the Greek word for motion, as is â€œcinema.â€

976
REVOLUTE JOINT
PRISMATIC JOINT

Chapter

25.

Robotics

has exactly six degrees of freedom, created by five revolute joints that generate rotational
motion and one prismatic joint that generates sliding motion. You can verify that the human
arm as a whole has more than six degrees of freedom by a simple experiment: put your hand
on the table and notice that you still have the freedom to rotate your elbow without changing
the configuration of your hand. Manipulators that have extra degrees of freedom are easier to
control than robots with only the minimum number of DOFs. Many industrial manipulators
therefore have seven DOFs, not six.
P

R
R

R
R

Î¸

R
(x, y)

(a)

(b)

Figure 25.4 (a) The Stanford Manipulator, an early robot arm with five revolute joints (R)
and one prismatic joint (P), for a total of six degrees of freedom. (b) Motion of a nonholonomic four-wheeled vehicle with front-wheel steering.

EFFECTIVE DOF
CONTROLLABLE DOF
NONHOLONOMIC

DIFFERENTIAL DRIVE

SYNCHRO DRIVE

For mobile robots, the DOFs are not necessarily the same as the number of actuated elements. Consider, for example, your average car: it can move forward or backward, and it can
turn, giving it two DOFs. In contrast, a carâ€™s kinematic configuration is three-dimensional:
on an open flat surface, one can easily maneuver a car to any (x, y) point, in any orientation.
(See Figure 25.4(b).) Thus, the car has three effective degrees of freedom but two controllable degrees of freedom. We say a robot is nonholonomic if it has more effective DOFs
than controllable DOFs and holonomic if the two numbers are the same. Holonomic robots
are easier to controlâ€”it would be much easier to park a car that could move sideways as well
as forward and backwardâ€”but holonomic robots are also mechanically more complex. Most
robot arms are holonomic, and most mobile robots are nonholonomic.
Mobile robots have a range of mechanisms for locomotion, including wheels, tracks,
and legs. Differential drive robots possess two independently actuated wheels (or tracks),
one on each side, as on a military tank. If both wheels move at the same velocity, the robot
moves on a straight line. If they move in opposite directions, the robot turns on the spot. An
alternative is the synchro drive, in which each wheel can move and turn around its own axis.
To avoid chaos, the wheels are tightly coordinated. When moving straight, for example, all
wheels point in the same direction and move at the same speed. Both differential and synchro
drives are nonholonomic. Some more expensive robots use holonomic drives, which have
three or more wheels that can be oriented and moved independently.
Some mobile robots possess arms. Figure 25.5(a) displays a two-armed robot. This
robotâ€™s arms use springs to compensate for gravity, and they provide minimal resistance to

Section 25.2.

Robot Hardware

977

(a)

(b)

Figure 25.5 (a) Mobile manipulator plugging its charge cable into a wall outlet. Image
c 2009. (b) One of Marc Raibertâ€™s legged robots in motion.
courtesy of Willow Garage, 

DYNAMICALLY
STABLE
STATICALLY STABLE

ELECTRIC MOTOR
PNEUMATIC
ACTUATION
HYDRAULIC
ACTUATION

external forces. Such a design minimizes the physical danger to people who might stumble
into such a robot. This is a key consideration in deploying robots in domestic environments.
Legs, unlike wheels, can handle rough terrain. However, legs are notoriously slow on
flat surfaces, and they are mechanically difficult to build. Robotics researchers have tried designs ranging from one leg up to dozens of legs. Legged robots have been made to walk, run,
and even hopâ€”as we see with the legged robot in Figure 25.5(b). This robot is dynamically
stable, meaning that it can remain upright while hopping around. A robot that can remain
upright without moving its legs is called statically stable. A robot is statically stable if its
center of gravity is above the polygon spanned by its legs. The quadruped (four-legged) robot
shown in Figure 25.6(a) may appear statically stable. However, it walks by lifting multiple
legs at the same time, which renders it dynamically stable. The robot can walk on snow and
ice, and it will not fall over even if you kick it (as demonstrated in videos available online).
Two-legged robots such as those in Figure 25.6(b) are dynamically stable.
Other methods of movement are possible: air vehicles use propellers or turbines; underwater vehicles use propellers or thrusters, similar to those used on submarines. Robotic
blimps rely on thermal effects to keep themselves aloft.
Sensors and effectors alone do not make a robot. A complete robot also needs a source
of power to drive its effectors. The electric motor is the most popular mechanism for both
manipulator actuation and locomotion, but pneumatic actuation using compressed gas and
hydraulic actuation using pressurized fluids also have their application niches.

978

Chapter

(a)

25.

Robotics

(b)

Figure 25.6 (a) Four-legged dynamically-stable robot â€œBig Dog.â€ Image courtesy Boston
c 2009. (b) 2009 RoboCup Standard Platform League competition, showing the
Dynamics, 
winning team, B-Human, from the DFKI center at the University of Bremen. Throughout the
match, B-Human outscored their opponents 64:1. Their success was built on probabilistic
state estimation using particle filters and Kalman filters; on machine-learning models for gait
c 2009.
optimization; and on dynamic kicking moves. Image courtesy DFKI, 

25.3

ROBOTIC P ERCEPTION
Perception is the process by which robots map sensor measurements into internal representations of the environment. Perception is difficult because sensors are noisy, and the environment is partially observable, unpredictable, and often dynamic. In other words, robots have
all the problems of state estimation (or filtering) that we discussed in Section 15.2. As a
rule of thumb, good internal representations for robots have three properties: they contain
enough information for the robot to make good decisions, they are structured so that they can
be updated efficiently, and they are natural in the sense that internal variables correspond to
natural state variables in the physical world.
In Chapter 15, we saw that Kalman filters, HMMs, and dynamic Bayes nets can represent the transition and sensor models of a partially observable environment, and we described
both exact and approximate algorithms for updating the belief stateâ€”the posterior probability distribution over the environment state variables. Several dynamic Bayes net models for
this process were shown in Chapter 15. For robotics problems, we include the robotâ€™s own
past actions as observed variables in the model. Figure 25.7 shows the notation used in this
chapter: Xt is the state of the environment (including the robot) at time t, Zt is the observation
received at time t, and At is the action taken after the observation is received.

Section 25.3.

Robotic Perception

979

Atâˆ’2

Atâˆ’1

At

Xtâˆ’1

Xt

Xt+1

Ztâˆ’1

Zt

Zt+1

Figure 25.7 Robot perception can be viewed as temporal inference from sequences of
actions and measurements, as illustrated by this dynamic Bayes network.

We would like to compute the new belief state, P(Xt+1 | z1:t+1 , a1:t ), from the current
belief state P(Xt | z1:t , a1:tâˆ’1 ) and the new observation zt+1 . We did this in Section 15.2,
but here there are two differences: we condition explicitly on the actions as well as the observations, and we deal with continuous rather than discrete variables. Thus, we modify the
recursive filtering equation (15.5 on page 572) to use integration rather than summation:
P(Xt+1 | z1:t+1 , a1:t )
= Î±P(zt+1 | Xt+1 )

MOTION MODEL


P(Xt+1 | xt , at ) P (xt | z1:t , a1:tâˆ’1 ) dxt .

(25.1)

This equation states that the posterior over the state variables X at time t + 1 is calculated
recursively from the corresponding estimate one time step earlier. This calculation involves
the previous action at and the current sensor measurement zt+1 . For example, if our goal
is to develop a soccer-playing robot, Xt+1 might be the location of the soccer ball relative
to the robot. The posterior P(Xt | z1:t , a1:tâˆ’1 ) is a probability distribution over all states that
captures what we know from past sensor measurements and controls. Equation (25.1) tells us
how to recursively estimate this location, by incrementally folding in sensor measurements
(e.g., camera images) and robot motion commands. The probability P(Xt+1 | xt , at ) is called
the transition model or motion model, and P(zt+1 | Xt+1 ) is the sensor model.

25.3.1 Localization and mapping
LOCALIZATION

Localization is the problem of finding out where things areâ€”including the robot itself.
Knowledge about where things are is at the core of any successful physical interaction with
the environment. For example, robot manipulators must know the location of objects they
seek to manipulate; navigating robots must know where they are to find their way around.
To keep things simple, let us consider a mobile robot that moves slowly in a flat 2D
world. Let us also assume the robot is given an exact map of the environment. (An example
of such a map appears in Figure 25.10.) The pose of such a mobile robot is defined by its
two Cartesian coordinates with values x and y and its heading with value Î¸, as illustrated in
Figure 25.8(a). If we arrange those three values in a vector, then any particular state is given
by Xt = (xt , yt , Î¸t ) . So far so good.

980

Chapter

25.

Robotics

Ï‰ t Î”t

xi, yi

Î¸t+1

vt Î”t

h(xt)

xt+1

Z1

Z2

Z3

Z4

Î¸t

xt
(a)

(b)

Figure 25.8 (a) A simplified kinematic model of a mobile robot. The robot is shown as a
circle with an interior line marking the forward direction. The state xt consists of the (xt , yt )
position (shown implicitly) and the orientation Î¸t . The new state xt+1 is obtained by an
update in position of vt Î”t and in orientation of Ï‰t Î”t . Also shown is a landmark at (xi , yi )
observed at time t. (b) The range-scan sensor model. Two possible robot poses are shown for
a given range scan (z1 , z2 , z3 , z4 ). It is much more likely that the pose on the left generated
the range scan than the pose on the right.

In the kinematic approximation, each action consists of the â€œinstantaneousâ€ specification of two velocitiesâ€”a translational velocity vt and a rotational velocity Ï‰t . For small time
intervals Î”t, a crude deterministic model of the motion of such robots is given by
âŽž
âŽ›
vt Î”t cos Î¸t
XÌ‚t+1 = f (Xt , vt , Ï‰t ) = Xt + âŽ vt Î”t sin Î¸t âŽ  .
  
Ï‰t Î”t
at
The notation XÌ‚ refers to a deterministic state prediction. Of course, physical robots are
somewhat unpredictable. This is commonly modeled by a Gaussian distribution with mean
f (Xt , vt , Ï‰t ) and covariance Î£x . (See Appendix A for a mathematical definition.)
P(Xt+1 | Xt , vt , Ï‰t ) = N (XÌ‚t+1 , Î£x ) .

LANDMARK

This probability distribution is the robotâ€™s motion model. It models the effects of the motion
at on the location of the robot.
Next, we need a sensor model. We will consider two kinds of sensor model. The
first assumes that the sensors detect stable, recognizable features of the environment called
landmarks. For each landmark, the range and bearing are reported. Suppose the robotâ€™s state
is xt = (xt , yt , Î¸t ) and it senses a landmark whose location is known to be (xi , yi ) . Without
noise, the range and bearing can be calculated by simple geometry. (See Figure 25.8(a).) The
exact prediction of the observed range and bearing would be



(xt âˆ’ xi )2 + (yt âˆ’ yi )2
.
zÌ‚t = h(xt ) =
t
arctan xyii âˆ’y
âˆ’xt âˆ’ Î¸t

Section 25.3.

Robotic Perception

981

Again, noise distorts our measurements. To keep things simple, one might assume Gaussian
noise with covariance Î£z , giving us the sensor model
P (zt | xt ) = N (zÌ‚t , Î£z ) .
A somewhat different sensor model is used for an array of range sensors, each of which
has a fixed bearing relative to the robot. Such sensors produce a vector of range values
zt = (z1 , . . . , zM ) . Given a pose xt , let zÌ‚j be the exact range along the jth beam direction
from xt to the nearest obstacle. As before, this will be corrupted by Gaussian noise. Typically,
we assume that the errors for the different beam directions are independent and identically
distributed, so we have
P (zt | xt ) = Î±

M


2

eâˆ’(zj âˆ’zÌ‚j )/2Ïƒ .

j =1

MONTE CARLO
LOCALIZATION

LINEARIZATION

Figure 25.8(b) shows an example of a four-beam range scan and two possible robot poses,
one of which is reasonably likely to have produced the observed scan and one of which is not.
Comparing the range-scan model to the landmark model, we see that the range-scan model
has the advantage that there is no need to identify a landmark before the range scan can be
interpreted; indeed, in Figure 25.8(b), the robot faces a featureless wall. On the other hand,
if there are visible, identifiable landmarks, they may provide instant localization.
Chapter 15 described the Kalman filter, which represents the belief state as a single
multivariate Gaussian, and the particle filter, which represents the belief state by a collection
of particles that correspond to states. Most modern localization algorithms use one of two
representations of the robotâ€™s belief P(Xt | z1:t , a1:tâˆ’1 ).
Localization using particle filtering is called Monte Carlo localization, or MCL. The
MCL alfgorithm is an instance of the particle-filtering algorithm of Figure 15.17 (page 598).
All we need to do is supply the appropriate motion model and sensor model. Figure 25.9
shows one version using the range-scan model. The operation of the algorithm is illustrated in
Figure 25.10 as the robot finds out where it is inside an office building. In the first image, the
particles are uniformly distributed based on the prior, indicating global uncertainty about the
robotâ€™s position. In the second image, the first set of measurements arrives and the particles
form clusters in the areas of high posterior belief. In the third, enough measurements are
available to push all the particles to a single location.
The Kalman filter is the other major way to localize. A Kalman filter represents the
posterior P(Xt | z1:t , a1:tâˆ’1 ) by a Gaussian. The mean of this Gaussian will be denoted Î¼t and
its covariance Î£t . The main problem with Gaussian beliefs is that they are only closed under
linear motion models f and linear measurement models h. For nonlinear f or h, the result of
updating a filter is in general not Gaussian. Thus, localization algorithms using the Kalman
filter linearize the motion and sensor models. Linearization is a local approximation of a
nonlinear function by a linear function. Figure 25.11 illustrates the concept of linearization
for a (one-dimensional) robot motion model. On the left, it depicts a nonlinear motion model
f (xt , at ) (the control at is omitted in this graph since it plays no role in the linearization).
On the right, this function is approximated by a linear function fËœ(xt , at ). This linear function
is tangent to f at the point Î¼t , the mean of our state estimate at time t. Such a linearization

982

Chapter

25.

Robotics

function M ONTE -C ARLO -L OCALIZATION(a, z , N , P (X  |X, v, Ï‰), P (z|z âˆ— ), m) returns
a set of samples for the next time step
inputs: a, robot velocities v and Ï‰
z, range scan z1 , . . . , zM
P (X  |X, v, Ï‰), motion model
P (z|z âˆ— ), range sensor noise model
m, 2D map of the environment
persistent: S, a vector of samples of size N
local variables: W , a vector of weights of size N
S  , a temporary vector of particles of size N
W  , a vector of weights of size N
if S is empty then
/* initialization phase */
for i = 1 to N do
S[i] â† sample from P (X0 )
for i = 1 to N do /* update cycle */
S  [i] â† sample from P (X  |X = S[i], v, Ï‰)
W  [i] â† 1
for j = 1 to M do
z âˆ— â† R AY C AST(j, X = S  [i], m)
W  [i] â† W  [i] Â· P (zj | z âˆ— )
S â† W EIGHTED -S AMPLE -W ITH -R EPLACEMENT(N ,S  ,W  )
return S
Figure 25.9 A Monte Carlo localization algorithm using a range-scan sensor model with
independent noise.

TAYLOR EXPANSION

SIMULTANEOUS
LOCALIZATION AND
MAPPING

is called (first degree) Taylor expansion. A Kalman filter that linearizes f and h via Taylor
expansion is called an extended Kalman filter (or EKF). Figure 25.12 shows a sequence
of estimates of a robot running an extended Kalman filter localization algorithm. As the
robot moves, the uncertainty in its location estimate increases, as shown by the error ellipses.
Its error decreases as it senses the range and bearing to a landmark with known location
and increases again as the robot loses sight of the landmark. EKF algorithms work well if
landmarks are easily identified. Otherwise, the posterior distribution may be multimodal, as
in Figure 25.10(b). The problem of needing to know the identity of landmarks is an instance
of the data association problem discussed in Figure 15.6.
In some situations, no map of the environment is available. Then the robot will have to
acquire a map. This is a bit of a chicken-and-egg problem: the navigating robot will have to
determine its location relative to a map it doesnâ€™t quite know, at the same time building this
map while it doesnâ€™t quite know its actual location. This problem is important for many robot
applications, and it has been studied extensively under the name simultaneous localization
and mapping, abbreviated as SLAM.
SLAM problems are solved using many different probabilistic techniques, including
the extended Kalman filter discussed above. Using the EKF is straightforward: just augment

Section 25.3.

Robotic Perception

983

Robot position

(a)

Robot position

(b)

Robot position

(c)

Figure 25.10 Monte Carlo localization, a particle filtering algorithm for mobile robot localization. (a) Initial, global uncertainty. (b) Approximately bimodal uncertainty after navigating in the (symmetric) corridor. (c) Unimodal uncertainty after entering a room and finding
it to be distinctive.

984

Chapter

25.

Robotics

~

Xt+1

Xt+1

f(Xt, at)

f(Xt, at) = f(Î¼t, at) + Ft(Xt âˆ’ Î¼t)
f(Xt, at)

~

Î£t+1 f(Î¼t, at)

Î£t+1 f( Î¼ , a )
t
t

Î¼t
Î£t

Î£t+1

Î¼t
Î£t

Xt

(a)

Xt

(b)

Figure 25.11 One-dimensional illustration of a linearized motion model: (a) The function
f , and the projection of a mean Î¼t and a covariance interval (based on Î£t ) into time t + 1.
(b) The linearized version is the tangent of f at Î¼t . The projection of the mean Î¼t is correct.
However, the projected covariance Î£Ìƒt+1 differs from Î£t+1 .

robot

landmark

Figure 25.12 Example of localization using the extended Kalman filter. The robot moves
on a straight line. As it progresses, its uncertainty increases gradually, as illustrated by the
error ellipses. When it observes a landmark with known position, the uncertainty is reduced.

the state vector to include the locations of the landmarks in the environment. Luckily, the
EKF update scales quadratically, so for small maps (e.g., a few hundred landmarks) the computation is quite feasible. Richer maps are often obtained using graph relaxation methods,
similar to the Bayesian network inference techniques discussed in Chapter 14. Expectationâ€“
maximization is also used for SLAM.

25.3.2 Other types of perception
Not all of robot perception is about localization or mapping. Robots also perceive the temperature, odors, acoustic signals, and so on. Many of these quantities can be estimated using
variants of dynamic Bayes networks. All that is required for such estimators are conditional
probability distributions that characterize the evolution of state variables over time, and sensor models that describe the relation of measurements to state variables.
It is also possible to program a robot as a reactive agent, without explicitly reasoning
about probability distributions over states. We cover that approach in Section 25.6.3.
The trend in robotics is clearly towards representations with well-defined semantics.

Section 25.3.

Robotic Perception

(a)

985

(b)

(c)

Figure 25.13 Sequence of â€œdrivable surfaceâ€ classifier results using adaptive vision. In
(a) only the road is classified as drivable (striped area). The V-shaped dark line shows where
the vehicle is heading. In (b) the vehicle is commanded to drive off the road, onto a grassy
surface, and the classifier is beginning to classify some of the grass as drivable. In (c) the
vehicle has updated its model of drivable surface to correspond to grass as well as road.

Probabilistic techniques outperform other approaches in many hard perceptual problems such
as localization and mapping. However, statistical techniques are sometimes too cumbersome,
and simpler solutions may be just as effective in practice. To help decide which approach to
take, experience working with real physical robots is your best teacher.

25.3.3 Machine learning in robot perception

LOW-DIMENSIONAL
EMBEDDING

Machine learning plays an important role in robot perception. This is particularly the case
when the best internal representation is not known. One common approach is to map highdimensional sensor streams into lower-dimensional spaces using unsupervised machine learning methods (see Chapter 18). Such an approach is called low-dimensional embedding.
Machine learning makes it possible to learn sensor and motion models from data, while simultaneously discovering a suitable internal representations.
Another machine learning technique enables robots to continuously adapt to broad
changes in sensor measurements. Picture yourself walking from a sun-lit space into a dark
neon-lit room. Clearly things are darker inside. But the change of light source also affects all
the colors: Neon light has a stronger component of green light than sunlight. Yet somehow
we seem not to notice the change. If we walk together with people into a neon-lit room, we
donâ€™t think that suddenly their faces turned green. Our perception quickly adapts to the new
lighting conditions, and our brain ignores the differences.
Adaptive perception techniques enable robots to adjust to such changes. One example
is shown in Figure 25.13, taken from the autonomous driving domain. Here an unmanned
ground vehicle adapts its classifier of the concept â€œdrivable surface.â€ How does this work?
The robot uses a laser to provide classification for a small area right in front of the robot.
When this area is found to be flat in the laser range scan, it is used as a positive training
example for the concept â€œdrivable surface.â€ A mixture-of-Gaussians technique similar to the
EM algorithm discussed in Chapter 20 is then trained to recognize the specific color and
texture coefficients of the small sample patch. The images in Figure 25.13 are the result of
applying this classifier to the full image.

986

Chapter

SELF-SUPERVISED
LEARNING

25.4

25.

Robotics

Methods that make robots collect their own training data (with labels!) are called selfsupervised. In this instance, the robot uses machine learning to leverage a short-range sensor
that works well for terrain classification into a sensor that can see much farther. That allows
the robot to drive faster, slowing down only when the sensor model says there is a change in
the terrain that needs to be examined more carefully by the short-range sensors.

P LANNING TO M OVE

POINT-TO-POINT
MOTION
COMPLIANT MOTION

PATH PLANNING

All of a robotâ€™s deliberations ultimately come down to deciding how to move effectors. The
point-to-point motion problem is to deliver the robot or its end effector to a designated target
location. A greater challenge is the compliant motion problem, in which a robot moves
while being in physical contact with an obstacle. An example of compliant motion is a robot
manipulator that screws in a light bulb, or a robot that pushes a box across a table top.
We begin by finding a suitable representation in which motion-planning problems can
be described and solved. It turns out that the configuration spaceâ€”the space of robot states
defined by location, orientation, and joint anglesâ€”is a better place to work than the original
3D space. The path planning problem is to find a path from one configuration to another in
configuration space. We have already encountered various versions of the path-planning problem throughout this book; the complication added by robotics is that path planning involves
continuous spaces. There are two main approaches: cell decomposition and skeletonization.
Each reduces the continuous path-planning problem to a discrete graph-search problem. In
this section, we assume that motion is deterministic and that localization of the robot is exact.
Subsequent sections will relax these assumptions.

25.4.1 Configuration space

WORKSPACE
REPRESENTATION

LINKAGE
CONSTRAINTS

We will start with a simple representation for a simple robot motion problem. Consider the
robot arm shown in Figure 25.14(a). It has two joints that move independently. Moving
the joints alters the (x, y) coordinates of the elbow and the gripper. (The arm cannot move
in the z direction.) This suggests that the robotâ€™s configuration can be described by a fourdimensional coordinate: (xe , ye ) for the location of the elbow relative to the environment and
(xg , yg ) for the location of the gripper. Clearly, these four coordinates characterize the full
state of the robot. They constitute what is known as workspace representation, since the
coordinates of the robot are specified in the same coordinate system as the objects it seeks to
manipulate (or to avoid). Workspace representations are well-suited for collision checking,
especially if the robot and all objects are represented by simple polygonal models.
The problem with the workspace representation is that not all workspace coordinates
are actually attainable, even in the absence of obstacles. This is because of the linkage constraints on the space of attainable workspace coordinates. For example, the elbow position
(xe , ye ) and the gripper position (xg , yg ) are always a fixed distance apart, because they are
joined by a rigid forearm. A robot motion planner defined over workspace coordinates faces
the challenge of generating paths that adhere to these constraints. This is particularly tricky

Section 25.4.

Planning to Move

987

table

eelb
table

sshou

vertical
obstacle

e

left wall

s

(a)

(b)

Figure 25.14 (a) Workspace representation of a robot arm with 2 DOFs. The workspace
is a box with a flat obstacle hanging from the ceiling. (b) Configuration space of the same
robot. Only white regions in the space are configurations that are free of collisions. The dot
in this diagram corresponds to the configuration of the robot shown on the left.

CONFIGURATION
SPACE

KINEMATICS

INVERSE
KINEMATICS

because the state space is continuous and the constraints are nonlinear. It turns out to be easier to plan with a configuration space representation. Instead of representing the state of the
robot by the Cartesian coordinates of its elements, we represent the state by a configuration
of the robotâ€™s joints. Our example robot possesses two joints. Hence, we can represent its
state with the two angles Ï•s and Ï•e for the shoulder joint and elbow joint, respectively. In
the absence of any obstacles, a robot could freely take on any value in configuration space. In
particular, when planning a path one could simply connect the present configuration and the
target configuration by a straight line. In following this path, the robot would then move its
joints at a constant velocity, until a target location is reached.
Unfortunately, configuration spaces have their own problems. The task of a robot is usually expressed in workspace coordinates, not in configuration space coordinates. This raises
the question of how to map between workspace coordinates and configuration space. Transforming configuration space coordinates into workspace coordinates is simple: it involves
a series of straightforward coordinate transformations. These transformations are linear for
prismatic joints and trigonometric for revolute joints. This chain of coordinate transformation
is known as kinematics.
The inverse problem of calculating the configuration of a robot whose effector location
is specified in workspace coordinates is known as inverse kinematics. Calculating the inverse
kinematics is hard, especially for robots with many DOFs. In particular, the solution is seldom
unique. Figure 25.14(a) shows one of two possible configurations that put the gripper in the
same location. (The other configuration would has the elbow below the shoulder.)

988

Chapter

25.

Robotics

conf-2
conf-1
conf-3

conf-3
conf-1

conf-2

e

s

(a)
Figure 25.15

FREE SPACE
OCCUPIED SPACE

(b)

Three robot configurations, shown in workspace and configuration space.

In general, this two-link robot arm has between zero and two inverse kinematic solutions for any set of workspace coordinates. Most industrial robots have sufficient degrees
of freedom to find infinitely many solutions to motion problems. To see how this is possible, simply imagine that we added a third revolute joint to our example robot, one whose
rotational axis is parallel to the ones of the existing joints. In such a case, we can keep the
location (but not the orientation!) of the gripper fixed and still freely rotate its internal joints,
for most configurations of the robot. With a few more joints (how many?) we can achieve the
same effect while keeping the orientation of the gripper constant as well. We have already
seen an example of this in the â€œexperimentâ€ of placing your hand on the desk and moving
your elbow. The kinematic constraint of your hand position is insufficient to determine the
configuration of your elbow. In other words, the inverse kinematics of your shoulderâ€“arm
assembly possesses an infinite number of solutions.
The second problem with configuration space representations arises from the obstacles that may exist in the robotâ€™s workspace. Our example in Figure 25.14(a) shows several
such obstacles, including a free-hanging obstacle that protrudes into the center of the robotâ€™s
workspace. In workspace, such obstacles take on simple geometric formsâ€”especially in
most robotics textbooks, which tend to focus on polygonal obstacles. But how do they look
in configuration space?
Figure 25.14(b) shows the configuration space for our example robot, under the specific
obstacle configuration shown in Figure 25.14(a). The configuration space can be decomposed
into two subspaces: the space of all configurations that a robot may attain, commonly called
free space, and the space of unattainable configurations, called occupied space. The white
area in Figure 25.14(b) corresponds to the free space. All other regions correspond to occu-

Section 25.4.

Planning to Move

989

pied space. The different shadings of the occupied space corresponds to the different objects
in the robotâ€™s workspace; the black region surrounding the entire free space corresponds to
configurations in which the robot collides with itself. It is easy to see that extreme values of
the shoulder or elbow angles cause such a violation. The two oval-shaped regions on both
sides of the robot correspond to the table on which the robot is mounted. The third oval region
corresponds to the left wall. Finally, the most interesting object in configuration space is the
vertical obstacle that hangs from the ceiling and impedes the robotâ€™s motions. This object has
a funny shape in configuration space: it is highly nonlinear and at places even concave. With
a little bit of imagination the reader will recognize the shape of the gripper at the upper left
end. We encourage the reader to pause for a moment and study this diagram. The shape of
this obstacle is not at all obvious! The dot inside Figure 25.14(b) marks the configuration of
the robot, as shown in Figure 25.14(a). Figure 25.15 depicts three additional configurations,
both in workspace and in configuration space. In configuration conf-1, the gripper encloses
the vertical obstacle.
Even if the robotâ€™s workspace is represented by flat polygons, the shape of the free space
can be very complicated. In practice, therefore, one usually probes a configuration space
instead of constructing it explicitly. A planner may generate a configuration and then test to
see if it is in free space by applying the robot kinematics and then checking for collisions in
workspace coordinates.

25.4.2 Cell decomposition methods
CELL
DECOMPOSITION

The first approach to path planning uses cell decompositionâ€”that is, it decomposes the
free space into a finite number of contiguous regions, called cells. These regions have the
important property that the path-planning problem within a single region can be solved by
simple means (e.g., moving along a straight line). The path-planning problem then becomes
a discrete graph-search problem, very much like the search problems introduced in Chapter 3.
The simplest cell decomposition consists of a regularly spaced grid. Figure 25.16(a)
shows a square grid decomposition of the space and a solution path that is optimal for this
grid size. Grayscale shading indicates the value of each free-space grid cellâ€”i.e., the cost of
the shortest path from that cell to the goal. (These values can be computed by a deterministic
form of the VALUE -I TERATION algorithm given in Figure 17.4 on page 653.) Figure 25.16(b)
shows the corresponding workspace trajectory for the arm. Of course, we can also use the Aâˆ—
algorithm to find a shortest path.
Such a decomposition has the advantage that it is extremely simple to implement, but
it also suffers from three limitations. First, it is workable only for low-dimensional configuration spaces, because the number of grid cells increases exponentially with d, the number of
dimensions. Sounds familiar? This is the curse!dimensionality@of dimensionality. Second,
there is the problem of what to do with cells that are â€œmixedâ€â€”that is, neither entirely within
free space nor entirely within occupied space. A solution path that includes such a cell may
not be a real solution, because there may be no way to cross the cell in the desired direction
in a straight line. This would make the path planner unsound. On the other hand, if we insist
that only completely free cells may be used, the planner will be incomplete, because it might

990

Chapter

25.

Robotics

goal
start

goal
start

(a)

(b)

Figure 25.16 (a) Value function and path found for a discrete grid cell approximation of
the configuration space. (b) The same path visualized in workspace coordinates. Notice how
the robot bends its elbow to avoid a collision with the vertical obstacle.

EXACT CELL
DECOMPOSITION

be the case that the only paths to the goal go through mixed cellsâ€”especially if the cell size
is comparable to that of the passageways and clearances in the space. And third, any path
through a discretized state space will not be smooth. It is generally difficult to guarantee that
a smooth solution exists near the discrete path. So a robot may not be able to execute the
solution found through this decomposition.
Cell decomposition methods can be improved in a number of ways, to alleviate some
of these problems. The first approach allows further subdivision of the mixed cellsâ€”perhaps
using cells of half the original size. This can be continued recursively until a path is found
that lies entirely within free cells. (Of course, the method only works if there is a way to
decide if a given cell is a mixed cell, which is easy only if the configuration space boundaries
have relatively simple mathematical descriptions.) This method is complete provided there is
a bound on the smallest passageway through which a solution must pass. Although it focuses
most of the computational effort on the tricky areas within the configuration space, it still
fails to scale well to high-dimensional problems because each recursive splitting of a cell
creates 2d smaller cells. A second way to obtain a complete algorithm is to insist on an exact
cell decomposition of the free space. This method must allow cells to be irregularly shaped
where they meet the boundaries of free space, but the shapes must still be â€œsimpleâ€ in the
sense that it should be easy to compute a traversal of any free cell. This technique requires
some quite advanced geometric ideas, so we shall not pursue it further here.
Examining the solution path shown in Figure 25.16(a), we can see an additional difficulty that will have to be resolved. The path contains arbitrarily sharp corners; a robot moving
at any finite speed could not execute such a path. This problem is solved by storing certain
continuous values for each grid cell. Consider an algorithm which stores, for each grid cell,

Section 25.4.

HYBRID A*

Planning to Move

991

the exact, continuous state that was attained with the cell was first expanded in the search.
Assume further, that when propagating information to nearby grid cells, we use this continuous state as a basis, and apply the continuous robot motion model for jumping to nearby cells.
In doing so, we can now guarantee that the resulting trajectory is smooth and can indeed be
executed by the robot. One algorithm that implements this is hybrid A*.

25.4.3 Modified cost functions

POTENTIAL FIELD

Notice that in Figure 25.16, the path goes very close to the obstacle. Anyone who has driven
a car knows that a parking space with one millimeter of clearance on either side is not really a
parking space at all; for the same reason, we would prefer solution paths that are robust with
respect to small motion errors.
This problem can be solved by introducing a potential field. A potential field is a
function defined over state space, whose value grows with the distance to the closest obstacle.
Figure 25.17(a) shows such a potential fieldâ€”the darker a configuration state, the closer it is
to an obstacle.
The potential field can be used as an additional cost term in the shortest-path calculation.
This induces an interesting tradeoff. On the one hand, the robot seeks to minimize path length
to the goal. On the other hand, it tries to stay away from obstacles by virtue of minimizing the
potential function. With the appropriate weight balancing the two objectives, a resulting path
may look like the one shown in Figure 25.17(b). This figure also displays the value function
derived from the combined cost function, again calculated by value iteration. Clearly, the
resulting path is longer, but it is also safer.
There exist many other ways to modify the cost function. For example, it may be
desirable to smooth the control parameters over time. For example, when driving a car, a
smooth path is better than a jerky one. In general, such higher-order constraints are not easy
to accommodate in the planning process, unless we make the most recent steering command
a part of the state. However, it is often easy to smooth the resulting trajectory after planning,
using conjugate gradient methods. Such post-planning smoothing is essential in many realworld applications.

25.4.4 Skeletonization methods
SKELETONIZATION

VORONOI GRAPH

The second major family of path-planning algorithms is based on the idea of skeletonization.
These algorithms reduce the robotâ€™s free space to a one-dimensional representation, for which
the planning problem is easier. This lower-dimensional representation is called a skeleton of
the configuration space.
Figure 25.18 shows an example skeletonization: it is a Voronoi graph of the free
spaceâ€”the set of all points that are equidistant to two or more obstacles. To do path planning with a Voronoi graph, the robot first changes its present configuration to a point on the
Voronoi graph. It is easy to show that this can always be achieved by a straight-line motion
in configuration space. Second, the robot follows the Voronoi graph until it reaches the point
nearest to the target configuration. Finally, the robot leaves the Voronoi graph and moves to
the target. Again, this final step involves straight-line motion in configuration space.

992

Chapter

start

(a)

25.

Robotics

goal

(b)

Figure 25.17 (a) A repelling potential field pushes the robot away from obstacles. (b)
Path found by simultaneously minimizing path length and the potential.

(a)

(b)

Figure 25.18 (a) The Voronoi graph is the set of points equidistant to two or more obstacles in configuration space. (b) A probabilistic roadmap, composed of 400 randomly chosen
points in free space.

In this way, the original path-planning problem is reduced to finding a path on the
Voronoi graph, which is generally one-dimensional (except in certain nongeneric cases) and
has finitely many points where three or more one-dimensional curves intersect. Thus, finding

Section 25.5.

993

the shortest path along the Voronoi graph is a discrete graph-search problem of the kind
discussed in Chapters 3 and 4. Following the Voronoi graph may not give us the shortest
path, but the resulting paths tend to maximize clearance. Disadvantages of Voronoi graph
techniques are that they are difficult to apply to higher-dimensional configuration spaces, and
that they tend to induce unnecessarily large detours when the configuration space is wide
open. Furthermore, computing the Voronoi graph can be difficult, especially in configuration
space, where the shapes of obstacles can be complex.
An alternative to the Voronoi graphs is the probabilistic roadmap, a skeletonization
approach that offers more possible routes, and thus deals better with wide-open spaces. Figure 25.18(b) shows an example of a probabilistic roadmap. The graph is created by randomly
generating a large number of configurations, and discarding those that do not fall into free
space. Two nodes are joined by an arc if it is â€œeasyâ€ to reach one node from the otherâ€“for
example, by a straight line in free space. The result of all this is a randomized graph in the
robotâ€™s free space. If we add the robotâ€™s start and goal configurations to this graph, path
planning amounts to a discrete graph search. Theoretically, this approach is incomplete, because a bad choice of random points may leave us without any paths from start to goal. It
is possible to bound the probability of failure in terms of the number of points generated
and certain geometric properties of the configuration space. It is also possible to direct the
generation of sample points towards the areas where a partial search suggests that a good
path may be found, working bidirectionally from both the start and the goal positions. With
these improvements, probabilistic roadmap planning tends to scale better to high-dimensional
configuration spaces than most alternative path-planning techniques.

PROBABILISTIC
ROADMAP

25.5

Planning Uncertain Movements

P LANNING U NCERTAIN M OVEMENTS

MOST LIKELY STATE

ONLINE REPLANNING

None of the robot motion-planning algorithms discussed thus far addresses a key characteristic of robotics problems: uncertainty. In robotics, uncertainty arises from partial observability
of the environment and from the stochastic (or unmodeled) effects of the robotâ€™s actions. Errors can also arise from the use of approximation algorithms such as particle filtering, which
does not provide the robot with an exact belief state even if the stochastic nature of the environment is modeled perfectly.
Most of todayâ€™s robots use deterministic algorithms for decision making, such as the
path-planning algorithms of the previous section. To do so, it is common practice to extract
the most likely state from the probability distribution produced by the state estimation algorithm. The advantage of this approach is purely computational. Planning paths through
configuration space is already a challenging problem; it would be worse if we had to work
with a full probability distribution over states. Ignoring uncertainty in this way works when
the uncertainty is small. In fact, when the environment model changes over time as the result
of incorporating sensor measurements, many robots plan paths online during plan execution.
This is the online replanning technique of Section 11.3.3.

994

NAVIGATION
FUNCTION

INFORMATION
GATHERING ACTION

COASTAL
NAVIGATION

Chapter

25.

Robotics

Unfortunately, ignoring the uncertainty does not always work. In some problems the
robotâ€™s uncertainty is simply too massive: How can we use a deterministic path planner to
control a mobile robot that has no clue where it is? In general, if the robotâ€™s true state is not
the one identified by the maximum likelihood rule, the resulting control will be suboptimal.
Depending on the magnitude of the error this can lead to all sorts of unwanted effects, such
as collisions with obstacles.
The field of robotics has adopted a range of techniques for accommodating uncertainty.
Some are derived from the algorithms given in Chapter 17 for decision making under uncertainty. If the robot faces uncertainty only in its state transition, but its state is fully observable,
the problem is best modeled as a Markov decision process (MDP). The solution of an MDP is
an optimal policy, which tells the robot what to do in every possible state. In this way, it can
handle all sorts of motion errors, whereas a single-path solution from a deterministic planner
would be much less robust. In robotics, policies are called navigation functions. The value
function shown in Figure 25.16(a) can be converted into such a navigation function simply
by following the gradient.
Just as in Chapter 17, partial observability makes the problem much harder. The resulting robot control problem is a partially observable MDP, or POMDP. In such situations, the
robot maintains an internal belief state, like the ones discussed in Section 25.3. The solution
to a POMDP is a policy defined over the robotâ€™s belief state. Put differently, the input to
the policy is an entire probability distribution. This enables the robot to base its decision not
only on what it knows, but also on what it does not know. For example, if it is uncertain
about a critical state variable, it can rationally invoke an information gathering action. This
is impossible in the MDP framework, since MDPs assume full observability. Unfortunately,
techniques that solve POMDPs exactly are inapplicable to roboticsâ€”there are no known techniques for high-dimensional continuous spaces. Discretization produces POMDPs that are far
too large to handle. One remedy is to make the minimization of uncertainty a control objective. For example, the coastal navigation heuristic requires the robot to stay near known
landmarks to decrease its uncertainty. Another approach applies variants of the probabilistic roadmap planning method to the belief space representation. Such methods tend to scale
better to large discrete POMDPs.

25.5.1 Robust methods
ROBUST CONTROL

FINE-MOTION
PLANNING

Uncertainty can also be handled using so-called robust control methods (see page 836) rather
than probabilistic methods. A robust method is one that assumes a bounded amount of uncertainty in each aspect of a problem, but does not assign probabilities to values within the
allowed interval. A robust solution is one that works no matter what actual values occur,
provided they are within the assumed interval. An extreme form of robust method is the conformant planning approach given in Chapter 11â€”it produces plans that work with no state
information at all.
Here, we look at a robust method that is used for fine-motion planning (or FMP) in
robotic assembly tasks. Fine-motion planning involves moving a robot arm in very close
proximity to a static environment object. The main difficulty with fine-motion planning is

Section 25.5.

Planning Uncertain Movements

995

initial
configuration

Cv

v

motion
envelope

Figure 25.19 A two-dimensional environment, velocity uncertainty cone, and envelope of
possible robot motions. The intended velocity is v, but with uncertainty the actual velocity
could be anywhere in Cv , resulting in a final configuration somewhere in the motion envelope,
which means we wouldnâ€™t know if we hit the hole or not.
Cv

initial
configuration
v

motion
envelope

Figure 25.20 The first motion command and the resulting envelope of possible robot motions. No matter what the error, we know the final configuration will be to the left of the
hole.

GUARDED MOTION

COMPLIANT MOTION

that the required motions and the relevant features of the environment are very small. At such
small scales, the robot is unable to measure or control its position accurately and may also be
uncertain of the shape of the environment itself; we will assume that these uncertainties are
all bounded. The solutions to FMP problems will typically be conditional plans or policies
that make use of sensor feedback during execution and are guaranteed to work in all situations
consistent with the assumed uncertainty bounds.
A fine-motion plan consists of a series of guarded motions. Each guarded motion
consists of (1) a motion command and (2) a termination condition, which is a predicate on the
robotâ€™s sensor values, and returns true to indicate the end of the guarded move. The motion
commands are typically compliant motions that allow the effector to slide if the motion
command would cause collision with an obstacle. As an example, Figure 25.19 shows a twodimensional configuration space with a narrow vertical hole. It could be the configuration
space for insertion of a rectangular peg into a hole or a car key into the ignition. The motion
commands are constant velocities. The termination conditions are contact with a surface. To
model uncertainty in control, we assume that instead of moving in the commanded direction,
the robotâ€™s actual motion lies in the cone Cv about it. The figure shows what would happen

996

Chapter

Cv

25.

Robotics

v

motion
envelope

Figure 25.21 The second motion command and the envelope of possible motions. Even
with error, we will eventually get into the hole.

if we commanded a velocity straight down from the initial configuration. Because of the
uncertainty in velocity, the robot could move anywhere in the conical envelope, possibly
going into the hole, but more likely landing to one side of it. Because the robot would not
then know which side of the hole it was on, it would not know which way to move.
A more sensible strategy is shown in Figures 25.20 and 25.21. In Figure 25.20, the
robot deliberately moves to one side of the hole. The motion command is shown in the figure,
and the termination test is contact with any surface. In Figure 25.21, a motion command is
given that causes the robot to slide along the surface and into the hole. Because all possible
velocities in the motion envelope are to the right, the robot will slide to the right whenever it
is in contact with a horizontal surface. It will slide down the right-hand vertical edge of the
hole when it touches it, because all possible velocities are down relative to a vertical surface.
It will keep moving until it reaches the bottom of the hole, because that is its termination
condition. In spite of the control uncertainty, all possible trajectories of the robot terminate
in contact with the bottom of the holeâ€”that is, unless surface irregularities cause the robot to
stick in one place.
As one might imagine, the problem of constructing fine-motion plans is not trivial; in
fact, it is a good deal harder than planning with exact motions. One can either choose a
fixed number of discrete values for each motion or use the environment geometry to choose
directions that give qualitatively different behavior. A fine-motion planner takes as input the
configuration-space description, the angle of the velocity uncertainty cone, and a specification
of what sensing is possible for termination (surface contact in this case). It should produce a
multistep conditional plan or policy that is guaranteed to succeed, if such a plan exists.
Our example assumes that the planner has an exact model of the environment, but it is
possible to allow for bounded error in this model as follows. If the error can be described in
terms of parameters, those parameters can be added as degrees of freedom to the configuration
space. In the last example, if the depth and width of the hole were uncertain, we could add
them as two degrees of freedom to the configuration space. It is impossible to move the
robot in these directions in the configuration space or to sense its position directly. But
both those restrictions can be incorporated when describing this problem as an FMP problem
by appropriately specifying control and sensor uncertainties. This gives a complex, fourdimensional planning problem, but exactly the same planning techniques can be applied.

Section 25.6.

Moving

997

Notice that unlike the decision-theoretic methods in Chapter 17, this kind of robust approach
results in plans designed for the worst-case outcome, rather than maximizing the expected
quality of the plan. Worst-case plans are optimal in the decision-theoretic sense only if failure
during execution is much worse than any of the other costs involved in execution.

25.6

M OVING
So far, we have talked about how to plan motions, but not about how to move. Our plansâ€”
particularly those produced by deterministic path plannersâ€”assume that the robot can simply
follow any path that the algorithm produces. In the real world, of course, this is not the case.
Robots have inertia and cannot execute arbitrary paths except at arbitrarily slow speeds. In
most cases, the robot gets to exert forces rather than specify positions. This section discusses
methods for calculating these forces.

25.6.1 Dynamics and control

DIFFERENTIAL
EQUATION

CONTROLLER

REFERENCE
CONTROLLER
REFERENCE PATH
OPTIMAL
CONTROLLERS

Section 25.2 introduced the notion of dynamic state, which extends the kinematic state of a
robot by its velocity. For example, in addition to the angle of a robot joint, the dynamic state
also captures the rate of change of the angle, and possibly even its momentary acceleration.
The transition model for a dynamic state representation includes the effect of forces on this
rate of change. Such models are typically expressed via differential equations, which are
equations that relate a quantity (e.g., a kinematic state) to the change of the quantity over
time (e.g., velocity). In principle, we could have chosen to plan robot motion using dynamic
models, instead of our kinematic models. Such a methodology would lead to superior robot
performance, if we could generate the plans. However, the dynamic state has higher dimension than the kinematic space, and the curse of dimensionality would render many motion
planning algorithms inapplicable for all but the most simple robots. For this reason, practical
robot system often rely on simpler kinematic path planners.
A common technique to compensate for the limitations of kinematic plans is to use a
separate mechanism, a controller, for keeping the robot on track. Controllers are techniques
for generating robot controls in real time using feedback from the environment, so as to
achieve a control objective. If the objective is to keep the robot on a preplanned path, it is
often referred to as a reference controller and the path is called a reference path. Controllers
that optimize a global cost function are known as optimal controllers. Optimal policies for
continuous MDPs are, in effect, optimal controllers.
On the surface, the problem of keeping a robot on a prespecified path appears to be
relatively straightforward. In practice, however, even this seemingly simple problem has its
pitfalls. Figure 25.22(a) illustrates what can go wrong; it shows the path of a robot that
attempts to follow a kinematic path. Whenever a deviation occursâ€”whether due to noise or
to constraints on the forces the robot can applyâ€”the robot provides an opposing force whose
magnitude is proportional to this deviation. Intuitively, this might appear plausible, since
deviations should be compensated by a counterforce to keep the robot on track. However,

998

Chapter

(a)

(b)

25.

Robotics

(c)

Figure 25.22 Robot arm control using (a) proportional control with gain factor 1.0, (b)
proportional control with gain factor 0.1, and (c) PD (proportional derivative) control with
gain factors 0.3 for the proportional component and 0.8 for the differential component. In all
cases the robot arm tries to follow the path shown in gray.

P CONTROLLER

as Figure 25.22(a) illustrates, our controller causes the robot to vibrate rather violently. The
vibration is the result of a natural inertia of the robot arm: once driven back to its reference
position the robot then overshoots, which induces a symmetric error with opposite sign. Such
overshooting may continue along an entire trajectory, and the resulting robot motion is far
from desirable.
Before we can define a better controller, let us formally describe what went wrong.
Controllers that provide force in negative proportion to the observed error are known as P
controllers. The letter â€˜Pâ€™ stands for proportional, indicating that the actual control is proportional to the error of the robot manipulator. More formally, let y(t) be the reference path,
parameterized by time index t. The control at generated by a P controller has the form:
at = KP (y(t) âˆ’ xt ) .

GAIN PARAMETER

STABLE
STRICTLY STABLE

Here xt is the state of the robot at time t and KP is a constant known as the gain parameter of
the controller and its value is called the gain factor); Kp regulates how strongly the controller
corrects for deviations between the actual state xt and the desired one y(t). In our example,
KP = 1. At first glance, one might think that choosing a smaller value for KP would
remedy the problem. Unfortunately, this is not the case. Figure 25.22(b) shows a trajectory
for KP = .1, still exhibiting oscillatory behavior. Lower values of the gain parameter may
simply slow down the oscillation, but do not solve the problem. In fact, in the absence of
friction, the P controller is essentially a spring law; so it will oscillate indefinitely around a
fixed target location.
Traditionally, problems of this type fall into the realm of control theory, a field of
increasing importance to researchers in AI. Decades of research in this field have led to a large
number of controllers that are superior to the simple control law given above. In particular, a
reference controller is said to be stable if small perturbations lead to a bounded error between
the robot and the reference signal. It is said to be strictly stable if it is able to return to and

Section 25.6.

PD CONTROLLER

PID CONTROLLER

Moving

999

then stay on its reference path upon such perturbations. Our P controller appears to be stable
but not strictly stable, since it fails to stay anywhere near its reference trajectory.
The simplest controller that achieves strict stability in our domain is a PD controller.
The letter â€˜Pâ€™ stands again for proportional, and â€˜Dâ€™ stands for derivative. PD controllers are
described by the following equation:
âˆ‚(y(t) âˆ’ xt )
.
(25.2)
at = KP (y(t) âˆ’ xt ) + KD
âˆ‚t
As this equation suggests, PD controllers extend P controllers by a differential component,
which adds to the value of at a term that is proportional to the first derivative of the error
y(t) âˆ’ xt over time. What is the effect of such a term? In general, a derivative term dampens
the system that is being controlled. To see this, consider a situation where the error (y(t)âˆ’xt )
is changing rapidly over time, as is the case for our P controller above. The derivative of this
error will then counteract the proportional term, which will reduce the overall response to
the perturbation. However, if the same error persists and does not change, the derivative will
vanish and the proportional term dominates the choice of control.
Figure 25.22(c) shows the result of applying this PD controller to our robot arm, using
as gain parameters KP = .3 and KD = .8. Clearly, the resulting path is much smoother, and
does not exhibit any obvious oscillations.
PD controllers do have failure modes, however. In particular, PD controllers may fail
to regulate an error down to zero, even in the absence of external perturbations. Often such
a situation is the result of a systematic external force that is not part of the model. An autonomous car driving on a banked surface, for example, may find itself systematically pulled
to one side. Wear and tear in robot arms cause similar systematic errors. In such situations,
an over-proportional feedback is required to drive the error closer to zero. The solution to this
problem lies in adding a third term to the control law, based on the integrated error over time:

âˆ‚(y(t) âˆ’ xt )
.
(25.3)
at = KP (y(t) âˆ’ xt ) + KI (y(t) âˆ’ xt )dt + KD
âˆ‚t
+
Here KI is yet another gain parameter. The term (y(t) âˆ’ xt )dt calculates the integral of the
error over time. The effect of this term is that long-lasting deviations between the reference
signal and the actual state are corrected. If, for example, xt is smaller than y(t) for a long
period of time, this integral will grow until the resulting control at forces this error to shrink.
Integral terms, then, ensure that a controller does not exhibit systematic error, at the expense
of increased danger of oscillatory behavior. A controller with all three terms is called a PID
controller (for proportional integral derivative). PID controllers are widely used in industry,
for a variety of control problems.

25.6.2 Potential-field control
We introduced potential fields as an additional cost function in robot motion planning, but
they can also be used for generating robot motion directly, dispensing with the path planning
phase altogether. To achieve this, we have to define an attractive force that pulls the robot
towards its goal configuration and a repellent potential field that pushes the robot away from
obstacles. Such a potential field is shown in Figure 25.23. Its single global minimum is

1000

Chapter

start

goal

25.

Robotics

goal

start

(a)

(b)

Figure 25.23 Potential field control. The robot ascends a potential field composed of
repelling forces asserted from the obstacles and an attracting force that corresponds to the
goal configuration. (a) Successful path. (b) Local optimum.

the goal configuration, and the value is the sum of the distance to this goal configuration
and the proximity to obstacles. No planning was involved in generating the potential field
shown in the figure. Because of this, potential fields are well suited to real-time control.
Figure 25.23(a) shows a trajectory of a robot that performs hill climbing in the potential
field. In many applications, the potential field can be calculated efficiently for any given
configuration. Moreover, optimizing the potential amounts to calculating the gradient of the
potential for the present robot configuration. These calculations can be extremely efficient,
especially when compared to path-planning algorithms, all of which are exponential in the
dimensionality of the configuration space (the DOFs) in the worst case.
The fact that the potential field approach manages to find a path to the goal in such
an efficient manner, even over long distances in configuration space, raises the question as
to whether there is a need for planning in robotics at all. Are potential field techniques
sufficient, or were we just lucky in our example? The answer is that we were indeed lucky.
Potential fields have many local minima that can trap the robot. In Figure 25.23(b), the robot
approaches the obstacle by simply rotating its shoulder joint, until it gets stuck on the wrong
side of the obstacle. The potential field is not rich enough to make the robot bend its elbow
so that the arm fits under the obstacle. In other words, potential field control is great for local
robot motion but sometimes we still need global planning. Another important drawback with
potential fields is that the forces they generate depend only on the obstacle and robot positions,
not on the robotâ€™s velocity. Thus, potential field control is really a kinematic method and may
fail if the robot is moving quickly.

Section 25.6.

Moving

1001
retract, lift higher
yes
S3

move
forward
stuck?

no

lift up

S4

set down
push backward
S2

(a)

S1

(b)

Figure 25.24 (a) Genghis, a hexapod robot. (b) An augmented finite state machine
(AFSM) for the control of a single leg. Notice that this AFSM reacts to sensor feedback:
if a leg is stuck during the forward swinging phase, it will be lifted increasingly higher.

25.6.3 Reactive control

REACTIVE CONTROL

GAIT

So far we have considered control decisions that require some model of the environment for
constructing either a reference path or a potential field. There are some difficulties with this
approach. First, models that are sufficiently accurate are often difficult to obtain, especially
in complex or remote environments, such as the surface of Mars, or for robots that have
few sensors. Second, even in cases where we can devise a model with sufficient accuracy,
computational difficulties and localization error might render these techniques impractical.
In some cases, a reflex agent architecture using reactive control is more appropriate.
For example, picture a legged robot that attempts to lift a leg over an obstacle. We could
give this robot a rule that says lift the leg a small height h and move it forward, and if the leg
encounters an obstacle, move it back and start again at a higher height. You could say that h
is modeling an aspect of the world, but we can also think of h as an auxiliary variable of the
robot controller, devoid of direct physical meaning.
One such example is the six-legged (hexapod) robot, shown in Figure 25.24(a), designed for walking through rough terrain. The robotâ€™s sensors are inadequate to obtain models of the terrain for path planning. Moreover, even if we added sufficiently accurate sensors,
the twelve degrees of freedom (two for each leg) would render the resulting path planning
problem computationally intractable.
It is possible, nonetheless, to specify a controller directly without an explicit environmental model. (We have already seen this with the PD controller, which was able to keep a
complex robot arm on target without an explicit model of the robot dynamics; it did, however,
require a reference path generated from a kinematic model.) For the hexapod robot we first
choose a gait, or pattern of movement of the limbs. One statically stable gait is to first move
the right front, right rear, and left center legs forward (keeping the other three fixed), and
then move the other three. This gait works well on flat terrain. On rugged terrain, obstacles
may prevent a leg from swinging forward. This problem can be overcome by a remarkably
simple control rule: when a legâ€™s forward motion is blocked, simply retract it, lift it higher,

1002

Chapter

25.

Robotics

Figure 25.25 Multiple exposures of an RC helicopter executing a flip based on a policy
learned with reinforcement learning. Images courtesy of Andrew Ng, Stanford University.

EMERGENT
BEHAVIOR

and try again. The resulting controller is shown in Figure 25.24(b) as a finite state machine;
it constitutes a reflex agent with state, where the internal state is represented by the index of
the current machine state (s1 through s4 ).
Variants of this simple feedback-driven controller have been found to generate remarkably robust walking patterns, capable of maneuvering the robot over rugged terrain. Clearly,
such a controller is model-free, and it does not deliberate or use search for generating controls. Environmental feedback plays a crucial role in the controllerâ€™s execution. The software
alone does not specify what will actually happen when the robot is placed in an environment.
Behavior that emerges through the interplay of a (simple) controller and a (complex) environment is often referred to as emergent behavior. Strictly speaking, all robots discussed
in this chapter exhibit emergent behavior, due to the fact that no model is perfect. Historically, however, the term has been reserved for control techniques that do not utilize explicit
environmental models. Emergent behavior is also characteristic of biological organisms.

25.6.4 Reinforcement learning control
One particularly exciting form of control is based on the policy search form of reinforcement
learning (see Section 21.5). This work has been enormously influential in recent years, at
is has solved challenging robotics problems for which previously no solution existed. An
example is acrobatic autonomous helicopter flight. Figure 25.25 shows an autonomous flip
of a small RC (radio-controlled) helicopter. This maneuver is challenging due to the highly
nonlinear nature of the aerodynamics involved. Only the most experienced of human pilots
are able to perform it. Yet a policy search method (as described in Chapter 21), using only a
few minutes of computation, learned a policy that can safely execute a flip every time.
Policy search needs an accurate model of the domain before it can find a policy. The
input to this model is the state of the helicopter at time t, the controls at time t, and the
resulting state at time t + Î”t. The state of a helicopter can be described by the 3D coordinates
of the vehicle, its yaw, pitch, and roll angles, and the rate of change of these six variables.
The controls are the manual controls of of the helicopter: throttle, pitch, elevator, aileron,
and rudder. All that remains is the resulting stateâ€”how are we going to define a model that
accurately says how the helicopter responds to each control? The answer is simple: Let an
expert human pilot fly the helicopter, and record the controls that the expert transmits over
the radio and the state variables of the helicopter. About four minutes of human-controlled
flight suffices to build a predictive model that is sufficiently accurate to simulate the vehicle.

Section 25.7.

Robotic Software Architectures

1003

What is remarkable about this example is the ease with which this learning approach
solves a challenging robotics problem. This is one of the many successes of machine learning
in scientific fields previously dominated by careful mathematical analysis and modeling.

25.7

ROBOTIC S OFTWARE A RCHITECTURES

SOFTWARE
ARCHITECTURE

HYBRID
ARCHITECTURE

A methodology for structuring algorithms is called a software architecture. An architecture
includes languages and tools for writing programs, as well as an overall philosophy for how
programs can be brought together.
Modern-day software architectures for robotics must decide how to combine reactive
control and model-based deliberative planning. In many ways, reactive and deliberate techniques have orthogonal strengths and weaknesses. Reactive control is sensor-driven and appropriate for making low-level decisions in real time. However, it rarely yields a plausible
solution at the global level, because global control decisions depend on information that cannot be sensed at the time of decision making. For such problems, deliberate planning is a
more appropriate choice.
Consequently, most robot architectures use reactive techniques at the lower levels of
control and deliberative techniques at the higher levels. We encountered such a combination
in our discussion of PD controllers, where we combined a (reactive) PD controller with a
(deliberate) path planner. Architectures that combine reactive and deliberate techniques are
called hybrid architectures.

25.7.1 Subsumption architecture
SUBSUMPTION
ARCHITECTURE

AUGMENTED FINITE
STATE MACHINE

The subsumption architecture (Brooks, 1986) is a framework for assembling reactive controllers out of finite state machines. Nodes in these machines may contain tests for certain
sensor variables, in which case the execution trace of a finite state machine is conditioned on
the outcome of such a test. Arcs can be tagged with messages that will be generated when
traversing them, and that are sent to the robotâ€™s motors or to other finite state machines. Additionally, finite state machines possess internal timers (clocks) that control the time it takes to
traverse an arc. The resulting machines are refereed to as augmented finite state machines,
or AFSMs, where the augmentation refers to the use of clocks.
An example of a simple AFSM is the four-state machine shown in Figure 25.24(b),
which generates cyclic leg motion for a hexapod walker. This AFSM implements a cyclic
controller, whose execution mostly does not rely on environmental feedback. The forward
swing phase, however, does rely on sensor feedback. If the leg is stuck, meaning that it has
failed to execute the forward swing, the robot retracts the leg, lifts it up a little higher, and
attempts to execute the forward swing once again. Thus, the controller is able to react to
contingencies arising from the interplay of the robot and its environment.
The subsumption architecture offers additional primitives for synchronizing AFSMs,
and for combining output values of multiple, possibly conflicting AFSMs. In this way, it
enables the programmer to compose increasingly complex controllers in a bottom-up fashion.

1004

Chapter

25.

Robotics

In our example, we might begin with AFSMs for individual legs, followed by an AFSM for
coordinating multiple legs. On top of this, we might implement higher-level behaviors such
as collision avoidance, which might involve backing up and turning.
The idea of composing robot controllers from AFSMs is quite intriguing. Imagine
how difficult it would be to generate the same behavior with any of the configuration-space
path-planning algorithms described in the previous section. First, we would need an accurate model of the terrain. The configuration space of a robot with six legs, each of which
is driven by two independent motors, totals eighteen dimensions (twelve dimensions for the
configuration of the legs, and six for the location and orientation of the robot relative to its
environment). Even if our computers were fast enough to find paths in such high-dimensional
spaces, we would have to worry about nasty effects such as the robot sliding down a slope.
Because of such stochastic effects, a single path through configuration space would almost
certainly be too brittle, and even a PID controller might not be able to cope with such contingencies. In other words, generating motion behavior deliberately is simply too complex a
problem for present-day robot motion planning algorithms.
Unfortunately, the subsumption architecture has its own problems. First, the AFSMs
are driven by raw sensor input, an arrangement that works if the sensor data is reliable and
contains all necessary information for decision making, but fails if sensor data has to be integrated in nontrivial ways over time. Subsumption-style controllers have therefore mostly been
applied to simple tasks, such as following a wall or moving towards visible light sources. Second, the lack of deliberation makes it difficult to change the task of the robot. A subsumptionstyle robot usually does just one task, and it has no notion of how to modify its controls to
accommodate different goals (just like the dung beetle on page 39). Finally, subsumptionstyle controllers tend to be difficult to understand. In practice, the intricate interplay between
dozens of interacting AFSMs (and the environment) is beyond what most human programmers can comprehend. For all these reasons, the subsumption architecture is rarely used in
robotics, despite its great historical importance. However, it has had an influence on other
architectures, and on individual components of some architectures.

25.7.2 Three-layer architecture
THREE-LAYER
ARCHITECTURE

REACTIVE LAYER

EXECUTIVE LAYER

Hybrid architectures combine reaction with deliberation. The most popular hybrid architecture is the three-layer architecture, which consists of a reactive layer, an executive layer,
and a deliberative layer.
The reactive layer provides low-level control to the robot. It is characterized by a tight
sensorâ€“action loop. Its decision cycle is often on the order of milliseconds.
The executive layer (or sequencing layer) serves as the glue between the reactive layer
and the deliberative layer. It accepts directives by the deliberative layer, and sequences them
for the reactive layer. For example, the executive layer might handle a set of via-points
generated by a deliberative path planner, and make decisions as to which reactive behavior
to invoke. Decision cycles at the executive layer are usually in the order of a second. The
executive layer is also responsible for integrating sensor information into an internal state
representation. For example, it may host the robotâ€™s localization and online mapping routines.

Section 25.7.

Robotic Software Architectures

SENSOR INTERFACE
RDDF database

1005

PERCEPTION

PLANNING&CONTROL

USER INTERFACE

Top level control

corridor

Touch screen UI

pause/disable command

Wireless E-Stop

Laser 1 interface
RDDF corridor (smoothed and original)

driving mode

Laser 2 interface
Laser 3 interface

road center

Road finder

Laser 4 interface

laser map

Laser 5 interface

Laser mapper

Camera interface

trajectory

map

VEHICLE
INTERFACE

vision map

Vision mapper

Radar interface

Path planner

Steering control

obstacle list

Radar mapper

Touareg interface

vehicle state (pose, velocity)

GPS position

vehicle
state

UKF Pose estimation

Throttle/brake control
Power server interface

vehicle state (pose, velocity)

GPS compass
IMU interface

Surface assessment

velocity limit

Wheel velocity
Brake/steering
heart beats

emergency stop

Linux processes start/stop
health status

Process controller

Health monitor

power on/off

data

GLOBAL
SERVICES

Data logger
Communication requests

File system
Communication channels

Inter-process communication (IPC) server

clocks

Time server

Figure 25.26 Software architecture of a robot car. This software implements a data
pipeline, in which all modules process data simultaneously.

DELIBERATIVE LAYER

The deliberative layer generates global solutions to complex tasks using planning.
Because of the computational complexity involved in generating such solutions, its decision
cycle is often in the order of minutes. The deliberative layer (or planning layer) uses models
for decision making. Those models might be either learned from data or supplied and may
utilize state information gathered at the executive layer.
Variants of the three-layer architecture can be found in most modern-day robot software
systems. The decomposition into three layers is not very strict. Some robot software systems
possess additional layers, such as user interface layers that control the interaction with people,
or a multiagent level for coordinating a robotâ€™s actions with that of other robots operating in
the same environment.

25.7.3 Pipeline architecture
PIPELINE
ARCHITECTURE

SENSOR INTERFACE
LAYER
PERCEPTION LAYER

Another architecture for robots is known as the pipeline architecture. Just like the subsumption architecture, the pipeline architecture executes multiple process in parallel. However, the
specific modules in this architecture resemble those in the three-layer architecture.
Figure 25.26 shows an example pipeline architecture, which is used to control an autonomous car. Data enters this pipeline at the sensor interface layer. The perception layer

1006

Chapter

(a)

25.

Robotics

(b)

Figure 25.27 (a) The Helpmate robot transports food and other medical items in dozens
of hospitals worldwide. (b) Kiva robots are part of a material-handling system for moving
shelves in fulfillment centers. Image courtesy of Kiva Systems.

PLANNING AND
CONTROL LAYER
VEHICLE INTERFACE
LAYER

25.8

then updates the robotâ€™s internal models of the environment based on this data. Next, these
models are handed to the planning and control layer, which adjusts the robotâ€™s internal
plans turns them into actual controls for the robot. Those are then communicated back to the
vehicle through the vehicle interface layer.
The key to the pipeline architecture is that this all happens in parallel. While the perception layer processes the most recent sensor data, the control layer bases its choices on
slightly older data. In this way, the pipeline architecture is similar to the human brain. We
donâ€™t switch off our motion controllers when we digest new sensor data. Instead, we perceive,
plan, and act all at the same time. Processes in the pipeline architecture run asynchronously,
and all computation is data-driven. The resulting system is robust, and it is fast.
The architecture in Figure 25.26 also contains other, cross-cutting modules, responsible
for establishing communication between the different elements of the pipeline.

A PPLICATION D OMAINS
Here are some of the prime application domains for robotic technology.
Industry and Agriculture. Traditionally, robots have been fielded in areas that require
difficult human labor, yet are structured enough to be amenable to robotic automation. The
best example is the assembly line, where manipulators routinely perform tasks such as assembly, part placement, material handling, welding, and painting. In many of these tasks,
robots have become more cost-effective than human workers. Outdoors, many of the heavy
machines that we use to harvest, mine, or excavate earth have been turned into robots. For

Section 25.8.

Application Domains

(a)

1007

(b)

Figure 25.28 (a) Robotic car B OSS, which won the DARPA Urban Challenge. Courtesy
of Carnegie Mellon University. (b) Surgical robots in the operating room. Image courtesy of
da Vinci Surgical Systems.

example, a project at Carnegie Mellon University has demonstrated that robots can strip paint
off large ships about 50 times faster than people can, and with a much reduced environmental
impact. Prototypes of autonomous mining robots have been found to be faster and more precise than people in transporting ore in underground mines. Robots have been used to generate
high-precision maps of abandoned mines and sewer systems. While many of these systems
are still in their prototype stages, it is only a matter of time until robots will take over much
of the semimechanical work that is presently performed by people.
Transportation. Robotic transportation has many facets: from autonomous helicopters
that deliver payloads to hard-to-reach locations, to automatic wheelchairs that transport people who are unable to control wheelchairs by themselves, to autonomous straddle carriers that
outperform skilled human drivers when transporting containers from ships to trucks on loading docks. A prime example of indoor transportation robots, or gofers, is the Helpmate robot
shown in Figure 25.27(a). This robot has been deployed in dozens of hospitals to transport
food and other items. In factory settings, autonomous vehicles are now routinely deployed
to transport goods in warehouses and between production lines. The Kiva system, shown in
Figure 25.27(b), helps workers at fulfillment centers package goods into shipping containers.
Many of these robots require environmental modifications for their operation. The most
common modifications are localization aids such as inductive loops in the floor, active beacons, or barcode tags. An open challenge in robotics is the design of robots that can use
natural cues, instead of artificial devices, to navigate, particularly in environments such as the
deep ocean where GPS is unavailable.
Robotic cars. Most of use cars every day. Many of us make cell phone calls while
driving. Some of us even text. The sad result: more than a million people die every year in
traffic accidents. Robotic cars like B OSS and S TANLEY offer hope: Not only will they make
driving much safer, but they will also free us from the need to pay attention to the road during
our daily commute.
Progress in robotic cars was stimulated by the DARPA Grand Challenge, a race over
100 miles of unrehearsed desert terrain, which represented a much more challenging task than

1008

Chapter

(a)

25.

Robotics

(b)

Figure 25.29 (a) A robot mapping an abandoned coal mine. (b) A 3D map of the mine
acquired by the robot.

had ever been accomplished before. Stanfordâ€™s S TANLEY vehicle completed the course in less
than seven hours in 2005, winning a $2 million prize and a place in the National Museum of
American History. Figure 25.28(a) depicts B OSS , which in 2007 won the DARPA Urban
Challenge, a complicated road race on city streets where robots faced other robots and had to
obey traffic rules.
Health care. Robots are increasingly used to assist surgeons with instrument placement
when operating on organs as intricate as brains, eyes, and hearts. Figure 25.28(b) shows such
a system. Robots have become indispensable tools in a range of surgical procedures, such as
hip replacements, thanks to their high precision. In pilot studies, robotic devices have been
found to reduce the danger of lesions when performing colonoscopy. Outside the operating
room, researchers have begun to develop robotic aides for elderly and handicapped people,
such as intelligent robotic walkers and intelligent toys that provide reminders to take medication and provide comfort. Researchers are also working on robotic devices for rehabilitation
that aid people in performing certain exercises.
Hazardous environments. Robots have assisted people in cleaning up nuclear waste,
most notably in Chernobyl and Three Mile Island. Robots were present after the collapse
of the World Trade Center, where they entered structures deemed too dangerous for human
search and rescue crews.
Some countries have used robots to transport ammunition and to defuse bombsâ€”a notoriously dangerous task. A number of research projects are presently developing prototype
robots for clearing minefields, on land and at sea. Most existing robots for these tasks are
teleoperatedâ€”a human operates them by remote control. Providing such robots with autonomy is an important next step.
Exploration. Robots have gone where no one has gone before, including the surface
of Mars (see Figure 25.2(b) and the cover). Robotic arms assist astronauts in deploying
and retrieving satellites and in building the International Space Station. Robots also help
explore under the sea. They are routinely used to acquire maps of sunken ships. Figure 25.29
shows a robot mapping an abandoned coal mine, along with a 3D model of the mine acquired

Section 25.8.

Application Domains

(a)

1009

(b)

Figure 25.30 (a) Roomba, the worldâ€™s best-selling mobile robot, vacuums floors. Image
c 2009. (b) Robotic hand modeled after human hand. Image courtesy
courtesy of iRobot, 
of University of Washington and Carnegie Mellon University.

DRONE

ROOMBA

ROBOTIC SOCCER

using range sensors. In 1996, a team of researches released a legged robot into the crater
of an active volcano to acquire data for climate research. Unmanned air vehicles known as
drones are used in military operations. Robots are becoming very effective tools for gathering
information in domains that are difficult (or dangerous) for people to access.
Personal Services. Service is an up-and-coming application domain of robotics. Service robots assist individuals in performing daily tasks. Commercially available domestic
service robots include autonomous vacuum cleaners, lawn mowers, and golf caddies. The
worldâ€™s most popular mobile robot is a personal service robot: the robotic vacuum cleaner
Roomba, shown in Figure 25.30(a). More than three million Roombas have been sold.
Roomba can navigate autonomously and perform its tasks without human help.
Other service robots operate in public places, such as robotic information kiosks that
have been deployed in shopping malls and trade fairs, or in museums as tour guides. Service tasks require human interaction, and the ability to cope robustly with unpredictable and
dynamic environments.
Entertainment. Robots have begun to conquer the entertainment and toy industry.
In Figure 25.6(b) we see robotic soccer, a competitive game very much like human soccer, but played with autonomous mobile robots. Robot soccer provides great opportunities
for research in AI, since it raises a range of problems relevant to many other, more serious
robot applications. Annual robotic soccer competitions have attracted large numbers of AI
researchers and added a lot of excitement to the field of robotics.
Human augmentation. A final application domain of robotic technology is that of
human augmentation. Researchers have developed legged walking machines that can carry
people around, very much like a wheelchair. Several research efforts presently focus on the
development of devices that make it easier for people to walk or move their arms by providing
additional forces through extraskeletal attachments. If such devices are attached permanently,

1010

Chapter

25.

Robotics

they can be thought of as artificial robotic limbs. Figure 25.30(b) shows a robotic hand that
may serve as a prosthetic device in the future.
Robotic teleoperation, or telepresence, is another form of human augmentation. Teleoperation involves carrying out tasks over long distances with the aid of robotic devices.
A popular configuration for robotic teleoperation is the masterâ€“slave configuration, where
a robot manipulator emulates the motion of a remote human operator, measured through a
haptic interface. Underwater vehicles are often teleoperated; the vehicles can go to a depth
that would be dangerous for humans but can still be guided by the human operator. All these
systems augment peopleâ€™s ability to interact with their environments. Some projects go as far
as replicating humans, at least at a very superficial level. Humanoid robots are now available
commercially through several companies in Japan.

25.9

S UMMARY
Robotics concerns itself with intelligent agents that manipulate the physical world. In this
chapter, we have learned the following basics of robot hardware and software.
â€¢ Robots are equipped with sensors for perceiving their environment and effectors with
which they can assert physical forces on their environment. Most robots are either
manipulators anchored at fixed locations or mobile robots that can move.
â€¢ Robotic perception concerns itself with estimating decision-relevant quantities from
sensor data. To do so, we need an internal representation and a method for updating
this internal representation over time. Common examples of hard perceptual problems
include localization, mapping, and object recognition.
â€¢ Probabilistic filtering algorithms such as Kalman filters and particle filters are useful
for robot perception. These techniques maintain the belief state, a posterior distribution
over state variables.
â€¢ The planning of robot motion is usually done in configuration space, where each point
specifies the location and orientation of the robot and its joint angles.
â€¢ Configuration space search algorithms include cell decomposition techniques, which
decompose the space of all configurations into finitely many cells, and skeletonization
techniques, which project configuration spaces onto lower-dimensional manifolds. The
motion planning problem is then solved using search in these simpler structures.
â€¢ A path found by a search algorithm can be executed by using the path as the reference
trajectory for a PID controller. Controllers are necessary in robotics to accommodate
small perturbations; path planning alone is usually insufficient.
â€¢ Potential field techniques navigate robots by potential functions, defined over the distance to obstacles and the goal location. Potential field techniques may get stuck in
local minima, but they can generate motion directly without the need for path planning.
â€¢ Sometimes it is easier to specify a robot controller directly, rather than deriving a path
from an explicit model of the environment. Such controllers can often be written as
simple finite state machines.

Bibliographical and Historical Notes

1011

â€¢ There exist different architectures for software design. The subsumption architecture enables programmers to compose robot controllers from interconnected finite state
machines. Three-layer architectures are common frameworks for developing robot
software that integrate deliberation, sequencing of subgoals, and control. The related
pipeline architecture processes data in parallel through a sequence of modules, corresponding to perception, modeling, planning, control, and robot interfaces.

B IBLIOGRAPHICAL

UNIMATE

PUMA

AND

H ISTORICAL N OTES

The word robot was popularized by Czech playwright Karel Capek in his 1921 play R.U.R.
(Rossumâ€™s Universal Robots). The robots, which were grown chemically rather than constructed mechanically, end up resenting their masters and decide to take over. It appears
(Glanc, 1978) it was Capekâ€™s brother, Josef, who first combined the Czech words â€œrobotaâ€
(obligatory work) and â€œrobotnikâ€ (serf) to yield â€œrobotâ€ in his 1917 short story Opilec.
The term robotics was first used by Asimov (1950). Robotics (under other names) has
a much longer history, however. In ancient Greek mythology, a mechanical man named Talos
was supposedly designed and built by Hephaistos, the Greek god of metallurgy. Wonderful
automata were built in the 18th centuryâ€”Jacques Vaucansonâ€™s mechanical duck from 1738
being one early exampleâ€”but the complex behaviors they exhibited were entirely fixed in
advance. Possibly the earliest example of a programmable robot-like device was the Jacquard
loom (1805), described on page 14.
The first commercial robot was a robot arm called Unimate, short for universal automation, developed by Joseph Engelberger and George Devol. In 1961, the first Unimate robot
was sold to General Motors, where it was used for manufacturing TV picture tubes. 1961
was also the year when Devol obtained the first U.S. patent on a robot. Eleven years later, in
1972, Nissan Corp. was among the first to automate an entire assembly line with robots, developed by Kawasaki with robots supplied by Engelberger and Devolâ€™s company Unimation.
This development initiated a major revolution that took place mostly in Japan and the U.S.,
and that is still ongoing. Unimation followed up in 1978 with the development of the PUMA
robot, short for Programmable Universal Machine for Assembly. The PUMA robot, initially
developed for General Motors, was the de facto standard for robotic manipulation for the two
decades that followed. At present, the number of operating robots is estimated at one million
worldwide, more than half of which are installed in Japan.
The literature on robotics research can be divided roughly into two parts: mobile robots
and stationary manipulators. Grey Walterâ€™s â€œturtle,â€ built in 1948, could be considered the
first autonomous mobile robot, although its control system was not programmable. The â€œHopkins Beast,â€ built in the early 1960s at Johns Hopkins University, was much more sophisticated; it had pattern-recognition hardware and could recognize the cover plate of a standard
AC power outlet. It was capable of searching for outlets, plugging itself in, and then recharging its batteries! Still, the Beast had a limited repertoire of skills. The first general-purpose
mobile robot was â€œShakey,â€ developed at what was then the Stanford Research Institute (now

1012

OCCUPANCY GRID

MARKOV
LOCALIZATION

RAOBLACKWELLIZED
PARTICLE FILTER
HANDâ€“EYE
MACHINES

PIANO MOVERS

Chapter

25.

Robotics

SRI) in the late 1960s (Fikes and Nilsson, 1971; Nilsson, 1984). Shakey was the first robot
to integrate perception, planning, and execution, and much subsequent research in AI was
influenced by this remarkable achievement. Shakey appears on the cover of this book with
project leader Charlie Rosen (1917â€“2002). Other influential projects include the Stanford
Cart and the CMU Rover (Moravec, 1983). Cox and Wilfong (1990) describes classic work
on autonomous vehicles.
The field of robotic mapping has evolved from two distinct origins. The first thread
began with work by Smith and Cheeseman (1986), who applied Kalman filters to the simultaneous localization and mapping problem. This algorithm was first implemented by
Moutarlier and Chatila (1989), and later extended by Leonard and Durrant-Whyte (1992);
see Dissanayake et al. (2001) for an overview of early Kalman filter variations. The second
thread began with the development of the occupancy grid representation for probabilistic
mapping, which specifies the probability that each (x, y) location is occupied by an obstacle (Moravec and Elfes, 1985). Kuipers and Levitt (1988) were among the first to propose
topological rather than metric mapping, motivated by models of human spatial cognition. A
seminal paper by Lu and Milios (1997) recognized the sparseness of the simultaneous localization and mapping problem, which gave rise to the development of nonlinear optimization
techniques by Konolige (2004) and Montemerlo and Thrun (2004), as well as hierarchical
methods by Bosse et al. (2004). Shatkay and Kaelbling (1997) and Thrun et al. (1998) introduced the EM algorithm into the field of robotic mapping for data association. An overview
of probabilistic mapping methods can be found in (Thrun et al., 2005).
Early mobile robot localization techniques are surveyed by Borenstein et al. (1996).
Although Kalman filtering was well known as a localization method in control theory for
decades, the general probabilistic formulation of the localization problem did not appear
in the AI literature until much later, through the work of Tom Dean and colleagues (Dean
et al., 1990, 1990) and of Simmons and Koenig (1995). The latter work introduced the term
Markov localization. The first real-world application of this technique was by Burgard et al.
(1999), through a series of robots that were deployed in museums. Monte Carlo localization based on particle filters was developed by Fox et al. (1999) and is now widely used.
The Rao-Blackwellized particle filter combines particle filtering for robot localization with
exact filtering for map building (Murphy and Russell, 2001; Montemerlo et al., 2002).
The study of manipulator robots, originally called handâ€“eye machines, has evolved
along quite different lines. The first major effort at creating a handâ€“eye machine was Heinrich Ernstâ€™s MH-1, described in his MIT Ph.D. thesis (Ernst, 1961). The Machine Intelligence
project at Edinburgh also demonstrated an impressive early system for vision-based assembly called F REDDY (Michie, 1972). After these pioneering efforts, a great deal of work focused on geometric algorithms for deterministic and fully observable motion planning problems. The PSPACE-hardness of robot motion planning was shown in a seminal paper by
Reif (1979). The configuration space representation is due to Lozano-Perez (1983). A series
of papers by Schwartz and Sharir on what they called piano movers problems (Schwartz
et al., 1987) was highly influential.
Recursive cell decomposition for configuration space planning was originated by Brooks
and Lozano-Perez (1985) and improved significantly by Zhu and Latombe (1991). The ear-

Bibliographical and Historical Notes

VISIBILITY GRAPH

GRASPING

HAPTIC FEEDBACK

VECTOR FIELD
HISTOGRAMS

1013

liest skeletonization algorithms were based on Voronoi diagrams (Rowat, 1979) and visibility graphs (Wesley and Lozano-Perez, 1979). Guibas et al. (1992) developed efficient
techniques for calculating Voronoi diagrams incrementally, and Choset (1996) generalized
Voronoi diagrams to broader motion-planning problems. John Canny (1988) established the
first singly exponential algorithm for motion planning. The seminal text by Latombe (1991)
covers a variety of approaches to motion-planning, as do the texts by Choset et al. (2004) and
LaValle (2006). Kavraki et al. (1996) developed probabilistic roadmaps, which are currently
one of the most effective methods. Fine-motion planning with limited sensing was investigated by Lozano-Perez et al. (1984) and Canny and Reif (1987). Landmark-based navigation (Lazanas and Latombe, 1992) uses many of the same ideas in the mobile robot arena.
Key work applying POMDP methods (Section 17.4) to motion planning under uncertainty in
robotics is due to Pineau et al. (2003) and Roy et al. (2005).
The control of robots as dynamical systemsâ€”whether for manipulation or navigationâ€”
has generated a huge literature that is barely touched on by this chapter. Important works
include a trilogy on impedance control by Hogan (1985) and a general study of robot dynamics by Featherstone (1987). Dean and Wellman (1991) were among the first to try to tie
together control theory and AI planning systems. Three classic textbooks on the mathematics
of robot manipulation are due to Paul (1981), Craig (1989), and Yoshikawa (1990). The area
of grasping is also important in roboticsâ€”the problem of determining a stable grasp is quite
difficult (Mason and Salisbury, 1985). Competent grasping requires touch sensing, or haptic
feedback, to determine contact forces and detect slip (Fearing and Hollerbach, 1985).
Potential-field control, which attempts to solve the motion planning and control problems simultaneously, was introduced into the robotics literature by Khatib (1986). In mobile
robotics, this idea was viewed as a practical solution to the collision avoidance problem, and
was later extended into an algorithm called vector field histograms by Borenstein (1991).
Navigation functions, the robotics version of a control policy for deterministic MDPs, were
introduced by Koditschek (1987). Reinforcement learning in robotics took off with the seminal work by Bagnell and Schneider (2001) and Ng et al. (2004), who developed the paradigm
in the context of autonomous helicopter control.
The topic of software architectures for robots engenders much religious debate. The
good old-fashioned AI candidateâ€”the three-layer architectureâ€”dates back to the design of
Shakey and is reviewed by Gat (1998). The subsumption architecture is due to Brooks (1986),
although similar ideas were developed independently by Braitenberg (1984), whose book,
Vehicles, describes a series of simple robots based on the behavioral approach. The success of Brooksâ€™s six-legged walking robot was followed by many other projects. Connell,
in his Ph.D. thesis (1989), developed a mobile robot capable of retrieving objects that was
entirely reactive. Extensions of the behavior-based paradigm to multirobot systems can be
found in (Mataric, 1997) and (Parker, 1996). GRL (Horswill, 2000) and C OLBERT (Konolige, 1997) abstract the ideas of concurrent behavior-based robotics into general robot control
languages. Arkin (1998) surveys some of the most popular approaches in this field.
Research on mobile robotics has been stimulated over the last decade by several important competitions. The earliest competition, AAAIâ€™s annual mobile robot competition, began
in 1992. The first competition winner was C ARMEL (Congdon et al., 1992). Progress has

1014

ROBOCUP

DARPA GRAND
CHALLENGE

URBAN CHALLENGE

Chapter

25.

Robotics

been steady and impressive: in more recent competitions robots entered the conference complex, found their way to the registration desk, registered for the conference, and even gave a
short talk. The Robocup competition, launched in 1995 by Kitano and colleagues (1997a),
aims to â€œdevelop a team of fully autonomous humanoid robots that can win against the human world champion team in soccerâ€ by 2050. Play occurs in leagues for simulated robots,
wheeled robots of different sizes, and humanoid robots. In 2009 teams from 43 countries
participated and the event was broadcast to millions of viewers. Visser and Burkhard (2007)
track the improvements that have been made in perception, team coordination, and low-level
skills over the past decade.
The DARPA Grand Challenge, organized by DARPA in 2004 and 2005, required
autonomous robots to travel more than 100 miles through unrehearsed desert terrain in less
than 10 hours (Buehler et al., 2006). In the original event in 2004, no robot traveled more
than 8 miles, leading many to believe the prize would never be claimed. In 2005, Stanfordâ€™s
robot S TANLEY won the competition in just under 7 hours of travel (Thrun, 2006). DARPA
then organized the Urban Challenge, a competition in which robots had to navigate 60 miles
in an urban environment with other traffic. Carnegie Mellon Universityâ€™s robot B OSS took
first place and claimed the $2 million prize (Urmson and Whittaker, 2008). Early pioneers in
the development of robotic cars included Dickmanns and Zapp (1987) and Pomerleau (1993).
Two early textbooks, by Dudek and Jenkin (2000) and Murphy (2000), cover robotics
generally. A more recent overview is due to Bekey (2008). An excellent book on robot
manipulation addresses advanced topics such as compliant motion (Mason, 2001). Robot
motion planning is covered in Choset et al. (2004) and LaValle (2006). Thrun et al. (2005)
provide an introduction into probabilistic robotics. The premiere conference for robotics is
Robotics: Science and Systems Conference, followed by the IEEE International Conference
on Robotics and Automation. Leading robotics journals include IEEE Robotics and Automation, the International Journal of Robotics Research, and Robotics and Autonomous Systems.

E XERCISES
25.1 Monte Carlo localization is biased for any finite sample sizeâ€”i.e., the expected value
of the location computed by the algorithm differs from the true expected valueâ€”because of
the way particle filtering works. In this question, you are asked to quantify this bias.
To simplify, consider a world with four possible robot locations: X = {x1 , x2 , x3 , x4 }.
Initially, we draw N â‰¥ 1 samples uniformly from among those locations. As usual, it is
perfectly acceptable if more than one sample is generated for any of the locations X. Let Z
be a Boolean sensor variable characterized by the following conditional probabilities:
P (z | x1 ) = 0.8

P (Â¬z | x1 ) = 0.2

P (z | x2 ) = 0.4

P (Â¬z | x2 ) = 0.6

P (z | x3 ) = 0.1

P (Â¬z | x3 ) = 0.9

P (z | x4 ) = 0.1

P (Â¬z | x4 ) = 0.9 .

Exercises

1015

B

A

A

B

Starting configuration <âˆ’0.5, 7>
Figure 25.31

Ending configuration <âˆ’0.5, âˆ’7>

A Robot manipulator in two of its possible configurations.

MCL uses these probabilities to generate particle weights, which are subsequently normalized
and used in the resampling process. For simplicity, let us assume we generate only one new
sample in the resampling process, regardless of N . This sample might correspond to any of
the four locations in X. Thus, the sampling process defines a probability distribution over X.
a. What is the resulting probability distribution over X for this new sample? Answer this
question separately for N = 1, . . . , 10, and for N = âˆž.
b. The difference between two probability distributions P and Q can be measured by the
KL divergence, which is defined as

P (xi )
.
P (xi ) log
KL(P, Q) =
Q(xi )
i

What are the KL divergences between the distributions in (a) and the true posterior?
c. What modification of the problem formulation (not the algorithm!) would guarantee
that the specific estimator above is unbiased even for finite values of N ? Provide at
least two such modifications (each of which should be sufficient).
25.2 Implement Monte Carlo localization for a simulated robot with range sensors. A grid
map and range data are available from the code repository at aima.cs.berkeley.edu.
You should demonstrate successful global localization of the robot.
25.3 Consider a robot with two simple manipulators, as shown in figure 25.31. Manipulator
A is a square block of side 2 which can slide back and on a rod that runs along the x-axis
from x=âˆ’10 to x=10. Manipulator B is a square block of side 2 which can slide back and
on a rod that runs along the y-axis from y=âˆ’10 to y=10. The rods lie outside the plane of

1016

Chapter

25.

Robotics

manipulation, so the rods do not interfere with the movement of the blocks. A configuration
is then a pair x, y where x is the x-coordinate of the center of manipulator A and where y is
the y-coordinate of the center of manipulator B. Draw the configuration space for this robot,
indicating the permitted and excluded zones.
25.4 Suppose that you are working with the robot in Exercise 25.3 and you are given the
problem of finding a path from the starting configuration of figure 25.31 to the ending configuration. Consider a potential function
1
D(A, Goal )2 + D(B, Goal )2 +
D(A, B)2
where D(A, B) is the distance between the closest points of A and B.
a. Show that hill climbing in this potential field will get stuck in a local minimum.
b. Describe a potential field where hill climbing will solve this particular problem. You
need not work out the exact numerical coefficients needed, just the general form of the
solution. (Hint: Add a term that â€œrewardsâ€ the hill climber for moving A out of Bâ€™s
way, even in a case like this where this does not reduce the distance from A to B in the
above sense.)
25.5 Consider the robot arm shown in Figure 25.14. Assume that the robotâ€™s base element
is 60cm long and that its upper arm and forearm are each 40cm long. As argued on page 987,
the inverse kinematics of a robot is often not unique. State an explicit closed-form solution of
the inverse kinematics for this arm. Under what exact conditions is the solution unique?
25.6 Implement an algorithm for calculating the Voronoi diagram of an arbitrary 2D environment, described by an n Ã— n Boolean array. Illustrate your algorithm by plotting the
Voronoi diagram for 10 interesting maps. What is the complexity of your algorithm?
25.7 This exercise explores the relationship between workspace and configuration space
using the examples shown in Figure 25.32.
a. Consider the robot configurations shown in Figure 25.32(a) through (c), ignoring the
obstacle shown in each of the diagrams. Draw the corresponding arm configurations in
configuration space. (Hint: Each arm configuration maps to a single point in configuration space, as illustrated in Figure 25.14(b).)
b. Draw the configuration space for each of the workspace diagrams in Figure 25.32(a)â€“
(c). (Hint: The configuration spaces share with the one shown in Figure 25.32(a) the
region that corresponds to self-collision, but differences arise from the lack of enclosing
obstacles and the different locations of the obstacles in these individual figures.)
c. For each of the black dots in Figure 25.32(e)â€“(f), draw the corresponding configurations
of the robot arm in workspace. Please ignore the shaded regions in this exercise.
d. The configuration spaces shown in Figure 25.32(e)â€“(f) have all been generated by a
single workspace obstacle (dark shading), plus the constraints arising from the selfcollision constraint (light shading). Draw, for each diagram, the workspace obstacle
that corresponds to the darkly shaded area.

Exercises

1017

(a)

(b)

(c)

(d)

(e)

(f)

Figure 25.32

Diagrams for Exercise 25.7.

e. Figure 25.32(d) illustrates that a single planar obstacle can decompose the workspace
into two disconnected regions. What is the maximum number of disconnected regions that can be created by inserting a planar obstacle into an obstacle-free, connected
workspace, for a 2DOF robot? Give an example, and argue why no larger number of
disconnected regions can be created. How about a non-planar obstacle?
25.8 Consider a mobile robot moving on a horizontal surface. Suppose that the robot can
execute two kinds of motions:
â€¢ Rolling forward a specified distance.
â€¢ Rotating in place through a specified angle.
The state of such a robot can be characterized in terms of three parameters x, y, Ï†, the xcoordinate and y-coordinate of the robot (more precisely, of its center of rotation) and the
robotâ€™s orientation expressed as the angle from the positive x direction. The action â€œRoll(D)â€
has the effect of changing state x, y, Ï† to x + D cos(Ï†), y + D sin(Ï†), Ï†, and the action
Rotate(Î¸) has the effect of changing state x, y, Ï† to x, y, Ï† + Î¸.
a. Suppose that the robot is initially at 0, 0, 0 and then executes the actions Rotate(60â—¦ ),
Roll(1), Rotate(25â—¦ ), Roll(2). What is the final state of the robot?

1018

Chapter

25.

Robotics

robot
sensor
range

goal

Figure 25.33

Simplified robot in a maze. See Exercise 25.9.

b. Now suppose that the robot has imperfect control of its own rotation, and that, if it
attempts to rotate by Î¸, it may actually rotate by any angle between Î¸ âˆ’ 10â—¦ and Î¸ + 10â—¦ .
In that case, if the robot attempts to carry out the sequence of actions in (A), there is
a range of possible ending states. What are the minimal and maximal values of the
x-coordinate, the y-coordinate and the orientation in the final state?
c. Let us modify the model in (B) to a probabilistic model in which, when the robot
attempts to rotate by Î¸, its actual angle of rotation follows a Gaussian distribution
with mean Î¸ and standard deviation 10â—¦ . Suppose that the robot executes the actions
Rotate(90â—¦ ), Roll(1). Give a simple argument that (a) the expected value of the location at the end is not equal to the result of rotating exactly 90â—¦ and then rolling forward
1 unit, and (b) that the distribution of locations at the end does not follow a Gaussian.
(Do not attempt to calculate the true mean or the true distribution.)
The point of this exercise is that rotational uncertainty quickly gives rise to a lot of
positional uncertainty and that dealing with rotational uncertainty is painful, whether
uncertainty is treated in terms of hard intervals or probabilistically, due to the fact that
the relation between orientation and position is both non-linear and non-monotonic.
25.9 Consider the simplified robot shown in Figure 25.33. Suppose the robotâ€™s Cartesian
coordinates are known at all times, as are those of its goal location. However, the locations
of the obstacles are unknown. The robot can sense obstacles in its immediate proximity, as
illustrated in this figure. For simplicity, let us assume the robotâ€™s motion is noise-free, and
the state space is discrete. Figure 25.33 is only one example; in this exercise you are required
to address all possible grid worlds with a valid path from the start to the goal location.
a. Design a deliberate controller that guarantees that the robot always reaches its goal
location if at all possible. The deliberate controller can memorize measurements in the
form of a map that is being acquired as the robot moves. Between individual moves, it
may spend arbitrary time deliberating.

Exercises

1019
b. Now design a reactive controller for the same task. This controller may not memorize
past sensor measurements. (It may not build a map!) Instead, it has to make all decisions
based on the current measurement, which includes knowledge of its own location and
that of the goal. The time to make a decision must be independent of the environment
size or the number of past time steps. What is the maximum number of steps that it may
take for your robot to arrive at the goal?
c. How will your controllers from (a) and (b) perform if any of the following six conditions
apply: continuous state space, noise in perception, noise in motion, noise in both perception and motion, unknown location of the goal (the goal can be detected only when
within sensor range), or moving obstacles. For each condition and each controller, give
an example of a situation where the robot fails (or explain why it cannot fail).
25.10 In Figure 25.24(b) on page 1001, we encountered an augmented finite state machine
for the control of a single leg of a hexapod robot. In this exercise, the aim is to design an
AFSM that, when combined with six copies of the individual leg controllers, results in efficient, stable locomotion. For this purpose, you have to augment the individual leg controller
to pass messages to your new AFSM and to wait until other messages arrive. Argue why your
controller is efficient, in that it does not unnecessarily waste energy (e.g., by sliding legs),
and in that it propels the robot at reasonably high speeds. Prove that your controller satisfies
the dynamic stability condition given on page 977.
25.11 (This exercise was first devised by Michael Genesereth and Nils Nilsson. It works
for first graders through graduate students.) Humans are so adept at basic household tasks
that they often forget how complex these tasks are. In this exercise you will discover the
complexity and recapitulate the last 30 years of developments in robotics. Consider the task
of building an arch out of three blocks. Simulate a robot with four humans as follows:
Brain. The Brain direct the hands in the execution of a plan to achieve the goal. The
Brain receives input from the Eyes, but cannot see the scene directly. The brain is the only
one who knows what the goal is.
Eyes. The Eyes report a brief description of the scene to the Brain: â€œThere is a red box
standing on top of a green box, which is on its sideâ€ Eyes can also answer questions from the
Brain such as, â€œIs there a gap between the Left Hand and the red box?â€ If you have a video
camera, point it at the scene and allow the eyes to look at the viewfinder of the video camera,
but not directly at the scene.
Left hand and right hand. One person plays each Hand. The two Hands stand next to
each other, each wearing an oven mitt on one hand, Hands execute only simple commands
from the Brainâ€”for example, â€œLeft Hand, move two inches forward.â€ They cannot execute
commands other than motions; for example, they cannot be commanded to â€œPick up the box.â€
The Hands must be blindfolded. The only sensory capability they have is the ability to tell
when their path is blocked by an immovable obstacle such as a table or the other Hand. In
such cases, they can beep to inform the Brain of the difficulty.

26

PHILOSOPHICAL
FOUNDATIONS

In which we consider what it means to think and whether artifacts could and
should ever do so.

WEAK AI
STRONG AI

26.1

Philosophers have been around far longer than computers and have been trying to resolve
some questions that relate to AI: How do minds work? Is it possible for machines to act
intelligently in the way that people do, and if they did, would they have real, conscious
minds? What are the ethical implications of intelligent machines?
First, some terminology: the assertion that machines could act as if they were intelligent
is called the weak AI hypothesis by philosophers, and the assertion that machines that do so
are actually thinking (not just simulating thinking) is called the strong AI hypothesis.
Most AI researchers take the weak AI hypothesis for granted, and donâ€™t care about the
strong AI hypothesisâ€”as long as their program works, they donâ€™t care whether you call it a
simulation of intelligence or real intelligence. All AI researchers should be concerned with
the ethical implications of their work.

W EAK AI: C AN M ACHINES ACT I NTELLIGENTLY ?
The proposal for the 1956 summer workshop that defined the field of Artificial Intelligence
(McCarthy et al., 1955) made the assertion that â€œEvery aspect of learning or any other feature
of intelligence can be so precisely described that a machine can be made to simulate it.â€ Thus,
AI was founded on the assumption that weak AI is possible. Others have asserted that weak
AI is impossible: â€œArtificial intelligence pursued within the cult of computationalism stands
not even a ghost of a chance of producing durable resultsâ€ (Sayre, 1993).
Clearly, whether AI is impossible depends on how it is defined. In Section 1.1, we defined AI as the quest for the best agent program on a given architecture. With this formulation,
AI is by definition possible: for any digital architecture with k bits of program storage there
are exactly 2k agent programs, and all we have to do to find the best one is enumerate and test
them all. This might not be feasible for large k, but philosophers deal with the theoretical,
not the practical.
1020

Section 26.1.

CAN MACHINES
THINK?
CAN SUBMARINES
SWIM?

TURING TEST

Weak AI: Can Machines Act Intelligently?

1021

Our definition of AI works well for the engineering problem of finding a good agent,
given an architecture. Therefore, weâ€™re tempted to end this section right now, answering the
title question in the affirmative. But philosophers are interested in the problem of comparing two architecturesâ€”human and machine. Furthermore, they have traditionally posed the
question not in terms of maximizing expected utility but rather as, â€œCan machines think?â€
The computer scientist Edsger Dijkstra (1984) said that â€œThe question of whether Machines Can Think . . . is about as relevant as the question of whether Submarines Can Swim.â€
The American Heritage Dictionaryâ€™s first definition of swim is â€œTo move through water by
means of the limbs, fins, or tail,â€ and most people agree that submarines, being limbless,
cannot swim. The dictionary also defines fly as â€œTo move through the air by means of wings
or winglike parts,â€ and most people agree that airplanes, having winglike parts, can fly. However, neither the questions nor the answers have any relevance to the design or capabilities of
airplanes and submarines; rather they are about the usage of words in English. (The fact that
ships do swim in Russian only amplifies this point.). The practical possibility of â€œthinking
machinesâ€ has been with us for only 50 years or so, not long enough for speakers of English to
settle on a meaning for the word â€œthinkâ€â€”does it require â€œa brainâ€ or just â€œbrain-like parts.â€
Alan Turing, in his famous paper â€œComputing Machinery and Intelligenceâ€ (1950), suggested that instead of asking whether machines can think, we should ask whether machines
can pass a behavioral intelligence test, which has come to be called the Turing Test. The test
is for a program to have a conversation (via online typed messages) with an interrogator for
five minutes. The interrogator then has to guess if the conversation is with a program or a
person; the program passes the test if it fools the interrogator 30% of the time. Turing conjectured that, by the year 2000, a computer with a storage of 109 units could be programmed
well enough to pass the test. He was wrongâ€”programs have yet to fool a sophisticated judge.
On the other hand, many people have been fooled when they didnâ€™t know they might
be chatting with a computer. The E LIZA program and Internet chatbots such as M GONZ
(Humphrys, 2008) and NATACHATA have fooled their correspondents repeatedly, and the
chatbot C YBER L OVER has attracted the attention of law enforcement because of its penchant
for tricking fellow chatters into divulging enough personal information that their identity can
be stolen. The Loebner Prize competition, held annually since 1991, is the longest-running
Turing Test-like contest. The competitions have led to better models of human typing errors.
Turing himself examined a wide variety of possible objections to the possibility of intelligent machines, including virtually all of those that have been raised in the half-century
since his paper appeared. We will look at some of them.

26.1.1 The argument from disability
The â€œargument from disabilityâ€ makes the claim that â€œa machine can never do X.â€ As examples of X, Turing lists the following:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right
from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone
fall in love with it, learn from experience, use words properly, be the subject of its own
thought, have as much diversity of behavior as man, do something really new.

1022

Chapter

26.

Philosophical Foundations

In retrospect, some of these are rather easyâ€”weâ€™re all familiar with computers that â€œmake
mistakes.â€ We are also familiar with a century-old technology that has had a proven ability
to â€œmake someone fall in love with itâ€â€”the teddy bear. Computer chess expert David Levy
predicts that by 2050 people will routinely fall in love with humanoid robots (Levy, 2007).
As for a robot falling in love, that is a common theme in fiction,1 but there has been only limited speculation about whether it is in fact likely (Kim et al., 2007). Programs do play chess,
checkers and other games; inspect parts on assembly lines, steer cars and helicopters; diagnose diseases; and do hundreds of other tasks as well as or better than humans. Computers
have made small but significant discoveries in astronomy, mathematics, chemistry, mineralogy, biology, computer science, and other fields. Each of these required performance at the
level of a human expert.
Given what we now know about computers, it is not surprising that they do well at
combinatorial problems such as playing chess. But algorithms also perform at human levels
on tasks that seemingly involve human judgment, or as Turing put it, â€œlearning from experienceâ€ and the ability to â€œtell right from wrong.â€ As far back as 1955, Paul Meehl (see also
Grove and Meehl, 1996) studied the decision-making processes of trained experts at subjective tasks such as predicting the success of a student in a training program or the recidivism
of a criminal. In 19 out of the 20 studies he looked at, Meehl found that simple statistical
learning algorithms (such as linear regression or naive Bayes) predict better than the experts.
The Educational Testing Service has used an automated program to grade millions of essay
questions on the GMAT exam since 1999. The program agrees with human graders 97% of
the time, about the same level that two human graders agree (Burstein et al., 2001).
It is clear that computers can do many things as well as or better than humans, including
things that people believe require great human insight and understanding. This does not mean,
of course, that computers use insight and understanding in performing these tasksâ€”those are
not part of behavior, and we address such questions elsewhereâ€”but the point is that oneâ€™s
first guess about the mental processes required to produce a given behavior is often wrong. It
is also true, of course, that there are many tasks at which computers do not yet excel (to put
it mildly), including Turingâ€™s task of carrying on an open-ended conversation.

26.1.2 The mathematical objection
It is well known, through the work of Turing (1936) and GoÌˆdel (1931), that certain mathematical questions are in principle unanswerable by particular formal systems. GoÌˆdelâ€™s incompleteness theorem (see Section 9.5) is the most famous example of this. Briefly, for any
formal axiomatic system F powerful enough to do arithmetic, it is possible to construct a
so-called GoÌˆdel sentence G(F ) with the following properties:
â€¢ G(F ) is a sentence of F , but cannot be proved within F .
â€¢ If F is consistent, then G(F ) is true.
1

For example, the opera CoppeÌlia (1870), the novel Do Androids Dream of Electric Sheep? (1968), the movies
AI (2001) and Wall-E (2008), and in song, Noel Cowardâ€™s 1955 version of Letâ€™s Do It: Letâ€™s Fall in Love predicted
â€œprobably weâ€™ll live to see machines do it.â€ He didnâ€™t.

Section 26.1.

Weak AI: Can Machines Act Intelligently?

1023

Philosophers such as J. R. Lucas (1961) have claimed that this theorem shows that machines
are mentally inferior to humans, because machines are formal systems that are limited by the
incompleteness theoremâ€”they cannot establish the truth of their own GoÌˆdel sentenceâ€”while
humans have no such limitation. This claim has caused decades of controversy, spawning a
vast literature, including two books by the mathematician Sir Roger Penrose (1989, 1994)
that repeat the claim with some fresh twists (such as the hypothesis that humans are different
because their brains operate by quantum gravity). We will examine only three of the problems
with the claim.
First, GoÌˆdelâ€™s incompleteness theorem applies only to formal systems that are powerful
enough to do arithmetic. This includes Turing machines, and Lucasâ€™s claim is in part based
on the assertion that computers are Turing machines. This is a good approximation, but is not
quite true. Turing machines are infinite, whereas computers are finite, and any computer can
therefore be described as a (very large) system in propositional logic, which is not subject to
GoÌˆdelâ€™s incompleteness theorem. Second, an agent should not be too ashamed that it cannot
establish the truth of some sentence while other agents can. Consider the sentence
J. R. Lucas cannot consistently assert that this sentence is true.

If Lucas asserted this sentence, then he would be contradicting himself, so therefore Lucas
cannot consistently assert it, and hence it must be true. We have thus demonstrated that there
is a sentence that Lucas cannot consistently assert while other people (and machines) can. But
that does not make us think less of Lucas. To take another example, no human could compute
the sum of a billion 10 digit numbers in his or her lifetime, but a computer could do it in
seconds. Still, we do not see this as a fundamental limitation in the humanâ€™s ability to think.
Humans were behaving intelligently for thousands of years before they invented mathematics,
so it is unlikely that formal mathematical reasoning plays more than a peripheral role in what
it means to be intelligent.
Third, and most important, even if we grant that computers have limitations on what
they can prove, there is no evidence that humans are immune from those limitations. It is
all too easy to show rigorously that a formal system cannot do X, and then claim that humans can do X using their own informal method, without giving any evidence for this claim.
Indeed, it is impossible to prove that humans are not subject to GoÌˆdelâ€™s incompleteness theorem, because any rigorous proof would require a formalization of the claimed unformalizable
human talent, and hence refute itself. So we are left with an appeal to intuition that humans
can somehow perform superhuman feats of mathematical insight. This appeal is expressed
with arguments such as â€œwe must assume our own consistency, if thought is to be possible at
allâ€ (Lucas, 1976). But if anything, humans are known to be inconsistent. This is certainly
true for everyday reasoning, but it is also true for careful mathematical thought. A famous
example is the four-color map problem. Alfred Kempe published a proof in 1879 that was
widely accepted and contributed to his election as a Fellow of the Royal Society. In 1890,
however, Percy Heawood pointed out a flaw and the theorem remained unproved until 1977.

1024

Chapter

26.

Philosophical Foundations

26.1.3 The argument from informality

QUALIFICATION
PROBLEM

One of the most influential and persistent criticisms of AI as an enterprise was raised by Turing as the â€œargument from informality of behavior.â€ Essentially, this is the claim that human
behavior is far too complex to be captured by any simple set of rules and that because computers can do no more than follow a set of rules, they cannot generate behavior as intelligent
as that of humans. The inability to capture everything in a set of logical rules is called the
qualification problem in AI.
The principal proponent of this view has been the philosopher Hubert Dreyfus, who
has produced a series of influential critiques of artificial intelligence: What Computers Canâ€™t
Do (1972), the sequel What Computers Still Canâ€™t Do (1992), and, with his brother Stuart,
Mind Over Machine (1986).
The position they criticize came to be called â€œGood Old-Fashioned AI,â€ or GOFAI, a
term coined by philosopher John Haugeland (1985). GOFAI is supposed to claim that all
intelligent behavior can be captured by a system that reasons logically from a set of facts and
rules describing the domain. It therefore corresponds to the simplest logical agent described
in Chapter 7. Dreyfus is correct in saying that logical agents are vulnerable to the qualification
problem. As we saw in Chapter 13, probabilistic reasoning systems are more appropriate for
open-ended domains. The Dreyfus critique therefore is not addressed against computers per
se, but rather against one particular way of programming them. It is reasonable to suppose,
however, that a book called What First-Order Logical Rule-Based Systems Without Learning
Canâ€™t Do might have had less impact.
Under Dreyfusâ€™s view, human expertise does include knowledge of some rules, but only
as a â€œholistic contextâ€ or â€œbackgroundâ€ within which humans operate. He gives the example
of appropriate social behavior in giving and receiving gifts: â€œNormally one simply responds
in the appropriate circumstances by giving an appropriate gift.â€ One apparently has â€œa direct
sense of how things are done and what to expect.â€ The same claim is made in the context of
chess playing: â€œA mere chess master might need to figure out what to do, but a grandmaster
just sees the board as demanding a certain move . . . the right response just pops into his or her
head.â€ It is certainly true that much of the thought processes of a present-giver or grandmaster
is done at a level that is not open to introspection by the conscious mind. But that does not
mean that the thought processes do not exist. The important question that Dreyfus does not
answer is how the right move gets into the grandmasterâ€™s head. One is reminded of Daniel
Dennettâ€™s (1984) comment,
It is rather as if philosophers were to proclaim themselves expert explainers of the methods of stage magicians, and then, when we ask how the magician does the sawing-thelady-in-half trick, they explain that it is really quite obvious: the magician doesnâ€™t really
saw her in half; he simply makes it appear that he does. â€œBut how does he do that?â€ we
ask. â€œNot our department,â€ say the philosophers.

Dreyfus and Dreyfus (1986) propose a five-stage process of acquiring expertise, beginning
with rule-based processing (of the sort proposed in GOFAI) and ending with the ability to
select correct responses instantaneously. In making this proposal, Dreyfus and Dreyfus in
effect move from being AI critics to AI theoristsâ€”they propose a neural network architecture

Section 26.1.

Weak AI: Can Machines Act Intelligently?

1025

organized into a vast â€œcase library,â€ but point out several problems. Fortunately, all of their
problems have been addressed, some with partial success and some with total success. Their
problems include the following:
1. Good generalization from examples cannot be achieved without background knowledge. They claim no one has any idea how to incorporate background knowledge into
the neural network learning process. In fact, we saw in Chapters 19 and 20 that there
are techniques for using prior knowledge in learning algorithms. Those techniques,
however, rely on the availability of knowledge in explicit form, something that Dreyfus
and Dreyfus strenuously deny. In our view, this is a good reason for a serious redesign
of current models of neural processing so that they can take advantage of previously
learned knowledge in the way that other learning algorithms do.
2. Neural network learning is a form of supervised learning (see Chapter 18), requiring
the prior identification of relevant inputs and correct outputs. Therefore, they claim,
it cannot operate autonomously without the help of a human trainer. In fact, learning
without a teacher can be accomplished by unsupervised learning (Chapter 20) and
reinforcement learning (Chapter 21).
3. Learning algorithms do not perform well with many features, and if we pick a subset
of features, â€œthere is no known way of adding new features should the current set prove
inadequate to account for the learned facts.â€ In fact, new methods such as support
vector machines handle large feature sets very well. With the introduction of large
Web-based data sets, many applications in areas such as language processing (Sha and
Pereira, 2003) and computer vision (Viola and Jones, 2002a) routinely handle millions
of features. We saw in Chapter 19 that there are also principled ways to generate new
features, although much more work is needed.
4. The brain is able to direct its sensors to seek relevant information and to process it
to extract aspects relevant to the current situation. But, Dreyfus and Dreyfus claim,
â€œCurrently, no details of this mechanism are understood or even hypothesized in a way
that could guide AI research.â€ In fact, the field of active vision, underpinned by the
theory of information value (Chapter 16), is concerned with exactly the problem of
directing sensors, and already some robots have incorporated the theoretical results
obtained. S TANLEY â€™s 132-mile trip through the desert (page 28) was made possible in
large part by an active sensing system of this kind.
In sum, many of the issues Dreyfus has focused onâ€”background commonsense knowledge,
the qualification problem, uncertainty, learning, compiled forms of decision makingâ€”are
indeed important issues, and have by now been incorporated into standard intelligent agent
design. In our view, this is evidence of AIâ€™s progress, not of its impossibility.
One of Dreyfusâ€™ strongest arguments is for situated agents rather than disembodied
logical inference engines. An agent whose understanding of â€œdogâ€ comes only from a limited
set of logical sentences such as â€œDog(x) â‡’ Mammal (x)â€ is at a disadvantage compared
to an agent that has watched dogs run, has played fetch with them, and has been licked by
one. As philosopher Andy Clark (1998) says, â€œBiological brains are first and foremost the
control systems for biological bodies. Biological bodies move and act in rich real-world

1026

EMBODIED
COGNITION

26.2

Chapter

26.

Philosophical Foundations

surroundings.â€ To understand how human (or other animal) agents work, we have to consider
the whole agent, not just the agent program. Indeed, the embodied cognition approach claims
that it makes no sense to consider the brain separately: cognition takes place within a body,
which is embedded in an environment. We need to study the system as a whole; the brain
augments its reasoning by referring to the environment, as the reader does in perceiving (and
creating) marks on paper to transfer knowledge. Under the embodied cognition program,
robotics, vision, and other sensors become central, not peripheral.

S TRONG AI: C AN M ACHINES R EALLY T HINK ?
Many philosophers have claimed that a machine that passes the Turing Test would still not
be actually thinking, but would be only a simulation of thinking. Again, the objection was
foreseen by Turing. He cites a speech by Professor Geoffrey Jefferson (1949):
Not until a machine could write a sonnet or compose a concerto because of thoughts and
emotions felt, and not by the chance fall of symbols, could we agree that machine equals
brainâ€”that is, not only write it but know that it had written it.

Turing calls this the argument from consciousnessâ€”the machine has to be aware of its own
mental states and actions. While consciousness is an important subject, Jeffersonâ€™s key point
actually relates to phenomenology, or the study of direct experience: the machine has to
actually feel emotions. Others focus on intentionalityâ€”that is, the question of whether the
machineâ€™s purported beliefs, desires, and other representations are actually â€œaboutâ€ something in the real world.
Turingâ€™s response to the objection is interesting. He could have presented reasons that
machines can in fact be conscious (or have phenomenology, or have intentions). Instead, he
maintains that the question is just as ill-defined as asking, â€œCan machines think?â€ Besides,
why should we insist on a higher standard for machines than we do for humans? After all,
in ordinary life we never have any direct evidence about the internal mental states of other
humans. Nevertheless, Turing says, â€œInstead of arguing continually over this point, it is usual
to have the polite convention that everyone thinks.â€
Turing argues that Jefferson would be willing to extend the polite convention to machines if only he had experience with ones that act intelligently. He cites the following dialog,
which has become such a part of AIâ€™s oral tradition that we simply have to include it:
HUMAN:

In the first line of your sonnet which reads â€œshall I compare thee to a summerâ€™s
day,â€ would not a â€œspring dayâ€ do as well or better?
MACHINE: It wouldnâ€™t scan.
HUMAN: How about â€œa winterâ€™s day.â€ That would scan all right.
MACHINE: Yes, but nobody wants to be compared to a winterâ€™s day.
HUMAN: Would you say Mr. Pickwick reminded you of Christmas?
MACHINE: In a way.
HUMAN: Yet Christmas is a winterâ€™s day, and I do not think Mr. Pickwick would mind
the comparison.

Section 26.2.

Strong AI: Can Machines Really Think?

1027

MACHINE:

I donâ€™t think youâ€™re serious. By a winterâ€™s day one means a typical winterâ€™s
day, rather than a special one like Christmas.

One can easily imagine some future time in which such conversations with machines are
commonplace, and it becomes customary to make no linguistic distinction between â€œrealâ€
and â€œartificialâ€ thinking. A similar transition occurred in the years after 1848, when artificial
urea was synthesized for the first time by Frederick WoÌˆhler. Prior to this event, organic and
inorganic chemistry were essentially disjoint enterprises and many thought that no process
could exist that would convert inorganic chemicals into organic material. Once the synthesis
was accomplished, chemists agreed that artificial urea was urea, because it had all the right
physical properties. Those who had posited an intrinsic property possessed by organic material that inorganic material could never have were faced with the impossibility of devising
any test that could reveal the supposed deficiency of artificial urea.
For thinking, we have not yet reached our 1848 and there are those who believe that
artificial thinking, no matter how impressive, will never be real. For example, the philosopher
John Searle (1980) argues as follows:
No one supposes that a computer simulation of a storm will leave us all wet . . . Why on
earth would anyone in his right mind suppose a computer simulation of mental processes
actually had mental processes? (pp. 37â€“38)

MINDâ€“BODY
PROBLEM

DUALISM

While it is easy to agree that computer simulations of storms do not make us wet, it is not
clear how to carry this analogy over to computer simulations of mental processes. After
all, a Hollywood simulation of a storm using sprinklers and wind machines does make the
actors wet, and a video game simulation of a storm does make the simulated characters wet.
Most people are comfortable saying that a computer simulation of addition is addition, and
of chess is chess. In fact, we typically speak of an implementation of addition or chess, not a
simulation. Are mental processes more like storms, or more like addition?
Turingâ€™s answerâ€”the polite conventionâ€”suggests that the issue will eventually go
away by itself once machines reach a certain level of sophistication. This would have the
effect of dissolving the difference between weak and strong AI. Against this, one may insist
that there is a factual issue at stake: humans do have real minds, and machines might or
might not. To address this factual issue, we need to understand how it is that humans have
real minds, not just bodies that generate neurophysiological processes. Philosophical efforts
to solve this mindâ€“body problem are directly relevant to the question of whether machines
could have real minds.
The mindâ€“body problem was considered by the ancient Greek philosophers and by various schools of Hindu thought, but was first analyzed in depth by the 17th-century French
philosopher and mathematician ReneÌ Descartes. His Meditations on First Philosophy (1641)
considered the mindâ€™s activity of thinking (a process with no spatial extent or material properties) and the physical processes of the body, concluding that the two must exist in separate
realmsâ€”what we would now call a dualist theory. The mindâ€“body problem faced by dualists is the question of how the mind can control the body if the two are really separate.
Descartes speculated that the two might interact through the pineal gland, which simply begs
the question of how the mind controls the pineal gland.

1028
MONISM
PHYSICALISM

MENTAL STATES

Chapter

26.

Philosophical Foundations

The monist theory of mind, often called physicalism, avoids this problem by asserting
the mind is not separate from the bodyâ€”that mental states are physical states. Most modern
philosophers of mind are physicalists of one form or another, and physicalism allows, at least
in principle, for the possibility of strong AI. The problem for physicalists is to explain how
physical statesâ€”in particular, the molecular configurations and electrochemical processes of
the brainâ€”can simultaneously be mental states, such as being in pain, enjoying a hamburger,
knowing that one is riding a horse, or believing that Vienna is the capital of Austria.

26.2.1 Mental states and the brain in a vat

INTENTIONAL STATE

WIDE CONTENT

NARROW CONTENT

Physicalist philosophers have attempted to explicate what it means to say that a personâ€”and,
by extension, a computerâ€”is in a particular mental state. They have focused in particular on
intentional states. These are states, such as believing, knowing, desiring, fearing, and so on,
that refer to some aspect of the external world. For example, the knowledge that one is eating
a hamburger is a belief about the hamburger and what is happening to it.
If physicalism is correct, it must be the case that the proper description of a personâ€™s
mental state is determined by that personâ€™s brain state. Thus, if I am currently focused on
eating a hamburger in a mindful way, my instantaneous brain state is an instance of the class of
mental states â€œknowing that one is eating a hamburger.â€ Of course, the specific configurations
of all the atoms of my brain are not essential: there are many configurations of my brain, or
of other peopleâ€™s brain, that would belong to the same class of mental states. The key point is
that the same brain state could not correspond to a fundamentally distinct mental state, such
as the knowledge that one is eating a banana.
The simplicity of this view is challenged by some simple thought experiments. Imagine, if you will, that your brain was removed from your body at birth and placed in a marvelously engineered vat. The vat sustains your brain, allowing it to grow and develop. At the
same time, electronic signals are fed to your brain from a computer simulation of an entirely
fictitious world, and motor signals from your brain are intercepted and used to modify the
simulation as appropriate.2 In fact, the simulated life you live replicates exactly the life you
would have lived, had your brain not been placed in the vat, including simulated eating of
simulated hamburgers. Thus, you could have a brain state identical to that of someone who is
really eating a real hamburger, but it would be literally false to say that you have the mental
state â€œknowing that one is eating a hamburger.â€ You arenâ€™t eating a hamburger, you have
never even experienced a hamburger, and you could not, therefore, have such a mental state.
This example seems to contradict the view that brain states determine mental states. One
way to resolve the dilemma is to say that the content of mental states can be interpreted from
two different points of view. The â€œwide contentâ€ view interprets it from the point of view
of an omniscient outside observer with access to the whole situation, who can distinguish
differences in the world. Under this view, the content of mental states involves both the brain
state and the environment history. Narrow content, on the other hand, considers only the
brain state. The narrow content of the brain states of a real hamburger-eater and a brain-in-avat â€œhamburgerâ€-â€œeaterâ€ is the same in both cases.
2

This situation may be familiar to those who have seen the 1999 film The Matrix.

Section 26.2.

Strong AI: Can Machines Really Think?

1029

Wide content is entirely appropriate if oneâ€™s goals are to ascribe mental states to others
who share oneâ€™s world, to predict their likely behavior and its effects, and so on. This is the
setting in which our ordinary language about mental content has evolved. On the other hand,
if one is concerned with the question of whether AI systems are really thinking and really
do have mental states, then narrow content is appropriate; it simply doesnâ€™t make sense to
say that whether or not an AI system is really thinking depends on conditions outside that
system. Narrow content is also relevant if we are thinking about designing AI systems or
understanding their operation, because it is the narrow content of a brain state that determines
what will be the (narrow content of the) next brain state. This leads naturally to the idea that
what matters about a brain stateâ€”what makes it have one kind of mental content and not
anotherâ€”is its functional role within the mental operation of the entity involved.

26.2.2 Functionalism and the brain replacement experiment
FUNCTIONALISM

The theory of functionalism says that a mental state is any intermediate causal condition
between input and output. Under functionalist theory, any two systems with isomorphic
causal processes would have the same mental states. Therefore, a computer program could
have the same mental states as a person. Of course, we have not yet said what â€œisomorphicâ€
really means, but the assumption is that there is some level of abstraction below which the
specific implementation does not matter.
The claims of functionalism are illustrated most clearly by the brain replacement experiment. This thought experiment was introduced by the philosopher Clark Glymour and
was touched on by John Searle (1980), but is most commonly associated with roboticist Hans
Moravec (1988). It goes like this: Suppose neurophysiology has developed to the point where
the inputâ€“output behavior and connectivity of all the neurons in the human brain are perfectly
understood. Suppose further that we can build microscopic electronic devices that mimic this
behavior and can be smoothly interfaced to neural tissue. Lastly, suppose that some miraculous surgical technique can replace individual neurons with the corresponding electronic
devices without interrupting the operation of the brain as a whole. The experiment consists
of gradually replacing all the neurons in someoneâ€™s head with electronic devices.
We are concerned with both the external behavior and the internal experience of the
subject, during and after the operation. By the definition of the experiment, the subjectâ€™s
external behavior must remain unchanged compared with what would be observed if the
operation were not carried out. 3 Now although the presence or absence of consciousness
cannot easily be ascertained by a third party, the subject of the experiment ought at least to
be able to record any changes in his or her own conscious experience. Apparently, there is
a direct clash of intuitions as to what would happen. Moravec, a robotics researcher and
functionalist, is convinced his consciousness would remain unaffected. Searle, a philosopher
and biological naturalist, is equally convinced his consciousness would vanish:
You find, to your total amazement, that you are indeed losing control of your external
behavior. You find, for example, that when doctors test your vision, you hear them say
â€œWe are holding up a red object in front of you; please tell us what you see.â€ You want
3

One can imagine using an identical â€œcontrolâ€ subject who is given a placebo operation, for comparison.

1030

Chapter

26.

Philosophical Foundations

to cry out â€œI canâ€™t see anything. Iâ€™m going totally blind.â€ But you hear your voice saying
in a way that is completely out of your control, â€œI see a red object in front of me.â€ . . .
your conscious experience slowly shrinks to nothing, while your externally observable
behavior remains the same. (Searle, 1992)

One can do more than argue from intuition. First, note that, for the external behavior to remain the same while the subject gradually becomes unconscious, it must be the case that the
subjectâ€™s volition is removed instantaneously and totally; otherwise the shrinking of awareness would be reflected in external behaviorâ€”â€œHelp, Iâ€™m shrinking!â€ or words to that effect.
This instantaneous removal of volition as a result of gradual neuron-at-a-time replacement
seems an unlikely claim to have to make.
Second, consider what happens if we do ask the subject questions concerning his or
her conscious experience during the period when no real neurons remain. By the conditions
of the experiment, we will get responses such as â€œI feel fine. I must say Iâ€™m a bit surprised
because I believed Searleâ€™s argument.â€ Or we might poke the subject with a pointed stick and
observe the response, â€œOuch, that hurt.â€ Now, in the normal course of affairs, the skeptic can
dismiss such outputs from AI programs as mere contrivances. Certainly, it is easy enough to
use a rule such as â€œIf sensor 12 reads â€˜Highâ€™ then output â€˜Ouch.â€™ â€ But the point here is that,
because we have replicated the functional properties of a normal human brain, we assume
that the electronic brain contains no such contrivances. Then we must have an explanation of
the manifestations of consciousness produced by the electronic brain that appeals only to the
functional properties of the neurons. And this explanation must also apply to the real brain,
which has the same functional properties. There are three possible conclusions:
1. The causal mechanisms of consciousness that generate these kinds of outputs in normal
brains are still operating in the electronic version, which is therefore conscious.
2. The conscious mental events in the normal brain have no causal connection to behavior,
and are missing from the electronic brain, which is therefore not conscious.
3. The experiment is impossible, and therefore speculation about it is meaningless.
EPIPHENOMENON

Although we cannot rule out the second possibility, it reduces consciousness to what philosophers call an epiphenomenal roleâ€”something that happens, but casts no shadow, as it were,
on the observable world. Furthermore, if consciousness is indeed epiphenomenal, then it
cannot be the case that the subject says â€œOuchâ€ because it hurtsâ€”that is, because of the conscious experience of pain. Instead, the brain must contain a second, unconscious mechanism
that is responsible for the â€œOuch.â€
Patricia Churchland (1986) points out that the functionalist arguments that operate at
the level of the neuron can also operate at the level of any larger functional unitâ€”a clump
of neurons, a mental module, a lobe, a hemisphere, or the whole brain. That means that if
you accept the notion that the brain replacement experiment shows that the replacement brain
is conscious, then you should also believe that consciousness is maintained when the entire
brain is replaced by a circuit that updates its state and maps from inputs to outputs via a huge
lookup table. This is disconcerting to many people (including Turing himself), who have
the intuition that lookup tables are not consciousâ€”or at least, that the conscious experiences
generated during table lookup are not the same as those generated during the operation of a

Section 26.2.

Strong AI: Can Machines Really Think?

1031

system that might be described (even in a simple-minded, computational sense) as accessing
and generating beliefs, introspections, goals, and so on.

26.2.3 Biological naturalism and the Chinese Room
BIOLOGICAL
NATURALISM

A strong challenge to functionalism has been mounted by John Searleâ€™s (1980) biological
naturalism, according to which mental states are high-level emergent features that are caused
by low-level physical processes in the neurons, and it is the (unspecified) properties of the
neurons that matter. Thus, mental states cannot be duplicated just on the basis of some program having the same functional structure with the same inputâ€“output behavior; we would
require that the program be running on an architecture with the same causal power as neurons.
To support his view, Searle describes a hypothetical system that is clearly running a program
and passes the Turing Test, but that equally clearly (according to Searle) does not understand
anything of its inputs and outputs. His conclusion is that running the appropriate program
(i.e., having the right outputs) is not a sufficient condition for being a mind.
The system consists of a human, who understands only English, equipped with a rule
book, written in English, and various stacks of paper, some blank, some with indecipherable
inscriptions. (The human therefore plays the role of the CPU, the rule book is the program,
and the stacks of paper are the storage device.) The system is inside a room with a small
opening to the outside. Through the opening appear slips of paper with indecipherable symbols. The human finds matching symbols in the rule book, and follows the instructions. The
instructions may include writing symbols on new slips of paper, finding symbols in the stacks,
rearranging the stacks, and so on. Eventually, the instructions will cause one or more symbols
to be transcribed onto a piece of paper that is passed back to the outside world.
So far, so good. But from the outside, we see a system that is taking input in the form
of Chinese sentences and generating answers in Chinese that are as â€œintelligentâ€ as those
in the conversation imagined by Turing.4 Searle then argues: the person in the room does
not understand Chinese (given). The rule book and the stacks of paper, being just pieces of
paper, do not understand Chinese. Therefore, there is no understanding of Chinese. Hence,
according to Searle, running the right program does not necessarily generate understanding.
Like Turing, Searle considered and attempted to rebuff a number of replies to his argument. Several commentators, including John McCarthy and Robert Wilensky, proposed
what Searle calls the systems reply. The objection is that asking if the human in the room
understands Chinese is analogous to asking if the CPU can take cube roots. In both cases,
the answer is no, and in both cases, according to the systems reply, the entire system does
have the capacity in question. Certainly, if one asks the Chinese Room whether it understands
Chinese, the answer would be affirmative (in fluent Chinese). By Turingâ€™s polite convention,
this should be enough. Searleâ€™s response is to reiterate the point that the understanding is not
in the human and cannot be in the paper, so there cannot be any understanding. He seems to
be relying on the argument that a property of the whole must reside in one of the parts. Yet
4 The fact that the stacks of paper might contain trillions of pages and the generation of answers would take
millions of years has no bearing on the logical structure of the argument. One aim of philosophical training is to
develop a finely honed sense of which objections are germane and which are not.

1032

Chapter

26.

Philosophical Foundations

water is wet, even though neither H nor O2 is. The real claim made by Searle rests upon the
following four axioms (Searle, 1990):
1.
2.
3.
4.

INTUITION PUMP

Computer programs are formal (syntactic).
Human minds have mental contents (semantics).
Syntax by itself is neither constitutive of nor sufficient for semantics.
Brains cause minds.

From the first three axioms Searle concludes that programs are not sufficient for minds. In
other words, an agent running a program might be a mind, but it is not necessarily a mind just
by virtue of running the program. From the fourth axiom he concludes â€œAny other system
capable of causing minds would have to have causal powers (at least) equivalent to those
of brains.â€ From there he infers that any artificial brain would have to duplicate the causal
powers of brains, not just run a particular program, and that human brains do not produce
mental phenomena solely by virtue of running a program.
The axioms are controversial. For example, axioms 1 and 2 rely on an unspecified
distinction between syntax and semantics that seems to be closely related to the distinction
between narrow and wide content. On the one hand, we can view computers as manipulating
syntactic symbols; on the other, we can view them as manipulating electric current, which
happens to be what brains mostly do (according to our current understanding). So it seems
we could equally say that brains are syntactic.
Assuming we are generous in interpreting the axioms, then the conclusionâ€”that programs are not sufficient for mindsâ€”does follow. But the conclusion is unsatisfactoryâ€”all
Searle has shown is that if you explicitly deny functionalism (that is what his axiom 3 does),
then you canâ€™t necessarily conclude that non-brains are minds. This is reasonable enoughâ€”
almost tautologicalâ€”so the whole argument comes down to whether axiom 3 can be accepted. According to Searle, the point of the Chinese Room argument is to provide intuitions
for axiom 3. The public reaction shows that the argument is acting as what Daniel Dennett
(1991) calls an intuition pump: it amplifies oneâ€™s prior intuitions, so biological naturalists
are more convinced of their positions, and functionalists are convinced only that axiom 3 is
unsupported, or that in general Searleâ€™s argument is unconvincing. The argument stirs up
combatants, but has done little to change anyoneâ€™s opinion. Searle remains undeterred, and
has recently started calling the Chinese Room a â€œrefutationâ€ of strong AI rather than just an
â€œargumentâ€ (Snell, 2008).
Even those who accept axiom 3, and thus accept Searleâ€™s argument, have only their intuitions to fall back on when deciding what entities are minds. The argument purports to show
that the Chinese Room is not a mind by virtue of running the program, but the argument says
nothing about how to decide whether the room (or a computer, some other type of machine,
or an alien) is a mind by virtue of some other reason. Searle himself says that some machines
do have minds: humans are biological machines with minds. According to Searle, human
brains may or may not be running something like an AI program, but if they are, that is not
the reason they are minds. It takes more to make a mindâ€”according to Searle, something
equivalent to the causal powers of individual neurons. What these powers are is left unspecified. It should be noted, however, that neurons evolved to fulfill functional rolesâ€”creatures

Section 26.2.

Strong AI: Can Machines Really Think?

1033

with neurons were learning and deciding long before consciousness appeared on the scene. It
would be a remarkable coincidence if such neurons just happened to generate consciousness
because of some causal powers that are irrelevant to their functional capabilities; after all, it
is the functional capabilities that dictate survival of the organism.
In the case of the Chinese Room, Searle relies on intuition, not proof: just look at the
room; whatâ€™s there to be a mind? But one could make the same argument about the brain:
just look at this collection of cells (or of atoms), blindly operating according to the laws of
biochemistry (or of physics)â€”whatâ€™s there to be a mind? Why can a hunk of brain be a mind
while a hunk of liver cannot? That remains the great mystery.

26.2.4 Consciousness, qualia, and the explanatory gap

CONSCIOUSNESS

QUALIA

INVERTED
SPECTRUM

EXPLANATORY GAP

Running through all the debates about strong AIâ€”the elephant in the debating room, so
to speakâ€”is the issue of consciousness. Consciousness is often broken down into aspects
such as understanding and self-awareness. The aspect we will focus on is that of subjective
experience: why it is that it feels like something to have certain brain states (e.g., while eating
a hamburger), whereas it presumably does not feel like anything to have other physical states
(e.g., while being a rock). The technical term for the intrinsic nature of experiences is qualia
(from the Latin word meaning, roughly, â€œsuch thingsâ€).
Qualia present a challenge for functionalist accounts of the mind because different
qualia could be involved in what are otherwise isomorphic causal processes. Consider, for
example, the inverted spectrum thought experiment, which the subjective experience of person X when seeing red objects is the same experience that the rest of us experience when
seeing green objects, and vice versa. X still calls red objects â€œred,â€ stops for red traffic lights,
and agrees that the redness of red traffic lights is a more intense red than the redness of the
setting sun. Yet, Xâ€™s subjective experience is just different.
Qualia are challenging not just for functionalism but for all of science. Suppose, for the
sake of argument, that we have completed the process of scientific research on the brainâ€”we
have found that neural process P12 in neuron N177 transforms molecule A into molecule B,
and so on, and on. There is simply no currently accepted form of reasoning that would lead
from such findings to the conclusion that the entity owning those neurons has any particular
subjective experience. This explanatory gap has led some philosophers to conclude that
humans are simply incapable of forming a proper understanding of their own consciousness.
Others, notably Daniel Dennett (1991), avoid the gap by denying the existence of qualia,
attributing them to a philosophical confusion.
Turing himself concedes that the question of consciousness is a difficult one, but denies
that it has much relevance to the practice of AI: â€œI do not wish to give the impression that I
think there is no mystery about consciousness . . . But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in
this paper.â€ We agree with Turingâ€”we are interested in creating programs that behave intelligently. The additional project of making them conscious is not one that we are equipped to
take on, nor one whose success we would be able to determine.

1034

26.3

Chapter

26.

Philosophical Foundations

T HE E THICS AND R ISKS OF D EVELOPING A RTIFICIAL I NTELLIGENCE
So far, we have concentrated on whether we can develop AI, but we must also consider
whether we should. If the effects of AI technology are more likely to be negative than positive,
then it would be the moral responsibility of workers in the field to redirect their research.
Many new technologies have had unintended negative side effects: nuclear fission brought
Chernobyl and the threat of global destruction; the internal combustion engine brought air
pollution, global warming, and the paving-over of paradise. In a sense, automobiles are
robots that have conquered the world by making themselves indispensable.
All scientists and engineers face ethical considerations of how they should act on the
job, what projects should or should not be done, and how they should be handled. See the
handbook on the Ethics of Computing (Berleur and Brunnstein, 2001). AI, however, seems
to pose some fresh problems beyond that of, say, building bridges that donâ€™t fall down:
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢

People might lose their jobs to automation.
People might have too much (or too little) leisure time.
People might lose their sense of being unique.
AI systems might be used toward undesirable ends.
The use of AI systems might result in a loss of accountability.
The success of AI might mean the end of the human race.

We will look at each issue in turn.
People might lose their jobs to automation. The modern industrial economy has become dependent on computers in general, and select AI programs in particular. For example,
much of the economy, especially in the United States, depends on the availability of consumer credit. Credit card applications, charge approvals, and fraud detection are now done
by AI programs. One could say that thousands of workers have been displaced by these AI
programs, but in fact if you took away the AI programs these jobs would not exist, because
human labor would add an unacceptable cost to the transactions. So far, automation through
information technology in general and AI in particular has created more jobs than it has
eliminated, and has created more interesting, higher-paying jobs. Now that the canonical AI
program is an â€œintelligent agentâ€ designed to assist a human, loss of jobs is less of a concern
than it was when AI focused on â€œexpert systemsâ€ designed to replace humans. But some
researchers think that doing the complete job is the right goal for AI. In reflecting on the 25th
Anniversary of the AAAI, Nils Nilsson (2005) set as a challenge the creation of human-level
AI that could pass the employment test rather than the Turing Testâ€”a robot that could learn
to do any one of a range of jobs. We may end up in a future where unemployment is high, but
even the unemployed serve as managers of their own cadre of robot workers.
People might have too much (or too little) leisure time. Alvin Toffler wrote in Future
Shock (1970), â€œThe work week has been cut by 50 percent since the turn of the century. It
is not out of the way to predict that it will be slashed in half again by 2000.â€ Arthur C.
Clarke (1968b) wrote that people in 2001 might be â€œfaced with a future of utter boredom,
where the main problem in life is deciding which of several hundred TV channels to select.â€

Section 26.3.

The Ethics and Risks of Developing Artificial Intelligence

1035

The only one of these predictions that has come close to panning out is the number of TV
channels. Instead, people working in knowledge-intensive industries have found themselves
part of an integrated computerized system that operates 24 hours a day; to keep up, they have
been forced to work longer hours. In an industrial economy, rewards are roughly proportional
to the time invested; working 10% more would tend to mean a 10% increase in income. In
an information economy marked by high-bandwidth communication and easy replication of
intellectual property (what Frank and Cook (1996) call the â€œWinner-Take-All Societyâ€), there
is a large reward for being slightly better than the competition; working 10% more could mean
a 100% increase in income. So there is increasing pressure on everyone to work harder. AI
increases the pace of technological innovation and thus contributes to this overall trend, but
AI also holds the promise of allowing us to take some time off and let our automated agents
handle things for a while. Tim Ferriss (2007) recommends using automation and outsourcing
to achieve a four-hour work week.
People might lose their sense of being unique. In Computer Power and Human Reason, Weizenbaum (1976), the author of the E LIZA program, points out some of the potential
threats that AI poses to society. One of Weizenbaumâ€™s principal arguments is that AI research
makes possible the idea that humans are automataâ€”an idea that results in a loss of autonomy
or even of humanity. We note that the idea has been around much longer than AI, going back
at least to Lâ€™Homme Machine (La Mettrie, 1748). Humanity has survived other setbacks to
our sense of uniqueness: De Revolutionibus Orbium Coelestium (Copernicus, 1543) moved
the Earth away from the center of the solar system, and Descent of Man (Darwin, 1871) put
Homo sapiens at the same level as other species. AI, if widely successful, may be at least as
threatening to the moral assumptions of 21st-century society as Darwinâ€™s theory of evolution
was to those of the 19th century.
AI systems might be used toward undesirable ends. Advanced technologies have
often been used by the powerful to suppress their rivals. As the number theorist G. H. Hardy
wrote (Hardy, 1940), â€œA science is said to be useful if its development tends to accentuate the
existing inequalities in the distribution of wealth, or more directly promotes the destruction
of human life.â€ This holds for all sciences, AI being no exception. Autonomous AI systems
are now commonplace on the battlefield; the U.S. military deployed over 5,000 autonomous
aircraft and 12,000 autonomous ground vehicles in Iraq (Singer, 2009). One moral theory
holds that military robots are like medieval armor taken to its logical extreme: no one would
have moral objections to a soldier wanting to wear a helmet when being attacked by large,
angry, axe-wielding enemies, and a teleoperated robot is like a very safe form of armor. On
the other hand, robotic weapons pose additional risks. To the extent that human decision
making is taken out of the firing loop, robots may end up making decisions that lead to the
killing of innocent civilians. At a larger scale, the possession of powerful robots (like the
possession of sturdy helmets) may give a nation overconfidence, causing it to go to war more
recklessly than necessary. In most wars, at least one party is overconfident in its military
abilitiesâ€”otherwise the conflict would have been resolved peacefully.
Weizenbaum (1976) also pointed out that speech recognition technology could lead to
widespread wiretapping, and hence to a loss of civil liberties. He didnâ€™t foresee a world with
terrorist threats that would change the balance of how much surveillance people are willing to

1036

Chapter

26.

Philosophical Foundations

accept, but he did correctly recognize that AI has the potential to mass-produce surveillance.
His prediction has in part come true: the U.K. now has an extensive network of surveillance
cameras, and other countries routinely monitor Web traffic and telephone calls. Some accept
that computerization leads to a loss of privacyâ€”Sun Microsystems CEO Scott McNealy has
said â€œYou have zero privacy anyway. Get over it.â€ David Brin (1998) argues that loss of
privacy is inevitable, and the way to combat the asymmetry of power of the state over the
individual is to make the surveillance accessible to all citizens. Etzioni (2004) argues for a
balancing of privacy and security; individual rights and community.
The use of AI systems might result in a loss of accountability. In the litigious atmosphere that prevails in the United States, legal liability becomes an important issue. When a
physician relies on the judgment of a medical expert system for a diagnosis, who is at fault if
the diagnosis is wrong? Fortunately, due in part to the growing influence of decision-theoretic
methods in medicine, it is now accepted that negligence cannot be shown if the physician
performs medical procedures that have high expected utility, even if the actual result is catastrophic for the patient. The question should therefore be â€œWho is at fault if the diagnosis is
unreasonable?â€ So far, courts have held that medical expert systems play the same role as
medical textbooks and reference books; physicians are responsible for understanding the reasoning behind any decision and for using their own judgment in deciding whether to accept
the systemâ€™s recommendations. In designing medical expert systems as agents, therefore,
the actions should be thought of not as directly affecting the patient but as influencing the
physicianâ€™s behavior. If expert systems become reliably more accurate than human diagnosticians, doctors might become legally liable if they donâ€™t use the recommendations of an expert
system. Atul Gawande (2002) explores this premise.
Similar issues are beginning to arise regarding the use of intelligent agents on the Internet. Some progress has been made in incorporating constraints into intelligent agents so that
they cannot, for example, damage the files of other users (Weld and Etzioni, 1994). The problem is magnified when money changes hands. If monetary transactions are made â€œon oneâ€™s
behalfâ€ by an intelligent agent, is one liable for the debts incurred? Would it be possible for
an intelligent agent to have assets itself and to perform electronic trades on its own behalf?
So far, these questions do not seem to be well understood. To our knowledge, no program
has been granted legal status as an individual for the purposes of financial transactions; at
present, it seems unreasonable to do so. Programs are also not considered to be â€œdriversâ€
for the purposes of enforcing traffic regulations on real highways. In California law, at least,
there do not seem to be any legal sanctions to prevent an automated vehicle from exceeding
the speed limits, although the designer of the vehicleâ€™s control mechanism would be liable in
the case of an accident. As with human reproductive technology, the law has yet to catch up
with the new developments.
The success of AI might mean the end of the human race. Almost any technology
has the potential to cause harm in the wrong hands, but with AI and robotics, we have the new
problem that the wrong hands might belong to the technology itself. Countless science fiction
stories have warned about robots or robotâ€“human cyborgs running amok. Early examples

Section 26.3.

The Ethics and Risks of Developing Artificial Intelligence

1037

include Mary Shelleyâ€™s Frankenstein, or the Modern Prometheus (1818)5 and Karel Capekâ€™s
play R.U.R. (1921), in which robots conquer the world. In movies, we have The Terminator
(1984), which combines the cliches of robots-conquer-the-world with time travel, and The
Matrix (1999), which combines robots-conquer-the-world with brain-in-a-vat.
It seems that robots are the protagonists of so many conquer-the-world stories because
they represent the unknown, just like the witches and ghosts of tales from earlier eras, or the
Martians from The War of the Worlds (Wells, 1898). The question is whether an AI system
poses a bigger risk than traditional software. We will look at three sources of risk.
First, the AI systemâ€™s state estimation may be incorrect, causing it to do the wrong
thing. For example, an autonomous car might incorrectly estimate the position of a car in the
adjacent lane, leading to an accident that might kill the occupants. More seriously, a missile
defense system might erroneously detect an attack and launch a counterattack, leading to
the death of billions. These risks are not really risks of AI systemsâ€”in both cases the same
mistake could just as easily be made by a human as by a computer. The correct way to mitigate
these risks is to design a system with checks and balances so that a single state-estimation
error does not propagate through the system unchecked.
Second, specifying the right utility function for an AI system to maximize is not so
easy. For example, we might propose a utility function designed to minimize human suffering,
expressed as an additive reward function over time as in Chapter 17. Given the way humans
are, however, weâ€™ll always find a way to suffer even in paradise; so the optimal decision for
the AI system is to terminate the human race as soon as possibleâ€”no humans, no suffering.
With AI systems, then, we need to be very careful what we ask for, whereas humans would
have no trouble realizing that the proposed utility function cannot be taken literally. On the
other hand, computers need not be tainted by the irrational behaviors described in Chapter 16.
Humans sometimes use their intelligence in aggressive ways because humans have some
innately aggressive tendencies, due to natural selection. The machines we build need not be
innately aggressive, unless we decide to build them that way (or unless they emerge as the
end product of a mechanism design that encourages aggressive behavior). Fortunately, there
are techniques, such as apprenticeship learning, that allows us to specify a utility function by
example. One can hope that a robot that is smart enough to figure out how to terminate the
human race is also smart enough to figure out that that was not the intended utility function.
Third, the AI systemâ€™s learning function may cause it to evolve into a system with
unintended behavior. This scenario is the most serious, and is unique to AI systems, so we
will cover it in more depth. I. J. Good wrote (1965),
ULTRAINTELLIGENT
MACHINE

Let an ultraintelligent machine be defined as a machine that can far surpass all the
intellectual activities of any man however clever. Since the design of machines is one of
these intellectual activities, an ultraintelligent machine could design even better machines;
there would then unquestionably be an â€œintelligence explosion,â€ and the intelligence of
man would be left far behind. Thus the first ultraintelligent machine is the last invention
that man need ever make, provided that the machine is docile enough to tell us how to
keep it under control.
5

As a young man, Charles Babbage was influenced by reading Frankenstein.

1038
TECHNOLOGICAL
SINGULARITY

TRANSHUMANISM

Chapter

26.

Philosophical Foundations

The â€œintelligence explosionâ€ has also been called the technological singularity by mathematics professor and science fiction author Vernor Vinge, who writes (1993), â€œWithin thirty
years, we will have the technological means to create superhuman intelligence. Shortly after,
the human era will be ended.â€ Good and Vinge (and many others) correctly note that the curve
of technological progress (on many measures) is growing exponentially at present (consider
Mooreâ€™s Law). However, it is a leap to extrapolate that the curve will continue to a singularity
of near-infinite growth. So far, every other technology has followed an S-shaped curve, where
the exponential growth eventually tapers off. Sometimes new technologies step in when the
old ones plateau; sometimes we hit hard limits. With less than a century of high-technology
history to go on, it is difficult to extrapolate hundreds of years ahead.
Note that the concept of ultraintelligent machines assumes that intelligence is an especially important attribute, and if you have enough of it, all problems can be solved. But
we know there are limits on computability and computational complexity. If the problem
of defining ultraintelligent machines (or even approximations to them) happens to fall in the
class of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then
even exponential progress in technology wonâ€™t helpâ€”the speed of light puts a strict upper
bound on how much computing can be done; problems beyond that limit will not be solved.
We still donâ€™t know where those upper bounds are.
Vinge is concerned about the coming singularity, but some computer scientists and
futurists relish it. Hans Moravec (2000) encourages us to give every advantage to our â€œmind
children,â€ the robots we create, which may surpass us in intelligence. There is even a new
wordâ€”transhumanismâ€”for the active social movement that looks forward to this future in
which humans are merged withâ€”or replaced byâ€”robotic and biotech inventions. Suffice it
to say that such issues present a challenge for most moral theorists, who take the preservation
of human life and the human species to be a good thing. Ray Kurzweil is currently the most
visible advocate for the singularity view, writing in The Singularity is Near (2005):
The Singularity will allow us to transcend these limitations of our biological bodies and
brain. We will gain power over our fates. Our mortality will be in our own hands. We
will be able to live as long as we want (a subtly different statement from saying we will
live forever). We will fully understand human thinking and will vastly extend and expand
its reach. By the end of this century, the nonbiological portion of our intelligence will be
trillions of trillions of times more powerful than unaided human intelligence.

Kurzweil also notes the potential dangers, writing â€œBut the Singularity will also amplify the
ability to act on our destructive inclinations, so its full story has not yet been written.â€
If ultraintelligent machines are a possibility, we humans would do well to make sure
that we design their predecessors in such a way that they design themselves to treat us well.
Science fiction writer Isaac Asimov (1942) was the first to address this issue, with his three
laws of robotics:
1. A robot may not injure a human being or, through inaction, allow a human being to
come to harm.
2. A robot must obey orders given to it by human beings, except where such orders would
conflict with the First Law.

Section 26.3.

The Ethics and Risks of Developing Artificial Intelligence

1039

3. A robot must protect its own existence as long as such protection does not conflict with
the First or Second Law.

FRIENDLY AI

These laws seem reasonable, at least to us humans.6 But the trick is how to implement these
laws. In the Asimov story Roundabout a robot is sent to fetch some selenium. Later the
robot is found wandering in a circle around the selenium source. Every time it heads toward
the source, it senses a danger, and the third law causes it to veer away. But every time it
veers away, the danger recedes, and the power of the second law takes over, causing it to
veer back towards the selenium. The set of points that define the balancing point between
the two laws defines a circle. This suggests that the laws are not logical absolutes, but rather
are weighed against each other, with a higher weighting for the earlier laws. Asimov was
probably thinking of an architecture based on control theoryâ€”perhaps a linear combination
of factorsâ€”while today the most likely architecture would be a probabilistic reasoning agent
that reasons over probability distributions of outcomes, and maximizes utility as defined by
the three laws. But presumably we donâ€™t want our robots to prevent a human from crossing
the street because of the nonzero chance of harm. That means that the negative utility for
harm to a human must be much greater than for disobeying, but that each of the utilities is
finite, not infinite.
Yudkowsky (2008) goes into more detail about how to design a Friendly AI. He asserts
that friendliness (a desire not to harm humans) should be designed in from the start, but that
the designers should recognize both that their own designs may be flawed, and that the robot
will learn and evolve over time. Thus the challenge is one of mechanism designâ€”to define a
mechanism for evolving AI systems under a system of checks and balances, and to give the
systems utility functions that will remain friendly in the face of such changes.
We canâ€™t just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time. For example, if technology had allowed
us to design a super-powerful AI agent in 1800 and endow it with the prevailing morals of
the time, it would be fighting today to reestablish slavery and abolish womenâ€™s right to vote.
On the other hand, if we build an AI agent today and tell it to evolve its utility function, how
can we assure that it wonâ€™t reason that â€œHumans think it is moral to kill annoying insects, in
part because insect brains are so primitive. But human brains are primitive compared to my
powers, so it must be moral for me to kill humans.â€
Omohundro (2008) hypothesizes that even an innocuous chess program could pose a
risk to society. Similarly, Marvin Minsky once suggested that an AI program designed to
solve the Riemann Hypothesis might end up taking over all the resources of Earth to build
more powerful supercomputers to help achieve its goal. The moral is that even if you only
want your program to play chess or prove theorems, if you give it the capability to learn
and alter itself, you need safeguards. Omohundro concludes that â€œSocial structures which
cause individuals to bear the cost of their negative externalities would go a long way toward
ensuring a stable and positive future,â€ This seems to be an excellent idea for society in general,
regardless of the possibility of ultraintelligent machines.
6

A robot might notice the inequity that a human is allowed to kill another in self-defense, but a robot is required
to sacrifice its own life to save a human.

1040

Chapter

26.

Philosophical Foundations

We should note that the idea of safeguards against change in utility function is not a
new one. In the Odyssey, Homer (ca. 700 B . C .) described Ulyssesâ€™ encounter with the sirens,
whose song was so alluring it compelled sailors to cast themselves into the sea. Knowing it
would have that effect on him, Ulysses ordered his crew to bind him to the mast so that he
could not perform the self-destructive act. It is interesting to think how similar safeguards
could be built into AI systems.
Finally, let us consider the robotâ€™s point of view. If robots become conscious, then to
treat them as mere â€œmachinesâ€ (e.g., to take them apart) might be immoral. Science fiction
writers have addressed the issue of robot rights. The movie A.I. (Spielberg, 2001) was based
on a story by Brian Aldiss about an intelligent robot who was programmed to believe that
he was human and fails to understand his eventual abandonment by his ownerâ€“mother. The
story (and the movie) argue for the need for a civil rights movement for robots.

26.4

S UMMARY
This chapter has addressed the following issues:
â€¢ Philosophers use the term weak AI for the hypothesis that machines could possibly
behave intelligently, and strong AI for the hypothesis that such machines would count
as having actual minds (as opposed to simulated minds).
â€¢ Alan Turing rejected the question â€œCan machines think?â€ and replaced it with a behavioral test. He anticipated many objections to the possibility of thinking machines.
Few AI researchers pay attention to the Turing Test, preferring to concentrate on their
systemsâ€™ performance on practical tasks, rather than the ability to imitate humans.
â€¢ There is general agreement in modern times that mental states are brain states.
â€¢ Arguments for and against strong AI are inconclusive. Few mainstream AI researchers
believe that anything significant hinges on the outcome of the debate.
â€¢ Consciousness remains a mystery.
â€¢ We identified six potential threats to society posed by AI and related technology. We
concluded that some of the threats are either unlikely or differ little from threats posed
by â€œunintelligentâ€ technologies. One threat in particular is worthy of further consideration: that ultraintelligent machines might lead to a future that is very different from
todayâ€”we may not like it, and at that point we may not have a choice. Such considerations lead inevitably to the conclusion that we must weigh carefully, and soon, the
possible consequences of AI research.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

Sources for the various responses to Turingâ€™s 1950 paper and for the main critics of weak
AI were given in the chapter. Although it became fashionable in the post-neural-network era

Bibliographical and Historical Notes

TWIN EARTHS

1041

to deride symbolic approaches, not all philosophers are critical of GOFAI. Some are, in fact,
ardent advocates and even practitioners. Zenon Pylyshyn (1984) has argued that cognition
can best be understood through a computational model, not only in principle but also as a
way of conducting research at present, and has specifically rebutted Dreyfusâ€™s criticisms of
the computational model of human cognition (Pylyshyn, 1974). Gilbert Harman (1983), in
analyzing belief revision, makes connections with AI research on truth maintenance systems.
Michael Bratman has applied his â€œbelief-desire-intentionâ€ model of human psychology (Bratman, 1987) to AI research on planning (Bratman, 1992). At the extreme end of strong AI,
Aaron Sloman (1978, p. xiii) has even described as â€œracialistâ€ the claim by Joseph Weizenbaum (1976) that intelligent machines can never be regarded as persons.
Proponents of the importance of embodiment in cognition include the philosophers
Merleau-Ponty, whose Phenomenology of Perception (1945) stressed the importance of the
body and the subjective interpretation of reality afforded by our senses, and Heidegger, whose
Being and Time (1927) asked what it means to actually be an agent, and criticized all of the
history of philosophy for taking this notion for granted. In the computer age, Alva Noe (2009)
and Andy Clark (1998, 2008) propose that our brains form a rather minimal representation
of the world, use the world itself in a just-in-time basis to maintain the illusion of a detailed
internal model, use props in the world (such as paper and pencil as well as computers) to
increase the capabilities of the mind. Pfeifer et al. (2006) and Lakoff and Johnson (1999)
present arguments for how the body helps shape cognition.
The nature of the mind has been a standard topic of philosophical theorizing from ancient times to the present. In the Phaedo, Plato specifically considered and rejected the idea
that the mind could be an â€œattunementâ€ or pattern of organization of the parts of the body, a
viewpoint that approximates the functionalist viewpoint in modern philosophy of mind. He
decided instead that the mind had to be an immortal, immaterial soul, separable from the
body and different in substanceâ€”the viewpoint of dualism. Aristotle distinguished a variety
of souls (Greek ÏˆÏ…Ï‡Î·) in living things, some of which, at least, he described in a functionalist
manner. (See Nussbaum (1978) for more on Aristotleâ€™s functionalism.)
Descartes is notorious for his dualistic view of the human mind, but ironically his historical influence was toward mechanism and physicalism. He explicitly conceived of animals as
automata, and he anticipated the Turing Test, writing â€œit is not conceivable [that a machine]
should produce different arrangements of words so as to give an appropriately meaningful
answer to whatever is said in its presence, as even the dullest of men can doâ€ (Descartes,
1637). Descartesâ€™s spirited defense of the animals-as-automata viewpoint actually had the
effect of making it easier to conceive of humans as automata as well, even though he himself
did not take this step. The book Lâ€™Homme Machine (La Mettrie, 1748) did explicitly argue
that humans are automata.
Modern analytic philosophy has typically accepted physicalism, but the variety of views
on the content of mental states is bewildering. The identification of mental states with brain
states is usually attributed to Place (1956) and Smart (1959). The debate between narrowcontent and wide-content views of mental states was triggered by Hilary Putnam (1975), who
introduced so-called twin earths (rather than brain-in-a-vat, as we did in the chapter) as a
device to generate identical brain states with different (wide) content.

1042

Chapter

26.

Philosophical Foundations

Functionalism is the philosophy of mind most naturally suggested by AI. The idea that
mental states correspond to classes of brain states defined functionally is due to Putnam
(1960, 1967) and Lewis (1966, 1980). Perhaps the most forceful proponent of functionalism is Daniel Dennett, whose ambitiously titled work Consciousness Explained (Dennett,
1991) has attracted many attempted rebuttals. Metzinger (2009) argues there is no such thing
as an objective self, that consciousness is the subjective appearance of a world. The inverted
spectrum argument concerning qualia was introduced by John Locke (1690). Frank Jackson (1982) designed an influential thought experiment involving Mary, a color scientist who
has been brought up in an entirely black-and-white world. Thereâ€™s Something About Mary
(Ludlow et al., 2004) collects several papers on this topic.
Functionalism has come under attack from authors who claim that they do not account
for the qualia or â€œwhat itâ€™s likeâ€ aspect of mental states (Nagel, 1974). Searle has focused
instead on the alleged inability of functionalism to account for intentionality (Searle, 1980,
1984, 1992). Churchland and Churchland (1982) rebut both these types of criticism. The
Chinese Room has been debated endlessly (Searle, 1980, 1990; Preston and Bishop, 2002).
Weâ€™ll just mention here a related work: Terry Bissonâ€™s (1990) science fiction story Theyâ€™re
Made out of Meat, in which alien robotic explorers who visit earth are incredulous to find
thinking human beings whose minds are made of meat. Presumably, the robotic alien equivalent of Searle believes that he can think due to the special causal powers of robotic circuits;
causal powers that mere meat-brains do not possess.
Ethical issues in AI predate the existence of the field itself. I. J. Goodâ€™s (1965) ultraintelligent machine idea was foreseen a hundred years earlier by Samuel Butler (1863).
Written four years after the publication of Darwinâ€™s On the Origins of Species and at a time
when the most sophisticated machines were steam engines, Butlerâ€™s article on Darwin Among
the Machines envisioned â€œthe ultimate development of mechanical consciousnessâ€ by natural
selection. The theme was reiterated by George Dyson (1998) in a book of the same title.
The philosophical literature on minds, brains, and related topics is large and difficult to
read without training in the terminology and methods of argument employed. The Encyclopedia of Philosophy (Edwards, 1967) is an impressively authoritative and very useful aid in
this process. The Cambridge Dictionary of Philosophy (Audi, 1999) is a shorter and more
accessible work, and the online Stanford Encyclopedia of Philosophy offers many excellent
articles and up-to-date references. The MIT Encyclopedia of Cognitive Science (Wilson and
Keil, 1999) covers the philosophy of mind as well as the biology and psychology of mind.
There are several general introductions to the philosophical â€œAI questionâ€ (Boden, 1990;
Haugeland, 1985; Copeland, 1993; McCorduck, 2004; Minsky, 2007). The Behavioral and
Brain Sciences, abbreviated BBS, is a major journal devoted to philosophical and scientific
debates about AI and neuroscience. Topics of ethics and responsibility in AI are covered in
the journals AI and Society and Journal of Artificial Intelligence and Law.

Exercises

1043

E XERCISES
26.1 Go through Turingâ€™s list of alleged â€œdisabilitiesâ€ of machines, identifying which have
been achieved, which are achievable in principle by a program, and which are still problematic because they require conscious mental states.
26.2 Find and analyze an account in the popular media of one or more of the arguments to
the effect that AI is impossible.
26.3 In the brain replacement argument, it is important to be able to restore the subjectâ€™s
brain to normal, such that its external behavior is as it would have been if the operation had
not taken place. Can the skeptic reasonably object that this would require updating those
neurophysiological properties of the neurons relating to conscious experience, as distinct
from those involved in the functional behavior of the neurons?
26.4 Suppose that a Prolog program containing many clauses about the rules of British
citizenship is compiled and run on an ordinary computer. Analyze the â€œbrain statesâ€ of the
computer under wide and narrow content.
26.5 Alan Perlis (1982) wrote, â€œA year spent in artificial intelligence is enough to make one
believe in Godâ€. He also wrote, in a letter to Philip Davis, that one of the central dreams of
computer science is that â€œthrough the performance of computers and their programs we will
remove all doubt that there is only a chemical distinction between the living and nonliving
world.â€ To what extent does the progress made so far in artificial intelligence shed light on
these issues? Suppose that at some future date, the AI endeavor has been completely successful; that is, we have build intelligent agents capable of carrying out any human cognitive task
at human levels of ability. To what extent would that shed light on these issues?
26.6 Compare the social impact of artificial intelligence in the last fifty years with the social
impact of the introduction of electric appliances and the internal combustion engine in the
fifty years between 1890 and 1940.
26.7 I. J. Good claims that intelligence is the most important quality, and that building
ultraintelligent machines will change everything. A sentient cheetah counters that â€œActually
speed is more important; if we could build ultrafast machines, that would change everything,â€
and a sentient elephant claims â€œYouâ€™re both wrong; what we need is ultrastrong machines.â€
What do you think of these arguments?
26.8 Analyze the potential threats from AI technology to society. What threats are most serious, and how might they be combated? How do they compare to the potential benefits?
26.9 How do the potential threats from AI technology compare with those from other computer science technologies, and to bio-, nano-, and nuclear technologies?
26.10 Some critics object that AI is impossible, while others object that it is too possible
and that ultraintelligent machines pose a threat. Which of these objections do you think is
more likely? Would it be a contradiction for someone to hold both positions?

27

AI: THE PRESENT AND
FUTURE

In which we take stock of where we are and where we are going, this being a good
thing to do before continuing.

In Chapter 2, we suggested that it would be helpful to view the AI task as that of designing
rational agentsâ€”that is, agents whose actions maximize their expected utility given their
percept histories. We showed that the design problem depends on the percepts and actions
available to the agent, the utility function that the agentâ€™s behavior should satisfy, and the
nature of the environment. A variety of different agent designs are possible, ranging from
reflex agents to fully deliberative, knowledge-based, decision-theoretic agents. Moreover,
the components of these designs can have a number of different instantiationsâ€”for example,
logical or probabilistic reasoning, and atomic, factored, or structured representations of states.
The intervening chapters presented the principles by which these components operate.
For all the agent designs and components, there has been tremendous progress both in
our scientific understanding and in our technological capabilities. In this chapter, we stand
back from the details and ask, â€œWill all this progress lead to a general-purpose intelligent
agent that can perform well in a wide variety of environments?â€ Section 27.1 looks at the
components of an intelligent agent to assess whatâ€™s known and whatâ€™s missing. Section 27.2
does the same for the overall agent architecture. Section 27.3 asks whether designing rational
agents is the right goal in the first place. (The answer is, â€œNot really, but itâ€™s OK for now.â€)
Finally, Section 27.4 examines the consequences of success in our endeavors.

27.1

AGENT C OMPONENTS
Chapter 2 presented several agent designs and their components. To focus our discussion
here, we will look at the utility-based agent, which we show again in Figure 27.1. When endowed with a learning component (Figure 2.15), this is the most general of our agent designs.
Letâ€™s see where the state of the art stands for each of the components.
Interaction with the environment through sensors and actuators: For much of the
history of AI, this has been a glaring weak point. With a few honorable exceptions, AI systems were built in such a way that humans had to supply the inputs and interpret the outputs,
1044

Section 27.1.

Agent Components

1045

Sensors
State
What the world
is like now

What my actions do

What it will be like
if I do action A

Utility

How happy I will be
in such a state

Environment

How the world evolves

What action I
should do now

Agent
Figure 27.1

Actuators

A model-based, utility-based agent, as first presented in Figure 2.14.

while robotic systems focused on low-level tasks in which high-level reasoning and planning were largely absent. This was due in part to the great expense and engineering effort
required to get real robots to work at all. The situation has changed rapidly in recent years
with the availability of ready-made programmable robots. These, in turn, have benefited
from small, cheap, high-resolution CCD cameras and compact, reliable motor drives. MEMS
(micro-electromechanical systems) technology has supplied miniaturized accelerometers, gyroscopes, and actuators for an artificial flying insect (Floreano et al., 2009). It may also be
possible to combine millions of MEMS devices to produce powerful macroscopic actuators.
Thus, we see that AI systems are at the cusp of moving from primarily software-only
systems to embedded robotic systems. The state of robotics today is roughly comparable to
the state of personal computers in about 1980: at that time researchers and hobbyists could
experiment with PCs, but it would take another decade before they became commonplace.
Keeping track of the state of the world: This is one of the core capabilities required
for an intelligent agent. It requires both perception and updating of internal representations.
Chapter 4 showed how to keep track of atomic state representations; Chapter 7 described
how to do it for factored (propositional) state representations; Chapter 12 extended this to
first-order logic; and Chapter 15 described filtering algorithms for probabilistic reasoning in
uncertain environments. Current filtering and perception algorithms can be combined to do a
reasonable job of reporting low-level predicates such as â€œthe cup is on the table.â€ Detecting
higher-level actions, such as â€œDr. Russell is having a cup of tea with Dr. Norvig while discussing plans for next week,â€ is more difficult. Currently it can be done (see Figure 24.25 on
page 961) only with the help of annotated examples.
Another problem is that, although the approximate filtering algorithms from Chapter 15
can handle quite large environments, they are still dealing with a factored representationâ€”
they have random variables, but do not represent objects and relations explicitly. Section 14.6
explained how probability and first-order logic can be combined to solve this problem, and

1046

Chapter

27.

AI: The Present and Future

Section 14.6.3 showed how we can handle uncertainty about the identity of objects. We expect
that the application of these ideas for tracking complex environments will yield huge benefits.
However, we are still faced with a daunting task of defining general, reusable representation
schemes for complex domains. As discussed in Chapter 12, we donâ€™t yet know how to do that
in general; only for isolated, simple domains. It is possible that a new focus on probabilistic
rather than logical representation coupled with aggressive machine learning (rather than handencoding of knowledge) will allow for progress.
Projecting, evaluating, and selecting future courses of action: The basic knowledgerepresentation requirements here are the same as for keeping track of the world; the primary
difficulty is coping with courses of actionâ€”such as having a conversation or a cup of teaâ€”
that consist eventually of thousands or millions of primitive steps for a real agent. It is only
by imposing hierarchical structure on behavior that we humans cope at all. We saw in
Section 11.2 how to use hierarchical representations to handle problems of this scale; furthermore, work in hierarchical reinforcement learning has succeeded in combining some
of these ideas with the techniques for decision making under uncertainty described in Chapter 17. As yet, algorithms for the partially observable case (POMDPs) are using the same
atomic state representation we used for the search algorithms of Chapter 3. There is clearly a
great deal of work to do here, but the technical foundations are largely in place. Section 27.2
discusses the question of how the search for effective long-range plans might be controlled.
Utility as an expression of preferences: In principle, basing rational decisions on the
maximization of expected utility is completely general and avoids many of the problems of
purely goal-based approaches, such as conflicting goals and uncertain attainment. As yet,
however, there has been very little work on constructing realistic utility functionsâ€”imagine,
for example, the complex web of interacting preferences that must be understood by an agent
operating as an office assistant for a human being. It has proven very difficult to decompose
preferences over complex states in the same way that Bayes nets decompose beliefs over
complex states. One reason may be that preferences over states are really compiled from
preferences over state histories, which are described by reward functions (see Chapter 17).
Even if the reward function is simple, the corresponding utility function may be very complex.
This suggests that we take seriously the task of knowledge engineering for reward functions
as a way of conveying to our agents what it is that we want them to do.
Learning: Chapters 18 to 21 described how learning in an agent can be formulated as
inductive learning (supervised, unsupervised, or reinforcement-based) of the functions that
constitute the various components of the agent. Very powerful logical and statistical techniques have been developed that can cope with quite large problems, reaching or exceeding
human capabilities in many tasksâ€”as long as we are dealing with a predefined vocabulary
of features and concepts. On the other hand, machine learning has made very little progress
on the important problem of constructing new representations at levels of abstraction higher
than the input vocabulary. In computer vision, for example, learning complex concepts such
as Classroom and Cafeteria would be made unnecessarily difficult if the agent were forced
to work from pixels as the input representation; instead, the agent needs to be able to form
intermediate concepts first, such as Desk and Tray , without explicit human supervision.
Similar considerations apply to learning behavior: HavingACupOfTea is a very important

Section 27.2.

DEEP BELIEF
NETWORKS

27.2

HYBRID
ARCHITECTURE

REAL-TIME AI

Agent Architectures

1047

high-level step in many plans, but how does it get into an action library that initially contains
much simpler actions such as RaiseArm and Swallow ? Perhaps this will incorporate some
of the ideas of deep belief networksâ€”Bayesian networks that have multiple layers of hidden
variables, as in the work of Hinton et al. (2006), Hawkins and Blakeslee (2004), and Bengio
and LeCun (2007).
The vast majority of machine learning research today assumes a factored representation, learning a function h : Rn â†’ R for regression and h : Rn â†’ {0, 1} for classification.
Learning researchers will need to adapt their very successful techniques for factored representations to structured representations, particularly hierarchical representations. The work
on inductive logic programming in Chapter 19 is a first step in this direction; the logical next
step is to combine these ideas with the probabilistic languages of Section 14.6.
Unless we understand such issues, we are faced with the daunting task of constructing
large commonsense knowledge bases by hand, an approach that has not fared well to date.
There is great promise in using the Web as a source of natural language text, images, and
videos to serve as a comprehensive knowledge base, but so far machine learning algorithms
are limited in the amount of organized knowledge they can extract from these sources.

AGENT A RCHITECTURES
It is natural to ask, â€œWhich of the agent architectures in Chapter 2 should an agent use?â€
The answer is, â€œAll of them!â€ We have seen that reflex responses are needed for situations
in which time is of the essence, whereas knowledge-based deliberation allows the agent to
plan ahead. A complete agent must be able to do both, using a hybrid architecture. One
important property of hybrid architectures is that the boundaries between different decision
components are not fixed. For example, compilation continually converts declarative information at the deliberative level into more efficient representations, eventually reaching the
reflex levelâ€”see Figure 27.2. (This is the purpose of explanation-based learning, as discussed
in Chapter 19.) Agent architectures such as S OAR (Laird et al., 1987) and T HEO (Mitchell,
1990) have exactly this structure. Every time they solve a problem by explicit deliberation,
they save away a generalized version of the solution for use by the reflex component. A
less studied problem is the reversal of this process: when the environment changes, learned
reflexes may no longer be appropriate and the agent must return to the deliberative level to
produce new behaviors.
Agents also need ways to control their own deliberations. They must be able to cease
deliberating when action is demanded, and they must be able to use the time available for
deliberation to execute the most profitable computations. For example, a taxi-driving agent
that sees an accident ahead must decide in a split second either to brake or to take evasive
action. It should also spend that split second thinking about the most important questions,
such as whether the lanes to the left and right are clear and whether there is a large truck
close behind, rather than worrying about wear and tear on the tires or where to pick up the
next passenger. These issues are usually studied under the heading of real-time AI. As AI

1048

Chapter

27.

AI: The Present and Future

Knowledge-based
deliberation
Compilation

Percepts

Reflex system

Actions

Figure 27.2 Compilation serves to convert deliberative decision making into more efficient, reflexive mechanisms.

ANYTIME
ALGORITHM

DECISIONTHEORETIC
METAREASONING

REFLECTIVE
ARCHITECTURE

systems move into more complex domains, all problems will become real-time, because the
agent will never have long enough to solve the decision problem exactly.
Clearly, there is a pressing need for general methods of controlling deliberation, rather
than specific recipes for what to think about in each situation. The first useful idea is to employ anytime algorithms (Dean and Boddy, 1988; Horvitz, 1987). An anytime algorithm is
an algorithm whose output quality improves gradually over time, so that it has a reasonable
decision ready whenever it is interrupted. Such algorithms are controlled by a metalevel decision procedure that assesses whether further computation is worthwhile. (See Section 3.5.4
for a brief description of metalevel decision making.) Example of an anytime algorithms
include iterative deepening in game-tree search and MCMC in Bayesian networks.
The second technique for controlling deliberation is decision-theoretic metareasoning
(Russell and Wefald, 1989, 1991; Horvitz, 1989; Horvitz and Breese, 1996). This method
applies the theory of information value (Chapter 16) to the selection of individual computations. The value of a computation depends on both its cost (in terms of delaying action) and
its benefits (in terms of improved decision quality). Metareasoning techniques can be used to
design better search algorithms and to guarantee that the algorithms have the anytime property. Metareasoning is expensive, of course, and compilation methods can be applied so that
the overhead is small compared to the costs of the computations being controlled. Metalevel
reinforcement learning may provide another way to acquire effective policies for controlling
deliberation: in essence, computations that lead to better decisions are reinforced, while those
that turn out to have no effect are penalized. This approach avoids the myopia problems of
the simple value-of-information calculation.
Metareasoning is one specific example of a reflective architectureâ€”that is, an architecture that enables deliberation about the computational entities and actions occurring within
the architecture itself. A theoretical foundation for reflective architectures can be built by
defining a joint state space composed from the environment state and the computational state
of the agent itself. Decision-making and learning algorithms can be designed that operate
over this joint state space and thereby serve to implement and improve the agentâ€™s computational activities. Eventually, we expect task-specific algorithms such as alphaâ€“beta search
and backward chaining to disappear from AI systems, to be replaced by general methods that
direct the agentâ€™s computations toward the efficient generation of high-quality decisions.

Section 27.3.

27.3

Are We Going in the Right Direction?

1049

A RE W E G OING IN THE R IGHT D IRECTION ?
The preceding section listed many advances and many opportunities for further progress. But
where is this all leading? Dreyfus (1992) gives the analogy of trying to get to the moon by
climbing a tree; one can report steady progress, all the way to the top of the tree. In this
section, we consider whether AIâ€™s current path is more like a tree climb or a rocket trip.
In Chapter 1, we said that our goal was to build agents that act rationally. However, we
also said that
. . . achieving perfect rationalityâ€”always doing the right thingâ€”is not feasible in complicated environments. The computational demands are just too high. For most of the book,
however, we will adopt the working hypothesis that perfect rationality is a good starting
point for analysis.

Now it is time to consider again what exactly the goal of AI is. We want to build agents, but
with what specification in mind? Here are four possibilities:
PERFECT
RATIONALITY

Perfect rationality. A perfectly rational agent acts at every instant in such a way as to
maximize its expected utility, given the information it has acquired from the environment. We
have seen that the calculations necessary to achieve perfect rationality in most environments
are too time consuming, so perfect rationality is not a realistic goal.

CALCULATIVE
RATIONALITY

Calculative rationality. This is the notion of rationality that we have used implicitly in designing logical and decision-theoretic agents, and most of theoretical AI research has focused
on this property. A calculatively rational agent eventually returns what would have been the
rational choice at the beginning of its deliberation. This is an interesting property for a system
to exhibit, but in most environments, the right answer at the wrong time is of no value. In
practice, AI system designers are forced to compromise on decision quality to obtain reasonable overall performance; unfortunately, the theoretical basis of calculative rationality does
not provide a well-founded way to make such compromises.

BOUNDED
RATIONALITY

Bounded rationality. Herbert Simon (1957) rejected the notion of perfect (or even approximately perfect) rationality and replaced it with bounded rationality, a descriptive theory of
decision making by real agents. He wrote,
The capacity of the human mind for formulating and solving complex problems is very
small compared with the size of the problems whose solution is required for objectively
rational behavior in the real worldâ€”or even for a reasonable approximation to such objective rationality.

He suggested that bounded rationality works primarily by satisficingâ€”that is, deliberating
only long enough to come up with an answer that is â€œgood enough.â€ Simon won the Nobel
Prize in economics for this work and has written about it in depth (Simon, 1982). It appears
to be a useful model of human behaviors in many cases. It is not a formal specification
for intelligent agents, however, because the definition of â€œgood enoughâ€ is not given by the
theory. Furthermore, satisficing seems to be just one of a large range of methods used to cope
with bounded resources.

1050
BOUNDED
OPTIMALITY

ASYMPTOTIC
BOUNDED
OPTIMALITY

Chapter

27.

AI: The Present and Future

Bounded optimality (BO). A bounded optimal agent behaves as well as possible, given its
computational resources. That is, the expected utility of the agent program for a bounded
optimal agent is at least as high as the expected utility of any other agent program running on
the same machine.
Of these four possibilities, bounded optimality seems to offer the best hope for a strong
theoretical foundation for AI. It has the advantage of being possible to achieve: there is always
at least one best programâ€”something that perfect rationality lacks. Bounded optimal agents
are actually useful in the real world, whereas calculatively rational agents usually are not, and
satisficing agents might or might not be, depending on how ambitious they are.
The traditional approach in AI has been to start with calculative rationality and then
make compromises to meet resource constraints. If the problems imposed by the constraints
are minor, one would expect the final design to be similar to a BO agent design. But as the
resource constraints become more criticalâ€”for example, as the environment becomes more
complexâ€”one would expect the two designs to diverge. In the theory of bounded optimality,
these constraints can be handled in a principled fashion.
As yet, little is known about bounded optimality. It is possible to construct bounded
optimal programs for very simple machines and for somewhat restricted kinds of environments (Etzioni, 1989; Russell et al., 1993), but as yet we have no idea what BO programs
are like for large, general-purpose computers in complex environments. If there is to be a
constructive theory of bounded optimality, we have to hope that the design of bounded optimal programs does not depend too strongly on the details of the computer being used. It
would make scientific research very difficult if adding a few kilobytes of memory to a gigabyte machine made a significant difference to the design of the BO program. One way to
make sure this cannot happen is to be slightly more relaxed about the criteria for bounded
optimality. By analogy with the notion of asymptotic complexity (Appendix A), we can define asymptotic bounded optimality (ABO) as follows (Russell and Subramanian, 1995).
Suppose a program P is bounded optimal for a machine M in a class of environments E,
where the complexity of environments in E is unbounded. Then program P  is ABO for M
in E if it can outperform P by running on a machine kM that is k times faster (or larger)
than M . Unless k were enormous, we would be happy with a program that was ABO for
a nontrivial environment on a nontrivial architecture. There would be little point in putting
enormous effort into finding BO rather than ABO programs, because the size and speed of
available machines tends to increase by a constant factor in a fixed amount of time anyway.
We can hazard a guess that BO or ABO programs for powerful computers in complex
environments will not necessarily have a simple, elegant structure. We have already seen that
general-purpose intelligence requires some reflex capability and some deliberative capability;
a variety of forms of knowledge and decision making; learning and compilation mechanisms
for all of those forms; methods for controlling reasoning; and a large store of domain-specific
knowledge. A bounded optimal agent must adapt to the environment in which it finds itself,
so that eventually its internal organization will reflect optimizations that are specific to the
particular environment. This is only to be expected, and it is similar to the way in which
racing cars restricted by engine capacity have evolved into extremely complex designs. We

Section 27.4.

What If AI Does Succeed?

1051

suspect that a science of artificial intelligence based on bounded optimality will involve a
good deal of study of the processes that allow an agent program to converge to bounded
optimality and perhaps less concentration on the details of the messy programs that result.
In sum, the concept of bounded optimality is proposed as a formal task for AI research
that is both well defined and feasible. Bounded optimality specifies optimal programs rather
than optimal actions. Actions are, after all, generated by programs, and it is over programs
that designers have control.

27.4

W HAT I F AI D OES S UCCEED ?
In David Lodgeâ€™s Small World (1984), a novel about the academic world of literary criticism,
the protagonist causes consternation by asking a panel of eminent but contradictory literary
theorists the following question: â€œWhat if you were right?â€ None of the theorists seems to
have considered this question before, perhaps because debating unfalsifiable theories is an end
in itself. Similar confusion can be evoked by asking AI researchers, â€œWhat if you succeed?â€
As Section 26.3 relates, there are ethical issues to consider. Intelligent computers are
more powerful than dumb ones, but will that power be used for good or ill? Those who strive
to develop AI have a responsibility to see that the impact of their work is a positive one. The
scope of the impact will depend on the degree of success of AI. Even modest successes in AI
have already changed the ways in which computer science is taught (Stein, 2002) and software
development is practiced. AI has made possible new applications such as speech recognition
systems, inventory control systems, surveillance systems, robots, and search engines.
We can expect that medium-level successes in AI would affect all kinds of people in
their daily lives. So far, computerized communication networks, such as cell phones and the
Internet, have had this kind of pervasive effect on society, but AI has not. AI has been at work
behind the scenesâ€”for example, in automatically approving or denying credit card transactions for every purchase made on the Webâ€”but has not been visible to the average consumer.
We can imagine that truly useful personal assistants for the office or the home would have a
large positive impact on peopleâ€™s lives, although they might cause some economic dislocation in the short term. Automated assistants for driving could prevent accidents, saving tens
of thousands of lives per year. A technological capability at this level might also be applied
to the development of autonomous weapons, which many view as undesirable. Some of the
biggest societal problems we face todayâ€”such as the harnessing of genomic information for
treating disease, the efficient management of energy resources, and the verification of treaties
concerning nuclear weaponsâ€”are being addressed with the help of AI technologies.
Finally, it seems likely that a large-scale success in AIâ€”the creation of human-level intelligence and beyondâ€”would change the lives of a majority of humankind. The very nature
of our work and play would be altered, as would our view of intelligence, consciousness, and
the future destiny of the human race. AI systems at this level of capability could threaten human autonomy, freedom, and even survival. For these reasons, we cannot divorce AI research
from its ethical consequences (see Section 26.3).

1052

Chapter

27.

AI: The Present and Future

Which way will the future go? Science fiction authors seem to favor dystopian futures
over utopian ones, probably because they make for more interesting plots. But so far, AI
seems to fit in with other revolutionary technologies (printing, plumbing, air travel, telephony)
whose negative repercussions are outweighed by their positive aspects.
In conclusion, we see that AI has made great progress in its short history, but the final
sentence of Alan Turingâ€™s (1950) essay on Computing Machinery and Intelligence is still
valid today:
We can see only a short distance ahead,
but we can see that much remains to be done.

A
A.1

C OMPLEXITY A NALYSIS AND O() N OTATION

BENCHMARKING

ANALYSIS OF
ALGORITHMS

MATHEMATICAL
BACKGROUND

Computer scientists are often faced with the task of comparing algorithms to see how fast
they run or how much memory they require. There are two approaches to this task. The first
is benchmarkingâ€”running the algorithms on a computer and measuring speed in seconds
and memory consumption in bytes. Ultimately, this is what really matters, but a benchmark
can be unsatisfactory because it is so specific: it measures the performance of a particular
program written in a particular language, running on a particular computer, with a particular
compiler and particular input data. From the single result that the benchmark provides, it
can be difficult to predict how well the algorithm would do on a different compiler, computer, or data set. The second approach relies on a mathematical analysis of algorithms,
independently of the particular implementation and input, as discussed below.

A.1.1 Asymptotic analysis
We will consider algorithm analysis through the following example, a program to compute
the sum of a sequence of numbers:

function S UMMATION(sequence) returns a number
sum â† 0
for i = 1 to L ENGTH(sequence) do
sum â† sum + sequence[i]
return sum

The first step in the analysis is to abstract over the input, in order to find some parameter or
parameters that characterize the size of the input. In this example, the input can be characterized by the length of the sequence, which we will call n. The second step is to abstract
over the implementation, to find some measure that reflects the running time of the algorithm
but is not tied to a particular compiler or computer. For the S UMMATION program, this could
be just the number of lines of code executed, or it could be more detailed, measuring the
number of additions, assignments, array references, and branches executed by the algorithm.
1053

1054

Appendix

A.

Mathematical background

Either way gives us a characterization of the total number of steps taken by the algorithm as
a function of the size of the input. We will call this characterization T (n). If we count lines
of code, we have T (n) = 2n + 2 for our example.
If all programs were as simple as S UMMATION , the analysis of algorithms would be a
trivial field. But two problems make it more complicated. First, it is rare to find a parameter
like n that completely characterizes the number of steps taken by an algorithm. Instead, the
best we can usually do is compute the worst case Tworst (n) or the average case Tavg (n).
Computing an average means that the analyst must assume some distribution of inputs.
The second problem is that algorithms tend to resist exact analysis. In that case, it is
necessary to fall back on an approximation. We say that the S UMMATION algorithm is O(n),
meaning that its measure is at most a constant times n, with the possible exception of a few
small values of n. More formally,
T (n) is O(f (n)) if T (n) â‰¤ kf (n) for some k, for all n > n0 .
ASYMPTOTIC
ANALYSIS

The O() notation gives us what is called an asymptotic analysis. We can say without question that, as n asymptotically approaches infinity, an O(n) algorithm is better than an O(n2 )
algorithm. A single benchmark figure could not substantiate such a claim.
The O() notation abstracts over constant factors, which makes it easier to use, but less
precise, than the T () notation. For example, an O(n2 ) algorithm will always be worse than
an O(n) in the long run, but if the two algorithms are T (n2 + 1) and T (100n + 1000), then
the O(n2 ) algorithm is actually better for n < 110.
Despite this drawback, asymptotic analysis is the most widely used tool for analyzing
algorithms. It is precisely because the analysis abstracts over both the exact number of operations (by ignoring the constant factor k) and the exact content of the input (by considering
only its size n) that the analysis becomes mathematically feasible. The O() notation is a good
compromise between precision and ease of analysis.

A.1.2 NP and inherently hard problems

COMPLEXITY
ANALYSIS

The analysis of algorithms and the O() notation allow us to talk about the efficiency of a
particular algorithm. However, they have nothing to say about whether there could be a better
algorithm for the problem at hand. The field of complexity analysis analyzes problems rather
than algorithms. The first gross division is between problems that can be solved in polynomial
time and problems that cannot be solved in polynomial time, no matter what algorithm is
used. The class of polynomial problemsâ€”those which can be solved in time O(nk ) for some
kâ€”is called P. These are sometimes called â€œeasyâ€ problems, because the class contains those
problems with running times like O(log n) and O(n). But it also contains those with time
O(n1000 ), so the name â€œeasyâ€ should not be taken too literally.
Another important class of problems is NP, the class of nondeterministic polynomial
problems. A problem is in this class if there is some algorithm that can guess a solution and
then verify whether the guess is correct in polynomial time. The idea is that if you have an
arbitrarily large number of processors, so that you can try all the guesses at once, or you are
very lucky and always guess right the first time, then the NP problems become P problems.
One of the biggest open questions in computer science is whether the class NP is equivalent

Section A.2.

NP-COMPLETE

CO-NP

CO-NP-COMPLETE

A.2

VECTOR

Vectors, Matrices, and Linear Algebra

1055

to the class P when one does not have the luxury of an infinite number of processors or
omniscient guessing. Most computer scientists are convinced that P = NP; that NP problems
are inherently hard and have no polynomial-time algorithms. But this has never been proven.
Those who are interested in deciding whether P = NP look at a subclass of NP called the
NP-complete problems. The word â€œcompleteâ€ is used here in the sense of â€œmost extremeâ€
and thus refers to the hardest problems in the class NP. It has been proven that either all
the NP-complete problems are in P or none of them is. This makes the class theoretically
interesting, but the class is also of practical interest because many important problems are
known to be NP-complete. An example is the satisfiability problem: given a sentence of
propositional logic, is there an assignment of truth values to the proposition symbols of the
sentence that makes it true? Unless a miracle occurs and P = NP, there can be no algorithm
that solves all satisfiability problems in polynomial time. However, AI is more interested in
whether there are algorithms that perform efficiently on typical problems drawn from a predetermined distribution; as we saw in Chapter 7, there are algorithms such as WALK SAT that
do quite well on many problems.
The class co-NP is the complement of NP, in the sense that, for every decision problem
in NP, there is a corresponding problem in co-NP with the â€œyesâ€ and â€œnoâ€ answers reversed.
We know that P is a subset of both NP and co-NP, and it is believed that there are problems
in co-NP that are not in P. The co-NP-complete problems are the hardest problems in co-NP.
The class #P (pronounced â€œsharp Pâ€) is the set of counting problems corresponding to
the decision problems in NP. Decision problems have a yes-or-no answer: is there a solution
to this 3-SAT formula? Counting problems have an integer answer: how many solutions are
there to this 3-SAT formula? In some cases, the counting problem is much harder than the
decision problem. For example, deciding whether a bipartite graph has a perfect matching
can be done in time O(V E) (where the graph has V vertices and E edges), but the counting
problem â€œhow many perfect matches does this bipartite graph haveâ€ is #P-complete, meaning
that it is hard as any problem in #P and thus at least as hard as any NP problem.
Another class is the class of PSPACE problemsâ€”those that require a polynomial amount
of space, even on a nondeterministic machine. It is believed that PSPACE-hard problems are
worse than NP-complete problems, although it could turn out that NP = PSPACE, just as it
could turn out that P = NP.

V ECTORS , M ATRICES , AND L INEAR A LGEBRA
Mathematicians define a vector as a member of a vector space, but we will use a more concrete definition: a vector is an ordered sequence of values. For example, in two-dimensional
space, we have vectors such as x = 3, 4 and y = 0, 2. We follow the convention of boldface characters for vector names, although some authors use arrows or bars over the names:
x or yÌ„. The elements of a vector can be accessed using subscripts: z = z1 , z2 , . . . , zn . One
confusing point: this book is synthesizing work from many subfields, which variously call
their sequences vectors, lists, or tuples, and variously use the notations 1, 2, [1, 2], or (1, 2).

1056

MATRIX

Appendix

A.

Mathematical background

The two fundamental operations on vectors are vector addition and scalar multiplication. The vector addition x + y is the elementwise sum: x + y = 3 + 0, 4 + 2 = 3, 6. Scalar
multiplication multiplies each element by a constant: 5x = 5 Ã— 3, 5 Ã— 4 = 15, 20.
The length of a vector is denoted |x|
and is computed by taking the square root of the
sum of the squares of the elements: |x| = (32 + 42 ) = 5. The dot product x Â· y (also called
scalar product)
of two vectors is the sum of the products of corresponding elements, that is,

x Â· y = i xi yi , or in our particular case, x Â· y = 3 Ã— 0 + 4 Ã— 2 = 8.
Vectors are often interpreted as directed line segments (arrows) in an n-dimensional
Euclidean space. Vector addition is then equivalent to placing the tail of one vector at the
head of the other, and the dot product x Â· y is equal to |x| |y| cos Î¸, where Î¸ is the angle
between x and y.
A matrix is a rectangular array of values arranged into rows and columns. Here is a
matrix A of size 3 Ã— 4:
âŽž
âŽ›
A1,1 A1,2 A1,3 A1,4
âŽ A2,1 A2,2 A2,3 A2,4 âŽ 
A3,1 A3,2 A3,3 A3,4
The first index of Ai,j specifies the row and the second the column. In programming languages, Ai,j is often written A[i,j] or A[i][j].
The sum of two matrices is defined by adding their corresponding elements; for example
(A + B)i,j = Ai,j + Bi,j . (The sum is undefined if A and B have different sizes.) We can also
define the multiplication of a matrix by a scalar: (cA)i,j = cAi,j . Matrix multiplication (the
product of two matrices) is more complicated. The product AB is defined only if A is of size
a Ã— b and B is of size b Ã— c (i.e., the second matrix has the same number of rows as the first
has columns); the result is a matrix of size a Ã— c. If the matrices are of appropriate size, then
the result is

Ai,j Bj,k .
(AB)i,k =
j

IDENTITY MATRIX
TRANSPOSE
INVERSE
SINGULAR

Matrix multiplication is not commutative, even for square matrices: AB = BA in general.
It is, however, associative: (AB)C = A(BC). Note that the dot product can be expressed in
terms of a transpose and a matrix multiplication:x Â· y = x y.
The identity matrix I has elements Ii,j equal to 1 when i = j and equal to 0 otherwise.
It has the property that AI = A for all A. The transpose of A, written A is formed by
turning rows into columns and vice versa, or, more formally, by A i,j = Aj,i . The inverse of
a square matrix A is another square matrix Aâˆ’1 such that Aâˆ’1 A = I. For a singular matrix,
the inverse does not exist. For a nonsingular matrix, it can be computed in O(n3 ) time.
Matrices are used to solve systems of linear equations in O(n3 ) time; the time is dominated by inverting a matrix of coefficients. Consider the following set of equations, for which
we want a solution in x, y, and z:
+2x + y âˆ’ z = 8
âˆ’3x âˆ’ y + 2z = âˆ’11
âˆ’2x + y + 2z = âˆ’3 .

Section A.3.

Probability Distributions

1057

We can represent this system as the matrix equation A x = b, where
âŽ›
âŽž
âŽ› âŽž
âŽ›
âŽž
2 1 âˆ’1
x
8
A = âŽ âˆ’3 âˆ’1 2 âŽ  ,
x = âŽ y âŽ ,
b = âŽ âˆ’11 âŽ  .
âˆ’2 1 2
z
âˆ’3
To solve A x = b we multiply both sides by Aâˆ’1 , yielding Aâˆ’1 Ax = Aâˆ’1 b, which simplifies
to x = Aâˆ’1 b. After inverting A and multiplying by b, we get the answer
âŽ› âŽž âŽ›
âŽž
x
2
x=âŽy âŽ =âŽ 3âŽ  .
z
âˆ’1

A.3

P ROBABILITY D ISTRIBUTIONS
A probability is a measure over a set of events that satisfies three axioms:
1. The measure of each event is between 0 and 1. We write this as 0 â‰¤ P (X = xi ) â‰¤ 1,
where X is a random variable representing an event and xi are the possible values of
X. In general, random variables are denoted by uppercase letters and their values by
lowercase letters.

2. The measure of the whole set is 1; that is, ni= 1 P (X = xi ) = 1.
3. The probability of a union of disjoint events is the sum of the probabilities of the individual events; that is, P (X = x1 âˆ¨ X = x2 ) = P (X = x1 ) + P (X = x2 ), where x1 and
x2 are disjoint.

PROBABILITY
DENSITY FUNCTION

A probabilistic model consists of a sample space of mutually exclusive possible outcomes,
together with a probability measure for each outcome. For example, in a model of the weather
tomorrow, the outcomes might be sunny, cloudy, rainy, and snowy. A subset of these outcomes constitutes an event. For example, the event of precipitation is the subset consisting of
{rainy, snowy}.
), . . . , P (X = xn ). We also
We use P(X) to denote the vector of values
 P (X = x1
use P (xi ) as an abbreviation for P (X = xi ) and x P (x) for ni= 1 P (X = xi ).
The conditional probability P (B|A) is defined as P (B âˆ©A)/P (A). A and B are conditionally independent if P (B|A) = P (B) (or equivalently, P (A|B) = P (A)). For continuous
variables, there are an infinite number of values, and unless there are point spikes, the probability of any one value is 0. Therefore, we define a probability density function, which we
also denote as P (Â·), but which has a slightly different meaning from the discrete probability
function. The density function P (x) for a random variable X, which might be thought of as
P (X = x), is intuitively defined as the ratio of the probability that X falls into an interval
around x, divided by the width of the interval, as the interval width goes to zero:
P (x) = lim P (x â‰¤ X â‰¤ x + dx)/dx .
dxâ†’0

1058

Appendix

A.

Mathematical background

The density function must be nonnegative for all x and must have
 âˆž
P (x) dx = 1 .
CUMULATIVE
PROBABILITY
DENSITY FUNCTION

âˆ’âˆž

We can also define a cumulative probability density function FX (x), which is the probability of a random variable being less than x:
 x
P (u) du .
FX (x) = P (X â‰¤ x) =
âˆ’âˆž

Note that the probability density function has units, whereas the discrete probability function
is unitless. For example, if values of X are measured in seconds, then the density is measured
in Hz (i.e., 1/sec). If values of X are points in three-dimensional space measured in meters,
then density is measured in 1/m3 .
GAUSSIAN
DISTRIBUTION

STANDARD NORMAL
DISTRIBUTION
MULTIVARIATE
GAUSSIAN

CUMULATIVE
DISTRIBUTION

One of the most important probability distributions is the Gaussian distribution, also
known as the normal distribution. A Gaussian distribution with mean Î¼ and standard deviation Ïƒ (and therefore variance Ïƒ 2 ) is defined as
1
2
2
P (x) = âˆš eâˆ’(xâˆ’Î¼) /(2Ïƒ ) ,
Ïƒ 2Ï€
where x is a continuous variable ranging from âˆ’âˆž to +âˆž. With mean Î¼ = 0 and variance
Ïƒ 2 = 1, we get the special case of the standard normal distribution. For a distribution over
a vector x in n dimensions, there is the multivariate Gaussian distribution:
â€œ
â€
âˆ’1
1
âˆ’ 12 (xâˆ’Î¼) Î£ (xâˆ’Î¼)


e
P (x) =
,
(2Ï€)n |Î£|
where Î¼ is the mean vector and Î£ is the covariance matrix (see below).
In one dimension, we can define the cumulative distribution function F (x) as the
probability that a random variable will be less than x. For the normal distribution, this is
x
zâˆ’Î¼
1
P (z)dz = (1 + erf( âˆš )) ,
F (x) =
2
Ïƒ 2
âˆ’âˆž

CENTRAL LIMIT
THEOREM

EXPECTATION

where erf(x) is the so-called error function, which has no closed-form representation.
The central limit theorem states that the distribution formed by sampling n independent random variables and taking their mean tends to a normal distribution as n tends to
infinity. This holds for almost any collection of random variables, even if they are not strictly
independent, unless the variance of any finite subset of variables dominates the others.
The expectation of a random variable, E(X), is the mean or average value, weighted
by the probability of each value. For a discrete variable it is:

xi P (X = xi ) .
E(X) =
i

For a continuous variable, replace the summation with an integral over the probability density
function, P (x):
âˆž
E(X) =
xP (x) dx ,
âˆ’âˆž

Bibliographical and Historical Notes
ROOT MEAN SQUARE

COVARIANCE

1059

The root mean square, RMS, of a set of values (often samples of a random variable) is
the square root of the mean of the squares of the values,

x21 + . . . + x2n
.
RMS (x1 , . . . , xn ) =
n
The covariance of two random variables is the expectation of the product of their differences
from their means:
cov(X, Y ) = E((X âˆ’ Î¼X )(Y âˆ’ Î¼Y )) .

COVARIANCE MATRIX

The covariance matrix, often denoted Î£, is a matrix of covariances between elements of a
vector of random variables. Given X = X1 , . . . Xn  , the entries of the covariance matrix
are as follows:
Î£i,j = cov(Xi , Xj ) = E((Xi âˆ’ Î¼i )(Xj âˆ’ Î¼j )) .
A few more miscellaneous points: we use log(x) for the natural logarithm, loge (x). We use
argmaxx f (x) for the value of x for which f (x) is maximal.

B IBLIOGRAPHICAL

AND

H ISTORICAL N OTES

The O() notation so widely used in computer science today was first introduced in the context
of number theory by the German mathematician P. G. H. Bachmann (1894). The concept of
NP-completeness was invented by Cook (1971), and the modern method for establishing a
reduction from one problem to another is due to Karp (1972). Cook and Karp have both won
the Turing award, the highest honor in computer science, for their work.
Classic works on the analysis and design of algorithms include those by Knuth (1973)
and Aho, Hopcroft, and Ullman (1974); more recent contributions are by Tarjan (1983) and
Cormen, Leiserson, and Rivest (1990). These books place an emphasis on designing and
analyzing algorithms to solve tractable problems. For the theory of NP-completeness and
other forms of intractability, see Garey and Johnson (1979) or Papadimitriou (1994). Good
texts on probability include Chung (1979), Ross (1988), and Bertsekas and Tsitsiklis (2008).

